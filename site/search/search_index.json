{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Formation Deep Learning et IA conversationnelle","text":""},{"location":"#bienvenue-dans-ce-parcours-dapprentissage","title":"Bienvenue dans ce parcours d'apprentissage","text":"<p>Cette formation intensive vous initie au Deep Learning \u00e0 travers une approche pratique et progressive, sp\u00e9cialement con\u00e7ue pour les \u00e9tudiants de BTS SIO. Vous d\u00e9couvrirez les fondamentaux des r\u00e9seaux de neurones, explorerez diff\u00e9rentes architectures, et d\u00e9velopperez un chatbot p\u00e9dagogique int\u00e9grant l'API Mistral AI.</p>"},{"location":"#organisation-du-parcours","title":"Organisation du parcours","text":"<p>Notre formation se compose de 4 modules de 4 heures chacun :</p> Module Titre Aper\u00e7u Module 1 Fondamentaux du Deep Learning Introduction pratique, concepts fondamentaux, anatomie des r\u00e9seaux de neurones Module 2 Architectures sp\u00e9cialis\u00e9es R\u00e9seaux convolutifs (CNN) pour la vision, r\u00e9seaux r\u00e9currents (RNN) pour le texte Module 3 D\u00e9veloppement d'applications pratiques Frameworks, optimisation, int\u00e9gration API, pr\u00e9paration au projet Module 4 Projet int\u00e9grateur - Chatbot p\u00e9dagogique D\u00e9veloppement du chatbot, finalisation, pr\u00e9sentation"},{"location":"#prerequis-techniques","title":"Pr\u00e9requis techniques","text":"<p>Pour suivre efficacement cette formation, vous devez :</p> <ul> <li>Poss\u00e9der des bases en programmation Python</li> <li>Disposer d'un compte Google pour acc\u00e9der \u00e0 Colab</li> <li>Avoir une curiosit\u00e9 pour l'intelligence artificielle</li> </ul>"},{"location":"#navigation-dans-ce-site","title":"Navigation dans ce site","text":"<p>Ce site contient toutes les ressources n\u00e9cessaires pour votre parcours :</p> <ul> <li>Carte de progression - Parcours d'apprentissage et comp\u00e9tences d\u00e9velopp\u00e9es</li> <li>Ressources - Documentation technique et mat\u00e9riel compl\u00e9mentaire</li> <li>\u00c9valuation - Crit\u00e8res et modalit\u00e9s d'\u00e9valuation</li> </ul>"},{"location":"#commencer-votre-parcours","title":"Commencer votre parcours","text":"<p>Pr\u00eat \u00e0 vous lancer dans l'univers du Deep Learning ? Deux options s'offrent \u00e0 vous :</p> <p>D\u00e9couvrir le projet chatbot Commencer le Module 1</p>"},{"location":"carte-progression/","title":"Carte de progression","text":""},{"location":"carte-progression/#gps-pedagogique-votre-itineraire-dapprentissage-du-deep-learning","title":"GPS p\u00e9dagogique : votre itin\u00e9raire d'apprentissage du Deep Learning","text":"<p>Cette carte de progression vous permettra de visualiser clairement les objectifs, les activit\u00e9s et les comp\u00e9tences d\u00e9velopp\u00e9es \u00e0 chaque \u00e9tape de votre formation en Deep Learning.</p>"},{"location":"carte-progression/#les-4-modules-du-parcours","title":"Les 4 modules du parcours","text":"<pre><code>flowchart LR\n    M1[Module 1\\nFondamentaux du DL] --&gt; M2[Module 2\\nArchitectures sp\u00e9cialis\u00e9es]\n    M2 --&gt; M3[Module 3\\nD\u00e9veloppement d'applications]\n    M3 --&gt; M4[Module 4\\nProjet chatbot p\u00e9dagogique]</code></pre>"},{"location":"carte-progression/#module-1-fondamentaux-du-deep-learning","title":"Module 1 : Fondamentaux du Deep Learning","text":"<p>Concepts cl\u00e9s : - Structure et fonctionnement d'un neurone artificiel - R\u00e9seaux de neurones multicouches - Forward et backpropagation - Fonctions d'activation (ReLU, Sigmoid, Softmax) - Diff\u00e9rences fondamentales entre Machine Learning classique et Deep Learning</p> <p>Activit\u00e9s pratiques : - Manipulation d'un r\u00e9seau de neurones sur donn\u00e9es MNIST - Comparaison directe ML vs DL sur le m\u00eame jeu de donn\u00e9es - Visualisation des couches internes d'un r\u00e9seau</p> <p>Livrables : - Notebook \"Hello World du Deep Learning\" compl\u00e9t\u00e9 - Sch\u00e9ma annot\u00e9 d'un r\u00e9seau de neurones</p>"},{"location":"carte-progression/#module-2-architectures-specialisees","title":"Module 2 : Architectures sp\u00e9cialis\u00e9es","text":"<p>Concepts cl\u00e9s pour les CNN : - Convolution et filtres - Pooling et r\u00e9duction de dimension - Feature maps et leur interpr\u00e9tation - Transfer learning avec mod\u00e8les pr\u00e9-entra\u00een\u00e9s</p> <p>Concepts cl\u00e9s pour les RNN : - Traitement de s\u00e9quences et donn\u00e9es temporelles - Probl\u00e8me de la disparition du gradient - Cellules LSTM et GRU - Applications au traitement du langage naturel</p> <p>Activit\u00e9s pratiques : - Impl\u00e9mentation d'un CNN pour la classification d'images - D\u00e9veloppement d'un RNN pour l'analyse de sentiment - Optimisation d'un mod\u00e8le de pr\u00e9vision des ventes</p>"},{"location":"carte-progression/#module-3-developpement-dapplications-pratiques","title":"Module 3 : D\u00e9veloppement d'applications pratiques","text":"<p>Concepts cl\u00e9s : - TensorFlow/Keras : mod\u00e8les fonctionnels et s\u00e9quentiels - Optimisation des hyperparam\u00e8tres - Techniques de r\u00e9gularisation (dropout, batch normalization) - API REST pour servir des mod\u00e8les - Int\u00e9gration de mod\u00e8les de langage (API Mistral)</p> <p>Activit\u00e9s pratiques : - Utilisation de mod\u00e8les pr\u00e9-entra\u00een\u00e9s - Optimisation des performances d'inf\u00e9rence - Premier test d'int\u00e9gration avec l'API Mistral - Conception du prototype de chatbot</p>"},{"location":"carte-progression/#module-4-projet-integrateur-chatbot-pedagogique","title":"Module 4 : Projet int\u00e9grateur - Chatbot p\u00e9dagogique","text":"<p>Concepts cl\u00e9s : - Prompt engineering pour mod\u00e8les de langage - Gestion du contexte conversationnel - Structures de donn\u00e9es pour bases de connaissances - Optimisation de l'exp\u00e9rience utilisateur - Techniques de d\u00e9ploiement</p> <p>Activit\u00e9s pratiques : - D\u00e9veloppement d'une interface conversationnelle - Int\u00e9gration avanc\u00e9e avec l'API Mistral - Structuration d'une base de connaissances - Tests et optimisation de l'exp\u00e9rience utilisateur</p>"},{"location":"carte-progression/#ce-que-vous-saurez-faire-apres-chaque-module","title":"Ce que vous saurez faire apr\u00e8s chaque module","text":""},{"location":"carte-progression/#apres-le-module-1","title":"Apr\u00e8s le Module 1","text":"<ul> <li>Expliquer le fonctionnement d'un r\u00e9seau de neurones de base</li> <li>Distinguer ML classique et Deep Learning dans des cas concrets</li> <li>Impl\u00e9menter un r\u00e9seau simple pour la classification d'images</li> <li>Interpr\u00e9ter les m\u00e9triques d'entra\u00eenement (pr\u00e9cision, perte)</li> </ul>"},{"location":"carte-progression/#apres-le-module-2","title":"Apr\u00e8s le Module 2","text":"<ul> <li>Impl\u00e9menter et adapter un CNN pour la vision par ordinateur</li> <li>D\u00e9velopper un RNN pour des t\u00e2ches de traitement de texte</li> <li>Visualiser et interpr\u00e9ter les feature maps d'un CNN</li> <li>Am\u00e9liorer un mod\u00e8le existant avec diff\u00e9rentes techniques</li> </ul>"},{"location":"carte-progression/#apres-le-module-3","title":"Apr\u00e8s le Module 3","text":"<ul> <li>Utiliser efficacement TensorFlow/Keras pour cr\u00e9er des mod\u00e8les</li> <li>Appliquer des techniques d'optimisation des performances</li> <li>Int\u00e9grer l'API Mistral dans une application simple</li> <li>Concevoir l'architecture d'un chatbot p\u00e9dagogique</li> </ul>"},{"location":"carte-progression/#apres-le-module-4","title":"Apr\u00e8s le Module 4","text":"<ul> <li>D\u00e9velopper un chatbot p\u00e9dagogique complet et fonctionnel</li> <li>Cr\u00e9er et g\u00e9rer une base de connaissances structur\u00e9e</li> <li>Optimiser l'exp\u00e9rience utilisateur d'un syst\u00e8me conversationnel</li> <li>Pr\u00e9senter et d\u00e9fendre un projet technique</li> </ul>"},{"location":"carte-progression/#concepts-cles-du-deep-learning-a-travers-le-parcours","title":"Concepts cl\u00e9s du Deep Learning \u00e0 travers le parcours","text":"<ul> <li>Neurones artificiels et r\u00e9seaux \u2192 Module 1</li> <li>Descente de gradient et r\u00e9tropropagation \u2192 Module 1</li> <li>Convolution et vision par ordinateur \u2192 Module 2</li> <li>M\u00e9moire r\u00e9currente et s\u00e9quences \u2192 Module 2</li> <li>Optimisation et hyperparam\u00e8tres \u2192 Module 3</li> <li>Mod\u00e8les de langage et g\u00e9n\u00e9ration de texte \u2192 Module 3, 4</li> <li>Syst\u00e8mes conversationnels \u2192 Module 4</li> <li>Architectures d'applications IA \u2192 Module 4</li> </ul>"},{"location":"carte-progression/#ressources-essentielles","title":"Ressources essentielles","text":"<ul> <li>Documentation TensorFlow/Keras - tensorflow.org/tutorials</li> <li>API Mistral - docs.mistral.ai</li> <li>Hugging Face - huggingface.co/docs</li> <li>FastAPI - fastapi.tiangolo.com</li> </ul> <p>Retour \u00e0 l'accueil Commencer le Module 1</p>"},{"location":"suivi-progression/","title":"Suivi de progression personnalis\u00e9","text":""},{"location":"suivi-progression/#votre-progression","title":"Votre progression","text":"Module 1 0% Module 2 0% Module 3 0% Module 4 0% Progression globale 0% Voir le d\u00e9tail de ma progression R\u00e9initialiser ma progression"},{"location":"suivi-progression/#tableau-de-bord","title":"Tableau de bord","text":"<p>Ce tableau de bord vous permet de suivre votre progression \u00e0 travers les diff\u00e9rents modules et activit\u00e9s de la formation Deep Learning. Cochez les cases au fur et \u00e0 mesure que vous compl\u00e9tez chaque partie.</p>"},{"location":"suivi-progression/#module-1-fondamentaux-du-deep-learning","title":"Module 1 : Fondamentaux du Deep Learning","text":"Section Activit\u00e9 Statut Date de compl\u00e9tion Introduction pratique D\u00e9monstrations d'applications \u2b1c Premier contact avec un r\u00e9seau de neurones \u2b1c Exp\u00e9rimentations guid\u00e9es \u2b1c Concepts fondamentaux Atelier \"Bo\u00eete noire\" \u2b1c D\u00e9fi de g\u00e9n\u00e9ralisation \u2b1c Exploration d'un neurone et d'un r\u00e9seau \u2b1c Mini-projet individuel Modification et am\u00e9lioration d'un r\u00e9seau \u2b1c Documentation des r\u00e9sultats \u2b1c Auto-\u00e9valuation QCM sur les concepts fondamentaux \u2b1c Sch\u00e9ma conceptuel compl\u00e9t\u00e9 \u2b1c"},{"location":"suivi-progression/#module-2-architectures-specialisees","title":"Module 2 : Architectures sp\u00e9cialis\u00e9es","text":"Section Activit\u00e9 Statut Date de compl\u00e9tion R\u00e9seaux convolutifs (CNN) Principes des CNN \u2b1c Impl\u00e9mentation d'un CNN pour MNIST \u2b1c Visualisation des filtres et feature maps \u2b1c Int\u00e9gration dans une application web \u2b1c R\u00e9seaux r\u00e9currents (RNN) Principes des RNN/LSTM \u2b1c Impl\u00e9mentation d'un mod\u00e8le d'analyse de sentiment \u2b1c Exp\u00e9rimentation avec l'API Mistral AI \u2b1c Challenge d'am\u00e9lioration Diagnostic d'un mod\u00e8le sous-optimal \u2b1c Exp\u00e9rimentation avec diff\u00e9rentes architectures \u2b1c Documentation des am\u00e9liorations \u2b1c"},{"location":"suivi-progression/#module-3-developpement-dapplications-pratiques","title":"Module 3 : D\u00e9veloppement d'applications pratiques","text":"Section Activit\u00e9 Statut Date de compl\u00e9tion Frameworks pour d\u00e9butants Installation et configuration de TensorFlow/Keras \u2b1c Utilisation de mod\u00e8les pr\u00e9-entra\u00een\u00e9s \u2b1c D\u00e9veloppement d'une API simple \u2b1c Am\u00e9lioration des performances Techniques d'optimisation \u2b1c Bonnes pratiques \u2b1c TP pratique d'am\u00e9lioration \u2b1c Pr\u00e9paration au projet final \u00c9tude du cahier des charges \u2b1c Analyse de cas r\u00e9els \u2b1c Prototype avec API Mistral \u2b1c"},{"location":"suivi-progression/#module-4-projet-integrateur-chatbot-pedagogique","title":"Module 4 : Projet int\u00e9grateur - Chatbot p\u00e9dagogique","text":"Section Activit\u00e9 Statut Date de compl\u00e9tion D\u00e9veloppement du chatbot Interface conversationnelle \u2b1c Int\u00e9gration avec API Mistral AI \u2b1c Base de connaissances \u2b1c Fonctionnalit\u00e9s p\u00e9dagogiques \u2b1c Finalisation et tests Tests fonctionnels \u2b1c Optimisation des performances \u2b1c Documentation technique \u2b1c Guide utilisateur \u2b1c Pr\u00e9sentation Pr\u00e9paration de la d\u00e9monstration \u2b1c Pr\u00e9sentation finale \u2b1c"},{"location":"suivi-progression/#livrables-soumis","title":"Livrables soumis","text":"Livrable Module Statut Date de soumission Note Fiche d'observations \"Hello World\" 1 \u2b1c Tableau comparatif ML vs DL 1 \u2b1c Sch\u00e9ma annot\u00e9 d'un r\u00e9seau de neurones 1 \u2b1c Rapport du mini-projet 1 \u2b1c Application CNN fonctionnelle 2 \u2b1c Mod\u00e8le RNN pour analyse de sentiment 2 \u2b1c Rapport d'analyse comparative 2 \u2b1c Mod\u00e8le optimis\u00e9 et documentation 3 \u2b1c Document de conception du chatbot 3 \u2b1c Code source du chatbot 4 \u2b1c Base de connaissances 4 \u2b1c Documentation technique 4 \u2b1c Guide utilisateur 4 \u2b1c Pr\u00e9sentation finale 4 \u2b1c"},{"location":"suivi-progression/#graphique-de-progression","title":"Graphique de progression","text":"<p>Pour visualiser votre progression globale, calculez le pourcentage d'activit\u00e9s compl\u00e9t\u00e9es pour chaque module :</p> <ul> <li>Module 1 : _ / 10 activit\u00e9s compl\u00e9t\u00e9es (_%)</li> <li>Module 2 : _ / 10 activit\u00e9s compl\u00e9t\u00e9es (_%)</li> <li>Module 3 : _ / 9 activit\u00e9s compl\u00e9t\u00e9es (_%)</li> <li>Module 4 : _ / 10 activit\u00e9s compl\u00e9t\u00e9es (_%)</li> </ul> <p>Progression globale : _ / 39 activit\u00e9s compl\u00e9t\u00e9es (_%)</p>"},{"location":"suivi-progression/#mes-badges","title":"Mes badges","text":"Compl\u00e9tez des sections pour d\u00e9bloquer des badges!"},{"location":"suivi-progression/#instructions-dutilisation","title":"Instructions d'utilisation","text":"<ol> <li>T\u00e9l\u00e9chargez ou imprimez cette page pour votre suivi personnel</li> <li>Cochez les cases (remplacez \u2b1c par \u2705) au fur et \u00e0 mesure de votre progression</li> <li>Notez la date de compl\u00e9tion pour chaque activit\u00e9</li> <li>Calculez r\u00e9guli\u00e8rement votre pourcentage de progression</li> <li>Partagez votre progression avec votre formateur lors des points d'\u00e9tape</li> </ol>"},{"location":"suivi-progression/#notes-personnelles-et-reflexions","title":"Notes personnelles et r\u00e9flexions","text":"<p>Utilisez cet espace pour noter vos observations, difficult\u00e9s rencontr\u00e9es et points forts identifi\u00e9s au cours de votre formation.</p> <p>Retour \u00e0 l'accueil</p>"},{"location":"evaluation/","title":"\u00c9valuation du parcours Deep Learning","text":""},{"location":"evaluation/#presentation-du-systeme-devaluation","title":"Pr\u00e9sentation du syst\u00e8me d'\u00e9valuation","text":"<p>Cette section d\u00e9taille les modalit\u00e9s d'\u00e9valuation du parcours sur le Deep Learning et du projet de chatbot p\u00e9dagogique. Elle vous permettra de comprendre clairement les attentes, les crit\u00e8res d'\u00e9valuation et les livrables requis.</p>"},{"location":"evaluation/#objectifs-de-levaluation","title":"Objectifs de l'\u00e9valuation","text":"<p>L'\u00e9valuation de ce parcours vise plusieurs objectifs :</p> <ol> <li>Mesurer votre compr\u00e9hension des concepts fondamentaux du Deep Learning</li> <li>\u00c9valuer votre capacit\u00e9 \u00e0 appliquer ces concepts dans un projet concret</li> <li>Valoriser le travail d'\u00e9quipe et la r\u00e9partition efficace des t\u00e2ches</li> <li>Pr\u00e9parer aux situations professionnelles en simulant un projet r\u00e9el d'entreprise</li> <li>Fournir un feedback constructif pour votre progression personnelle</li> </ol>"},{"location":"evaluation/#repartition-globale-de-levaluation","title":"R\u00e9partition globale de l'\u00e9valuation","text":"<p>L'\u00e9valuation globale du parcours se d\u00e9compose comme suit :</p> Composante Pond\u00e9ration Description Participation active 10% Engagement dans les activit\u00e9s, pertinence des contributions Mini-projets 30% Qualit\u00e9 des livrables des s\u00e9ances 2 et 3 Projet final - Produit 30% Fonctionnalit\u00e9 et qualit\u00e9 technique du chatbot Projet final - Processus 15% Organisation, m\u00e9thodologie, r\u00e9partition des t\u00e2ches Projet final - Pr\u00e9sentation 15% Qualit\u00e9 de la pr\u00e9sentation et de la documentation"},{"location":"evaluation/#documents-devaluation-disponibles","title":"Documents d'\u00e9valuation disponibles","text":""},{"location":"evaluation/#criteres-devaluation-detailles","title":"Crit\u00e8res d'\u00e9valuation d\u00e9taill\u00e9s","text":"<p>Ce document pr\u00e9sente en d\u00e9tail tous les crit\u00e8res utilis\u00e9s pour l'\u00e9valuation de chaque composante du parcours :      - Description pr\u00e9cise de chaque crit\u00e8re      - Bar\u00e8me de notation et pond\u00e9ration      - Exemples de livrables attendus pour chaque niveau de performance      - Conseils pour maximiser votre score</p>"},{"location":"evaluation/#grille-de-repartition-des-taches","title":"Grille de r\u00e9partition des t\u00e2ches","text":"<p>Cette grille vous aide \u00e0 organiser efficacement le travail au sein de votre \u00e9quipe :</p> <pre><code> - Identification des r\u00f4les et responsabilit\u00e9s\n - Planning des jalons interm\u00e9diaires\n - Suivi de l'avancement des t\u00e2ches\n - Gestion des risques et plan de contingence\n</code></pre>"},{"location":"evaluation/#fiche-devaluation-finale","title":"Fiche d'\u00e9valuation finale","text":"<p>La fiche utilis\u00e9e lors de l'\u00e9valuation finale du projet de chatbot p\u00e9dagogique :</p> <pre><code> - Crit\u00e8res sp\u00e9cifiques pour chaque aspect du projet\n - Bar\u00e8me de notation d\u00e9taill\u00e9\n - Espace pour les commentaires et feedback\n - Auto-\u00e9valuation pr\u00e9alable \u00e0 remplir par l'\u00e9quipe\n</code></pre>"},{"location":"evaluation/#checklist-dauto-evaluation","title":"Checklist d'auto-\u00e9valuation","text":"<p>Cette checklist vous permet de v\u00e9rifier que votre projet r\u00e9pond \u00e0 tous les crit\u00e8res avant la soumission finale:</p> <ul> <li>Liste compl\u00e8te des fonctionnalit\u00e9s \u00e0 impl\u00e9menter</li> <li>Points techniques \u00e0 v\u00e9rifier</li> <li>Aspects de documentation \u00e0 ne pas oublier</li> <li>Conseils pour la pr\u00e9sentation finale</li> </ul>"},{"location":"evaluation/#mini-projets-evalues","title":"Mini-projets \u00e9valu\u00e9s","text":"<p>Les mini-projets des s\u00e9ances 2 et 3 font partie int\u00e9grante de l'\u00e9valuation continue :</p>"},{"location":"evaluation/#mini-projet-cnn-10","title":"Mini-projet CNN (10%)","text":"<pre><code> - Impl\u00e9mentation d'un r\u00e9seau convolutif pour la classification d'images\n - Visualisation et interpr\u00e9tation des filtres et feature maps\n - Int\u00e9gration dans une application web simple\n - \u00c9valuation des performances sur diff\u00e9rents jeux de donn\u00e9es\n</code></pre>"},{"location":"evaluation/#mini-projet-rnn-10","title":"Mini-projet RNN (10%)","text":"<pre><code> - Impl\u00e9mentation d'un mod\u00e8le LSTM pour l'analyse de sentiment\n - Exp\u00e9rimentation avec l'API Mistral AI pour le traitement du langage\n - Comparaison entre approche par r\u00e9seau de neurones et API de mod\u00e8le de langage\n - Documentation des r\u00e9sultats et limitations\n</code></pre>"},{"location":"evaluation/#projet-damelioration-10","title":"Projet d'am\u00e9lioration (10%)","text":"<pre><code> - Diagnostic des probl\u00e8mes d'un mod\u00e8le de pr\u00e9vision des ventes sous-optimal\n - Application de techniques d'am\u00e9lioration cibl\u00e9es\n - Mesure et analyse comparative des performances\n - Documentation des exp\u00e9rimentations et conclusions\n</code></pre>"},{"location":"evaluation/#evaluation-du-projet-final","title":"\u00c9valuation du projet final","text":"<p>Le projet final de chatbot p\u00e9dagogique constitue la partie la plus importante de l'\u00e9valuation (60% au total) :</p>"},{"location":"evaluation/#produit-final-30","title":"Produit final (30%)","text":"<pre><code> - Interface conversationnelle fonctionnelle et intuitive\n - Int\u00e9gration avanc\u00e9e avec l'API Mistral AI\n - Base de connaissances compl\u00e8te et structur\u00e9e sur le Deep Learning\n - Fonctionnalit\u00e9s p\u00e9dagogiques (explications, exemples, exercices)\n - Performances techniques (temps de r\u00e9ponse, gestion des erreurs)\n</code></pre>"},{"location":"evaluation/#processus-de-developpement-15","title":"Processus de d\u00e9veloppement (15%)","text":"<pre><code> - Organisation de l'\u00e9quipe et r\u00e9partition des t\u00e2ches\n - Respect des jalons interm\u00e9diaires\n - Tests fonctionnels et validation des performances\n - Adaptation aux difficult\u00e9s techniques rencontr\u00e9es\n</code></pre>"},{"location":"evaluation/#presentation-et-documentation-15","title":"Pr\u00e9sentation et documentation (15%)","text":"<pre><code> - Pr\u00e9sentation claire et d\u00e9monstration convaincante\n - Documentation technique d\u00e9taill\u00e9e\n - Guide utilisateur complet\n - Explication des choix techniques et architecture\n</code></pre>"},{"location":"evaluation/#calendrier-devaluation","title":"Calendrier d'\u00e9valuation","text":"S\u00e9ance \u00c9valuation Livrables attendus S\u00e9ance 2 Mini-projets CNN et RNN Mod\u00e8les fonctionnels et rapports d'analyse S\u00e9ance 3 Projet d'am\u00e9lioration Mod\u00e8le optimis\u00e9 et documentation S\u00e9ance 3 Pr\u00e9paration du projet Document de conception du chatbot S\u00e9ance 4 D\u00e9veloppement Chatbot fonctionnel et documentation S\u00e9ance 4 Pr\u00e9sentation finale D\u00e9monstration et d\u00e9fense du projet"},{"location":"evaluation/#conseils-pour-reussir","title":"Conseils pour r\u00e9ussir","text":"<ol> <li>Commencez par l'essentiel - Assurez-vous que les fonctionnalit\u00e9s de base sont solides avant d'ajouter des \u00e9l\u00e9ments avanc\u00e9s</li> <li>Documentez au fur et \u00e0 mesure - Ne laissez pas la documentation pour la fin</li> <li>Testez r\u00e9guli\u00e8rement - Identifiez et corrigez les probl\u00e8mes t\u00f4t</li> <li>R\u00e9partissez \u00e9quitablement les t\u00e2ches - Utilisez la grille de r\u00e9partition pour organiser le travail</li> <li>Pr\u00e9parez soigneusement votre d\u00e9monstration - Pr\u00e9voyez un sc\u00e9nario qui met en valeur les points forts de votre solution</li> <li>G\u00e9rez efficacement le temps - Respectez les jalons interm\u00e9diaires pour \u00e9viter le stress de derni\u00e8re minute</li> <li>Communiquez avec l'enseignant - Demandez de l'aide si vous rencontrez des difficult\u00e9s</li> </ol>"},{"location":"evaluation/#auto-evaluation","title":"Auto-\u00e9valuation","text":"<p>Avant chaque remise, nous vous encourageons \u00e0 r\u00e9aliser une auto-\u00e9valuation \u00e0 l'aide des grilles fournies. Cette pratique vous permettra de :</p> <pre><code> - Identifier les points forts et axes d'am\u00e9lioration de votre travail\n - V\u00e9rifier que tous les crit\u00e8res sont bien pris en compte\n - Prioriser les aspects \u00e0 finaliser ou am\u00e9liorer\n - Pr\u00e9parer votre argumentaire pour la pr\u00e9sentation\n</code></pre>"},{"location":"evaluation/#ressources-supplementaires","title":"Ressources suppl\u00e9mentaires","text":"<ul> <li>Guide de bonnes pratiques pour la documentation technique</li> <li>Conseils pour une pr\u00e9sentation efficace</li> <li>Comp\u00e9tences recherch\u00e9es en stage BTS SIO</li> </ul> <p>L'\u00e9valuation est con\u00e7ue comme un outil p\u00e9dagogique pour vous guider et vous motiver tout au long de ce parcours d'apprentissage. Gardez \u00e0 l'esprit que l'objectif principal est d'acqu\u00e9rir des comp\u00e9tences pratiques en Deep Learning que vous pourrez valoriser dans votre parcours professionnel.</p> <p>[</p>"},{"location":"evaluation/checklist-auto-evaluation/","title":"Checklist d'auto-\u00e9valuation du projet chatbot","text":"<p>Utilisez cette checklist pour vous assurer que votre projet chatbot p\u00e9dagogique r\u00e9pond \u00e0 tous les crit\u00e8res d'\u00e9valuation avant la soumission finale. Cette liste vous permettra d'identifier rapidement les points \u00e0 am\u00e9liorer.</p>"},{"location":"evaluation/checklist-auto-evaluation/#fonctionnalites-du-produit","title":"Fonctionnalit\u00e9s du produit","text":""},{"location":"evaluation/checklist-auto-evaluation/#interface-conversationnelle","title":"Interface conversationnelle","text":"<ul> <li> Zone de saisie des messages fonctionnelle</li> <li> Affichage clair des messages utilisateur et assistant</li> <li> Indicateur de chargement pendant le traitement</li> <li> Historique de conversation visible et navigable</li> <li> Remise \u00e0 z\u00e9ro de la conversation possible</li> <li> Interface responsive (mobile/desktop)</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#gestion-du-contexte","title":"Gestion du contexte","text":"<ul> <li> Conservation du contexte entre les messages</li> <li> R\u00e9f\u00e9rences coh\u00e9rentes aux questions/r\u00e9ponses pr\u00e9c\u00e9dentes</li> <li> Limitation de la taille du contexte pour optimiser l'API</li> <li> Priorit\u00e9 aux informations r\u00e9centes en cas de contexte trop long</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#base-de-connaissances","title":"Base de connaissances","text":"<ul> <li> Structure hi\u00e9rarchique des concepts de Deep Learning</li> <li> Couverture compl\u00e8te du programme des 4 s\u00e9ances</li> <li> Contenu adapt\u00e9 aux diff\u00e9rents niveaux (d\u00e9butant/interm\u00e9diaire/avanc\u00e9)</li> <li> Exemples concrets pour chaque concept principal</li> <li> Sources et r\u00e9f\u00e9rences techniques v\u00e9rifi\u00e9es</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#integration-api-mistral","title":"Int\u00e9gration API Mistral","text":"<ul> <li> Connexion fonctionnelle avec gestion des erreurs</li> <li> Optimisation des prompts pour obtenir des r\u00e9ponses p\u00e9dagogiques</li> <li> Gestion des limites de l'API (quotas, d\u00e9lais de r\u00e9ponse)</li> <li> Param\u00e8tres adapt\u00e9s selon le type de question (temperature, max_tokens)</li> <li> M\u00e9canisme de fallback en cas d'\u00e9chec de l'API</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#fonctionnalites-pedagogiques","title":"Fonctionnalit\u00e9s p\u00e9dagogiques","text":"<ul> <li> Adaptation du niveau d'explication au profil de l'utilisateur</li> <li> G\u00e9n\u00e9ration d'exemples pertinents</li> <li> Quiz ou exercices interactifs</li> <li> Suggestions de concepts \u00e0 explorer</li> <li> Visualisations ou sch\u00e9mas explicatifs (si applicable)</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#aspects-techniques","title":"Aspects techniques","text":""},{"location":"evaluation/checklist-auto-evaluation/#code-et-architecture","title":"Code et architecture","text":"<ul> <li> Structure modulaire et bien organis\u00e9e</li> <li> S\u00e9paration claire frontend/backend</li> <li> Nommage explicite des variables et fonctions</li> <li> Commentaires sur le code complexe</li> <li> Gestion des exceptions et des cas d'erreur</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#performance","title":"Performance","text":"<ul> <li> Temps de r\u00e9ponse raisonnable (&lt;5s)</li> <li> Optimisation des appels \u00e0 l'API</li> <li> Syst\u00e8me de cache pour les questions fr\u00e9quentes</li> <li> Chargement optimis\u00e9 des ressources</li> <li> Fonctionnement fluide m\u00eame avec un historique long</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#securite-et-bonnes-pratiques","title":"S\u00e9curit\u00e9 et bonnes pratiques","text":"<ul> <li> Gestion s\u00e9curis\u00e9e de la cl\u00e9 API (variables d'environnement)</li> <li> Validation des entr\u00e9es utilisateur</li> <li> Protection contre les injections</li> <li> Absence de secrets dans le code source</li> <li> Respect des bonnes pratiques de d\u00e9veloppement web</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#documentation","title":"Documentation","text":""},{"location":"evaluation/checklist-auto-evaluation/#documentation-technique","title":"Documentation technique","text":"<ul> <li> Architecture du syst\u00e8me expliqu\u00e9e</li> <li> Diagrammes et sch\u00e9mas d'explication</li> <li> Instructions d'installation claires</li> <li> Description des d\u00e9pendances</li> <li> Explication des choix techniques</li> <li> Structure des fichiers d\u00e9taill\u00e9e</li> <li> API et interfaces document\u00e9es</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#guide-utilisateur","title":"Guide utilisateur","text":"<ul> <li> Instructions de d\u00e9marrage</li> <li> Description des fonctionnalit\u00e9s</li> <li> Exemples d'utilisation</li> <li> FAQ avec questions courantes</li> <li> Troubleshooting des probl\u00e8mes communs</li> <li> Captures d'\u00e9cran illustratives</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#base-de-code","title":"Base de code","text":"<ul> <li> README.md complet</li> <li> Commentaires pertinents dans le code</li> <li> Fichier requirements.txt ou package.json</li> <li> Fichier .env.example (sans la vraie cl\u00e9 API)</li> <li> Organisation claire des dossiers</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#presentation","title":"Pr\u00e9sentation","text":""},{"location":"evaluation/checklist-auto-evaluation/#support-visuel","title":"Support visuel","text":"<ul> <li> Diapositives claires et professionnelles</li> <li> Structure logique de la pr\u00e9sentation</li> <li> \u00c9quilibre entre texte et visuels</li> <li> Mise en \u00e9vidence des points forts</li> <li> Aper\u00e7u de l'architecture et des fonctionnalit\u00e9s</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#demonstration","title":"D\u00e9monstration","text":"<ul> <li> Sc\u00e9nario de d\u00e9monstration pr\u00e9par\u00e9</li> <li> Test des fonctionnalit\u00e9s cl\u00e9s en direct</li> <li> Plan B en cas de probl\u00e8me technique</li> <li> Exemples vari\u00e9s montrant diff\u00e9rentes capacit\u00e9s</li> <li> Timing respect\u00e9</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#processus-de-developpement","title":"Processus de d\u00e9veloppement","text":""},{"location":"evaluation/checklist-auto-evaluation/#organisation-de-lequipe","title":"Organisation de l'\u00e9quipe","text":"<ul> <li> R\u00e9partition \u00e9quilibr\u00e9e des t\u00e2ches</li> <li> Communication r\u00e9guli\u00e8re document\u00e9e</li> <li> Utilisation d'outils de gestion de projet</li> <li> Revues de code entre membres</li> <li> Journal des d\u00e9cisions importantes</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#gestion-du-temps","title":"Gestion du temps","text":"<ul> <li> Respect des jalons interm\u00e9diaires</li> <li> Planification des t\u00e2ches document\u00e9e</li> <li> Priorisation des fonctionnalit\u00e9s essentielles</li> <li> Finalisation dans les d\u00e9lais</li> <li> Adaptation aux impr\u00e9vus</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#comment-utiliser-cette-checklist","title":"Comment utiliser cette checklist","text":"<ol> <li>Parcourez cette liste au moins une semaine avant la date de rendu finale</li> <li>Cochez les \u00e9l\u00e9ments d\u00e9j\u00e0 impl\u00e9ment\u00e9s ou compl\u00e9t\u00e9s</li> <li>Priorisez les \u00e9l\u00e9ments non coch\u00e9s selon leur importance et difficult\u00e9</li> <li>Attribuez des responsables pour chaque \u00e9l\u00e9ment restant</li> <li>Planifiez des points de contr\u00f4le r\u00e9guliers pour suivre l'avancement</li> <li>V\u00e9rifiez \u00e0 nouveau tous les points avant la soumission finale</li> </ol> <p>Cette checklist vous aidera \u00e0 vous assurer que vous n'avez rien oubli\u00e9 et que votre projet est aussi complet que possible.</p> <p>Retour \u00e0 l'index d'\u00e9valuation</p>"},{"location":"evaluation/criteres-evaluation-seance4/","title":"Fiche d'\u00e9valuation - Projet chatbot p\u00e9dagogique Deep Learning","text":""},{"location":"evaluation/criteres-evaluation-seance4/#informations-generales","title":"Informations g\u00e9n\u00e9rales","text":"<p>Nom de l'\u00e9quipe : ________</p> <p>Membres :  1. _____ 2. _____</p> <p>Date de remise : ________</p> <p>URL du d\u00e9p\u00f4t Git : ________</p>"},{"location":"evaluation/criteres-evaluation-seance4/#1-qualite-du-produit-final-30","title":"1. Qualit\u00e9 du produit final (30%)","text":""},{"location":"evaluation/criteres-evaluation-seance4/#11-fonctionnalites-implementees","title":"1.1 Fonctionnalit\u00e9s impl\u00e9ment\u00e9es","text":"Fonctionnalit\u00e9 Non impl\u00e9ment\u00e9e (0) Partiellement impl\u00e9ment\u00e9e (1) Fonctionnelle avec limitations (2) Pleinement fonctionnelle (3) Interface conversationnelle \u25a1 \u25a1 \u25a1 \u25a1 Gestion du contexte de conversation \u25a1 \u25a1 \u25a1 \u25a1 Base de connaissances structur\u00e9e \u25a1 \u25a1 \u25a1 \u25a1 Int\u00e9gration API Mistral \u25a1 \u25a1 \u25a1 \u25a1 Explications des concepts DL \u25a1 \u25a1 \u25a1 \u25a1 G\u00e9n\u00e9ration d'exemples \u25a1 \u25a1 \u25a1 \u25a1 Exercices interactifs \u25a1 \u25a1 \u25a1 \u25a1 <p>Commentaires : <pre><code>_______________________________________________________________________\n_______________________________________________________________________\n_______________________________________________________________________\n</code></pre></p>"},{"location":"evaluation/criteres-evaluation-seance4/#12-experience-utilisateur","title":"1.2 Exp\u00e9rience utilisateur","text":"Crit\u00e8re Insuffisant (0) Satisfaisant (1) Bon (2) Excellent (3) Intuitivit\u00e9 de l'interface \u25a1 \u25a1 \u25a1 \u25a1 Qualit\u00e9 des r\u00e9ponses g\u00e9n\u00e9r\u00e9es \u25a1 \u25a1 \u25a1 \u25a1 Temps de r\u00e9ponse \u25a1 \u25a1 \u25a1 \u25a1 Gestion des erreurs \u25a1 \u25a1 \u25a1 \u25a1 Adaptation au niveau de l'utilisateur \u25a1 \u25a1 \u25a1 \u25a1 <p>Commentaires : <pre><code>_______________________________________________________________________\n_______________________________________________________________________\n_______________________________________________________________________\n</code></pre></p>"},{"location":"evaluation/criteres-evaluation-seance4/#13-qualite-technique","title":"1.3 Qualit\u00e9 technique","text":"Crit\u00e8re Insuffisant (0) Satisfaisant (1) Bon (2) Excellent (3) Structure du code \u25a1 \u25a1 \u25a1 \u25a1 Gestion de l'API Mistral \u25a1 \u25a1 \u25a1 \u25a1 Performance et optimisation \u25a1 \u25a1 \u25a1 \u25a1 Robustesse aux entr\u00e9es impr\u00e9vues \u25a1 \u25a1 \u25a1 \u25a1 <p>Commentaires : <pre><code>_______________________________________________________________________\n_______________________________________________________________________\n_______________________________________________________________________\n</code></pre></p> <p>Score section 1 : _____ / 60 points</p>"},{"location":"evaluation/criteres-evaluation-seance4/#2-exactitude-du-contenu-pedagogique-20","title":"2. Exactitude du contenu p\u00e9dagogique (20%)","text":"Concept Inexact (0) Partiellement exact (1) Pr\u00e9cis (2) Tr\u00e8s pr\u00e9cis et complet (3) Introduction au Deep Learning \u25a1 \u25a1 \u25a1 \u25a1 R\u00e9seaux de neurones artificiels \u25a1 \u25a1 \u25a1 \u25a1 Convolution Neural Networks (CNN) \u25a1 \u25a1 \u25a1 \u25a1 Recurrent Neural Networks (RNN) \u25a1 \u25a1 \u25a1 \u25a1 Apprentissage et optimisation \u25a1 \u25a1 \u25a1 \u25a1 Frameworks (TensorFlow, Keras, etc.) \u25a1 \u25a1 \u25a1 \u25a1 Applications pratiques \u25a1 \u25a1 \u25a1 \u25a1 <p>Commentaires : <pre><code>_______________________________________________________________________\n_______________________________________________________________________\n_______________________________________________________________________\n</code></pre></p> <p>Score section 2 : _____ / 21 points</p>"},{"location":"evaluation/criteres-evaluation-seance4/#3-organisation-et-processus-de-developpement-15","title":"3. Organisation et processus de d\u00e9veloppement (15%)","text":"Crit\u00e8re Insuffisant (0) Satisfaisant (1) Bon (2) Excellent (3) R\u00e9partition des t\u00e2ches \u25a1 \u25a1 \u25a1 \u25a1 Utilisation du contr\u00f4le de version \u25a1 \u25a1 \u25a1 \u25a1 Planification et respect des d\u00e9lais \u25a1 \u25a1 \u25a1 \u25a1 Communication dans l'\u00e9quipe \u25a1 \u25a1 \u25a1 \u25a1 Adaptation aux difficult\u00e9s rencontr\u00e9es \u25a1 \u25a1 \u25a1 \u25a1 <p>Commentaires : <pre><code>_______________________________________________________________________\n_______________________________________________________________________\n_______________________________________________________________________\n</code></pre></p> <p>Score section 3 : _____ / 15 points</p>"},{"location":"evaluation/criteres-evaluation-seance4/#4-documentation-et-presentation-15","title":"4. Documentation et pr\u00e9sentation (15%)","text":""},{"location":"evaluation/criteres-evaluation-seance4/#41-documentation-technique","title":"4.1 Documentation technique","text":"Crit\u00e8re Insuffisant (0) Satisfaisant (1) Bon (2) Excellent (3) Installation et configuration \u25a1 \u25a1 \u25a1 \u25a1 Architecture du syst\u00e8me \u25a1 \u25a1 \u25a1 \u25a1 Structure de la base de connaissances \u25a1 \u25a1 \u25a1 \u25a1 API et interfaces \u25a1 \u25a1 \u25a1 \u25a1"},{"location":"evaluation/criteres-evaluation-seance4/#42-guide-utilisateur","title":"4.2 Guide utilisateur","text":"Crit\u00e8re Insuffisant (0) Satisfaisant (1) Bon (2) Excellent (3) Clart\u00e9 des instructions \u25a1 \u25a1 \u25a1 \u25a1 Exemples d'utilisation \u25a1 \u25a1 \u25a1 \u25a1 Gestion des probl\u00e8mes courants \u25a1 \u25a1 \u25a1 \u25a1"},{"location":"evaluation/criteres-evaluation-seance4/#43-presentation-finale","title":"4.3 Pr\u00e9sentation finale","text":"Crit\u00e8re Insuffisant (0) Satisfaisant (1) Bon (2) Excellent (3) Structure et clart\u00e9 \u25a1 \u25a1 \u25a1 \u25a1 D\u00e9monstration fonctionnelle \u25a1 \u25a1 \u25a1 \u25a1 R\u00e9ponses aux questions \u25a1 \u25a1 \u25a1 \u25a1 <p>Commentaires : <pre><code>_______________________________________________________________________\n_______________________________________________________________________\n_______________________________________________________________________\n</code></pre></p> <p>Score section 4 : _____ / 33 points</p>"},{"location":"evaluation/criteres-evaluation-seance4/#5-innovation-et-valeur-ajoutee-10","title":"5. Innovation et valeur ajout\u00e9e (10%)","text":"Crit\u00e8re Insuffisant (0) Satisfaisant (1) Bon (2) Excellent (3) Fonctionnalit\u00e9s originales \u25a1 \u25a1 \u25a1 \u25a1 Approche p\u00e9dagogique innovante \u25a1 \u25a1 \u25a1 \u25a1 Potentiel d'utilisation r\u00e9elle \u25a1 \u25a1 \u25a1 \u25a1 Adaptabilit\u00e9 \u00e0 d'autres contextes \u25a1 \u25a1 \u25a1 \u25a1 <p>Commentaires : <pre><code>_______________________________________________________________________\n_______________________________________________________________________\n_______________________________________________________________________\n</code></pre></p> <p>Score section 5 : _____ / 12 points</p>"},{"location":"evaluation/criteres-evaluation-seance4/#6-participation-active-10","title":"6. Participation active (10%)","text":"Membre Limit\u00e9e (0-3) R\u00e9guli\u00e8re (4-6) Importante (7-9) Exceptionnelle (10) Membre 1 : ___ \u25a1 \u25a1 \u25a1 \u25a1 Membre 2 : ___ \u25a1 \u25a1 \u25a1 \u25a1 <p>Commentaires : <pre><code>_______________________________________________________________________\n_______________________________________________________________________\n_______________________________________________________________________\n</code></pre></p> <p>Score section 6 : _____ / 20 points</p>"},{"location":"evaluation/criteres-evaluation-seance4/#recapitulatif-de-levaluation","title":"R\u00e9capitulatif de l'\u00e9valuation","text":"Section Points obtenus Points possibles Pourcentage Pond\u00e9ration Score pond\u00e9r\u00e9 1. Qualit\u00e9 du produit final 60 30% 2. Exactitude du contenu p\u00e9dagogique 21 20% 3. Organisation et processus 15 15% 4. Documentation et pr\u00e9sentation 33 15% 5. Innovation et valeur ajout\u00e9e 12 10% 6. Participation active 20 10% TOTAL 100% <p>Note finale : _____ / 20</p>"},{"location":"evaluation/criteres-evaluation-seance4/#feedback-global","title":"Feedback global","text":"<p>Forces du projet : <pre><code>_______________________________________________________________________\n_______________________________________________________________________\n_______________________________________________________________________\n</code></pre></p> <p>Axes d'am\u00e9lioration : <pre><code>_______________________________________________________________________\n_______________________________________________________________________\n_______________________________________________________________________\n</code></pre></p> <p>Conseils pour l'avenir : <pre><code>_______________________________________________________________________\n_______________________________________________________________________\n_______________________________________________________________________\n</code></pre></p>"},{"location":"evaluation/criteres-evaluation/","title":"Crit\u00e8res d'\u00e9valuation","text":"<p>Ce document pr\u00e9sente les crit\u00e8res d'\u00e9valuation du projet de chatbot p\u00e9dagogique sur le Deep Learning.</p>"},{"location":"evaluation/criteres-evaluation/#repartition-globale","title":"R\u00e9partition globale","text":"Composante % Description Participation active 10% Engagement dans les activit\u00e9s et contributions Mini-projets 30% Qualit\u00e9 des livrables des s\u00e9ances 2 et 3 Projet final - Produit 30% Fonctionnalit\u00e9 et qualit\u00e9 du chatbot Projet final - Processus 15% Organisation et m\u00e9thodologie Projet final - Pr\u00e9sentation 15% Pr\u00e9sentation et documentation"},{"location":"evaluation/criteres-evaluation/#detail-des-criteres","title":"D\u00e9tail des crit\u00e8res","text":""},{"location":"evaluation/criteres-evaluation/#1-participation-active-10","title":"1. Participation active (10%)","text":"<ul> <li>Pr\u00e9sence et engagement : Assiduit\u00e9 et implication dans les activit\u00e9s</li> <li>Pertinence des interventions : Qualit\u00e9 des questions et contributions</li> <li>Collaboration : Attitude constructive dans le travail d'\u00e9quipe</li> </ul>"},{"location":"evaluation/criteres-evaluation/#2-mini-projets-30","title":"2. Mini-projets (30%)","text":""},{"location":"evaluation/criteres-evaluation/#mini-projet-cnn-10","title":"Mini-projet CNN (10%)","text":"<ul> <li>Impl\u00e9mentation fonctionnelle du mod\u00e8le CNN</li> <li>Visualisation et compr\u00e9hension des feature maps</li> <li>Int\u00e9gration dans l'application web</li> <li>Analyse des performances et limitations</li> </ul>"},{"location":"evaluation/criteres-evaluation/#mini-projet-rnn-10","title":"Mini-projet RNN (10%)","text":"<ul> <li>Impl\u00e9mentation du mod\u00e8le LSTM pour analyse de sentiment</li> <li>Pr\u00e9traitement des donn\u00e9es textuelles</li> <li>Exp\u00e9rimentation avec l'API Mistral AI</li> <li>Documentation des r\u00e9sultats</li> </ul>"},{"location":"evaluation/criteres-evaluation/#projet-damelioration-10","title":"Projet d'am\u00e9lioration (10%)","text":"<ul> <li>Diagnostic pertinent du mod\u00e8le sous-optimal</li> <li>Techniques d'optimisation appliqu\u00e9es</li> <li>Am\u00e9lioration mesurable des performances</li> <li>Rapport d'analyse complet</li> </ul>"},{"location":"evaluation/criteres-evaluation/#3-projet-final-produit-30","title":"3. Projet final - Produit (30%)","text":"<ul> <li>Fonctionnalit\u00e9 du chatbot (12%)</li> <li>Interface conversationnelle intuitive</li> <li>Gestion du contexte et des sessions</li> <li>Qualit\u00e9 des r\u00e9ponses p\u00e9dagogiques</li> <li>Robustesse face aux requ\u00eates vari\u00e9es</li> <li>Base de connaissances (9%)</li> <li>Couverture compl\u00e8te des concepts du Deep Learning vus en cours</li> <li>Organisation logique et hi\u00e9rarchis\u00e9e</li> <li>Adaptation au niveau de l'utilisateur (d\u00e9butant/interm\u00e9diaire/avanc\u00e9)</li> <li>Exemples pertinents et analogies p\u00e9dagogiques</li> <li>Int\u00e9gration technique (9%)</li> <li>Utilisation optimale de l'API Mistral AI</li> <li>Architecture backend bien structur\u00e9e</li> <li>Performances et temps de r\u00e9ponse optimis\u00e9s</li> <li>Gestion des erreurs et cas limites</li> </ul>"},{"location":"evaluation/criteres-evaluation/#4-projet-final-processus-15","title":"4. Projet final - Processus (15%)","text":"<ul> <li>Organisation de l'\u00e9quipe (5%)</li> <li>R\u00e9partition \u00e9quilibr\u00e9e des t\u00e2ches</li> <li>Utilisation d'outils de suivi</li> <li>Communication efficace entre membres</li> <li>Gestion du projet (5%)</li> <li>Respect des jalons interm\u00e9diaires</li> <li>Adaptation aux difficult\u00e9s rencontr\u00e9es</li> <li>Documentation du processus de d\u00e9veloppement</li> <li>M\u00e9thodologie technique (5%)</li> <li>Approche structur\u00e9e du d\u00e9veloppement</li> <li>Tests et validation des fonctionnalit\u00e9s</li> <li>Gestion de version et organisation du code</li> </ul>"},{"location":"evaluation/criteres-evaluation/#5-projet-final-presentation-15","title":"5. Projet final - Pr\u00e9sentation (15%)","text":"<ul> <li>Pr\u00e9sentation orale (7%)</li> <li>Clart\u00e9 et structure de l'expos\u00e9</li> <li>Qualit\u00e9 de la d\u00e9monstration en direct</li> <li>R\u00e9ponses pertinentes aux questions</li> <li>Respect du temps imparti</li> <li>Documentation (8%)</li> <li>Documentation technique compl\u00e8te</li> <li>Guide utilisateur clair</li> <li>Qualit\u00e9 de la structure du code source</li> <li>Explications des choix d'impl\u00e9mentation</li> </ul>"},{"location":"evaluation/criteres-evaluation/#livrables-attendus","title":"Livrables attendus","text":""},{"location":"evaluation/criteres-evaluation/#a-lissue-de-la-seance-2","title":"\u00c0 l'issue de la s\u00e9ance 2","text":"<ul> <li>Mod\u00e8le CNN fonctionnel avec visualisations</li> <li>Mod\u00e8le RNN pour analyse de texte</li> <li>Analyse comparative des performances</li> </ul>"},{"location":"evaluation/criteres-evaluation/#a-lissue-de-la-seance-3","title":"\u00c0 l'issue de la s\u00e9ance 3","text":"<ul> <li>Application optimis\u00e9e et rapport d'optimisation</li> <li>Premier test d'int\u00e9gration avec l'API Mistral</li> <li>Document de conception du chatbot</li> </ul>"},{"location":"evaluation/criteres-evaluation/#a-lissue-de-la-seance-4","title":"\u00c0 l'issue de la s\u00e9ance 4","text":"<ul> <li>Code source complet du chatbot</li> <li>Base de connaissances sur le Deep Learning</li> <li>Documentation technique et guide utilisateur</li> <li>Support de pr\u00e9sentation</li> </ul>"},{"location":"evaluation/criteres-evaluation/#conseils-pour-maximiser-votre-evaluation","title":"Conseils pour maximiser votre \u00e9valuation","text":"<ol> <li>Commencez par l'essentiel : Assurez-vous que les fonctionnalit\u00e9s de base sont solides avant d'ajouter des \u00e9l\u00e9ments avanc\u00e9s</li> <li>Documentez au fur et \u00e0 mesure : Ne laissez pas la documentation pour la fin</li> <li>Testez abondamment : Envisagez diff\u00e9rents sc\u00e9narios d'utilisation</li> <li>Montrez votre compr\u00e9hension : Expliquez vos choix techniques dans la documentation</li> <li>Pr\u00e9parez une d\u00e9mo convaincante : Identifiez les sc\u00e9narios qui mettent en valeur votre travail</li> </ol>"},{"location":"evaluation/grille-repartition-taches/","title":"Grille de r\u00e9partition des t\u00e2ches","text":""},{"location":"evaluation/grille-repartition-taches/#organisation-du-travail-dequipe-pour-le-projet-chatbot-pedagogique","title":"Organisation du travail d'\u00e9quipe pour le projet chatbot p\u00e9dagogique","text":"<p>Cette grille vous aidera \u00e0 organiser efficacement le travail au sein de votre \u00e9quipe pour le d\u00e9veloppement du chatbot p\u00e9dagogique sur le Deep Learning. Une bonne r\u00e9partition des t\u00e2ches est essentielle pour la r\u00e9ussite du projet et fait partie des crit\u00e8res d'\u00e9valuation.</p>"},{"location":"evaluation/grille-repartition-taches/#composition-de-lequipe","title":"Composition de l'\u00e9quipe","text":"Nom et pr\u00e9nom R\u00f4le principal Comp\u00e9tences cl\u00e9s Contact <p>\u00c0 compl\u00e9ter avec les informations de votre \u00e9quipe (2 personnes maximum par \u00e9quipe)</p>"},{"location":"evaluation/grille-repartition-taches/#domaines-de-responsabilite","title":"Domaines de responsabilit\u00e9","text":"<p>Pour \u00e9quilibrer le travail entre les membres de l'\u00e9quipe, nous recommandons de r\u00e9partir les responsabilit\u00e9s selon les domaines suivants. Chaque \u00e9tudiant doit prendre en charge une partie technique et une partie contenu/documentation.</p>"},{"location":"evaluation/grille-repartition-taches/#repartition-suggeree-des-responsabilites","title":"R\u00e9partition sugg\u00e9r\u00e9e des responsabilit\u00e9s","text":""},{"location":"evaluation/grille-repartition-taches/#membre-1","title":"Membre 1","text":"<p>Responsabilit\u00e9s techniques :   - Conception de l'architecture globale du chatbot   - D\u00e9veloppement de la logique de traitement des requ\u00eates   - Int\u00e9gration technique avec l'API Mistral AI   - Tests d'int\u00e9gration</p> <p>Responsabilit\u00e9s de contenu :   - Structuration de la base de connaissances sur le Deep Learning   - Organisation des concepts en niveaux de difficult\u00e9   - Validation de l'exactitude des informations techniques</p>"},{"location":"evaluation/grille-repartition-taches/#membre-2","title":"Membre 2","text":"<p>Responsabilit\u00e9s techniques :   - Conception de l'interface conversationnelle   - D\u00e9veloppement de l'interface utilisateur   - Gestion des flux de conversation   - Tests utilisateurs</p> <p>Responsabilit\u00e9s de contenu :   - Recherche et r\u00e9daction des contenus p\u00e9dagogiques   - Cr\u00e9ation des exemples et illustrations   - Pr\u00e9paration de la documentation technique et du guide utilisateur</p>"},{"location":"evaluation/grille-repartition-taches/#planification-des-taches-par-seance","title":"Planification des t\u00e2ches par s\u00e9ance","text":""},{"location":"evaluation/grille-repartition-taches/#a-lissue-de-la-seance-2","title":"\u00c0 l'issue de la s\u00e9ance 2","text":"T\u00e2che Responsable Deadline Statut Ma\u00eetriser les principes des CNN et RNN Comprendre l'int\u00e9gration des mod\u00e8les dans une application web Compl\u00e9ter les mini-projets CNN et RNN R\u00e9fl\u00e9chir aux applications potentielles pour le chatbot"},{"location":"evaluation/grille-repartition-taches/#a-lissue-de-la-seance-3","title":"\u00c0 l'issue de la s\u00e9ance 3","text":"T\u00e2che Responsable Deadline Statut Document d'architecture du chatbot Premi\u00e8re version de la structure de la base de connaissances Prototype d'interface minimaliste Premier test d'int\u00e9gration avec API Mistral Plan de projet d\u00e9taill\u00e9"},{"location":"evaluation/grille-repartition-taches/#pour-la-seance-4-finalisation","title":"Pour la s\u00e9ance 4 (finalisation)","text":"T\u00e2che Responsable Deadline Statut Interface conversationnelle compl\u00e8te Int\u00e9gration avanc\u00e9e avec l'API Mistral AI Base de connaissances compl\u00e8te Fonctionnalit\u00e9s p\u00e9dagogiques Tests fonctionnels Documentation technique Guide utilisateur Pr\u00e9paration de la d\u00e9monstration"},{"location":"evaluation/grille-repartition-taches/#suivi-des-reunions-dequipe","title":"Suivi des r\u00e9unions d'\u00e9quipe","text":"Date Dur\u00e9e Participants Sujets abord\u00e9s D\u00e9cisions prises Prochaines actions <p>Planifiez au moins une r\u00e9union d'\u00e9quipe entre chaque s\u00e9ance</p>"},{"location":"evaluation/grille-repartition-taches/#gestion-des-risques","title":"Gestion des risques","text":"<p>Identifiez les principaux risques pour votre projet et pr\u00e9voyez des strat\u00e9gies d'att\u00e9nuation :</p> Risque Probabilit\u00e9 (1-5) Impact (1-5) Strat\u00e9gie d'att\u00e9nuation Responsable Difficult\u00e9s d'int\u00e9gration avec l'API Mistral Probl\u00e8mes de coh\u00e9rence dans la base de connaissances D\u00e9passement des d\u00e9lais Conflits techniques ou de conception Limites de l'API gratuite (quotas)"},{"location":"evaluation/grille-repartition-taches/#outils-de-collaboration","title":"Outils de collaboration","text":"<p>Listez les outils que vous utiliserez pour la collaboration d'\u00e9quipe :</p> <ul> <li>Gestion de code : ___ (ex: GitHub, GitLab)</li> <li>Communication : ___ (ex: Discord, Slack)</li> <li>Partage de documents : ___ (ex: Google Drive, OneDrive)</li> <li>Suivi de projet : ___ (ex: Trello, GitHub Projects)</li> </ul>"},{"location":"evaluation/grille-repartition-taches/#processus-de-developpement","title":"Processus de d\u00e9veloppement","text":"<p>D\u00e9crivez bri\u00e8vement votre approche pour le d\u00e9veloppement du chatbot :</p> <ol> <li>Phase d'exploration : </li> <li>Tester l'API Mistral avec diff\u00e9rents types de prompts</li> <li>Explorer des exemples de chatbots p\u00e9dagogiques existants</li> <li> <p>D\u00e9finir les limites et possibilit\u00e9s techniques</p> </li> <li> <p>Phase de conception :</p> </li> <li>D\u00e9finir l'architecture technique</li> <li>Structurer la base de connaissances</li> <li> <p>Concevoir les flux de conversation</p> </li> <li> <p>Phase de d\u00e9veloppement :</p> </li> <li>Impl\u00e9mentation par composants</li> <li>Revues de code r\u00e9guli\u00e8res</li> <li> <p>Tests continus</p> </li> <li> <p>Phase de finalisation :</p> </li> <li>Tests fonctionnels complets</li> <li>Documentation</li> <li>Pr\u00e9paration de la pr\u00e9sentation</li> </ol>"},{"location":"evaluation/grille-repartition-taches/#engagement-de-lequipe","title":"Engagement de l'\u00e9quipe","text":"<p>En tant que membres de l'\u00e9quipe, nous nous engageons \u00e0 :    - Respecter les d\u00e9lais et les responsabilit\u00e9s assign\u00e9es    - Communiquer r\u00e9guli\u00e8rement sur l'avancement de nos t\u00e2ches    - Demander de l'aide si nous rencontrons des difficult\u00e9s    - Contribuer activement \u00e0 la r\u00e9ussite collective du projet    - Documenter notre travail pour faciliter l'int\u00e9gration</p>"},{"location":"evaluation/grille-repartition-taches/#evaluation-de-la-repartition-des-taches","title":"\u00c9valuation de la r\u00e9partition des t\u00e2ches","text":"<p>Cette grille sera utilis\u00e9e dans l'\u00e9valuation finale du projet, dans la composante \"Projet final - Processus\" (15% de la note finale). Les crit\u00e8res suivants seront consid\u00e9r\u00e9s :</p> <ul> <li>\u00c9quilibre dans la r\u00e9partition des responsabilit\u00e9s</li> <li>Ad\u00e9quation entre les comp\u00e9tences et les r\u00f4les assign\u00e9s</li> <li>Respect des engagements pris</li> <li>Qualit\u00e9 de la collaboration et de la communication</li> <li>Capacit\u00e9 d'adaptation face aux difficult\u00e9s rencontr\u00e9es</li> </ul> <p>Consultez la grille d\u00e9taill\u00e9e d'\u00e9valuation pour plus d'informations sur les autres crit\u00e8res d'\u00e9valuation du projet.</p>"},{"location":"module1/","title":"Module 1 : Fondamentaux du Deep Learning","text":""},{"location":"module1/#objectifs-du-module","title":"Objectifs du module","text":"<p>\u00c0 l'issue de ce module, vous serez capable de :</p> <ul> <li>Manipuler concr\u00e8tement un r\u00e9seau de neurones simple</li> <li>Comprendre les diff\u00e9rences entre Machine Learning classique et Deep Learning</li> <li>Expliquer le fonctionnement de base d'un r\u00e9seau de neurones</li> <li>Appliquer des techniques d'am\u00e9lioration d'un mod\u00e8le de Deep Learning</li> </ul>"},{"location":"module1/#programme-4h","title":"Programme (4h)","text":"<p>Ce module se d\u00e9roule en quatre phases distinctes, chacune con\u00e7ue pour vous faire d\u00e9couvrir le Deep Learning par la pratique plut\u00f4t que par la th\u00e9orie.</p>"},{"location":"module1/#phase-1-introduction-pratique-1h","title":"Phase 1 : Introduction pratique (1h)","text":"<p>D\u00e9couvrez le Deep Learning \u00e0 travers des exemples concrets, sans vous pr\u00e9occuper de la th\u00e9orie pour le moment.</p> <ul> <li>D\u00e9monstrations d'applications concr\u00e8tes (GitHub Copilot, reconnaissance d'objets...)</li> <li>Premier contact avec un r\u00e9seau de neurones simple</li> <li>Challenge d'exp\u00e9rimentation guid\u00e9e sur un mod\u00e8le MNIST</li> </ul>"},{"location":"module1/#phase-2-concepts-fondamentaux-1h30","title":"Phase 2 : Concepts fondamentaux (1h30)","text":"<p>Comparez les approches du Machine Learning classique et du Deep Learning pour comprendre leurs diff\u00e9rences fondamentales.</p> <ul> <li>Atelier \"Bo\u00eete noire\" : exploration parall\u00e8le des deux approches</li> <li>D\u00e9fi de g\u00e9n\u00e9ralisation sur des donn\u00e9es modifi\u00e9es</li> <li>Exploration interactive d'un neurone et d'un r\u00e9seau simple</li> </ul>"},{"location":"module1/#phase-3-mini-projet-individuel-1h","title":"Phase 3 : Mini-projet individuel (1h)","text":"<p>Mettez en pratique vos connaissances en modifiant et am\u00e9liorant un r\u00e9seau de neurones.</p> <ul> <li>Modification des hyperparam\u00e8tres</li> <li>Test avec diff\u00e9rentes architectures</li> <li>Analyse de l'impact des changements sur les performances</li> <li>Documentation des r\u00e9sultats dans un rapport synth\u00e9tique</li> </ul>"},{"location":"module1/#phase-4-auto-evaluation-et-synthese-30min","title":"Phase 4 : Auto-\u00e9valuation et synth\u00e8se (30min)","text":"<p>Consolidez vos connaissances et \u00e9valuez votre compr\u00e9hension de mani\u00e8re autonome.</p> <ul> <li>QCM sur les concepts fondamentaux</li> <li>Synth\u00e8se personnelle \u00e0 r\u00e9diger</li> <li>Sch\u00e9ma conceptuel \u00e0 compl\u00e9ter</li> </ul> <p>Ressources : - QCM d'\u00e9valuation - Sch\u00e9ma \u00e0 compl\u00e9ter</p>"},{"location":"module1/#livrables-attendus","title":"Livrables attendus","text":"<p>\u00c0 l'issue de ce module, vous devrez avoir produit :</p> <ul> <li>La fiche d'observations compl\u00e9t\u00e9e sur le \"Hello World du Deep Learning\"</li> <li>Le tableau comparatif Machine Learning vs Deep Learning rempli</li> <li>Le sch\u00e9ma annot\u00e9 d'un r\u00e9seau de neurones</li> <li>Le rapport du mini-projet avec analyse des modifications effectu\u00e9es</li> </ul>"},{"location":"module1/#ressources-complementaires","title":"Ressources compl\u00e9mentaires","text":"<ul> <li>Glossaire du Deep Learning - Les termes essentiels expliqu\u00e9s simplement</li> <li>Guide d'utilisation de Google Colab - Pour vous aider \u00e0 utiliser cet outil</li> </ul>"},{"location":"module1/#competences-bts-sio-developpees","title":"Comp\u00e9tences BTS SIO d\u00e9velopp\u00e9es","text":"<p>Ce module vous permet d'acqu\u00e9rir plusieurs comp\u00e9tences du r\u00e9f\u00e9rentiel BTS SIO :</p> Comp\u00e9tence Description Activit\u00e9s associ\u00e9es B1.3 Gestion des donn\u00e9es Manipulation des datasets d'images B2.2 Conception et d\u00e9veloppement Am\u00e9lioration des mod\u00e8les de Deep Learning B2.3 Conception et d\u00e9veloppement d'IHM Analyse des interfaces de notebooks interactifs B3.2 V\u00e9rification et validation \u00c9valuation de la performance des mod\u00e8les"},{"location":"module1/#pret-a-commencer","title":"Pr\u00eat \u00e0 commencer ?","text":"<p>Plongez dans le monde fascinant du Deep Learning en commen\u00e7ant par la premi\u00e8re phase d'introduction pratique !</p> <p>Commencer par l'introduction pratique</p>"},{"location":"module1/concepts-fondamentaux/","title":"Concepts fondamentaux","text":"<p>Voici une version du contenu adapt\u00e9 pour une pratique individuelle sans l'aide de l'enseignant :</p>"},{"location":"module1/concepts-fondamentaux/#phase-2-decouverte-des-concepts-par-lexperimentation","title":"Phase 2 : D\u00e9couverte des concepts par l'exp\u00e9rimentation","text":""},{"location":"module1/concepts-fondamentaux/#objectifs-de-la-phase","title":"Objectifs de la phase","text":"<p>Dans cette phase, vous allez : - Comparer exp\u00e9rimentalement le Machine Learning classique et le Deep Learning - Observer les diff\u00e9rences fondamentales en termes de pr\u00e9paration des donn\u00e9es et de performances - D\u00e9couvrir l'anatomie d'un r\u00e9seau de neurones en manipulant directement ses composants - Comprendre par la pratique comment l'information circule dans un r\u00e9seau de neurones</p>"},{"location":"module1/concepts-fondamentaux/#comparaison-pratique-machine-learning-vs-deep-learning-30-min","title":"Comparaison pratique : Machine Learning vs Deep Learning (30 min)","text":""},{"location":"module1/concepts-fondamentaux/#objectif","title":"Objectif","text":"<p>Comprendre par l'observation directe les diff\u00e9rences fondamentales entre le Machine Learning classique et le Deep Learning, en les appliquant au m\u00eame jeu de donn\u00e9es.</p>"},{"location":"module1/concepts-fondamentaux/#instructions-pour-une-pratique-individuelle","title":"Instructions pour une pratique individuelle","text":"<ol> <li>Ouvrez deux notebooks Google Colab dans des onglets s\u00e9par\u00e9s :</li> <li>Machine Learning classique (Random Forest)</li> <li> <p>Deep Learning (CNN)</p> </li> <li> <p>Suivez les instructions dans chaque notebook et ex\u00e9cutez les cellules dans l'ordre indiqu\u00e9.</p> </li> <li> <p>Pendant que vous explorez les deux approches, prenez des notes sur :</p> </li> <li>Comment chaque approche traite les donn\u00e9es MNIST (chiffres manuscrits)</li> <li>Les diff\u00e9rences dans la pr\u00e9paration des donn\u00e9es</li> <li>La complexit\u00e9 d'impl\u00e9mentation de chaque approche</li> <li>Le temps d'entra\u00eenement respectif</li> <li>Les performances sur donn\u00e9es normales et bruit\u00e9es</li> </ol>"},{"location":"module1/concepts-fondamentaux/#points-cles-a-identifier-par-vous-meme","title":"Points cl\u00e9s \u00e0 identifier par vous-m\u00eame","text":"<p>\u00c0 travers cette exp\u00e9rimentation, identifiez ces concepts fondamentaux :</p> <ul> <li>Comment les caract\u00e9ristiques (features) sont trait\u00e9es dans chaque approche</li> <li>Le r\u00f4le de la repr\u00e9sentation des donn\u00e9es</li> <li>La capacit\u00e9 d'abstraction des diff\u00e9rents mod\u00e8les</li> <li>Les compromis entre temps d'entra\u00eenement et performance</li> </ul>"},{"location":"module1/concepts-fondamentaux/#tableau-comparatif-a-remplir","title":"Tableau comparatif \u00e0 remplir","text":"<p>Utilisez ce tableau pour noter vos observations :</p> Aspect observ\u00e9 Machine Learning (Random Forest) Deep Learning (CNN) Pr\u00e9paration des donn\u00e9es Extraction de caract\u00e9ristiques Temps d'entra\u00eenement Pr\u00e9cision globale Pr\u00e9cision sur donn\u00e9es bruit\u00e9es Facilit\u00e9 d'impl\u00e9mentation"},{"location":"module1/concepts-fondamentaux/#exploration-pratique-anatomie-dun-reseau-de-neurones-45-min","title":"Exploration pratique : Anatomie d'un r\u00e9seau de neurones (45 min)","text":"<p>Dans cette partie, vous allez explorer individuellement le fonctionnement interne d'un r\u00e9seau de neurones.</p>"},{"location":"module1/concepts-fondamentaux/#materiel-pour-la-pratique-individuelle","title":"Mat\u00e9riel pour la pratique individuelle","text":"<ul> <li>Notebook interactif \"Anatomie d'un r\u00e9seau de neurones\"</li> <li>Sch\u00e9ma \u00e0 compl\u00e9ter pour la synth\u00e8se</li> <li>Fiche r\u00e9capitulative des termes techniques</li> </ul>"},{"location":"module1/concepts-fondamentaux/#instructions-etape-par-etape","title":"Instructions \u00e9tape par \u00e9tape","text":""},{"location":"module1/concepts-fondamentaux/#partie-1-exploration-dun-neurone-unique-15-min","title":"Partie 1 : Exploration d'un neurone unique (15 min)","text":"<p>Dans cette partie, vous allez manipuler un neurone artificiel unique pour comprendre son fonctionnement de base.</p> <ol> <li>Ouvrez le notebook \"Anatomie d'un r\u00e9seau de neurones\" dans Google Colab</li> <li>Ex\u00e9cutez les cellules d'importation des biblioth\u00e8ques et de configuration</li> <li>Localisez la section \"Neurone unique\" et ex\u00e9cutez la cellule d'initialisation</li> <li>Exp\u00e9rimentez avec les contr\u00f4les interactifs pour :</li> <li>Modifier les valeurs d'entr\u00e9e (x\u2081, x\u2082)</li> <li>Ajuster les poids (w\u2081, w\u2082)</li> <li>Changer la valeur du biais (b)</li> <li>Observer l'effet sur la sortie du neurone</li> </ol> <p>Questions \u00e0 explorer par vous-m\u00eame :</p> <ul> <li>Que se passe-t-il si tous les poids sont \u00e0 z\u00e9ro ?</li> <li>Comment pouvez-vous configurer le neurone pour qu'il s'active uniquement si les deux entr\u00e9es sont \u00e9lev\u00e9es ?</li> <li>Quel est l'effet du biais sur le \"seuil\" d'activation ?</li> <li>Comment la fonction d'activation ReLU transforme-t-elle la sortie ?</li> </ul>"},{"location":"module1/concepts-fondamentaux/#partie-2-de-lunique-au-reseau-15-min","title":"Partie 2 : De l'unique au r\u00e9seau (15 min)","text":"<p>Passez maintenant \u00e0 un petit r\u00e9seau de neurones pour comprendre comment l'information circule \u00e0 travers les couches.</p> <ol> <li>Localisez la section \"R\u00e9seau simple\" et ex\u00e9cutez les cellules d'initialisation</li> <li>Explorez le r\u00e9seau compos\u00e9 de :</li> <li>Une couche d'entr\u00e9e (2 neurones)</li> <li>Une couche cach\u00e9e (3 neurones)</li> <li>Une couche de sortie (1 neurone)</li> <li>R\u00e9alisez les exp\u00e9riences suivantes par vous-m\u00eame :</li> <li>Observez comment le signal se propage \u00e0 travers les couches</li> <li>Suivez le parcours d'une information sp\u00e9cifique (valeur d'entr\u00e9e)</li> <li>Identifiez les \"motifs d'activation\" qui se forment pour diff\u00e9rentes entr\u00e9es</li> <li>Testez diff\u00e9rentes fonctions d'activation (ReLU, Sigmoid, Tanh)</li> </ol> <p>Exercice pratique :  Essayez de configurer manuellement les poids pour que le r\u00e9seau r\u00e9alise la fonction logique XOR (entr\u00e9es : [0,0]\u21920, [0,1]\u21921, [1,0]\u21921, [1,1]\u21920).</p>"},{"location":"module1/concepts-fondamentaux/#partie-3-visualisation-de-lentrainement-10-min","title":"Partie 3 : Visualisation de l'entra\u00eenement (10 min)","text":"<p>Dans cette partie, vous allez observer comment un r\u00e9seau apprend au fil du temps.</p> <ol> <li>Localisez la section \"Entra\u00eenement\" et ex\u00e9cutez la cellule d'initialisation</li> <li>Lancez la visualisation de l'entra\u00eenement en temps r\u00e9el</li> <li>Observez :</li> <li>L'\u00e9volution des poids \u00e0 chaque it\u00e9ration</li> <li>Comment la \"fronti\u00e8re de d\u00e9cision\" se modifie</li> <li>La diminution de l'erreur au fil des \u00e9poques</li> <li>Essayez de modifier par vous-m\u00eame :</li> <li>Le taux d'apprentissage (learning rate)</li> <li>La complexit\u00e9 du probl\u00e8me (type de donn\u00e9es)</li> <li>L'architecture du r\u00e9seau (nombre de neurones)</li> </ol>"},{"location":"module1/concepts-fondamentaux/#partie-4-synthese-et-verbalisation-5-min","title":"Partie 4 : Synth\u00e8se et verbalisation (5 min)","text":"<ol> <li>Compl\u00e9tez le sch\u00e9ma du r\u00e9seau de neurones fourni en fin de notebook</li> <li>Identifiez et nommez correctement :</li> <li>Les entr\u00e9es et sorties</li> <li>Les poids et biais</li> <li>Les fonctions d'activation</li> <li>Les couches cach\u00e9es</li> <li>R\u00e9digez un court paragraphe (5-7 lignes) expliquant avec vos propres mots :</li> <li>Comment un r\u00e9seau de neurones traite l'information</li> <li>Comment il peut apprendre \u00e0 partir d'exemples</li> </ol>"},{"location":"module1/concepts-fondamentaux/#points-cles-a-retenir","title":"Points cl\u00e9s \u00e0 retenir","text":"<p>\u00c0 travers cette exploration, vous devriez avoir d\u00e9couvert :</p> <ul> <li>Le r\u00f4le fondamental des poids et biais</li> <li>L'importance des fonctions d'activation pour introduire la non-lin\u00e9arit\u00e9</li> <li>Comment l'information se propage \u00e0 travers un r\u00e9seau (forward propagation)</li> <li>Les bases du processus d'apprentissage (ajustement des poids)</li> </ul>"},{"location":"module1/concepts-fondamentaux/#defi-de-generalisation-10-min","title":"D\u00e9fi de g\u00e9n\u00e9ralisation (10 min)","text":"<p>Pour approfondir votre compr\u00e9hension, r\u00e9alisez ce d\u00e9fi suppl\u00e9mentaire :</p> <ol> <li>Retournez aux notebooks de la premi\u00e8re partie (ML classique et Deep Learning)</li> <li>Localisez la section \"D\u00e9fi de g\u00e9n\u00e9ralisation\" dans chaque notebook</li> <li>Ex\u00e9cutez les cellules qui permettent de tester les mod\u00e8les sur :</li> <li>Des images avec du bruit ajout\u00e9</li> <li>Des images avec rotation l\u00e9g\u00e8re</li> <li>Notez les performances des deux approches sur ces donn\u00e9es modifi\u00e9es</li> <li>Analysez par vous-m\u00eame :</li> <li>Lequel des mod\u00e8les g\u00e9n\u00e9ralise le mieux aux nouvelles donn\u00e9es ?</li> <li>Pourquoi existe-t-il cette diff\u00e9rence ?</li> <li>Quels avantages et inconv\u00e9nients pr\u00e9sentent chaque approche ?</li> </ol>"},{"location":"module1/concepts-fondamentaux/#auto-evaluation","title":"Auto-\u00e9valuation","text":"<p>Pour v\u00e9rifier votre compr\u00e9hension, posez-vous ces questions :</p> <ol> <li>Pourriez-vous expliquer \u00e0 un camarade la diff\u00e9rence principale entre ML classique et Deep Learning ?</li> <li>Sauriez-vous d\u00e9crire le fonctionnement d'un neurone artificiel et son r\u00f4le dans un r\u00e9seau ?</li> <li>Comprenez-vous comment un r\u00e9seau de neurones \"apprend\" \u00e0 partir de donn\u00e9es ?</li> <li>Pouvez-vous identifier les situations o\u00f9 le Deep Learning serait pr\u00e9f\u00e9rable au ML classique, et vice versa ?</li> </ol>"},{"location":"module1/concepts-fondamentaux/#ressources-supplementaires-pour-approfondir-par-vous-meme","title":"Ressources suppl\u00e9mentaires pour approfondir par vous-m\u00eame","text":"<ul> <li>Visualisations interactives : Playground TensorFlow</li> <li>Neural Networks and Deep Learning - Un livre en ligne gratuit (en anglais)</li> <li>3Blue1Brown: Neural Networks - Une excellente s\u00e9rie de vid\u00e9os explicatives</li> </ul>"},{"location":"module1/concepts-fondamentaux/#conclusion","title":"Conclusion","text":"<p>Cette phase vous a permis de passer de l'observation pure \u00e0 une compr\u00e9hension plus approfondie des m\u00e9canismes internes du Deep Learning, tout en conservant une approche tr\u00e8s pratique et exp\u00e9rimentale. Les concepts d\u00e9couverts serviront de fondation pour la suite du parcours.</p> <p>Retour \u00e0 la S\u00e9ance 1  [Continuer vers le mini-projet(mini-projet.md)]{ .md-button .md-button--primary }</p>"},{"location":"module1/introduction-pratique/","title":"Introduction pratique","text":"<p>Voici la mise en forme corrig\u00e9e avec les retours \u00e0 la ligne appropri\u00e9s :</p>"},{"location":"module1/introduction-pratique/#introduction-pratique-au-deep-learning","title":"Introduction pratique au Deep Learning","text":""},{"location":"module1/introduction-pratique/#objectifs-de-cette-section","title":"Objectifs de cette section","text":"<p>Dans cette premi\u00e8re approche du Deep Learning, vous allez :</p> <ul> <li>D\u00e9couvrir des applications concr\u00e8tes et impressionnantes du Deep Learning</li> <li>Manipuler votre premier r\u00e9seau de neurones sans pr\u00e9requis th\u00e9oriques</li> <li>Exp\u00e9rimenter l'impact des modifications sur les performances d'un mod\u00e8le</li> <li>D\u00e9velopper une intuition sur le fonctionnement des r\u00e9seaux de neurones</li> </ul>"},{"location":"module1/introduction-pratique/#approche-pedagogique-dabord-la-pratique-ensuite-la-theorie","title":"Approche p\u00e9dagogique : d'abord la pratique, ensuite la th\u00e9orie","text":"<p>Contrairement \u00e0 l'approche traditionnelle qui commence par la th\u00e9orie, nous allons d'abord vous faire manipuler des mod\u00e8les de Deep Learning pour \u00e9veiller votre curiosit\u00e9 et vous donner une intuition pratique. Les concepts th\u00e9oriques seront introduits progressivement, en s'appuyant sur votre exp\u00e9rience directe.</p>"},{"location":"module1/introduction-pratique/#phase-1-applications-du-deep-learning-15-min","title":"Phase 1 : Applications du Deep Learning (15 min)","text":""},{"location":"module1/introduction-pratique/#demonstration-1-github-copilot","title":"D\u00e9monstration 1 : GitHub Copilot","text":"<p>GitHub Copilot est un assistant de programmation bas\u00e9 sur un mod\u00e8le de Deep Learning. Il analyse le contexte de votre code et sugg\u00e8re des compl\u00e9ments pertinents.</p> <p>Comment \u00e7a fonctionne :</p> <ul> <li>Entra\u00een\u00e9 sur des millions de d\u00e9p\u00f4ts GitHub publics</li> <li>Utilise un mod\u00e8le de langage bas\u00e9 sur des architectures avanc\u00e9es</li> <li>Analyse le contexte (code existant, commentaires, noms de fonctions)</li> <li>G\u00e9n\u00e8re des suggestions pertinentes en temps r\u00e9el</li> </ul> <p>Exemple pratique :</p> <ul> <li>\u00c9criture d'une fonction \u00e0 partir d'un simple commentaire</li> <li>Compl\u00e9tion de code automatique</li> <li>G\u00e9n\u00e9ration de tests unitaires</li> </ul>"},{"location":"module1/introduction-pratique/#demonstration-2-reconnaissance-dobjets-en-temps-reel","title":"D\u00e9monstration 2 : Reconnaissance d'objets en temps r\u00e9el","text":"<p>La reconnaissance d'objets est l'une des applications les plus visibles du Deep Learning. Nous utiliserons l'application Teachable Machine pour une d\u00e9monstration en direct.</p> <p>Points \u00e0 observer :</p> <ul> <li>D\u00e9tection en temps r\u00e9el d'objets pr\u00e9sents dans la salle</li> <li>Niveau de confiance (pourcentage) pour chaque pr\u00e9diction</li> <li>Robustesse face aux variations (angle, \u00e9clairage)</li> </ul> <p>Comment \u00e7a fonctionne :</p> <ul> <li>Utilise des r\u00e9seaux de neurones convolutifs (CNN)</li> <li>D\u00e9tecte des caract\u00e9ristiques visuelles \u00e0 diff\u00e9rents niveaux d'abstraction</li> <li>Identifie et localise les objets dans l'image</li> </ul>"},{"location":"module1/introduction-pratique/#demonstration-3-generation-de-texte","title":"D\u00e9monstration 3 : G\u00e9n\u00e9ration de texte","text":"<p>Les mod\u00e8les de langage comme GPT ou Mistral peuvent g\u00e9n\u00e9rer du texte coh\u00e9rent et contextuellement pertinent sur pratiquement n'importe quel sujet.</p> <p>Exp\u00e9rimentation :</p> <ul> <li>Essai de diff\u00e9rentes amorces (technique, cr\u00e9ative, formelle)</li> <li>Observation de l'adaptation au style et au contexte</li> <li>Analyse de la coh\u00e9rence des textes g\u00e9n\u00e9r\u00e9s</li> </ul> <p>Applications professionnelles :</p> <ul> <li>G\u00e9n\u00e9ration automatique de descriptions de produits</li> <li>Cr\u00e9ation d'assistants virtuels pour guider les utilisateurs</li> <li>Production de r\u00e9sum\u00e9s de documents techniques</li> <li>Suggestions de r\u00e9ponses dans une application de service client</li> </ul>"},{"location":"module1/introduction-pratique/#phase-2-premier-contact-avec-un-reseau-de-neurones-30-min","title":"Phase 2 : Premier contact avec un r\u00e9seau de neurones (30 min)","text":""},{"location":"module1/introduction-pratique/#instructions-detaillees","title":"Instructions d\u00e9taill\u00e9es","text":""},{"location":"module1/introduction-pratique/#1-creation-dun-notebook-dans-google-colab","title":"1. Cr\u00e9ation d'un notebook dans Google Colab","text":"<p>Google Colab est un environnement Jupyter Notebook h\u00e9berg\u00e9 qui permet d'ex\u00e9cuter du code Python dans le cloud, sans installation locale.</p> <ol> <li>Ouvrez Google Colab</li> <li>Connectez-vous avec votre compte Google</li> <li>Cliquez sur \"Fichier\" &gt; \"Nouveau notebook\"</li> </ol>"},{"location":"module1/introduction-pratique/#2-exploration-du-hello-world-du-deep-learning","title":"2. Exploration du \"Hello World du Deep Learning\"","text":"<p>Le \"Hello World\" du Deep Learning est la reconnaissance de chiffres manuscrits avec le dataset MNIST. Vous allez impl\u00e9menter un r\u00e9seau de neurones simple capable de reconna\u00eetre des chiffres \u00e9crits \u00e0 la main.</p> <p>Suivez ces \u00e9tapes :</p> <ul> <li>Copier-coller les cellules depuis le notebook de r\u00e9f\u00e9rence</li> <li>Ex\u00e9cuter chaque cellule en cliquant sur le bouton \u25b6\ufe0f ou avec Ctrl+Entr\u00e9e</li> <li>Observer et analyser les r\u00e9sultats \u00e0 chaque \u00e9tape</li> <li>Compl\u00e9ter la fiche d'observations</li> </ul>"},{"location":"module1/introduction-pratique/#3-structure-du-notebook","title":"3. Structure du notebook","text":"<p>Le notebook est organis\u00e9 en plusieurs sections pour vous guider :</p> <ol> <li>Introduction - Pr\u00e9sentation du probl\u00e8me de reconnaissance de chiffres</li> <li>Configuration - Importation des biblioth\u00e8ques n\u00e9cessaires</li> <li>Chargement des donn\u00e9es - Pr\u00e9paration du dataset MNIST</li> <li>Cr\u00e9ation du mod\u00e8le - Architecture du r\u00e9seau de neurones</li> <li>Entra\u00eenement - Processus d'apprentissage du mod\u00e8le</li> <li>Visualisation - Graphiques d'\u00e9volution de l'apprentissage</li> <li>Pr\u00e9dictions - Test du mod\u00e8le sur de nouvelles donn\u00e9es</li> <li>Dessin interactif - Interface pour tester avec vos propres dessins</li> <li>Exp\u00e9rimentation - Suggestions de modifications \u00e0 essayer</li> </ol>"},{"location":"module1/introduction-pratique/#4-experimentations-guidees","title":"4. Exp\u00e9rimentations guid\u00e9es","text":"<p>Apr\u00e8s avoir ex\u00e9cut\u00e9 le notebook de base, essayez ces modifications pour observer leur impact :</p> <ol> <li> <p>Modification de l'architecture :</p> <ul> <li>Augmenter/diminuer le nombre de neurones dans chaque couche</li> <li>Ajouter ou supprimer des couches dans le r\u00e9seau</li> <li>Essayer d'ajouter une couche Dropout (qui d\u00e9sactive al\u00e9atoirement certains neurones pendant l'entra\u00eenement)</li> </ul> </li> <li> <p>Ajustement des param\u00e8tres d'entra\u00eenement :</p> <ul> <li>Changer le nombre de cycles d'entra\u00eenement (\u00e9poques)</li> <li>Modifier le nombre d'exemples trait\u00e9s \u00e0 la fois (taille du batch)</li> <li>Tester diff\u00e9rentes m\u00e9thodes d'apprentissage (optimiseurs)</li> </ul> </li> <li> <p>Test avec vos propres dessins :</p> <ul> <li>Utiliser l'interface de dessin pour tester des chiffres manuscrits</li> <li>Observer comment le mod\u00e8le r\u00e9agit \u00e0 diff\u00e9rents styles d'\u00e9criture</li> <li>Analyser les pr\u00e9dictions erron\u00e9es et tenter de comprendre pourquoi</li> </ul> </li> </ol>"},{"location":"module1/introduction-pratique/#phase-3-reflexion-et-documentation-15-min","title":"Phase 3 : R\u00e9flexion et documentation (15 min)","text":"<p>Apr\u00e8s vos exp\u00e9rimentations, prenez le temps de r\u00e9fl\u00e9chir \u00e0 ce que vous avez observ\u00e9 :</p> <ol> <li> <p>Compl\u00e9tez la fiche d'observations :</p> <ul> <li>Notez les performances initiales du mod\u00e8le</li> <li>Documentez l'impact de vos modifications</li> <li>Analysez les cas o\u00f9 le mod\u00e8le \u00e9choue</li> </ul> </li> <li> <p>Questions de r\u00e9flexion :</p> <ul> <li>Qu'est-ce qui semble avoir le plus d'impact sur les performances ?</li> <li>Quelles sont les limites du mod\u00e8le que vous avez observ\u00e9es ?</li> <li>Quelles applications pratiques pourriez-vous envisager avec cette technologie ?</li> </ul> </li> <li> <p>Partage d'exp\u00e9rience :</p> <ul> <li>\u00c9changez avec vos camarades sur vos observations</li> <li>Comparez les r\u00e9sultats de vos diff\u00e9rentes modifications</li> <li>Discutez des surprises ou des difficult\u00e9s rencontr\u00e9es</li> </ul> </li> </ol>"},{"location":"module1/introduction-pratique/#conclusion-et-transition","title":"Conclusion et transition","text":"<p>Cette introduction pratique vous a permis de manipuler directement un r\u00e9seau de neurones sans vous pr\u00e9occuper imm\u00e9diatement des concepts th\u00e9oriques sous-jacents. Vous avez pu observer comment un mod\u00e8le apprend \u00e0 reconna\u00eetre des chiffres manuscrits et comment diverses modifications peuvent affecter ses performances.</p> <p>Dans la prochaine section, nous approfondirons les concepts fondamentaux du Deep Learning en nous appuyant sur votre exp\u00e9rience pratique. Nous comparerons \u00e9galement le Deep Learning avec les approches classiques du Machine Learning pour mieux comprendre ses particularit\u00e9s et ses avantages.</p>"},{"location":"module1/introduction-pratique/#ressources-complementaires","title":"Ressources compl\u00e9mentaires","text":"<ul> <li>Guide d'utilisation de Google Colab - Pour vous aider \u00e0 utiliser cet environnement</li> <li>Glossaire du Deep Learning - D\u00e9finitions des termes techniques rencontr\u00e9s</li> <li>TensorFlow Playground - Interface interactive pour exp\u00e9rimenter avec des r\u00e9seaux de neurones simples</li> </ul> <p>Retour au Module 1 Continuer vers les Concepts fondamentaux</p>"},{"location":"module1/mini-projet/","title":"Mini-projet individuel : Am\u00e9lioration d'un mod\u00e8le de Deep Learning","text":""},{"location":"module1/mini-projet/#objectifs","title":"Objectifs","text":"<p>Ce mini-projet individuel vous permettra de :</p> <ul> <li>Appliquer les connaissances acquises sur les r\u00e9seaux de neurones</li> <li>Exp\u00e9rimenter avec diff\u00e9rentes architectures et hyperparam\u00e8tres</li> <li>D\u00e9velopper une m\u00e9thode d'analyse des performances</li> <li>Documenter vos r\u00e9sultats de fa\u00e7on professionnelle</li> </ul>"},{"location":"module1/mini-projet/#contexte","title":"Contexte","text":"<p>Vous \u00eates stagiaire dans une entreprise qui souhaite impl\u00e9menter un syst\u00e8me de reconnaissance automatique de chiffres manuscrits pour traiter des formulaires papier. Un premier mod\u00e8le a \u00e9t\u00e9 d\u00e9velopp\u00e9, mais sa pr\u00e9cision est encore insuffisante pour une utilisation en production.</p> <p>Votre mission est d'am\u00e9liorer ce mod\u00e8le existant en explorant diff\u00e9rentes configurations et en justifiant vos choix techniques.</p>"},{"location":"module1/mini-projet/#instructions-detaillees","title":"Instructions d\u00e9taill\u00e9es","text":""},{"location":"module1/mini-projet/#etape-1-preparation-de-lenvironnement-5-min","title":"\u00c9tape 1 : Pr\u00e9paration de l'environnement (5 min)","text":"<ol> <li>Cr\u00e9ez un nouveau notebook dans Google Colab</li> <li> <p>Importez les biblioth\u00e8ques n\u00e9cessaires :    <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.utils import to_categorical\n</code></pre></p> </li> <li> <p>Chargez et pr\u00e9parez les donn\u00e9es MNIST :    <pre><code># Chargement des donn\u00e9es\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Normalisation et reshaping\nX_train = X_train.reshape(-1, 28, 28, 1) / 255.0\nX_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n\n# Conversion des labels en cat\u00e9gories\ny_train_cat = to_categorical(y_train, 10)\ny_test_cat = to_categorical(y_test, 10)\n</code></pre></p> </li> </ol>"},{"location":"module1/mini-projet/#etape-2-implementation-du-modele-de-reference-10-min","title":"\u00c9tape 2 : Impl\u00e9mentation du mod\u00e8le de r\u00e9f\u00e9rence (10 min)","text":"<p>Le mod\u00e8le de r\u00e9f\u00e9rence fourni par l'entreprise est un CNN simple :</p> <pre><code>def create_baseline_model():\n    model = Sequential([\n        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n        MaxPooling2D(pool_size=(2, 2)),\n        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dense(10, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n\n# Cr\u00e9er et entra\u00eener le mod\u00e8le de r\u00e9f\u00e9rence\nbaseline_model = create_baseline_model()\nbaseline_history = baseline_model.fit(\n    X_train, y_train_cat,\n    epochs=5,\n    batch_size=128,\n    validation_split=0.2,\n    verbose=1\n)\n\n# \u00c9valuer le mod\u00e8le de r\u00e9f\u00e9rence\nbaseline_score = baseline_model.evaluate(X_test, y_test_cat, verbose=0)\nprint(f\"Mod\u00e8le de r\u00e9f\u00e9rence - Pr\u00e9cision: {baseline_score[1]*100:.2f}%\")\n</code></pre>"},{"location":"module1/mini-projet/#etape-3-analyse-et-modifications-30-min","title":"\u00c9tape 3 : Analyse et modifications (30 min)","text":"<p>Votre t\u00e2che consiste \u00e0 am\u00e9liorer ce mod\u00e8le de r\u00e9f\u00e9rence. Exp\u00e9rimentez avec au moins trois des modifications suivantes :</p> <ol> <li>Modification de l'architecture</li> <li>Ajouter/enlever des couches de convolution</li> <li>Modifier le nombre de filtres</li> <li> <p>Changer la taille des noyaux de convolution</p> </li> <li> <p>Techniques de r\u00e9gularisation</p> </li> <li>Ajouter des couches de Dropout</li> <li>Utiliser de la r\u00e9gularisation L1/L2</li> <li> <p>Impl\u00e9menter du Batch Normalization</p> </li> <li> <p>Optimisation des hyperparam\u00e8tres</p> </li> <li>Tester diff\u00e9rents optimiseurs (SGD, RMSprop, Adam)</li> <li>Modifier le taux d'apprentissage</li> <li> <p>Varier la taille du batch</p> </li> <li> <p>Augmentation de donn\u00e9es</p> </li> <li>Rotation des images</li> <li>Zoom</li> <li> <p>D\u00e9calage</p> </li> <li> <p>Strat\u00e9gies d'entra\u00eenement</p> </li> <li>Modifier le nombre d'\u00e9poques</li> <li>Utiliser un learning rate scheduler</li> <li>Impl\u00e9menter early stopping</li> </ol> <p>Pour chaque modification, cr\u00e9ez une fonction qui retourne le mod\u00e8le modifi\u00e9 :</p> <pre><code>def create_improved_model_1():\n    # Votre impl\u00e9mentation avec la premi\u00e8re modification\n    model = Sequential([\n        # Votre architecture modifi\u00e9e\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n\n# Similairement pour les autres modifications\n</code></pre>"},{"location":"module1/mini-projet/#etape-4-evaluation-comparative-10-min","title":"\u00c9tape 4 : \u00c9valuation comparative (10 min)","text":"<p>Cr\u00e9ez une fonction d'\u00e9valuation pour comparer syst\u00e9matiquement vos mod\u00e8les :</p> <pre><code>def evaluate_model(model_creator, model_name, epochs=5):\n    model = model_creator()\n\n    # Mesurer le temps d'entra\u00eenement\n    import time\n    start_time = time.time()\n\n    # Entra\u00eener le mod\u00e8le\n    history = model.fit(\n        X_train, y_train_cat,\n        epochs=epochs,\n        batch_size=128,\n        validation_split=0.2,\n        verbose=1\n    )\n\n    training_time = time.time() - start_time\n\n    # \u00c9valuer sur l'ensemble de test\n    test_score = model.evaluate(X_test, y_test_cat, verbose=0)\n    test_accuracy = test_score[1]\n\n    # Visualiser l'\u00e9volution de l'apprentissage\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Train')\n    plt.plot(history.history['val_accuracy'], label='Validation')\n    plt.title(f'{model_name} - Pr\u00e9cision')\n    plt.xlabel('\u00c9poque')\n    plt.ylabel('Pr\u00e9cision')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Train')\n    plt.plot(history.history['val_loss'], label='Validation')\n    plt.title(f'{model_name} - Perte')\n    plt.xlabel('\u00c9poque')\n    plt.ylabel('Perte')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    # Retourner les m\u00e9triques principales\n    return {\n        'accuracy': test_accuracy,\n        'training_time': training_time,\n        'history': history.history\n    }\n\n# \u00c9valuer tous les mod\u00e8les\nbaseline_results = evaluate_model(create_baseline_model, \"Mod\u00e8le de r\u00e9f\u00e9rence\")\nmodel1_results = evaluate_model(create_improved_model_1, \"Mod\u00e8le am\u00e9lior\u00e9 1\")\n# ... \u00e9valuez vos autres mod\u00e8les\n</code></pre>"},{"location":"module1/mini-projet/#etape-5-analyse-des-erreurs-5-min","title":"\u00c9tape 5 : Analyse des erreurs (5 min)","text":"<p>Pour mieux comprendre les forces et faiblesses de vos mod\u00e8les, analysez les erreurs de classification :</p> <pre><code>def analyze_errors(model, name):\n    # Obtenir les pr\u00e9dictions\n    predictions = model.predict(X_test)\n    pred_classes = np.argmax(predictions, axis=1)\n    true_classes = np.argmax(y_test_cat, axis=1)\n\n    # Identifier les erreurs\n    misclassified = np.where(pred_classes != true_classes)[0]\n\n    if len(misclassified) &gt; 0:\n        # Afficher quelques exemples mal class\u00e9s\n        plt.figure(figsize=(12, 4))\n        for i, idx in enumerate(misclassified[:10]):\n            plt.subplot(2, 5, i+1)\n            plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n            plt.title(f\"R\u00e9el: {true_classes[idx]}\\nPr\u00e9dit: {pred_classes[idx]}\")\n            plt.axis('off')\n        plt.suptitle(f\"Erreurs de classification - {name}\")\n        plt.tight_layout()\n        plt.show()\n\n    # Calculer le taux d'erreur par chiffre\n    error_by_digit = {}\n    for digit in range(10):\n        digit_indices = np.where(true_classes == digit)[0]\n        if len(digit_indices) &gt; 0:\n            digit_errors = np.sum(pred_classes[digit_indices] != digit)\n            error_rate = digit_errors / len(digit_indices)\n            error_by_digit[digit] = error_rate\n\n    # Afficher un graphique des taux d'erreur par chiffre\n    plt.figure(figsize=(10, 5))\n    digits = list(error_by_digit.keys())\n    error_rates = [error_by_digit[d] for d in digits]\n    plt.bar(digits, error_rates)\n    plt.title(f\"Taux d'erreur par chiffre - {name}\")\n    plt.xlabel(\"Chiffre\")\n    plt.ylabel(\"Taux d'erreur\")\n    plt.xticks(range(10))\n    plt.ylim(0, max(error_rates) * 1.2)\n    plt.grid(axis='y', alpha=0.3)\n    plt.show()\n\n# Analyser les erreurs du meilleur mod\u00e8le\nanalyze_errors(best_model, \"Meilleur mod\u00e8le\")\n</code></pre>"},{"location":"module1/mini-projet/#etape-6-generalisations-et-robustesse-5-min","title":"\u00c9tape 6 : G\u00e9n\u00e9ralisations et robustesse (5 min)","text":"<p>Testez la robustesse de votre meilleur mod\u00e8le face \u00e0 des donn\u00e9es alt\u00e9r\u00e9es :</p> <pre><code>def test_robustness(model, name):\n    # Cr\u00e9er des versions bruit\u00e9es et rot\u00e9es des images de test\n    noisy_X_test = X_test + np.random.normal(0, 0.1, X_test.shape)\n    noisy_X_test = np.clip(noisy_X_test, 0, 1)  # S'assurer que les valeurs restent entre 0 et 1\n\n    from scipy.ndimage import rotate\n    rotated_X_test = np.zeros_like(X_test)\n    for i in range(len(X_test)):\n        angle = np.random.uniform(-15, 15)\n        rotated_X_test[i, :, :, 0] = rotate(X_test[i, :, :, 0], angle, reshape=False)\n\n    # \u00c9valuer sur les donn\u00e9es alt\u00e9r\u00e9es\n    normal_score = model.evaluate(X_test, y_test_cat, verbose=0)[1]\n    noisy_score = model.evaluate(noisy_X_test, y_test_cat, verbose=0)[1]\n    rotated_score = model.evaluate(rotated_X_test, y_test_cat, verbose=0)[1]\n\n    # Afficher les r\u00e9sultats\n    print(f\"Performances du mod\u00e8le {name}:\")\n    print(f\"- Donn\u00e9es normales: {normal_score*100:.2f}%\")\n    print(f\"- Donn\u00e9es bruit\u00e9es: {noisy_score*100:.2f}%\")\n    print(f\"- Donn\u00e9es rot\u00e9es: {rotated_score*100:.2f}%\")\n\n    # Visualiser quelques exemples\n    plt.figure(figsize=(15, 5))\n    for i in range(5):\n        # Image originale\n        plt.subplot(3, 5, i+1)\n        plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n        plt.axis('off')\n        if i == 2:\n            plt.title(\"Images originales\")\n\n        # Image bruit\u00e9e\n        plt.subplot(3, 5, i+6)\n        plt.imshow(noisy_X_test[i].reshape(28, 28), cmap='gray')\n        plt.axis('off')\n        if i == 2:\n            plt.title(\"Images bruit\u00e9es\")\n\n        # Image rot\u00e9e\n        plt.subplot(3, 5, i+11)\n        plt.imshow(rotated_X_test[i].reshape(28, 28), cmap='gray')\n        plt.axis('off')\n        if i == 2:\n            plt.title(\"Images rot\u00e9es\")\n\n    plt.tight_layout()\n    plt.show()\n\n# Tester la robustesse du meilleur mod\u00e8le\ntest_robustness(best_model, \"Meilleur mod\u00e8le\")\n</code></pre>"},{"location":"module1/mini-projet/#etape-7-comparaison-finale-et-documentation-5-min","title":"\u00c9tape 7 : Comparaison finale et documentation (5 min)","text":"<p>Cr\u00e9ez un tableau r\u00e9capitulatif de tous les mod\u00e8les test\u00e9s :</p> <pre><code>def compare_models(results_dict):\n    # Cr\u00e9er un tableau comparatif\n    models = list(results_dict.keys())\n    metrics = ['accuracy', 'training_time']\n\n    # Afficher le tableau\n    print(\"-\" * 60)\n    print(f\"{'Mod\u00e8le':&lt;25} {'Pr\u00e9cision':&lt;15} {'Temps (s)':&lt;15}\")\n    print(\"-\" * 60)\n\n    for model_name in models:\n        acc = results_dict[model_name]['accuracy'] * 100\n        time = results_dict[model_name]['training_time']\n        print(f\"{model_name:&lt;25} {acc:&lt;15.2f} {time:&lt;15.2f}\")\n\n    print(\"-\" * 60)\n\n    # Visualiser la comparaison\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    accuracies = [results_dict[m]['accuracy'] * 100 for m in models]\n    plt.bar(models, accuracies)\n    plt.title('Comparaison des pr\u00e9cisions')\n    plt.ylabel('Pr\u00e9cision (%)')\n    plt.xticks(rotation=45, ha='right')\n    plt.ylim(min(accuracies) * 0.95, 100)\n\n    plt.subplot(1, 2, 2)\n    times = [results_dict[m]['training_time'] for m in models]\n    plt.bar(models, times)\n    plt.title('Comparaison des temps d\\'entra\u00eenement')\n    plt.ylabel('Temps (secondes)')\n    plt.xticks(rotation=45, ha='right')\n\n    plt.tight_layout()\n    plt.show()\n\n# Comparer tous les mod\u00e8les\nall_results = {\n    \"Mod\u00e8le de r\u00e9f\u00e9rence\": baseline_results,\n    \"Mod\u00e8le am\u00e9lior\u00e9 1\": model1_results,\n    # Ajoutez vos autres mod\u00e8les\n}\n\ncompare_models(all_results)\n</code></pre>"},{"location":"module1/mini-projet/#livrable-attendu","title":"Livrable attendu","text":"<p>Vous devez soumettre un notebook complet incluant :</p> <ol> <li>Introduction : Pr\u00e9sentation du contexte et des objectifs</li> <li>Mod\u00e8le de r\u00e9f\u00e9rence : Impl\u00e9mentation et r\u00e9sultats du mod\u00e8le initial</li> <li>Am\u00e9liorations propos\u00e9es : Description d\u00e9taill\u00e9e de chaque modification avec justification</li> <li>R\u00e9sultats comparatifs : Tableau et graphiques comparant tous les mod\u00e8les test\u00e9s</li> <li>Analyse des erreurs : \u00c9tude des cas o\u00f9 votre meilleur mod\u00e8le \u00e9choue encore</li> <li>Test de robustesse : \u00c9valuation de la performance sur des donn\u00e9es alt\u00e9r\u00e9es</li> <li>Conclusion : Synth\u00e8se des r\u00e9sultats et recommandations pour l'entreprise</li> </ol>"},{"location":"module1/mini-projet/#criteres-devaluation","title":"Crit\u00e8res d'\u00e9valuation","text":"<p>Votre mini-projet sera \u00e9valu\u00e9 selon les crit\u00e8res suivants :</p> Crit\u00e8re Points Description Qualit\u00e9 du code 4 Code bien structur\u00e9, comment\u00e9 et fonctionnel Pertinence des modifications 6 Choix judicieux et justifi\u00e9 des modifications apport\u00e9es Am\u00e9lioration effective 4 Gain de performances par rapport au mod\u00e8le de r\u00e9f\u00e9rence Analyse critique 3 Capacit\u00e9 \u00e0 analyser les forces et faiblesses des mod\u00e8les Documentation 3 Clart\u00e9 et compl\u00e9tude du rapport"},{"location":"module1/mini-projet/#conseils-pour-reussir","title":"Conseils pour r\u00e9ussir","text":"<ul> <li>Commencez simple : N'essayez pas d'impl\u00e9menter toutes les modifications \u00e0 la fois</li> <li>Exp\u00e9rimentez m\u00e9thodiquement : Changez un param\u00e8tre \u00e0 la fois pour bien comprendre son impact</li> <li>Documentez vos observations : Notez les effets de chaque modification sur les performances</li> <li>Analysez les erreurs : Comprendre pourquoi le mod\u00e8le se trompe est aussi important que d'am\u00e9liorer son score</li> <li>Justifiez vos choix : Expliquez pourquoi vous avez opt\u00e9 pour certaines modifications plut\u00f4t que d'autres</li> </ul>"},{"location":"module1/mini-projet/#ressources-complementaires","title":"Ressources compl\u00e9mentaires","text":"<ul> <li>Guide Keras pour les CNN</li> <li>TensorFlow Data Augmentation Tutorial</li> <li>Conseils pour am\u00e9liorer les performances des mod\u00e8les</li> </ul> <p>Retour au Module 1 Continuer vers l'Auto-\u00e9valuation</p>"},{"location":"module1/qcm-evaluation-module1/","title":"QCM d'auto-\u00e9valuation - Module 1 : Fondamentaux du Deep Learning","text":"<p>Ce QCM vous permettra d'\u00e9valuer votre compr\u00e9hension des concepts fondamentaux du Deep Learning vus durant cette premi\u00e8re s\u00e9ance.</p>"},{"location":"module1/qcm-evaluation-module1/#instructions","title":"Instructions","text":"<ul> <li>Cochez la ou les r\u00e9ponses correctes pour chaque question</li> <li>Certaines questions peuvent avoir plusieurs r\u00e9ponses correctes</li> <li>\u00c0 la fin du questionnaire, calculez votre score gr\u00e2ce au corrig\u00e9 fourni</li> <li>Dur\u00e9e recommand\u00e9e : 15 minutes</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#questions","title":"Questions","text":""},{"location":"module1/qcm-evaluation-module1/#1-quest-ce-qui-differencie-principalement-le-deep-learning-du-machine-learning-classique","title":"1. Qu'est-ce qui diff\u00e9rencie principalement le Deep Learning du Machine Learning classique ?","text":"<ul> <li> Le Deep Learning est plus rapide \u00e0 entra\u00eener</li> <li> Le Deep Learning utilise toujours du mat\u00e9riel sp\u00e9cialis\u00e9 (GPU)</li> <li> Le Deep Learning extrait automatiquement les caract\u00e9ristiques pertinentes des donn\u00e9es</li> <li> Le Deep Learning fonctionne uniquement sur des images</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#2-quels-sont-les-composants-fondamentaux-dun-reseau-de-neurones-plusieurs-reponses-possibles","title":"2. Quels sont les composants fondamentaux d'un r\u00e9seau de neurones ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Neurones</li> <li> Poids</li> <li> Biais</li> <li> Algorithmes de tri</li> <li> Fonctions d'activation</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#3-quelle-est-la-principale-fonction-du-processus-de-forward-propagation","title":"3. Quelle est la principale fonction du processus de \"forward propagation\" ?","text":"<ul> <li> Ajuster les poids du r\u00e9seau</li> <li> Calculer l'erreur entre les pr\u00e9dictions et les valeurs r\u00e9elles</li> <li> Propager les entr\u00e9es \u00e0 travers le r\u00e9seau pour produire une sortie</li> <li> R\u00e9duire la taille du r\u00e9seau</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#4-quest-ce-quune-epoque-dentrainement","title":"4. Qu'est-ce qu'une \u00e9poque d'entra\u00eenement ?","text":"<ul> <li> Une m\u00e9thode d'initialisation des poids</li> <li> Un passage complet \u00e0 travers l'ensemble des donn\u00e9es d'entra\u00eenement</li> <li> Une fonction d'activation sp\u00e9cifique</li> <li> Le temps n\u00e9cessaire pour entra\u00eener un mod\u00e8le</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#5-parmi-ces-fonctions-dactivation-laquelle-nest-pas-couramment-utilisee-dans-les-reseaux-de-neurones","title":"5. Parmi ces fonctions d'activation, laquelle n'est pas couramment utilis\u00e9e dans les r\u00e9seaux de neurones ?","text":"<ul> <li> ReLU</li> <li> Sigmoid</li> <li> Tanh</li> <li> Logarithmic</li> <li> Softmax</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#6-quel-probleme-les-reseaux-cnn-convolutional-neural-networks-sont-ils-specifiquement-concus-pour-resoudre","title":"6. Quel probl\u00e8me les r\u00e9seaux CNN (Convolutional Neural Networks) sont-ils sp\u00e9cifiquement con\u00e7us pour r\u00e9soudre ?","text":"<ul> <li> Traitement du langage naturel</li> <li> Analyse d'images</li> <li> Pr\u00e9diction de valeurs boursi\u00e8res</li> <li> G\u00e9n\u00e9ration de musique</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#7-dans-le-contexte-du-deep-learning-que-signifie-le-terme-overfitting","title":"7. Dans le contexte du Deep Learning, que signifie le terme \"overfitting\" ?","text":"<ul> <li> Le mod\u00e8le est trop simple pour capturer les motifs dans les donn\u00e9es</li> <li> Le mod\u00e8le m\u00e9morise les donn\u00e9es d'entra\u00eenement au d\u00e9triment de la g\u00e9n\u00e9ralisation</li> <li> Le mod\u00e8le s'entra\u00eene trop rapidement</li> <li> Le mod\u00e8le utilise trop de ressources computationnelles</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#8-quest-ce-que-le-learning-rate-dans-un-reseau-de-neurones","title":"8. Qu'est-ce que le \"learning rate\" dans un r\u00e9seau de neurones ?","text":"<ul> <li> Le taux de neurones activ\u00e9s dans le r\u00e9seau</li> <li> La vitesse \u00e0 laquelle le r\u00e9seau traite les donn\u00e9es</li> <li> Le param\u00e8tre qui contr\u00f4le l'ampleur des ajustements des poids pendant l'entra\u00eenement</li> <li> Le pourcentage de donn\u00e9es utilis\u00e9es pour l'entra\u00eenement</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#9-quelles-sont-les-applications-pratiques-du-deep-learning-que-nous-avons-observees-plusieurs-reponses-possibles","title":"9. Quelles sont les applications pratiques du Deep Learning que nous avons observ\u00e9es ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Compl\u00e9tion de code (GitHub Copilot)</li> <li> Reconnaissance d'objets dans des images</li> <li> G\u00e9n\u00e9ration de texte</li> <li> Optimisation des performances mat\u00e9rielles</li> <li> Reconnaissance de chiffres manuscrits</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#10-lorsque-nous-avons-modifie-larchitecture-du-reseau-dans-le-hello-world-du-deep-learning-quavons-nous-observe-plusieurs-reponses-possibles","title":"10. Lorsque nous avons modifi\u00e9 l'architecture du r\u00e9seau dans le \"Hello World du Deep Learning\", qu'avons-nous observ\u00e9 ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Ajouter plus de neurones am\u00e9liore toujours les performances</li> <li> Trop de neurones peut causer du surapprentissage</li> <li> Le choix de la fonction d'activation influence les performances</li> <li> Le temps d'entra\u00eenement est ind\u00e9pendant du nombre de neurones</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#11-quelle-technique-avons-nous-utilisee-pour-comparer-la-generalisation-du-machine-learning-classique-et-du-deep-learning","title":"11. Quelle technique avons-nous utilis\u00e9e pour comparer la g\u00e9n\u00e9ralisation du Machine Learning classique et du Deep Learning ?","text":"<ul> <li> Mesure du temps d'inf\u00e9rence</li> <li> Test sur des donn\u00e9es intentionnellement bruit\u00e9es</li> <li> Visualisation des fronti\u00e8res de d\u00e9cision</li> <li> Comparaison des fonctions d'activation</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#12-dans-un-neurone-artificiel-quel-est-le-role-du-biais-bias","title":"12. Dans un neurone artificiel, quel est le r\u00f4le du biais (bias) ?","text":"<ul> <li> Il permet d'acc\u00e9l\u00e9rer l'apprentissage</li> <li> Il ajuste le seuil d'activation du neurone</li> <li> Il limite la valeur maximale de sortie</li> <li> Il connecte les neurones entre eux</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#13-lors-de-lamelioration-du-modele-cnn-dans-le-mini-projet-quelle-modification-a-generalement-eu-limpact-le-plus-positif","title":"13. Lors de l'am\u00e9lioration du mod\u00e8le CNN dans le mini-projet, quelle modification a g\u00e9n\u00e9ralement eu l'impact le plus positif ?","text":"<ul> <li> Augmentation du learning rate</li> <li> R\u00e9duction du nombre d'\u00e9poques</li> <li> Ajout de couches de convolution</li> <li> Suppression des couches de pooling</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#14-quest-ce-qui-caracterise-un-reseau-de-neurones-profond","title":"14. Qu'est-ce qui caract\u00e9rise un r\u00e9seau de neurones \"profond\" ?","text":"<ul> <li> Il utilise uniquement des GPUs pour l'entra\u00eenement</li> <li> Il poss\u00e8de plusieurs couches cach\u00e9es</li> <li> Il traite uniquement des images haute r\u00e9solution</li> <li> Il est entra\u00een\u00e9 sur d'\u00e9normes quantit\u00e9s de donn\u00e9es</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#15-quels-avantages-le-deep-learning-presente-t-il-par-rapport-au-machine-learning-classique-pour-la-reconnaissance-dimages-plusieurs-reponses-possibles","title":"15. Quels avantages le Deep Learning pr\u00e9sente-t-il par rapport au Machine Learning classique pour la reconnaissance d'images ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Il ne n\u00e9cessite pas d'extraction manuelle de caract\u00e9ristiques</li> <li> Il est toujours plus pr\u00e9cis quelles que soient les donn\u00e9es</li> <li> Il peut apprendre des repr\u00e9sentations hi\u00e9rarchiques des caract\u00e9ristiques</li> <li> Il n\u00e9cessite moins de donn\u00e9es d'entra\u00eenement</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#auto-evaluation","title":"Auto-\u00e9valuation","text":"<p>Une fois le QCM compl\u00e9t\u00e9, v\u00e9rifiez vos r\u00e9ponses avec le corrig\u00e9 ci-dessous.</p>"},{"location":"module1/qcm-evaluation-module1/#corrige","title":"Corrig\u00e9","text":"<ol> <li>c</li> <li>a, b, c, e</li> <li>c</li> <li>b</li> <li>d</li> <li>b</li> <li>b</li> <li>c</li> <li>a, b, c, e</li> <li>b, c</li> <li>b</li> <li>b</li> <li>c</li> <li>b</li> <li>a, c</li> </ol>"},{"location":"module1/qcm-evaluation-module1/#calcul-de-votre-score","title":"Calcul de votre score","text":"<p>Comptez 1 point par r\u00e9ponse correcte. Pour les questions \u00e0 choix multiples, comptez 1 point uniquement si toutes vos s\u00e9lections sont correctes.</p> <p>Total des points possibles : 15</p>"},{"location":"module1/qcm-evaluation-module1/#interpretation","title":"Interpr\u00e9tation","text":"<ul> <li>12-15 points : Excellente ma\u00eetrise des concepts fondamentaux du Deep Learning</li> <li>9-11 points : Bonne compr\u00e9hension, quelques points \u00e0 clarifier</li> <li>6-8 points : Compr\u00e9hension de base, r\u00e9vision n\u00e9cessaire de certains concepts</li> <li>0-5 points : R\u00e9vision approfondie recommand\u00e9e avant de poursuivre</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#pour-approfondir","title":"Pour approfondir","text":"<p>Si vous avez obtenu moins de 12 points, nous vous recommandons de revoir les concepts sur lesquels vous avez fait des erreurs. Consultez les ressources suivantes : - Le notebook \"Hello World du Deep Learning\" - La section \"Concepts fondamentaux\" du cours - Le glossaire des termes du Deep Learning</p>"},{"location":"module1/ressources/anatomie-reseau/","title":"Anatomie d'un r\u00e9seau de neurones","text":"<p>Ce document contient le code et les explications pour le notebook d'exploration interactive d'un r\u00e9seau de neurones. Vous pouvez copier-coller chaque section dans une cellule Google Colab.</p>"},{"location":"module1/ressources/anatomie-reseau/#cellule-1-markdown-introduction","title":"Cellule 1 (Markdown) - Introduction","text":"<pre><code># Anatomie d'un r\u00e9seau de neurones\n\n## Exploration interactive du fonctionnement interne d'un r\u00e9seau de neurones\n\nDans ce notebook, nous allons explorer de mani\u00e8re interactive le fonctionnement interne d'un r\u00e9seau de neurones. Vous pourrez manipuler directement les composants fondamentaux (neurones, poids, biais) et observer leur impact sur les pr\u00e9dictions.\n\n### Objectifs :\n- Comprendre le fonctionnement d'un neurone artificiel\n- Visualiser l'effet des poids et du biais sur les d\u00e9cisions\n- Explorer le flux d'information dans un r\u00e9seau multicouche\n- Observer l'\u00e9volution des poids pendant l'entra\u00eenement\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-2-code-configuration-initiale","title":"Cellule 2 (Code) - Configuration initiale","text":"<pre><code># Partie 1: Configuration initiale\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom google.colab import output\noutput.enable_custom_widget_manager()\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nfrom matplotlib.colors import LinearSegmentedColormap\n\nprint(\"Configuration termin\u00e9e!\")\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-3-markdown-exploration-dun-neurone-unique","title":"Cellule 3 (Markdown) - Exploration d'un neurone unique","text":"<pre><code>## Exploration d'un neurone unique\n\nDans cette partie, nous allons observer le fonctionnement d'un neurone artificiel, l'unit\u00e9 fondamentale des r\u00e9seaux de neurones.\n\nUn neurone artificiel effectue deux op\u00e9rations principales :\n1. Une **somme pond\u00e9r\u00e9e** des entr\u00e9es (z = w\u2081x\u2081 + w\u2082x\u2082 + ... + b)\n2. L'application d'une **fonction d'activation** qui introduit la non-lin\u00e9arit\u00e9 (a = f(z))\n\nUtilisez les contr\u00f4les interactifs ci-dessous pour observer comment un neurone traite l'information.\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-4-code-fonctions-du-neurone","title":"Cellule 4 (Code) - Fonctions du neurone","text":"<pre><code># Fonction pour calculer la sortie d'un neurone\ndef neuron_output(x1, x2, w1, w2, b, activation=\"relu\"):\n    # Calcul de la somme pond\u00e9r\u00e9e\n    z = x1 * w1 + x2 * w2 + b\n\n    # Application de la fonction d'activation\n    if activation == \"relu\":\n        a = max(0, z)\n    elif activation == \"sigmoid\":\n        a = 1 / (1 + np.exp(-z))\n    elif activation == \"tanh\":\n        a = np.tanh(z)\n    else:\n        a = z  # Lin\u00e9aire\n\n    return z, a\n\n# Fonction pour visualiser un neurone\ndef visualize_neuron(x1, x2, w1, w2, b, activation=\"relu\"):\n    # Calculer la sortie\n    z, a = neuron_output(x1, x2, w1, w2, b, activation)\n\n    # Cr\u00e9er la figure\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n    # 1. Repr\u00e9sentation du neurone\n    ax = axes[0]\n    ax.set_xlim(-0.5, 2.5)\n    ax.set_ylim(-0.5, 2.5)\n\n    # Dessiner le neurone\n    circle = plt.Circle((1, 1), 0.4, fill=True, color='lightblue', alpha=0.7)\n    ax.add_artist(circle)\n\n    # Dessiner les entr\u00e9es\n    ax.plot(0, 0.7, 'ro', markersize=10)\n    ax.plot(0, 1.3, 'ro', markersize=10)\n\n    # Dessiner la sortie\n    ax.plot(2, 1, 'go', markersize=10)\n\n    # Ajouter les connexions\n    ax.arrow(0, 0.7, 0.6, 0.1, head_width=0.1, head_length=0.1, fc='black', ec='black', linewidth=2)\n    ax.arrow(0, 1.3, 0.6, -0.1, head_width=0.1, head_length=0.1, fc='black', ec='black', linewidth=2)\n    ax.arrow(1.4, 1, 0.6, 0, head_width=0.1, head_length=0.1, fc='black', ec='black', linewidth=2)\n\n    # Ajouter les textes\n    ax.text(-0.1, 0.7, f\"x\u2081 = {x1:.2f}\", fontsize=12, ha='right')\n    ax.text(-0.1, 1.3, f\"x\u2082 = {x2:.2f}\", fontsize=12, ha='right')\n    ax.text(1, 1, f\"z = {z:.2f}\\na = {a:.2f}\", fontsize=12, ha='center')\n    ax.text(0.5, 0.95, f\"w\u2081 = {w1:.2f}\", fontsize=10, rotation=15)\n    ax.text(0.5, 1.15, f\"w\u2082 = {w2:.2f}\", fontsize=10, rotation=-15)\n    ax.text(2.1, 1, f\"Sortie = {a:.2f}\", fontsize=12, ha='left')\n    ax.text(1, 0.5, f\"Biais = {b:.2f}\", fontsize=10)\n\n    ax.set_title(\"Neurone artificiel\", fontsize=14)\n    ax.set_axis_off()\n\n    # 2. Repr\u00e9sentation de la fonction d'activation\n    ax = axes[1]\n    x = np.linspace(-5, 5, 100)\n\n    if activation == \"relu\":\n        y = np.maximum(0, x)\n        title = \"Fonction d'activation: ReLU\"\n    elif activation == \"sigmoid\":\n        y = 1 / (1 + np.exp(-x))\n        title = \"Fonction d'activation: Sigmoid\"\n    elif activation == \"tanh\":\n        y = np.tanh(x)\n        title = \"Fonction d'activation: Tanh\"\n    else:\n        y = x\n        title = \"Fonction d'activation: Lin\u00e9aire\"\n\n    ax.plot(x, y, 'b-', linewidth=2)\n    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\n    # Marquer le point correspondant \u00e0 z\n    ax.plot(z, a, 'ro', markersize=8)\n    ax.plot([z, z], [0, a], 'r--', alpha=0.5)\n    ax.plot([0, z], [a, a], 'r--', alpha=0.5)\n\n    ax.set_xlim(-5, 5)\n    ax.set_ylim(-1.5, 1.5)\n    ax.set_xlabel(\"z (somme pond\u00e9r\u00e9e)\")\n    ax.set_ylabel(\"a (activation)\")\n    ax.set_title(title, fontsize=14)\n    ax.grid(True, alpha=0.3)\n\n    # 3. Visualisation de la fronti\u00e8re de d\u00e9cision\n    ax = axes[2]\n\n    # Cr\u00e9er des points pour former une grille\n    grid_size = 20\n    x1_values = np.linspace(0, 1, grid_size)\n    x2_values = np.linspace(0, 1, grid_size)\n    x1_grid, x2_grid = np.meshgrid(x1_values, x2_values)\n\n    # Calculer la sortie pour chaque point de la grille\n    z_grid = x1_grid * w1 + x2_grid * w2 + b\n\n    if activation == \"relu\":\n        a_grid = np.maximum(0, z_grid)\n    elif activation == \"sigmoid\":\n        a_grid = 1 / (1 + np.exp(-z_grid))\n    elif activation == \"tanh\":\n        a_grid = np.tanh(z_grid)\n    else:\n        a_grid = z_grid\n\n    # Cr\u00e9er une carte de couleur\n    cmap = plt.get_cmap('coolwarm')\n\n    # Tracer la heatmap\n    im = ax.imshow(a_grid, origin='lower', extent=[0, 1, 0, 1], \n                   cmap=cmap, vmin=0, vmax=1)\n    plt.colorbar(im, ax=ax, label=\"Activation\")\n\n    # Ajouter le point actuel\n    ax.plot(x1, x2, 'ko', markersize=8)\n\n    # Tracer la fronti\u00e8re de d\u00e9cision (a = 0.5)\n    if activation in [\"sigmoid\", \"tanh\"]:\n        threshold = 0.5\n        CS = ax.contour(x1_grid, x2_grid, a_grid, levels=[threshold], \n                         colors='k', linestyles='--')\n        ax.clabel(CS, inline=True, fontsize=10, fmt={threshold: \"a = 0.5\"})\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xlabel(\"x\u2081\")\n    ax.set_ylabel(\"x\u2082\")\n    ax.set_title(\"Carte d'activation\", fontsize=14)\n\n    plt.tight_layout()\n    plt.show()\n\n    return a\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-5-code-interface-interactive-pour-un-neurone","title":"Cellule 5 (Code) - Interface interactive pour un neurone","text":"<pre><code># Cr\u00e9er des widgets interactifs pour le neurone\nw1_slider = widgets.FloatSlider(value=1.0, min=-3.0, max=3.0, step=0.1, description='Poids w\u2081:')\nw2_slider = widgets.FloatSlider(value=1.0, min=-3.0, max=3.0, step=0.1, description='Poids w\u2082:')\nb_slider = widgets.FloatSlider(value=0.0, min=-3.0, max=3.0, step=0.1, description='Biais:')\nx1_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='Entr\u00e9e x\u2081:')\nx2_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='Entr\u00e9e x\u2082:')\nactivation_dropdown = widgets.Dropdown(\n    options=['relu', 'sigmoid', 'tanh', 'linear'],\n    value='relu',\n    description='Activation:'\n)\n\n# Fonction pour mettre \u00e0 jour la visualisation\ndef update_neuron_visualization(w1, w2, b, x1, x2, activation):\n    clear_output(wait=True)\n    output = visualize_neuron(x1, x2, w1, w2, b, activation)\n    print(f\"Sortie du neurone: {output:.4f}\")\n\n    # Expliquer le calcul\n    z = x1 * w1 + x2 * w2 + b\n    print(f\"\\nCalcul d\u00e9taill\u00e9:\")\n    print(f\"z = (x\u2081 \u00d7 w\u2081) + (x\u2082 \u00d7 w\u2082) + b\")\n    print(f\"z = ({x1:.2f} \u00d7 {w1:.2f}) + ({x2:.2f} \u00d7 {w2:.2f}) + {b:.2f}\")\n    print(f\"z = {x1*w1:.2f} + {x2*w2:.2f} + {b:.2f}\")\n    print(f\"z = {z:.2f}\")\n\n    if activation == \"relu\":\n        print(f\"a = ReLU(z) = max(0, z) = max(0, {z:.2f}) = {max(0, z):.2f}\")\n    elif activation == \"sigmoid\":\n        sig_z = 1 / (1 + np.exp(-z))\n        print(f\"a = Sigmoid(z) = 1 / (1 + e^(-z)) = 1 / (1 + e^(-{z:.2f})) = {sig_z:.2f}\")\n    elif activation == \"tanh\":\n        tanh_z = np.tanh(z)\n        print(f\"a = tanh(z) = tanh({z:.2f}) = {tanh_z:.2f}\")\n    else:\n        print(f\"a = z = {z:.2f}\")  # Lin\u00e9aire\n\n# Interface interactive pour le neurone\nneuron_output = widgets.interactive_output(\n    update_neuron_visualization,\n    {'w1': w1_slider, 'w2': w2_slider, 'b': b_slider, \n     'x1': x1_slider, 'x2': x2_slider, 'activation': activation_dropdown}\n)\n\n# Afficher les widgets\nprint(\"Utilisez les contr\u00f4les ci-dessous pour modifier les propri\u00e9t\u00e9s du neurone:\")\ndisplay(widgets.VBox([\n    widgets.HBox([x1_slider, x2_slider]),\n    widgets.HBox([w1_slider, w2_slider]),\n    widgets.HBox([b_slider, activation_dropdown])\n]))\ndisplay(neuron_output)\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-6-markdown-de-lunique-au-reseau","title":"Cellule 6 (Markdown) - De l'unique au r\u00e9seau","text":"<pre><code>## De l'unique au r\u00e9seau\n\nMaintenant que nous avons explor\u00e9 un neurone unique, passons \u00e0 un r\u00e9seau simple. Un r\u00e9seau de neurones est compos\u00e9 de plusieurs neurones organis\u00e9s en couches, o\u00f9 l'information se propage de l'entr\u00e9e vers la sortie.\n\nLe r\u00e9seau ci-dessous contient :\n- Une couche d'entr\u00e9e (2 neurones)\n- Une couche cach\u00e9e (nombre ajustable de neurones)\n- Une couche de sortie (1 neurone)\n\nObservez comment l'information circule \u00e0 travers le r\u00e9seau et comment les diff\u00e9rents poids affectent les activations.\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-7-code-fonctions-du-reseau","title":"Cellule 7 (Code) - Fonctions du r\u00e9seau","text":"<pre><code># Fonction pour cr\u00e9er et visualiser un r\u00e9seau simple\ndef create_simple_network(hidden_units=3, activation='relu'):\n    # Cr\u00e9er un mod\u00e8le s\u00e9quentiel\n    model = Sequential([\n        Dense(hidden_units, activation=activation, input_shape=(2,)),\n        Dense(1, activation='sigmoid')\n    ])\n\n    # Compiler le mod\u00e8le (bien que nous ne l'entra\u00eenerons pas)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    return model\n\n# Fonction pour visualiser un r\u00e9seau simple\ndef visualize_network(inputs, weights1=None, biases1=None, weights2=None, biases2=None, hidden_units=3, activation='relu'):\n    # Cr\u00e9er le mod\u00e8le si non fourni\n    model = create_simple_network(hidden_units, activation)\n\n    # Si des poids sont fournis, les appliquer\n    if weights1 is not None and biases1 is not None and weights2 is not None and biases2 is not None:\n        model.layers[0].set_weights([weights1, biases1])\n        model.layers[1].set_weights([weights2, biases2])\n\n    # Convertir les entr\u00e9es pour pr\u00e9diction\n    x = np.array([inputs])\n\n    # Obtenir les activations interm\u00e9diaires\n    intermediate_layer_model = tf.keras.Model(inputs=model.input,\n                                             outputs=model.layers[0].output)\n    intermediate_activations = intermediate_layer_model.predict(x)[0]\n\n    # Obtenir les activations de sortie\n    output_activation = model.predict(x)[0][0]\n\n    # Extraire les poids et biais\n    weights1, biases1 = model.layers[0].get_weights()\n    weights2, biases2 = model.layers[1].get_weights()\n\n    # Cr\u00e9er la figure pour visualiser le r\u00e9seau\n    plt.figure(figsize=(12, 8))\n\n    # D\u00e9finir les positions des neurones\n    input_layer_y = np.array([0.2, 0.8])\n    hidden_layer_y = np.linspace(0.1, 0.9, hidden_units)\n    output_layer_y = np.array([0.5])\n\n    input_layer_x = 0.1\n    hidden_layer_x = 0.5\n    output_layer_x = 0.9\n\n    # Dessiner les neurones d'entr\u00e9e\n    for i, y in enumerate(input_layer_y):\n        plt.scatter(input_layer_x, y, s=200, c='blue', alpha=0.7)\n        plt.text(input_layer_x, y, f\"x{i+1}={inputs[i]:.2f}\", fontsize=12, ha='center', va='center', color='white')\n\n    # Dessiner les neurones cach\u00e9s\n    for i, y in enumerate(hidden_layer_y):\n        # Calculer la somme pond\u00e9r\u00e9e\n        z = np.dot(inputs, weights1[:, i]) + biases1[i]\n\n        # Appliquer l'activation\n        if activation == 'relu':\n            a = max(0, z)\n        elif activation == 'sigmoid':\n            a = 1 / (1 + np.exp(-z))\n        elif activation == 'tanh':\n            a = np.tanh(z)\n        else:\n            a = z\n\n        # Couleur bas\u00e9e sur l'activation\n        color = plt.cm.viridis(a)\n\n        plt.scatter(hidden_layer_x, y, s=200, c=[color], alpha=0.7)\n        plt.text(hidden_layer_x, y, f\"{a:.2f}\", fontsize=12, ha='center', va='center', color='white')\n\n    # Dessiner le neurone de sortie\n    plt.scatter(output_layer_x, output_layer_y, s=200, c='red', alpha=0.7)\n    plt.text(output_layer_x, output_layer_y, f\"{output_activation:.2f}\", fontsize=12, ha='center', va='center', color='white')\n\n    # Dessiner les connexions entre couches d'entr\u00e9e et cach\u00e9e\n    for i, y_in in enumerate(input_layer_y):\n        for j, y_hid in enumerate(hidden_layer_y):\n            # Couleur et \u00e9paisseur bas\u00e9es sur le poids\n            weight = weights1[i, j]\n            width = abs(weight) * 3\n            color = 'red' if weight &lt; 0 else 'green'\n            alpha = min(abs(weight), 1.0)\n\n            plt.plot([input_layer_x, hidden_layer_x], [y_in, y_hid], \n                    c=color, linewidth=width, alpha=alpha)\n\n    # Dessiner les connexions entre couche cach\u00e9e et sortie\n    for i, y_hid in enumerate(hidden_layer_y):\n        # Couleur et \u00e9paisseur bas\u00e9es sur le poids\n        weight = weights2[i, 0]\n        width = abs(weight) * 3\n        color = 'red' if weight &lt; 0 else 'green'\n        alpha = min(abs(weight), 1.0)\n\n        plt.plot([hidden_layer_x, output_layer_x], [y_hid, output_layer_y], \n                c=color, linewidth=width, alpha=alpha)\n\n    # \u00c9tiquettes\n    plt.text(input_layer_x, 0.03, \"Couche d'entr\u00e9e\", fontsize=14, ha='center')\n    plt.text(hidden_layer_x, 0.03, \"Couche cach\u00e9e\", fontsize=14, ha='center')\n    plt.text(output_layer_x, 0.03, \"Couche de sortie\", fontsize=14, ha='center')\n\n    # Enlever les axes\n    plt.axis('off')\n    plt.title(f\"R\u00e9seau de neurones - Activation cach\u00e9e: {activation}\", fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\n    # Afficher les calculs d\u00e9taill\u00e9s\n    print(\"\\nCalculs d\u00e9taill\u00e9s pour chaque neurone de la couche cach\u00e9e:\")\n    for i in range(hidden_units):\n        z = np.dot(inputs, weights1[:, i]) + biases1[i]\n        print(f\"\\nNeurone cach\u00e9 {i+1}:\")\n        print(f\"z = (x\u2081 \u00d7 w\u2081,{i+1}) + (x\u2082 \u00d7 w\u2082,{i+1}) + b{i+1}\")\n        print(f\"z = ({inputs[0]:.2f} \u00d7 {weights1[0, i]:.2f}) + ({inputs[1]:.2f} \u00d7 {weights1[1, i]:.2f}) + {biases1[i]:.2f}\")\n        print(f\"z = {inputs[0] * weights1[0, i]:.2f} + {inputs[1] * weights1[1, i]:.2f} + {biases1[i]:.2f} = {z:.2f}\")\n\n        if activation == 'relu':\n            a = max(0, z)\n            print(f\"a = ReLU(z) = max(0, {z:.2f}) = {a:.2f}\")\n        elif activation == 'sigmoid':\n            a = 1 / (1 + np.exp(-z))\n            print(f\"a = Sigmoid(z) = 1 / (1 + e^(-{z:.2f})) = {a:.2f}\")\n        elif activation == 'tanh':\n            a = np.tanh(z)\n            print(f\"a = tanh(z) = tanh({z:.2f}) = {a:.2f}\")\n        else:\n            a = z\n            print(f\"a = z = {z:.2f}\")\n\n    print(\"\\nCalcul pour le neurone de sortie:\")\n    z_out = np.dot(intermediate_activations, weights2[:, 0]) + biases2[0]\n    print(f\"z = \u03a3(a_cach\u00e9 \u00d7 w_sortie) + b_sortie = {z_out:.2f}\")\n    print(f\"sortie = Sigmoid(z) = 1 / (1 + e^(-{z_out:.2f})) = {output_activation:.2f}\")\n\n    return model, weights1, biases1, weights2, biases2\n\n# Fonction pour g\u00e9n\u00e9rer des poids al\u00e9atoires\ndef generate_random_weights(hidden_units=3):\n    # G\u00e9n\u00e9rer des poids al\u00e9atoires pour la premi\u00e8re couche\n    weights1 = np.random.normal(0, 1, (2, hidden_units))\n    biases1 = np.random.normal(0, 1, hidden_units)\n\n    # G\u00e9n\u00e9rer des poids al\u00e9atoires pour la couche de sortie\n    weights2 = np.random.normal(0, 1, (hidden_units, 1))\n    biases2 = np.random.normal(0, 1, 1)\n\n    return weights1, biases1, weights2, biases2\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-8-code-interface-interactive-pour-le-reseau","title":"Cellule 8 (Code) - Interface interactive pour le r\u00e9seau","text":"<pre><code># Cr\u00e9er des widgets interactifs pour le r\u00e9seau\nx1_net_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='Entr\u00e9e x\u2081:')\nx2_net_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='Entr\u00e9e x\u2082:')\nhidden_units_slider = widgets.IntSlider(value=3, min=1, max=5, description='Neurones cach\u00e9s:')\nactivation_net_dropdown = widgets.Dropdown(\n    options=['relu', 'sigmoid', 'tanh', 'linear'],\n    value='relu',\n    description='Activation:'\n)\nrandom_button = widgets.Button(description=\"Poids al\u00e9atoires\")\n\n# Variables pour stocker les poids courants\ncurrent_weights1, current_biases1, current_weights2, current_biases2 = generate_random_weights()\n\n# Fonction pour visualiser le r\u00e9seau\ndef update_network_visualization(x1, x2, hidden_units, activation):\n    global current_weights1, current_biases1, current_weights2, current_biases2\n\n    # Ajuster les dimensions des poids si n\u00e9cessaire\n    if current_weights1.shape[1] != hidden_units:\n        current_weights1, current_biases1, current_weights2, current_biases2 = generate_random_weights(hidden_units)\n\n    # Visualiser le r\u00e9seau\n    inputs = np.array([x1, x2])\n    _, w1, b1, w2, b2 = visualize_network(\n        inputs, current_weights1, current_biases1, current_weights2, current_biases2, \n        hidden_units, activation\n    )\n\n    # Mettre \u00e0 jour les poids courants\n    current_weights1, current_biases1 = w1, b1\n    current_weights2, current_biases2 = w2, b2\n\n# Fonction pour g\u00e9n\u00e9rer de nouveaux poids al\u00e9atoires\ndef regenerate_weights(b):\n    global current_weights1, current_biases1, current_weights2, current_biases2\n    current_weights1, current_biases1, current_weights2, current_biases2 = generate_random_weights(\n        hidden_units_slider.value\n    )\n    # Mettre \u00e0 jour la visualisation\n    update_network_visualization(\n        x1_net_slider.value, x2_net_slider.value,\n        hidden_units_slider.value, activation_net_dropdown.value\n    )\n\n# Associer la fonction au bouton\nrandom_button.on_click(regenerate_weights)\n\n# Interface interactive pour le r\u00e9seau\nnetwork_output = widgets.interactive_output(\n    update_network_visualization,\n    {'x1': x1_net_slider, 'x2': x2_net_slider, \n     'hidden_units': hidden_units_slider, 'activation': activation_net_dropdown}\n)\n\n# Afficher les widgets pour le r\u00e9seau\nprint(\"\\nExplorez le comportement d'un r\u00e9seau simple:\")\ndisplay(widgets.VBox([\n    widgets.HBox([x1_net_slider, x2_net_slider]),\n    widgets.HBox([hidden_units_slider, activation_net_dropdown]),\n    random_button\n]))\ndisplay(network_output)\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-9-markdown-visualisation-de-lentrainement","title":"Cellule 9 (Markdown) - Visualisation de l'entra\u00eenement","text":"<pre><code>## Visualisation de l'entra\u00eenement\n\nDans cette derni\u00e8re partie, nous allons observer l'\u00e9volution des poids pendant l'entra\u00eenement d'un r\u00e9seau de neurones sur un probl\u00e8me classique : le probl\u00e8me XOR.\n\nLe probl\u00e8me XOR (OU exclusif) consiste \u00e0 pr\u00e9dire la sortie de la fonction logique XOR :\n- (0,0) \u2192 0\n- (0,1) \u2192 1\n- (1,0) \u2192 1\n- (1,1) \u2192 0\n\nCe probl\u00e8me n'est pas lin\u00e9airement s\u00e9parable, ce qui signifie qu'il ne peut pas \u00eatre r\u00e9solu par un seul neurone.\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-10-code-generation-de-donnees-xor","title":"Cellule 10 (Code) - G\u00e9n\u00e9ration de donn\u00e9es XOR","text":"<pre><code># G\u00e9n\u00e9rer des donn\u00e9es XOR\ndef generate_xor_data(n_samples=100):\n    X = np.random.rand(n_samples, 2)\n    y = np.logical_xor(X[:, 0] &gt; 0.5, X[:, 1] &gt; 0.5).astype(np.float32)\n    return X, y\n\n# Afficher quelques exemples de donn\u00e9es XOR\nX_sample, y_sample = generate_xor_data(20)\nplt.figure(figsize=(6, 6))\nplt.scatter(X_sample[:, 0], X_sample[:, 1], c=y_sample, cmap='coolwarm', s=100)\nplt.xlabel('x\u2081')\nplt.ylabel('x\u2082')\nplt.title('Probl\u00e8me XOR')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"Exemples de donn\u00e9es XOR:\")\nfor i in range(5):\n    x1, x2 = X_sample[i]\n    y = y_sample[i]\n    print(f\"x1={x1:.2f}, x2={x2:.2f} \u2192 y={y:.0f}\")\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-11-code-creation-et-entrainement-du-modele-xor","title":"Cellule 11 (Code) - Cr\u00e9ation et entra\u00eenement du mod\u00e8le XOR","text":"<pre><code># Cr\u00e9er un mod\u00e8le pour r\u00e9soudre XOR\nlearning_rate = 0.1\nhidden_units = 4\nepochs = 20\n\n# G\u00e9n\u00e9rer des donn\u00e9es\nX_train, y_train = generate_xor_data(200)\n\n# Cr\u00e9er un mod\u00e8le\nmodel = Sequential([\n    Dense(hidden_units, activation='relu', input_shape=(2,)),\n    Dense(1, activation='sigmoid')\n])\n\n# Compiler avec un optimiseur personnalis\u00e9\noptimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Entra\u00eener le mod\u00e8le\nhistory = model.fit(\n    X_train, y_train,\n    epochs=epochs,\n    batch_size=32,\n    verbose=1\n)\n\n# Afficher les r\u00e9sultats d'entra\u00eenement\nplt.figure(figsize=(12, 5))\n\n# Graphique de pr\u00e9cision\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], '-o')\nplt.title('Pr\u00e9cision pendant l\\'entra\u00eenement')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Pr\u00e9cision')\nplt.grid(True, alpha=0.3)\n\n# Graphique de perte\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], '-o')\nplt.title('Perte pendant l\\'entra\u00eenement')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Perte')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-12-code-visualisation-de-la-frontiere-de-decision","title":"Cellule 12 (Code) - Visualisation de la fronti\u00e8re de d\u00e9cision","text":"<p>```python</p>"},{"location":"module1/ressources/anatomie-reseau/#visualiser-la-frontiere-de-decision-finale","title":"Visualiser la fronti\u00e8re de d\u00e9cision finale","text":"<p>h = 0.01 x_min, x_max = 0, 1 y_min, y_max = 0, 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) grid_points = np.c_[xx.ravel</p>"},{"location":"module1/ressources/deep-learning/","title":"Machine Learning Classique - Classification MNIST avec Random Forest","text":"<p>Ce document contient le code et les explications pour le notebook de classification d'images MNIST avec Random Forest (approche Machine Learning classique). Vous pouvez copier-coller chaque section dans une cellule Google Colab.</p>"},{"location":"module1/ressources/deep-learning/#cellule-1-markdown-introduction","title":"Cellule 1 (Markdown) - Introduction","text":"<pre><code># Classification avec Machine Learning classique\n\n## Reconnaissance de chiffres manuscrits avec Random Forest\n\nDans ce notebook, nous allons impl\u00e9menter une approche de Machine Learning classique pour la classification des chiffres manuscrits en utilisant le dataset MNIST. Nous utiliserons l'algorithme Random Forest, qui est bas\u00e9 sur un ensemble d'arbres de d\u00e9cision.\n\n### Objectifs :\n- Comprendre comment pr\u00e9parer des donn\u00e9es d'images pour le ML classique\n- Impl\u00e9menter un classificateur Random Forest\n- \u00c9valuer ses performances et ses limites\n- Comparer cette approche avec le Deep Learning\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-2-code-importation-des-bibliotheques","title":"Cellule 2 (Code) - Importation des biblioth\u00e8ques","text":"<pre><code># Importation des biblioth\u00e8ques n\u00e9cessaires\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom google.colab import output\noutput.enable_custom_widget_manager()\nimport ipywidgets as widgets\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-3-markdown-chargement-des-donnees","title":"Cellule 3 (Markdown) - Chargement des donn\u00e9es","text":"<pre><code>## Chargement et exploration des donn\u00e9es\n\nLe dataset MNIST contient 70 000 images de chiffres manuscrits (0-9) en niveaux de gris. Chaque image est de taille 28x28 pixels, ce qui donne 784 pixels par image.\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-4-code-chargement-des-donnees-mnist","title":"Cellule 4 (Code) - Chargement des donn\u00e9es MNIST","text":"<pre><code>print(\"Chargement du jeu de donn\u00e9es MNIST...\")\n# Utilisation du jeu de donn\u00e9es MNIST int\u00e9gr\u00e9 \u00e0 sklearn\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\nX, y = mnist[\"data\"], mnist[\"target\"]\nX = X / 255.0  # Normalisation des valeurs de pixels entre 0 et 1\ny = y.astype(np.uint8)  # Conversion des labels en entiers\n\n# Exploration des donn\u00e9es\nprint(f\"Dimensions du jeu de donn\u00e9es: {X.shape}\")\nprint(f\"Nombre de classes: {len(np.unique(y))}\")\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-5-code-visualisation-des-exemples","title":"Cellule 5 (Code) - Visualisation des exemples","text":"<pre><code># Affichage de quelques exemples\nplt.figure(figsize=(10, 5))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.imshow(X[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"Label: {y[i]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.suptitle(\"Exemples de chiffres manuscrits\", y=1.05)\nplt.show()\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-6-markdown-preparation-des-donnees","title":"Cellule 6 (Markdown) - Pr\u00e9paration des donn\u00e9es","text":"<pre><code>## Pr\u00e9paration des donn\u00e9es pour Machine Learning classique\n\nContrairement aux r\u00e9seaux de neurones convolutifs (CNN), les algorithmes de ML classiques comme Random Forest ne sont pas con\u00e7us pour traiter directement des images. Nous devons donc :\n\n1. R\u00e9duire la dimensionnalit\u00e9 des donn\u00e9es (784 caract\u00e9ristiques est trop \u00e9lev\u00e9)\n2. Extraire des caract\u00e9ristiques pertinentes\n\nNous utiliserons l'Analyse en Composantes Principales (PCA) pour r\u00e9duire la dimensionnalit\u00e9 tout en conservant l'essentiel de l'information.\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-7-code-preparation-des-donnees","title":"Cellule 7 (Code) - Pr\u00e9paration des donn\u00e9es","text":"<pre><code>print(\"\\n--- Pr\u00e9paration des donn\u00e9es pour Random Forest ---\")\nprint(\"Pour le Machine Learning classique, nous devons souvent extraire des caract\u00e9ristiques manuellement.\")\n\n# R\u00e9duction de dimension avec PCA pour acc\u00e9l\u00e9rer l'entra\u00eenement\nprint(\"Application d'une r\u00e9duction de dimension (PCA)...\")\nn_components = 50  # R\u00e9duire de 784 \u00e0 50 caract\u00e9ristiques\n\n# S\u00e9paration en ensembles d'entra\u00eenement et de test\n# Utilisation d'un \u00e9chantillon r\u00e9duit pour acc\u00e9l\u00e9rer la d\u00e9monstration\nX_sample = X[:10000]\ny_sample = y[:10000]\n\nX_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n\nprint(f\"Taille de l'ensemble d'entra\u00eenement: {X_train.shape}\")\nprint(f\"Taille de l'ensemble de test: {X_test.shape}\")\n\n# Cr\u00e9ation d'un pipeline d'extraction de caract\u00e9ristiques\nfeature_pipeline = Pipeline([\n    ('scaler', StandardScaler()),  # Normalisation des donn\u00e9es\n    ('pca', PCA(n_components=n_components))  # R\u00e9duction de dimension par PCA\n])\n\n# Application aux donn\u00e9es\nprint(\"Extraction de caract\u00e9ristiques...\")\nX_train_features = feature_pipeline.fit_transform(X_train)\nX_test_features = feature_pipeline.transform(X_test)\n\nprint(f\"Dimensions apr\u00e8s extraction de caract\u00e9ristiques: {X_train_features.shape}\")\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-8-markdown-entrainement-du-modele","title":"Cellule 8 (Markdown) - Entra\u00eenement du mod\u00e8le","text":"<pre><code>## Entra\u00eenement du mod\u00e8le Random Forest\n\nNous allons maintenant entra\u00eener un classificateur Random Forest sur nos donn\u00e9es pr\u00e9trait\u00e9es. Random Forest est un algorithme d'ensemble qui combine les pr\u00e9dictions de plusieurs arbres de d\u00e9cision pour am\u00e9liorer la pr\u00e9cision et contr\u00f4ler le sur-apprentissage.\n\nPrincipaux hyperparam\u00e8tres :\n- **n_estimators** : Nombre d'arbres dans la for\u00eat\n- **max_depth** : Profondeur maximale de chaque arbre\n- **min_samples_split** : Nombre minimum d'\u00e9chantillons requis pour diviser un n\u0153ud\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-9-code-entrainement-du-modele","title":"Cellule 9 (Code) - Entra\u00eenement du mod\u00e8le","text":"<pre><code>print(\"\\n--- Entra\u00eenement du mod\u00e8le Random Forest ---\")\n\n# Param\u00e8tres du mod\u00e8le - vous pouvez les modifier\nn_estimators = 100  # Nombre d'arbres\nmax_depth = 10      # Profondeur maximale des arbres\nmin_samples_split = 2  # Nombre minimum d'\u00e9chantillons requis pour diviser un n\u0153ud\n\n# Cr\u00e9ation du mod\u00e8le\nrf_model = RandomForestClassifier(\n    n_estimators=n_estimators,\n    max_depth=max_depth,\n    min_samples_split=min_samples_split,\n    random_state=42,\n    n_jobs=-1  # Utiliser tous les c\u0153urs disponibles\n)\n\n# Mesure du temps d'entra\u00eenement\nstart_time = time.time()\nprint(\"Entra\u00eenement du mod\u00e8le en cours...\")\nrf_model.fit(X_train_features, y_train)\nend_time = time.time()\ntraining_time = end_time - start_time\n\nprint(f\"Temps d'entra\u00eenement: {training_time:.2f} secondes\")\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-10-markdown-evaluation-du-modele","title":"Cellule 10 (Markdown) - \u00c9valuation du mod\u00e8le","text":"<pre><code>## \u00c9valuation du mod\u00e8le\n\n\u00c9valuons maintenant les performances de notre mod\u00e8le Random Forest sur l'ensemble de test. Nous utiliserons plusieurs m\u00e9triques :\n- Pr\u00e9cision globale (accuracy)\n- Matrice de confusion\n- Rapport de classification d\u00e9taill\u00e9 (pr\u00e9cision, rappel, F1-score pour chaque classe)\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-11-code-evaluation-et-metriques","title":"Cellule 11 (Code) - \u00c9valuation et m\u00e9triques","text":"<pre><code>print(\"\\n--- \u00c9valuation du mod\u00e8le Random Forest ---\")\n\n# Pr\u00e9dictions sur l'ensemble de test\ny_pred = rf_model.predict(X_test_features)\n\n# Calcul des m\u00e9triques\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(f\"Pr\u00e9cision globale: {accuracy*100:.2f}%\")\nprint(\"\\nMatrice de confusion:\")\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Pr\u00e9dictions')\nplt.ylabel('Valeurs r\u00e9elles')\nplt.title('Matrice de confusion')\nplt.show()\n\nprint(\"\\nRapport de classification:\")\nprint(class_report)\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-12-markdown-analyse-des-erreurs","title":"Cellule 12 (Markdown) - Analyse des erreurs","text":"<pre><code>## Analyse des erreurs\n\nExaminons quelques exemples que notre mod\u00e8le a mal classifi\u00e9s pour mieux comprendre ses limites.\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-13-code-visualisation-des-erreurs","title":"Cellule 13 (Code) - Visualisation des erreurs","text":"<pre><code>print(\"\\n--- Analyse des erreurs ---\")\n\n# Identifier les erreurs\nerror_indices = np.where(y_pred != y_test)[0]\nn_errors = min(10, len(error_indices))  # Afficher max 10 erreurs\n\nif n_errors &gt; 0:\n    plt.figure(figsize=(12, 4))\n    for i, idx in enumerate(error_indices[:n_errors]):\n        plt.subplot(2, 5, i + 1)\n        # R\u00e9cup\u00e9rer l'image originale\n        img = X_test[idx].reshape(28, 28)\n        plt.imshow(img, cmap='gray')\n        plt.title(f\"R\u00e9el: {y_test[idx]}\\nPr\u00e9dit: {y_pred[idx]}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.suptitle(\"Exemples d'erreurs de classification\", y=1.05)\n    plt.show()\nelse:\n    print(\"Aucune erreur trouv\u00e9e dans l'\u00e9chantillon de test!\")\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-14-markdown-importance-des-caracteristiques","title":"Cellule 14 (Markdown) - Importance des caract\u00e9ristiques","text":"<pre><code>## Importance des caract\u00e9ristiques\n\nUn avantage des mod\u00e8les comme Random Forest est leur interpr\u00e9tabilit\u00e9. Nous pouvons examiner quelles caract\u00e9ristiques (ici, quelles composantes principales) le mod\u00e8le consid\u00e8re comme les plus importantes pour faire ses pr\u00e9dictions.\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-15-code-visualisation-de-limportance-des-caracteristiques","title":"Cellule 15 (Code) - Visualisation de l'importance des caract\u00e9ristiques","text":"<pre><code>print(\"\\n--- Importance des caract\u00e9ristiques ---\")\n# Visualiser l'importance des composantes principales\nfeature_importance = rf_model.feature_importances_\nsorted_idx = np.argsort(feature_importance)[::-1]\n\nplt.figure(figsize=(10, 5))\nplt.bar(range(20), feature_importance[sorted_idx[:20]])\nplt.xticks(range(20), [f\"Feat {i}\" for i in sorted_idx[:20]], rotation=90)\nplt.xlabel('Composantes principales')\nplt.ylabel('Importance')\nplt.title('Top 20 des composantes principales les plus importantes')\nplt.tight_layout()\nplt.show()\n\nprint(\"Les caract\u00e9ristiques les plus importantes sont les composantes principales qui capturent le plus de variance dans les donn\u00e9es.\")\n</code></pre>"},{"location":"module1/ressources/fiche-observations/","title":"\ud83d\udccb Fiche d'observations - Hello World du Deep Learning","text":"<p>Cette fiche d'observations vous accompagne \u00e9tape par \u00e9tape dans l'exploration du notebook. Pour chaque section, notez les r\u00e9f\u00e9rences aux cellules correspondantes du notebook.</p>"},{"location":"module1/ressources/fiche-observations/#informations-generales","title":"Informations g\u00e9n\u00e9rales","text":"<p>Nom et pr\u00e9nom : ____</p> <p>Date : ____</p> <p>Groupe : ____</p>"},{"location":"module1/ressources/fiche-observations/#partie-1-configuration-et-verification-de-lenvironnement-cellule-2","title":"Partie 1 : Configuration et v\u00e9rification de l'environnement (Cellule 2)","text":"Question Observation Version de TensorFlow d\u00e9tect\u00e9e GPU disponible ? (Oui/Non) Quelle est l'importance d'avoir un GPU pour le Deep Learning ?"},{"location":"module1/ressources/fiche-observations/#partie-2-chargement-et-preparation-des-donnees-cellule-3","title":"Partie 2 : Chargement et pr\u00e9paration des donn\u00e9es (Cellule 3)","text":"Question Observation Combien d'exemples d'entra\u00eenement sont disponibles ? Combien d'exemples de test sont disponibles ? Quelle est la dimension des images ? Pourquoi normalise-t-on les valeurs des pixels entre 0 et 1 ? D'apr\u00e8s les exemples affich\u00e9s, quelles difficult\u00e9s pourrait rencontrer le mod\u00e8le ?"},{"location":"module1/ressources/fiche-observations/#partie-3-architecture-du-modele-cellule-4","title":"Partie 3 : Architecture du mod\u00e8le (Cellule 4)","text":"<p>Dessinez le sch\u00e9ma simplifi\u00e9 de l'architecture du r\u00e9seau de neurones utilis\u00e9 :</p> <pre><code>[Sch\u00e9ma \u00e0 compl\u00e9ter]\n</code></pre> Question Observation Combien de couches comporte le mod\u00e8le ? Combien de param\u00e8tres entra\u00eenables au total ? Quel est le r\u00f4le des couches de convolution ? Quel est le r\u00f4le des couches de pooling ? Pourquoi utilise-t-on 'softmax' comme activation de la derni\u00e8re couche ?"},{"location":"module1/ressources/fiche-observations/#partie-4-entrainement-du-modele-cellule-5","title":"Partie 4 : Entra\u00eenement du mod\u00e8le (Cellule 5)","text":"Question Observation Combien d'\u00e9poques ont \u00e9t\u00e9 effectu\u00e9es ? Quelle est la pr\u00e9cision finale sur les donn\u00e9es d'entra\u00eenement ? Quelle est la pr\u00e9cision finale sur les donn\u00e9es de validation ? Quelle est la pr\u00e9cision sur l'ensemble de test ? Y a-t-il un signe de surapprentissage (overfitting) ? Pourquoi ?"},{"location":"module1/ressources/fiche-observations/#partie-5-visualisation-des-resultats-cellule-6","title":"Partie 5 : Visualisation des r\u00e9sultats (Cellule 6)","text":"<p>Analysez les graphiques d'apprentissage :</p> Question Observation La courbe de pr\u00e9cision d'entra\u00eenement est-elle croissante ? La courbe de perte d'entra\u00eenement est-elle d\u00e9croissante ? Y a-t-il un \u00e9cart important entre les courbes d'entra\u00eenement et de validation ? D'apr\u00e8s vous, l'entra\u00eenement a-t-il \u00e9t\u00e9 suffisant (nombre d'\u00e9poques) ?"},{"location":"module1/ressources/fiche-observations/#partie-6-predictions-sur-des-exemples-de-test-cellule-7","title":"Partie 6 : Pr\u00e9dictions sur des exemples de test (Cellule 7)","text":"<p>Observez les 10 exemples de pr\u00e9diction :</p> Question Observation Combien de pr\u00e9dictions sont correctes sur les 10 exemples ? Pour les pr\u00e9dictions incorrectes, quelles pourraient \u00eatre les raisons d'erreur ? Certains chiffres semblent-ils plus difficiles \u00e0 reconna\u00eetre que d'autres ?"},{"location":"module1/ressources/fiche-observations/#partie-7-test-avec-votre-propre-dessin-cellule-8","title":"Partie 7 : Test avec votre propre dessin (Cellule 8)","text":"Question Observation Quels chiffres avez-vous dessin\u00e9s ? Combien ont \u00e9t\u00e9 correctement pr\u00e9dits ? Pour ceux mal pr\u00e9dits, quelle \u00e9tait la pr\u00e9diction et pourquoi selon vous ? Comment le pr\u00e9traitement de l'image a-t-il transform\u00e9 votre dessin ?"},{"location":"module1/ressources/fiche-observations/#partie-8-experimentations-cellule-9","title":"Partie 8 : Exp\u00e9rimentations (Cellule 9)","text":"<p>Documentez vos exp\u00e9rimentations en modifiant le mod\u00e8le ou les param\u00e8tres :</p>"},{"location":"module1/ressources/fiche-observations/#experimentation-1","title":"Exp\u00e9rimentation 1","text":"<p>Modification effectu\u00e9e : _______</p> Param\u00e8tre Valeur originale Nouvelle valeur <p>R\u00e9sultats : - Pr\u00e9cision test : _% - Observations : _____</p>"},{"location":"module1/ressources/fiche-observations/#experimentation-2","title":"Exp\u00e9rimentation 2","text":"<p>Modification effectu\u00e9e : _______</p> Param\u00e8tre Valeur originale Nouvelle valeur <p>R\u00e9sultats : - Pr\u00e9cision test : _% - Observations : _____</p>"},{"location":"module1/ressources/fiche-observations/#conclusion","title":"Conclusion","text":"Question R\u00e9ponse Quels sont les 3 principaux apprentissages de ce TP ? 1.2.3. Quelles am\u00e9liorations pourriez-vous sugg\u00e9rer pour ce mod\u00e8le ? Comment ce mod\u00e8le se compare-t-il aux capacit\u00e9s humaines de reconnaissance de chiffres ? Quelles autres applications de la vision par ordinateur vous int\u00e9ressent ?"},{"location":"module1/ressources/fiche-observations/#glossaire-des-termes-cles-rencontres","title":"Glossaire des termes cl\u00e9s rencontr\u00e9s","text":"Terme Votre d\u00e9finition Convolution Pooling Epoch (\u00e9poque) Batch Dropout Softmax Overfitting (surapprentissage)"},{"location":"module1/ressources/glossaire-dl/","title":"Glossaire","text":"<p>Voici le glossaire du Deep Learning avec des liens vers les d\u00e9finitions des termes techniques mentionn\u00e9s :</p>"},{"location":"module1/ressources/glossaire-dl/#glossaire-du-deep-learning","title":"Glossaire du Deep Learning","text":""},{"location":"module1/ressources/glossaire-dl/#termes-fondamentaux","title":"Termes fondamentaux","text":"Terme D\u00e9finition Exemple concret Deep Learning Sous-domaine du Machine Learning utilisant des r\u00e9seaux de neurones \u00e0 plusieurs couches pour mod\u00e9liser des abstractions de haut niveau dans les donn\u00e9es. Reconnaissance d'objets dans des photos. R\u00e9seau de neurones Syst\u00e8me inspir\u00e9 du cerveau humain compos\u00e9 de n\u0153uds (neurones) interconnect\u00e9s qui traitent les informations. R\u00e9seau capable de reconna\u00eetre des chiffres manuscrits. Neurone artificiel Unit\u00e9 de calcul de base dans un r\u00e9seau de neurones qui re\u00e7oit des entr\u00e9es, applique une transformation et produit une sortie. Un neurone qui s'active quand il d\u00e9tecte un contour vertical. Couche Ensemble de neurones situ\u00e9s au m\u00eame niveau dans le r\u00e9seau. Couche d'entr\u00e9e, couche cach\u00e9e, couche de sortie. Poids Valeurs num\u00e9riques qui d\u00e9finissent l'importance relative de chaque connexion entre les neurones. Un poids \u00e9lev\u00e9 (ex: 0.8) indique une forte influence. Biais Valeur ajout\u00e9e \u00e0 la somme pond\u00e9r\u00e9e des entr\u00e9es d'un neurone pour ajuster le seuil d'activation. Permet \u00e0 un neurone de s'activer m\u00eame si toutes les entr\u00e9es sont nulles. Fonction d'activation Fonction math\u00e9matique qui d\u00e9termine la sortie d'un neurone en fonction de ses entr\u00e9es. ReLU, Sigmoid, Tanh."},{"location":"module1/ressources/glossaire-dl/#architectures-de-reseaux","title":"Architectures de r\u00e9seaux","text":"Terme D\u00e9finition Cas d'utilisation R\u00e9seau dense R\u00e9seau o\u00f9 chaque neurone est connect\u00e9 \u00e0 tous les neurones de la couche pr\u00e9c\u00e9dente. Classification d'images simples, pr\u00e9diction de valeurs. R\u00e9seau convolutif (CNN) R\u00e9seau sp\u00e9cialis\u00e9 dans le traitement des donn\u00e9es en grille comme les images, utilisant des filtres pour d\u00e9tecter des caract\u00e9ristiques. Reconnaissance d'objets, classification d'images. R\u00e9seau r\u00e9current (RNN) R\u00e9seau avec des connexions formant des cycles, adapt\u00e9 aux donn\u00e9es s\u00e9quentielles. Traduction automatique, g\u00e9n\u00e9ration de texte. LSTM/GRU Types de RNN capables de m\u00e9moriser l'information sur de longues s\u00e9quences gr\u00e2ce \u00e0 des m\u00e9canismes de m\u00e9moire. Analyse de texte long, pr\u00e9diction de s\u00e9ries temporelles. Transformer Architecture bas\u00e9e sur des m\u00e9canismes d'attention, sans r\u00e9currence, permettant de traiter les donn\u00e9es en parall\u00e8le. Mod\u00e8les de langage avanc\u00e9s comme GPT, BERT, Mistral. Autoencoder R\u00e9seau qui apprend \u00e0 encoder puis d\u00e9coder les donn\u00e9es pour r\u00e9duire la dimensionnalit\u00e9 ou d\u00e9tecter des anomalies. R\u00e9duction de dimensionnalit\u00e9, d\u00e9tection d'anomalies. GAN (Generative Adversarial Network) Deux r\u00e9seaux en comp\u00e9tition : un g\u00e9n\u00e9rateur cr\u00e9e des donn\u00e9es et un discriminateur essaie de les distinguer des donn\u00e9es r\u00e9elles. Cr\u00e9ation d'images r\u00e9alistes, deepfakes."},{"location":"module1/ressources/glossaire-dl/#apprentissage","title":"Apprentissage","text":"Terme D\u00e9finition Exemple Forward propagation Passage des donn\u00e9es d'entr\u00e9e \u00e0 travers le r\u00e9seau pour produire une pr\u00e9diction. Calcul de la sortie d'un mod\u00e8le pour une image d'entr\u00e9e. Loss (perte) Mesure de l'\u00e9cart entre les pr\u00e9dictions du mod\u00e8le et les valeurs r\u00e9elles. Erreur quadratique moyenne, entropie crois\u00e9e. Backpropagation Algorithme qui calcule le gradient de l'erreur par rapport aux poids du r\u00e9seau pour les ajuster. Calcul de la contribution de chaque poids \u00e0 l'erreur totale. Descente de gradient Algorithme d'optimisation qui ajuste les poids du r\u00e9seau pour minimiser l'erreur. Modification it\u00e9rative des poids dans la direction du gradient n\u00e9gatif. \u00c9poque Un passage complet \u00e0 travers l'ensemble des donn\u00e9es d'entra\u00eenement. Entra\u00eener un mod\u00e8le pendant 10 \u00e9poques. Batch Sous-ensemble des donn\u00e9es trait\u00e9 avant une mise \u00e0 jour des poids. Traiter les donn\u00e9es par lots de 32 exemples. Optimiseur Algorithme qui impl\u00e9mente la descente de gradient pour ajuster les poids du r\u00e9seau. Adam, SGD, RMSprop. Learning rate Taux qui contr\u00f4le l'ampleur des ajustements des poids lors de l'entra\u00eenement. Trop \u00e9lev\u00e9 : divergence, trop faible : apprentissage lent."},{"location":"module1/ressources/glossaire-dl/#techniques-specifiques","title":"Techniques sp\u00e9cifiques","text":"Terme D\u00e9finition Utilisation Transfer learning R\u00e9utilisation d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 sur une nouvelle t\u00e2che pour b\u00e9n\u00e9ficier de ses connaissances. Adapter un mod\u00e8le ImageNet pour reconna\u00eetre des maladies de plantes. Fine-tuning Ajustement d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 sur des donn\u00e9es sp\u00e9cifiques pour am\u00e9liorer ses performances sur une t\u00e2che particuli\u00e8re. R\u00e9entra\u00eener les derni\u00e8res couches d'un mod\u00e8le BERT pour la classification de texte. Data augmentation G\u00e9n\u00e9ration de nouvelles donn\u00e9es d'entra\u00eenement par transformation des donn\u00e9es existantes pour augmenter la diversit\u00e9. Rotation, mise \u00e0 l'\u00e9chelle, distorsion d'images. Dropout Technique o\u00f9 des neurones sont al\u00e9atoirement d\u00e9sactiv\u00e9s pendant l'entra\u00eenement pour r\u00e9duire l'overfitting. Force le r\u00e9seau \u00e0 \u00eatre redondant et robuste. Batch normalization Normalisation des activations d'une couche pour stabiliser et acc\u00e9l\u00e9rer l'apprentissage. Am\u00e9liore la convergence et permet d'utiliser des taux d'apprentissage plus \u00e9lev\u00e9s. Early stopping Arr\u00eat de l'entra\u00eenement quand les performances sur la validation cessent de s'am\u00e9liorer pour \u00e9viter l'overfitting. Emp\u00eache le surajustement aux donn\u00e9es d'entra\u00eenement. Embedding Conversion de donn\u00e9es cat\u00e9gorielles en vecteurs denses pour les repr\u00e9senter dans un espace continu. Word embeddings dans le NLP (Word2Vec, GloVe)."},{"location":"module1/ressources/glossaire-dl/#convolutions-et-cnn","title":"Convolutions et CNN","text":"Terme D\u00e9finition R\u00f4le Filtre (kernel) Matrice de poids appliqu\u00e9e \u00e0 une r\u00e9gion de l'image pour d\u00e9tecter des caract\u00e9ristiques sp\u00e9cifiques. D\u00e9tecte des caract\u00e9ristiques sp\u00e9cifiques (bords, textures). Feature map Sortie d'un filtre de convolution appliqu\u00e9 \u00e0 une image, repr\u00e9sentant les caract\u00e9ristiques d\u00e9tect\u00e9es. Carte d'activation des caract\u00e9ristiques d\u00e9tect\u00e9es. Pooling Op\u00e9ration de sous-\u00e9chantillonnage r\u00e9duisant les dimensions de la feature map pour g\u00e9n\u00e9raliser les caract\u00e9ristiques. R\u00e9duit la complexit\u00e9 computationnelle et contr\u00f4le l'overfitting. Padding Ajout de pixels (g\u00e9n\u00e9ralement z\u00e9ros) aux bords d'une image pour conserver les dimensions apr\u00e8s convolution. Permet de conserver les dimensions de l'image apr\u00e8s l'application des filtres. Stride Pas de d\u00e9placement du filtre sur l'image, contr\u00f4lant le chevauchement des champs r\u00e9ceptifs. Contr\u00f4le la taille de la feature map et la quantit\u00e9 de chevauchement."},{"location":"module1/ressources/glossaire-dl/#metriques-devaluation","title":"M\u00e9triques d'\u00e9valuation","text":"M\u00e9trique D\u00e9finition Cas d'usage Accuracy Proportion de pr\u00e9dictions correctes parmi toutes les pr\u00e9dictions. Classification \u00e9quilibr\u00e9e. Precision Proportion des pr\u00e9dictions positives qui sont correctes. Quand les faux positifs sont co\u00fbteux. Recall Proportion des cas positifs r\u00e9els correctement identifi\u00e9s. Quand les faux n\u00e9gatifs sont co\u00fbteux. F1-Score Moyenne harmonique de la pr\u00e9cision et du rappel, \u00e9quilibrant les deux m\u00e9triques. Classification avec classes d\u00e9s\u00e9quilibr\u00e9es. ROC-AUC Aire sous la courbe ROC, mesurant la qualit\u00e9 de la discrimination entre les classes. \u00c9valuation des mod\u00e8les de classification. MAE (Mean Absolute Error) Moyenne des valeurs absolues des erreurs entre les pr\u00e9dictions et les valeurs r\u00e9elles. R\u00e9gression, quand les \u00e9carts importants ne sont pas surpond\u00e9r\u00e9s. RMSE (Root Mean Squared Error) Racine carr\u00e9e de la moyenne des carr\u00e9s des erreurs entre les pr\u00e9dictions et les valeurs r\u00e9elles. R\u00e9gression, p\u00e9nalise davantage les grands \u00e9carts."},{"location":"module1/ressources/glossaire-dl/#problemes-courants","title":"Probl\u00e8mes courants","text":"Terme D\u00e9finition Solution possible Overfitting Le mod\u00e8le apprend trop bien les donn\u00e9es d'entra\u00eenement au d\u00e9triment de la g\u00e9n\u00e9ralisation sur de nouvelles donn\u00e9es. R\u00e9gularisation, dropout, plus de donn\u00e9es. Underfitting Le mod\u00e8le est trop simple pour capturer la complexit\u00e9 des donn\u00e9es, r\u00e9sultant en de mauvaises performances. Augmenter la complexit\u00e9 du mod\u00e8le, entra\u00eener plus longtemps. Vanishing gradient Probl\u00e8me o\u00f9 le gradient devient tr\u00e8s petit, ralentissant l'apprentissage dans les couches profondes. Utiliser ReLU, LSTM, initialisation des poids adapt\u00e9e. Exploding gradient Probl\u00e8me o\u00f9 le gradient devient tr\u00e8s grand, d\u00e9stabilisant l'apprentissage. Gradient clipping, normalisation des poids. Imbalanced data Jeu de donn\u00e9es o\u00f9 certaines classes sont beaucoup plus fr\u00e9quentes que d'autres, biaisant le mod\u00e8le. R\u00e9\u00e9chantillonnage, pond\u00e9ration des classes, techniques d'augmentation."},{"location":"module1/ressources/glossaire-dl/#termes-relatifs-aux-modeles-de-langage","title":"Termes relatifs aux mod\u00e8les de langage","text":"Terme D\u00e9finition Exemple Token Unit\u00e9 de base du texte pour les mod\u00e8les de langage, comme un mot, sous-mot ou caract\u00e8re. \"Je suis pr\u00eat\" \u2192 [\"Je\", \"suis\", \"pr\u00eat\"]. Tokenization Processus de d\u00e9coupage du texte en tokens pour les traiter dans un mod\u00e8le de langage. \"Je suis pr\u00eat\" \u2192 [\"Je\", \"suis\", \"pr\u00eat\"]. Prompt Texte initial fourni \u00e0 un mod\u00e8le de langage pour guider sa g\u00e9n\u00e9ration de texte. \"R\u00e9dige un po\u00e8me sur le printemps:\". Context window Nombre maximum de tokens qu'un mod\u00e8le peut traiter en une fois, d\u00e9terminant la quantit\u00e9 d'information contextuelle. GPT-4 a une fen\u00eatre contextuelle de 8k-32k tokens. Attention M\u00e9canisme permettant au mod\u00e8le de se concentrer sur diff\u00e9rentes parties de l'entr\u00e9e pour g\u00e9n\u00e9rer une sortie pertinente. Self-attention dans les Transformers. Fine-tuning Adaptation d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 \u00e0 une t\u00e2che sp\u00e9cifique en ajustant ses poids sur des donn\u00e9es sp\u00e9cifiques. Ajuster GPT pour une t\u00e2che de customer support. Few-shot learning Capacit\u00e9 d'un mod\u00e8le \u00e0 apprendre \u00e0 partir de tr\u00e8s peu d'exemples, souvent en fournissant quelques exemples dans le prompt. Donner 2-3 exemples dans le prompt pour guider le mod\u00e8le."},{"location":"module1/ressources/glossaire-dl/#frameworks-et-outils","title":"Frameworks et outils","text":"Terme D\u00e9finition Cas d'utilisation TensorFlow Framework de Machine Learning d\u00e9velopp\u00e9 par Google, utilis\u00e9 pour cr\u00e9er et entra\u00eener des mod\u00e8les de Deep Learning. D\u00e9ploiement en production, applications mobiles. PyTorch Framework de Machine Learning d\u00e9velopp\u00e9 par Facebook, connu pour sa flexibilit\u00e9 et sa facilit\u00e9 d'utilisation. Recherche, prototypage rapide. Keras API de haut niveau s'ex\u00e9cutant sur TensorFlow, facilitant le d\u00e9veloppement rapide de mod\u00e8les de Deep Learning. D\u00e9veloppement rapide de prototypes. Hugging Face Biblioth\u00e8que pour les mod\u00e8les de NLP pr\u00e9-entra\u00een\u00e9s, facilitant leur utilisation et leur fine-tuning. Utilisation de BERT, GPT et autres mod\u00e8les de langage. ONNX Format d'\u00e9change pour mod\u00e8les de Machine Learning, permettant l'interop\u00e9rabilit\u00e9 entre diff\u00e9rents frameworks. Transfert de mod\u00e8les entre TensorFlow, PyTorch, etc. TensorBoard Outil de visualisation pour TensorFlow, permettant de suivre les m\u00e9triques d'entra\u00eenement et de visualiser les graphes de mod\u00e8les. Suivi des m\u00e9triques d'entra\u00eenement. MLflow Plateforme pour g\u00e9rer le cycle de vie des mod\u00e8les de Machine Learning, incluant le suivi des exp\u00e9riences et la gestion des mod\u00e8les. Suivi des exp\u00e9riences, gestion des mod\u00e8les."},{"location":"module1/ressources/glossaire-dl/#applications-du-deep-learning","title":"Applications du Deep Learning","text":"Application Description Architecture typique Computer Vision Domaine du Deep Learning d\u00e9di\u00e9 \u00e0 l'analyse et la compr\u00e9hension d'images et de vid\u00e9os. CNN (ResNet, YOLO, EfficientNet). Natural Language Processing (NLP) Domaine du Deep Learning d\u00e9di\u00e9 au traitement et \u00e0 la g\u00e9n\u00e9ration de texte. Transformers (BERT, GPT, T5). Speech Recognition Conversion de la parole en texte \u00e0 l'aide de mod\u00e8les de Deep Learning. RNN, Transformers (Wav2Vec). Recommendation Systems Syst\u00e8mes qui sugg\u00e8rent du contenu personnalis\u00e9 en fonction des pr\u00e9f\u00e9rences de l'utilisateur. R\u00e9seaux de neurones profonds, embeddings. Generative AI Cr\u00e9ation de contenu nouveau (images, texte, audio) \u00e0 l'aide de mod\u00e8les de Deep Learning. GANs, Diffusion Models, Transformers. Reinforcement Learning Apprentissage par essai-erreur et r\u00e9compense, o\u00f9 un agent apprend \u00e0 prendre des d\u00e9cisions pour maximiser une r\u00e9compense. Deep Q-Networks, Policy Gradients. Time Series Analysis Pr\u00e9diction de valeurs futures dans des s\u00e9quences temporelles \u00e0 l'aide de mod\u00e8les de Deep Learning. LSTM, Transformers temporels."},{"location":"module1/ressources/glossaire-dl/#explications-des-termes-techniques","title":"Explications des termes techniques","text":""},{"location":"module1/ressources/glossaire-dl/#fonctions-dactivation","title":"Fonctions d'activation","text":"<ul> <li>ReLU (Rectified Linear Unit) : Fonction d'activation qui retourne 0 si l'entr\u00e9e est n\u00e9gative et l'entr\u00e9e elle-m\u00eame si elle est positive. Elle est couramment utilis\u00e9e dans les r\u00e9seaux de neurones pour introduire de la non-lin\u00e9arit\u00e9.</li> <li>Sigmoid : Fonction d'activation qui mappe les valeurs d'entr\u00e9e \u00e0 une plage de 0 \u00e0 1, souvent utilis\u00e9e pour les probl\u00e8mes de classification binaire.</li> <li>Tanh (Hyperbolic Tangent) : Fonction d'activation qui mappe les valeurs d'entr\u00e9e \u00e0 une plage de -1 \u00e0 1, souvent utilis\u00e9e dans les r\u00e9seaux r\u00e9currents.</li> </ul>"},{"location":"module1/ressources/glossaire-dl/#optimiseurs","title":"Optimiseurs","text":"<ul> <li>Adam (Adaptive Moment Estimation) : Algorithme d'optimisation qui combine les avantages de deux autres extensions de la descente de gradient stochastique, \u00e0 savoir AdaGrad et RMSProp. Il est largement utilis\u00e9 pour entra\u00eener des r\u00e9seaux de neurones.</li> <li>SGD (Stochastic Gradient Descent) : Algorithme d'optimisation qui met \u00e0 jour les poids du r\u00e9seau en utilisant une estimation stochastique du gradient de la fonction de perte.</li> <li>RMSprop : Algorithme d'optimisation qui adapte le taux d'apprentissage pour chaque param\u00e8tre, ce qui permet de stabiliser et d'acc\u00e9l\u00e9rer l'entra\u00eenement.</li> </ul>"},{"location":"module1/ressources/glossaire-dl/#modeles-de-langage","title":"Mod\u00e8les de langage","text":"<ul> <li>Word2Vec : Mod\u00e8le de langage qui apprend des repr\u00e9sentations vectorielles des mots (embeddings) en utilisant des r\u00e9seaux de neurones. Il est utilis\u00e9 pour capturer les relations s\u00e9mantiques entre les mots.</li> <li>GloVe (Global Vectors for Word Representation) : Mod\u00e8le de langage qui apprend des embeddings de mots en utilisant une matrice de co-occurrence des mots dans un corpus.</li> </ul>"},{"location":"module1/ressources/glossaire-dl/#modeles-de-reconnaissance-vocale","title":"Mod\u00e8les de reconnaissance vocale","text":"<ul> <li>Wav2Vec : Mod\u00e8le de reconnaissance vocale qui apprend des repr\u00e9sentations vectorielles des segments audio en utilisant des r\u00e9seaux de neurones. Il est utilis\u00e9 pour convertir la parole en texte.</li> </ul>"},{"location":"module1/ressources/glossaire-dl/#architectures-de-reseaux_1","title":"Architectures de r\u00e9seaux","text":"<ul> <li>ResNet (Residual Networks) : Architecture de r\u00e9seau de neurones convolutifs qui utilise des connexions r\u00e9siduelles pour permettre l'entra\u00eenement de r\u00e9seaux tr\u00e8s profonds sans d\u00e9gradation des performances.</li> <li>YOLO (You Only Look Once) : Architecture de r\u00e9seau de neurones convolutifs utilis\u00e9e pour la d\u00e9tection d'objets en temps r\u00e9el. Elle divise l'image en une grille et pr\u00e9dit des bo\u00eetes englobantes et des classes pour chaque cellule de la grille.</li> <li>EfficientNet : Architecture de r\u00e9seau de neurones convolutifs qui utilise une approche de mise \u00e0 l'\u00e9chelle compos\u00e9e pour optimiser la pr\u00e9cision et l'efficacit\u00e9 du mod\u00e8le.</li> </ul>"},{"location":"module1/ressources/glossaire-dl/#modeles-de-langage-avances","title":"Mod\u00e8les de langage avanc\u00e9s","text":"<ul> <li>BERT (Bidirectional Encoder Representations from Transformers) : Mod\u00e8le de langage bas\u00e9 sur les Transformers qui utilise des m\u00e9canismes d'attention bidirectionnelle pour capturer le contexte des mots dans une phrase. Il est largement utilis\u00e9 pour des t\u00e2ches de traitement du langage naturel.</li> <li>GPT (Generative Pre-trained Transformer) : Mod\u00e8le de langage bas\u00e9 sur les Transformers qui est pr\u00e9-entra\u00een\u00e9 sur un grand corpus de texte et peut \u00eatre fine-tun\u00e9 pour des t\u00e2ches sp\u00e9cifiques. Il est utilis\u00e9 pour la g\u00e9n\u00e9ration de texte et d'autres t\u00e2ches de traitement du langage naturel.</li> </ul>"},{"location":"module1/ressources/guide-colab/","title":"Guide d'utilisation de Google Colab","text":""},{"location":"module1/ressources/guide-colab/#introduction-a-google-colab","title":"Introduction \u00e0 Google Colab","text":"<p>Google Colab (ou Colaboratory) est un environnement de notebook Jupyter h\u00e9berg\u00e9 par Google. Il permet d'ex\u00e9cuter du code Python dans votre navigateur et est particuli\u00e8rement adapt\u00e9 au machine learning, \u00e0 l'analyse de donn\u00e9es et \u00e0 l'\u00e9ducation.</p>"},{"location":"module1/ressources/guide-colab/#avantages-de-google-colab","title":"Avantages de Google Colab","text":"<ul> <li>Gratuit : pas besoin d'installer Python ou des biblioth\u00e8ques sur votre ordinateur</li> <li>Puissant : acc\u00e8s \u00e0 des GPU et TPU gratuits</li> <li>Collaboratif : facilit\u00e9 de partage et de travail en \u00e9quipe</li> <li>Pr\u00eat \u00e0 l'emploi : biblioth\u00e8ques populaires d\u00e9j\u00e0 install\u00e9es (TensorFlow, PyTorch, etc.)</li> </ul>"},{"location":"module1/ressources/guide-colab/#acceder-a-google-colab","title":"Acc\u00e9der \u00e0 Google Colab","text":"<ol> <li>Allez sur colab.research.google.com</li> <li>Connectez-vous avec votre compte Google</li> <li>Sur la page d'accueil, vous pouvez:</li> <li>Cr\u00e9er un nouveau notebook</li> <li>Ouvrir un notebook existant</li> <li>Acc\u00e9der \u00e0 des tutoriels</li> </ol>"},{"location":"module1/ressources/guide-colab/#interface-de-colab","title":"Interface de Colab","text":"<p>L'interface de Colab est compos\u00e9e de:</p> <ol> <li>Barre de menu : Fichier, \u00c9dition, Affichage, etc.</li> <li>Barre d'outils : actions rapides</li> <li>Panneau de cellules : o\u00f9 vous \u00e9crivez et ex\u00e9cutez votre code</li> <li>Panneau lat\u00e9ral : pour acc\u00e9der aux fichiers, tableaux, etc.</li> </ol>"},{"location":"module1/ressources/guide-colab/#types-de-cellules","title":"Types de cellules","text":"<p>Dans Colab, il existe deux types principaux de cellules:</p> <ul> <li>Cellules de code : pour ex\u00e9cuter du code Python</li> <li>Cellules de texte : pour \u00e9crire des commentaires en Markdown</li> </ul>"},{"location":"module1/ressources/guide-colab/#cellules-de-code","title":"Cellules de code","text":"<pre><code># Exemple de cellule de code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.plot(x, y)\nplt.title(\"Fonction sinus\")\nplt.show()\n</code></pre>"},{"location":"module1/ressources/guide-colab/#cellules-de-texte-markdown","title":"Cellules de texte (Markdown)","text":"<p>Les cellules de texte utilisent la syntaxe Markdown:</p> <pre><code># Titre principal\n## Sous-titre\n\nTexte normal avec **texte en gras** et *texte en italique*.\n\nListe \u00e0 puces:\n- Item 1\n- Item 2\n\n\u00c9quation math\u00e9matique: $y = mx + b$\n</code></pre>"},{"location":"module1/ressources/guide-colab/#executer-du-code","title":"Ex\u00e9cuter du code","text":"<p>Pour ex\u00e9cuter une cellule: - Cliquez sur le bouton \u25b6\ufe0f \u00e0 gauche de la cellule - Ou utilisez le raccourci clavier <code>Shift+Enter</code></p> <p>Le r\u00e9sultat s'affiche directement sous la cellule.</p>"},{"location":"module1/ressources/guide-colab/#raccourcis-clavier-utiles","title":"Raccourcis clavier utiles","text":"<ul> <li><code>Ctrl+Enter</code> : Ex\u00e9cuter la cellule</li> <li><code>Shift+Enter</code> : Ex\u00e9cuter la cellule et passer \u00e0 la suivante</li> <li><code>Alt+Enter</code> : Ex\u00e9cuter la cellule et ins\u00e9rer une nouvelle cellule en dessous</li> <li><code>Ctrl+M D</code> : Supprimer la cellule</li> <li><code>Ctrl+M A</code> : Ins\u00e9rer une cellule au-dessus</li> <li><code>Ctrl+M B</code> : Ins\u00e9rer une cellule en-dessous</li> <li><code>Ctrl+M M</code> : Transformer en cellule Markdown</li> <li><code>Ctrl+M Y</code> : Transformer en cellule de code</li> </ul>"},{"location":"module1/ressources/guide-colab/#utiliser-le-gputpu","title":"Utiliser le GPU/TPU","text":"<p>Pour acc\u00e9l\u00e9rer l'ex\u00e9cution de votre code:</p> <ol> <li>Cliquez sur <code>Modifier</code> &gt; <code>Param\u00e8tres du notebook</code></li> <li>Sous <code>Acc\u00e9l\u00e9rateur mat\u00e9riel</code>, s\u00e9lectionnez <code>GPU</code> ou <code>TPU</code></li> <li>Cliquez sur <code>Enregistrer</code></li> </ol>"},{"location":"module1/ressources/guide-colab/#installer-des-bibliotheques","title":"Installer des biblioth\u00e8ques","text":"<p>Colab poss\u00e8de d\u00e9j\u00e0 de nombreuses biblioth\u00e8ques install\u00e9es, mais vous pouvez en ajouter d'autres:</p> <pre><code>!pip install nom_de_la_biblioth\u00e8que\n</code></pre> <p>Exemple: <pre><code>!pip install transformers\n</code></pre></p> <p>Apr\u00e8s l'installation, red\u00e9marrez l'environnement d'ex\u00e9cution: 1. <code>Ex\u00e9cution</code> &gt; <code>Red\u00e9marrer l'environnement d'ex\u00e9cution...</code></p>"},{"location":"module1/ressources/guide-colab/#gerer-les-fichiers","title":"G\u00e9rer les fichiers","text":""},{"location":"module1/ressources/guide-colab/#importer-des-fichiers","title":"Importer des fichiers","text":"<ol> <li>Cliquez sur l'ic\u00f4ne \ud83d\udcc2 dans le panneau lat\u00e9ral gauche</li> <li>Cliquez sur <code>Importer</code> pour t\u00e9l\u00e9charger un fichier</li> </ol> <p>Ou via le code: <pre><code>from google.colab import files\nuploaded = files.upload()\n</code></pre></p>"},{"location":"module1/ressources/guide-colab/#acceder-aux-fichiers-de-google-drive","title":"Acc\u00e9der aux fichiers de Google Drive","text":"<pre><code>from google.colab import drive\ndrive.mount('/content/drive')\n\n# Acc\u00e9der aux fichiers dans Drive\n!ls \"/content/drive/My Drive\"\n</code></pre>"},{"location":"module1/ressources/guide-colab/#telecharger-des-fichiers","title":"T\u00e9l\u00e9charger des fichiers","text":"<pre><code>from google.colab import files\nfiles.download('nom_du_fichier.ext')\n</code></pre>"},{"location":"module1/ressources/guide-colab/#enregistrer-votre-travail","title":"Enregistrer votre travail","text":"<p>Colab enregistre automatiquement votre travail dans Google Drive, mais vous pouvez aussi:</p> <ol> <li><code>Fichier</code> &gt; <code>Enregistrer une copie dans Drive</code></li> <li><code>Fichier</code> &gt; <code>T\u00e9l\u00e9charger</code> &gt; <code>T\u00e9l\u00e9charger .ipynb</code></li> </ol>"},{"location":"module1/ressources/guide-colab/#partager-un-notebook","title":"Partager un notebook","text":"<ol> <li>Cliquez sur <code>Partager</code> en haut \u00e0 droite</li> <li>Entrez les adresses e-mail ou obtenez un lien de partage</li> <li>D\u00e9finissez les autorisations d'acc\u00e8s (Lecteur ou \u00c9diteur)</li> </ol>"},{"location":"module1/ressources/guide-colab/#depannage-courant","title":"D\u00e9pannage courant","text":""},{"location":"module1/ressources/guide-colab/#erreur-cuda-out-of-memory","title":"Erreur \"CUDA out of memory\"","text":"<ul> <li>Red\u00e9marrez l'environnement d'ex\u00e9cution (Ex\u00e9cution &gt; Red\u00e9marrer...)</li> <li>R\u00e9duisez la taille de votre mod\u00e8le ou de vos donn\u00e9es</li> <li>Utilisez un lot (batch) plus petit</li> </ul>"},{"location":"module1/ressources/guide-colab/#deconnexion-apres-inactivite","title":"D\u00e9connexion apr\u00e8s inactivit\u00e9","text":"<ul> <li>Colab se d\u00e9connecte apr\u00e8s environ 90 minutes d'inactivit\u00e9</li> <li>Utilisez <code>Outils</code> &gt; <code>Param\u00e8tres</code> &gt; <code>Param\u00e8tres avanc\u00e9s</code> &gt; <code>D\u00e9sactiver l'interruption apr\u00e8s inactivit\u00e9</code></li> </ul>"},{"location":"module1/ressources/guide-colab/#limites-de-temps-dexecution","title":"Limites de temps d'ex\u00e9cution","text":"<ul> <li>Les sessions sont limit\u00e9es \u00e0 environ 12 heures</li> <li>Pour des calculs plus longs, enregistrez p\u00e9riodiquement votre travail</li> </ul>"},{"location":"module1/ressources/guide-colab/#perte-de-variables","title":"Perte de variables","text":"<ul> <li>Si vous ex\u00e9cutez les cellules dans un ordre diff\u00e9rent, certaines variables peuvent \u00eatre perdues</li> <li>Mieux vaut ex\u00e9cuter les cellules dans l'ordre s\u00e9quentiel</li> </ul>"},{"location":"module1/ressources/guide-colab/#astuces-pour-les-tps-de-deep-learning","title":"Astuces pour les TPs de Deep Learning","text":"<ol> <li> <p>V\u00e9rifiez l'acc\u00e9l\u00e9rateur mat\u00e9riel avant de commencer un entra\u00eenement lourd    <pre><code>import tensorflow as tf\nprint(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))\n</code></pre></p> </li> <li> <p>Sauvegardez vos mod\u00e8les r\u00e9guli\u00e8rement    <pre><code>model.save('mon_modele.h5')\n</code></pre></p> </li> <li> <p>Visualisez vos donn\u00e9es avant l'entra\u00eenement    <pre><code>import matplotlib.pyplot as plt\nplt.imshow(X_train[0])\nplt.show()\n</code></pre></p> </li> <li> <p>Utilisez tqdm pour les barres de progression    <pre><code>!pip install tqdm\nfrom tqdm.notebook import tqdm\n\nfor epoch in tqdm(range(100)):\n    # votre boucle d'entra\u00eenement\n</code></pre></p> </li> <li> <p>Profitez de TensorBoard <pre><code>%load_ext tensorboard\n%tensorboard --logdir logs\n</code></pre></p> </li> </ol>"},{"location":"module1/ressources/guide-colab/#ressources-supplementaires","title":"Ressources suppl\u00e9mentaires","text":"<ul> <li>Documentation officielle de Google Colab</li> <li>Tutoriels TensorFlow dans Colab</li> <li>Tutoriels PyTorch dans Colab</li> </ul> <p>Bonne exploration et bon apprentissage du Deep Learning avec Google Colab!</p>"},{"location":"module1/ressources/hello-world-dl/","title":"Hello World du Deep Learning - Reconnaissance de chiffres manuscrits","text":"<p>Ce notebook vous guidera \u00e0 travers la cr\u00e9ation, l'entra\u00eenement et l'utilisation d'un r\u00e9seau de neurones pour reconna\u00eetre des chiffres manuscrits. Copiez chaque cellule dans votre notebook Google Colab et ex\u00e9cutez-les dans l'ordre.</p>"},{"location":"module1/ressources/hello-world-dl/#cellule-1-introduction-cellule-markdown","title":"Cellule 1 : Introduction (Cellule Markdown)","text":"<pre><code># \ud83d\ude80 Hello World du Deep Learning\n\n## Reconnaissance de chiffres manuscrits avec TensorFlow et Keras\n\n### Objectifs de ce notebook\n\n- Charger et pr\u00e9parer un jeu de donn\u00e9es de chiffres manuscrits\n- Cr\u00e9er un r\u00e9seau de neurones simple\n- Entra\u00eener le mod\u00e8le\n- Visualiser les r\u00e9sultats\n- Tester le mod\u00e8le avec vos propres dessins\n\n### BTS SIO - D\u00e9couverte du Deep Learning\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-2-configuration-cellule-code","title":"Cellule 2 : Configuration (Cellule Code)","text":"<pre><code># Importation des biblioth\u00e8ques n\u00e9cessaires\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# V\u00e9rification de la version de TensorFlow\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\n\n# V\u00e9rification du GPU (m\u00e9thode recommand\u00e9e)\nprint(\"GPU disponible :\", len(tf.config.list_physical_devices('GPU')) &gt; 0)\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-3-chargement-des-donnees-cellule-code","title":"Cellule 3 : Chargement des donn\u00e9es (Cellule Code)","text":"<pre><code># Chargement du dataset MNIST\n(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n\n# Pr\u00e9traitement des donn\u00e9es\nX_train = X_train.reshape((60000, 28, 28, 1)) / 255.0\nX_test = X_test.reshape((10000, 28, 28, 1)) / 255.0\n\n# Conversion des labels en cat\u00e9gories\ny_train = keras.utils.to_categorical(y_train)\ny_test = keras.utils.to_categorical(y_test)\n\n# Affichage de quelques exemples\nplt.figure(figsize=(10, 2))\nfor i in range(10):\n    plt.subplot(1, 10, i+1)\n    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n    plt.axis('off')\nplt.suptitle(\"Exemples de chiffres manuscrits\")\nplt.show()\n\nprint(f\"Nombre d'exemples d'entra\u00eenement : {X_train.shape[0]}\")\nprint(f\"Nombre d'exemples de test : {X_test.shape[0]}\")\nprint(f\"Dimensions d'une image : {X_train.shape[1:3]}\")\nprint(f\"Valeurs des pixels apr\u00e8s normalisation : de 0 \u00e0 1\")\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-4-creation-du-modele-cellule-code","title":"Cellule 4 : Cr\u00e9ation du mod\u00e8le (Cellule Code)","text":"<pre><code># Cr\u00e9ation du mod\u00e8le de r\u00e9seau de neurones\n# On utilise Input comme premi\u00e8re couche (recommand\u00e9)\ninputs = keras.Input(shape=(28, 28, 1))\n\n# Couche de convolution\nx = layers.Conv2D(32, (3, 3), activation='relu')(inputs)\nx = layers.MaxPooling2D((2, 2))(x)\n\n# Couche de convolution suppl\u00e9mentaire\nx = layers.Conv2D(64, (3, 3), activation='relu')(x)\nx = layers.MaxPooling2D((2, 2))(x)\n\n# Aplatissement\nx = layers.Flatten()(x)\n\n# Couche dense\nx = layers.Dense(64, activation='relu')(x)\n\n# Couche de sortie\noutputs = layers.Dense(10, activation='softmax')(x)\n\n# Cr\u00e9ation du mod\u00e8le\nmodel = keras.Model(inputs, outputs, name=\"mnist_model\")\n\n# Compilation du mod\u00e8le\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Affichage du r\u00e9sum\u00e9 du mod\u00e8le\nmodel.summary()\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-5-entrainement-cellule-code","title":"Cellule 5 : Entra\u00eenement (Cellule Code)","text":"<pre><code># Entra\u00eenement du mod\u00e8le\n# Note : Nombre d'\u00e9poques r\u00e9duit pour la d\u00e9monstration\nhistory = model.fit(\n    X_train, y_train,\n    epochs=5,\n    batch_size=64,\n    validation_split=0.2,\n    verbose=1\n)\n\n# \u00c9valuation du mod\u00e8le\ntest_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"\\nPr\u00e9cision sur l'ensemble de test : {test_accuracy*100:.2f}%\")\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-6-visualisation-cellule-code","title":"Cellule 6 : Visualisation (Cellule Code)","text":"<pre><code># Visualisation de la pr\u00e9cision et de la perte\nplt.figure(figsize=(12, 4))\n\n# Pr\u00e9cision\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Pr\u00e9cision entra\u00eenement')\nplt.plot(history.history['val_accuracy'], label='Pr\u00e9cision validation')\nplt.title('Pr\u00e9cision du mod\u00e8le')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Pr\u00e9cision')\nplt.legend()\n\n# Perte\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Perte entra\u00eenement')\nplt.plot(history.history['val_loss'], label='Perte validation')\nplt.title('Perte du mod\u00e8le')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Perte')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-7-predictions-cellule-code","title":"Cellule 7 : Pr\u00e9dictions (Cellule Code)","text":"<pre><code># Pr\u00e9dictions et visualisation\n# Pr\u00e9dire sur quelques images de test\npredictions = model.predict(X_test[:10])\n\nplt.figure(figsize=(15, 6))\nfor i in range(10):\n    plt.subplot(2, 10, i+1)\n    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"R\u00e9el: {np.argmax(y_test[i])}\")\n    plt.axis('off')\n\n    plt.subplot(2, 10, i+11)\n    plt.bar(range(10), predictions[i])\n    plt.title(f\"Pr\u00e9dit: {np.argmax(predictions[i])}\")\n    plt.xticks(range(10))\n    plt.ylim(0, 1)\n\nplt.suptitle(\"Pr\u00e9dictions du mod\u00e8le\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-8-dessin-interactif-cellule-code","title":"Cellule 8 : Dessin interactif (Cellule Code)","text":"<pre><code># Interface interactive pour dessiner et pr\u00e9dire\n# Cette cellule permet de dessiner un chiffre directement dans Colab\n\nfrom google.colab import output\nfrom IPython.display import display, HTML\nimport io\nimport base64\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Fonction pour cr\u00e9er un canvas HTML\ndef create_canvas():\n    canvas_html = \"\"\"\n    &lt;canvas id=\"canvas\" width=\"280\" height=\"280\" style=\"border: 2px solid black; background-color: white;\"&gt;&lt;/canvas&gt;\n    &lt;div style=\"margin-top: 10px;\"&gt;\n      &lt;button id=\"predict_button\" style=\"padding: 5px 10px; background-color: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer;\"&gt;Pr\u00e9dire&lt;/button&gt;\n      &lt;button id=\"clear_button\" style=\"margin-left: 10px; padding: 5px 10px; background-color: #f44336; color: white; border: none; border-radius: 4px; cursor: pointer;\"&gt;Effacer&lt;/button&gt;\n    &lt;/div&gt;\n    &lt;div id=\"result\" style=\"margin-top: 10px; font-weight: bold;\"&gt;&lt;/div&gt;\n\n    &lt;script&gt;\n      var canvas = document.getElementById('canvas');\n      var ctx = canvas.getContext('2d');\n      var isDrawing = false;\n\n      // Remplir le fond en blanc d\u00e8s le d\u00e9part\n      ctx.fillStyle = \"white\";\n      ctx.fillRect(0, 0, canvas.width, canvas.height);\n\n      ctx.lineWidth = 15;\n      ctx.lineCap = 'round';\n      ctx.lineJoin = 'round';\n      ctx.strokeStyle = 'black';\n\n      canvas.addEventListener('mousedown', function(e) {\n        isDrawing = true;\n        ctx.beginPath();\n        ctx.moveTo(e.clientX - canvas.getBoundingClientRect().left, e.clientY - canvas.getBoundingClientRect().top);\n      });\n\n      canvas.addEventListener('mousemove', function(e) {\n        if (isDrawing) {\n          ctx.lineTo(e.clientX - canvas.getBoundingClientRect().left, e.clientY - canvas.getBoundingClientRect().top);\n          ctx.stroke();\n        }\n      });\n\n      canvas.addEventListener('mouseup', function() {\n        isDrawing = false;\n      });\n\n      canvas.addEventListener('mouseleave', function() {\n        isDrawing = false;\n      });\n\n      document.getElementById('clear_button').addEventListener('click', function() {\n        ctx.fillStyle = \"white\";\n        ctx.fillRect(0, 0, canvas.width, canvas.height);\n        document.getElementById('result').innerHTML = '';\n      });\n\n      document.getElementById('predict_button').addEventListener('click', function() {\n        var imageData = canvas.toDataURL('image/png');\n        document.getElementById('result').innerHTML = 'Analyse en cours...';\n        google.colab.kernel.invokeFunction('notebook.predict', [imageData], {});\n      });\n    &lt;/script&gt;\n    \"\"\"\n    return canvas_html\n\n# Fonction pour pr\u00e9traiter l'image dessin\u00e9e\ndef preprocess_image(image_data):\n    image_data = image_data.split(',')[1]\n    image = Image.open(io.BytesIO(base64.b64decode(image_data)))\n\n    # Convertir en niveaux de gris et redimensionner\n    image = image.convert('L').resize((28, 28))\n\n    # Convertir en tableau numpy\n    image_array = np.array(image)\n\n    # V\u00e9rification si inversion des couleurs est n\u00e9cessaire\n    if np.mean(image_array) &gt; 127:  # Fond clair, chiffre sombre\n        image_array = 255 - image_array\n\n    # Normaliser les pixels\n    image_array = image_array / 255.0\n\n    return image_array\n\n# Fonction de pr\u00e9diction\ndef predict_digit(image_data):\n    image_array = preprocess_image(image_data)\n    image_array = image_array.reshape(1, 28, 28, 1)\n\n    # Affichage de l'image pr\u00e9trait\u00e9e pour v\u00e9rifier\n    plt.figure(figsize=(6, 3))\n    plt.subplot(1, 2, 1)\n    plt.imshow(image_array.reshape(28, 28), cmap='gray')\n    plt.title(\"Image pr\u00e9trait\u00e9e\")\n    plt.axis('off')\n\n    # Pr\u00e9diction\n    prediction = model.predict(image_array)[0]\n    digit = np.argmax(prediction)\n    confidence = prediction[digit] * 100\n\n    # Affichage du graphique des probabilit\u00e9s\n    plt.subplot(1, 2, 2)\n    plt.bar(range(10), prediction)\n    plt.title(f\"Pr\u00e9diction: {digit}\")\n    plt.xlabel(\"Chiffre\")\n    plt.ylabel(\"Confiance\")\n    plt.xticks(range(10))\n    plt.show()\n\n    # Afficher le r\u00e9sultat dans le notebook\n    output.eval_js(f\"\"\"\n    document.getElementById('result').innerHTML = 'Pr\u00e9diction: {digit} (Confiance: {confidence:.2f}%)';\n    \"\"\")\n\n# Enregistrer la fonction pour \u00eatre appel\u00e9e depuis JavaScript\noutput.register_callback('notebook.predict', predict_digit)\n\n# Afficher le canvas\ndisplay(HTML(create_canvas()))\nprint(\"Dessinez un chiffre dans le canvas ci-dessus et cliquez sur 'Pr\u00e9dire'\")\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-9-experimentation-cellule-markdown","title":"Cellule 9 : Exp\u00e9rimentation (Cellule Markdown)","text":"<pre><code>## \ud83e\uddea Exp\u00e9rimentations\n\nVoici quelques modifications que vous pouvez essayer pour am\u00e9liorer ou observer les effets sur le mod\u00e8le :\n\n1. **Modifier l'architecture du r\u00e9seau :**\n   - Augmenter/diminuer le nombre de neurones dans chaque couche\n   - Ajouter ou supprimer des couches dans le r\u00e9seau\n   - Essayer d'ajouter une couche Dropout (qui d\u00e9sactive al\u00e9atoirement certains neurones pendant l'entra\u00eenement)\n\n2. **Ajuster les param\u00e8tres d'entra\u00eenement :**\n   - Changer le nombre de cycles d'entra\u00eenement (\u00e9poques)\n   - Modifier le nombre d'exemples trait\u00e9s \u00e0 la fois (taille du batch)\n   - Tester diff\u00e9rentes m\u00e9thodes d'apprentissage (optimiseurs)\n\nPour chaque modification, observez l'impact sur :\n- La pr\u00e9cision finale\n- La vitesse d'entra\u00eenement\n- Les courbes d'apprentissage\n- Le comportement face \u00e0 vos propres dessins\n\nN'h\u00e9sitez pas \u00e0 documenter vos observations dans la fiche fournie.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/","title":"Machine Learning Classique - Classification MNIST avec Random Forest","text":"<p>Ce document contient le code et les explications pour le notebook de classification d'images MNIST avec Random Forest (approche Machine Learning classique). Vous pouvez copier-coller chaque section dans une cellule Google Colab.</p>"},{"location":"module1/ressources/machine-learning-classique/#cellule-1-markdown-introduction","title":"Cellule 1 (Markdown) - Introduction","text":"<pre><code># Classification avec Machine Learning classique\n\n## Reconnaissance de chiffres manuscrits avec Random Forest\n\nDans ce notebook, nous allons impl\u00e9menter une approche de Machine Learning classique pour la classification des chiffres manuscrits en utilisant le dataset MNIST. Nous utiliserons l'algorithme Random Forest, qui est bas\u00e9 sur un ensemble d'arbres de d\u00e9cision.\n\n### Objectifs :\n- Comprendre comment pr\u00e9parer des donn\u00e9es d'images pour le ML classique\n- Impl\u00e9menter un classificateur Random Forest\n- \u00c9valuer ses performances et ses limites\n- Comparer cette approche avec le Deep Learning\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-2-code-importation-des-bibliotheques","title":"Cellule 2 (Code) - Importation des biblioth\u00e8ques","text":"<pre><code># Importation des biblioth\u00e8ques n\u00e9cessaires\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom google.colab import output\noutput.enable_custom_widget_manager()\nimport ipywidgets as widgets\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-3-markdown-chargement-des-donnees","title":"Cellule 3 (Markdown) - Chargement des donn\u00e9es","text":"<pre><code>## Chargement et exploration des donn\u00e9es\n\nLe dataset MNIST contient 70 000 images de chiffres manuscrits (0-9) en niveaux de gris. Chaque image est de taille 28x28 pixels, ce qui donne 784 pixels par image.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-4-code-chargement-des-donnees-mnist","title":"Cellule 4 (Code) - Chargement des donn\u00e9es MNIST","text":"<pre><code>print(\"Chargement du jeu de donn\u00e9es MNIST...\")\n# Utilisation du jeu de donn\u00e9es MNIST int\u00e9gr\u00e9 \u00e0 sklearn\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\nX, y = mnist[\"data\"], mnist[\"target\"]\nX = X / 255.0  # Normalisation des valeurs de pixels entre 0 et 1\ny = y.astype(np.uint8)  # Conversion des labels en entiers\n\n# Exploration des donn\u00e9es\nprint(f\"Dimensions du jeu de donn\u00e9es: {X.shape}\")\nprint(f\"Nombre de classes: {len(np.unique(y))}\")\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-5-code-visualisation-des-exemples","title":"Cellule 5 (Code) - Visualisation des exemples","text":"<pre><code># Affichage de quelques exemples\nplt.figure(figsize=(10, 5))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.imshow(X[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"Label: {y[i]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.suptitle(\"Exemples de chiffres manuscrits\", y=1.05)\nplt.show()\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-6-markdown-preparation-des-donnees","title":"Cellule 6 (Markdown) - Pr\u00e9paration des donn\u00e9es","text":"<pre><code>## Pr\u00e9paration des donn\u00e9es pour Machine Learning classique\n\nContrairement aux r\u00e9seaux de neurones convolutifs (CNN), les algorithmes de ML classiques comme Random Forest ne sont pas con\u00e7us pour traiter directement des images. Nous devons donc :\n\n1. R\u00e9duire la dimensionnalit\u00e9 des donn\u00e9es (784 caract\u00e9ristiques est trop \u00e9lev\u00e9)\n2. Extraire des caract\u00e9ristiques pertinentes\n\nNous utiliserons l'Analyse en Composantes Principales (PCA) pour r\u00e9duire la dimensionnalit\u00e9 tout en conservant l'essentiel de l'information.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-7-code-preparation-des-donnees","title":"Cellule 7 (Code) - Pr\u00e9paration des donn\u00e9es","text":"<pre><code>print(\"\\n--- Pr\u00e9paration des donn\u00e9es pour Random Forest ---\")\nprint(\"Pour le Machine Learning classique, nous devons souvent extraire des caract\u00e9ristiques manuellement.\")\n\n# R\u00e9duction de dimension avec PCA pour acc\u00e9l\u00e9rer l'entra\u00eenement\nprint(\"Application d'une r\u00e9duction de dimension (PCA)...\")\nn_components = 50  # R\u00e9duire de 784 \u00e0 50 caract\u00e9ristiques\n\n# S\u00e9paration en ensembles d'entra\u00eenement et de test\n# Utilisation d'un \u00e9chantillon r\u00e9duit pour acc\u00e9l\u00e9rer la d\u00e9monstration\nX_sample = X[:10000]\ny_sample = y[:10000]\n\nX_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n\nprint(f\"Taille de l'ensemble d'entra\u00eenement: {X_train.shape}\")\nprint(f\"Taille de l'ensemble de test: {X_test.shape}\")\n\n# Cr\u00e9ation d'un pipeline d'extraction de caract\u00e9ristiques\nfeature_pipeline = Pipeline([\n    ('scaler', StandardScaler()),  # Normalisation des donn\u00e9es\n    ('pca', PCA(n_components=n_components))  # R\u00e9duction de dimension par PCA\n])\n\n# Application aux donn\u00e9es\nprint(\"Extraction de caract\u00e9ristiques...\")\nX_train_features = feature_pipeline.fit_transform(X_train)\nX_test_features = feature_pipeline.transform(X_test)\n\nprint(f\"Dimensions apr\u00e8s extraction de caract\u00e9ristiques: {X_train_features.shape}\")\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-8-markdown-entrainement-du-modele","title":"Cellule 8 (Markdown) - Entra\u00eenement du mod\u00e8le","text":"<pre><code>## Entra\u00eenement du mod\u00e8le Random Forest\n\nNous allons maintenant entra\u00eener un classificateur Random Forest sur nos donn\u00e9es pr\u00e9trait\u00e9es. Random Forest est un algorithme d'ensemble qui combine les pr\u00e9dictions de plusieurs arbres de d\u00e9cision pour am\u00e9liorer la pr\u00e9cision et contr\u00f4ler le sur-apprentissage.\n\nPrincipaux hyperparam\u00e8tres :\n- **n_estimators** : Nombre d'arbres dans la for\u00eat\n- **max_depth** : Profondeur maximale de chaque arbre\n- **min_samples_split** : Nombre minimum d'\u00e9chantillons requis pour diviser un n\u0153ud\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-9-code-entrainement-du-modele","title":"Cellule 9 (Code) - Entra\u00eenement du mod\u00e8le","text":"<pre><code>print(\"\\n--- Entra\u00eenement du mod\u00e8le Random Forest ---\")\n\n# Param\u00e8tres du mod\u00e8le - vous pouvez les modifier\nn_estimators = 100  # Nombre d'arbres\nmax_depth = 10      # Profondeur maximale des arbres\nmin_samples_split = 2  # Nombre minimum d'\u00e9chantillons requis pour diviser un n\u0153ud\n\n# Cr\u00e9ation du mod\u00e8le\nrf_model = RandomForestClassifier(\n    n_estimators=n_estimators,\n    max_depth=max_depth,\n    min_samples_split=min_samples_split,\n    random_state=42,\n    n_jobs=-1  # Utiliser tous les c\u0153urs disponibles\n)\n\n# Mesure du temps d'entra\u00eenement\nstart_time = time.time()\nprint(\"Entra\u00eenement du mod\u00e8le en cours...\")\nrf_model.fit(X_train_features, y_train)\nend_time = time.time()\ntraining_time = end_time - start_time\n\nprint(f\"Temps d'entra\u00eenement: {training_time:.2f} secondes\")\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-10-markdown-evaluation-du-modele","title":"Cellule 10 (Markdown) - \u00c9valuation du mod\u00e8le","text":"<pre><code>## \u00c9valuation du mod\u00e8le\n\n\u00c9valuons maintenant les performances de notre mod\u00e8le Random Forest sur l'ensemble de test. Nous utiliserons plusieurs m\u00e9triques :\n- Pr\u00e9cision globale (accuracy)\n- Matrice de confusion\n- Rapport de classification d\u00e9taill\u00e9 (pr\u00e9cision, rappel, F1-score pour chaque classe)\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-11-code-evaluation-et-metriques","title":"Cellule 11 (Code) - \u00c9valuation et m\u00e9triques","text":"<pre><code>print(\"\\n--- \u00c9valuation du mod\u00e8le Random Forest ---\")\n\n# Pr\u00e9dictions sur l'ensemble de test\ny_pred = rf_model.predict(X_test_features)\n\n# Calcul des m\u00e9triques\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(f\"Pr\u00e9cision globale: {accuracy*100:.2f}%\")\nprint(\"\\nMatrice de confusion:\")\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Pr\u00e9dictions')\nplt.ylabel('Valeurs r\u00e9elles')\nplt.title('Matrice de confusion')\nplt.show()\n\nprint(\"\\nRapport de classification:\")\nprint(class_report)\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-12-markdown-analyse-des-erreurs","title":"Cellule 12 (Markdown) - Analyse des erreurs","text":"<pre><code>## Analyse des erreurs\n\nExaminons quelques exemples que notre mod\u00e8le a mal classifi\u00e9s pour mieux comprendre ses limites.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-13-code-visualisation-des-erreurs","title":"Cellule 13 (Code) - Visualisation des erreurs","text":"<pre><code>print(\"\\n--- Analyse des erreurs ---\")\n\n# Identifier les erreurs\nerror_indices = np.where(y_pred != y_test)[0]\nn_errors = min(10, len(error_indices))  # Afficher max 10 erreurs\n\nif n_errors &gt; 0:\n    plt.figure(figsize=(12, 4))\n    for i, idx in enumerate(error_indices[:n_errors]):\n        plt.subplot(2, 5, i + 1)\n        # R\u00e9cup\u00e9rer l'image originale\n        img = X_test[idx].reshape(28, 28)\n        plt.imshow(img, cmap='gray')\n        plt.title(f\"R\u00e9el: {y_test[idx]}\\nPr\u00e9dit: {y_pred[idx]}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.suptitle(\"Exemples d'erreurs de classification\", y=1.05)\n    plt.show()\nelse:\n    print(\"Aucune erreur trouv\u00e9e dans l'\u00e9chantillon de test!\")\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-14-markdown-importance-des-caracteristiques","title":"Cellule 14 (Markdown) - Importance des caract\u00e9ristiques","text":"<pre><code>## Importance des caract\u00e9ristiques\n\nUn avantage des mod\u00e8les comme Random Forest est leur interpr\u00e9tabilit\u00e9. Nous pouvons examiner quelles caract\u00e9ristiques (ici, quelles composantes principales) le mod\u00e8le consid\u00e8re comme les plus importantes pour faire ses pr\u00e9dictions.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-15-code-visualisation-de-limportance-des-caracteristiques","title":"Cellule 15 (Code) - Visualisation de l'importance des caract\u00e9ristiques","text":"<pre><code>print(\"\\n--- Importance des caract\u00e9ristiques ---\")\n# Visualiser l'importance des composantes principales\nfeature_importance = rf_model.feature_importances_\nsorted_idx = np.argsort(feature_importance)[::-1]\n\nplt.figure(figsize=(10, 5))\nplt.bar(range(20), feature_importance[sorted_idx[:20]])\nplt.xticks(range(20), [f\"Feat {i}\" for i in sorted_idx[:20]], rotation=90)\nplt.xlabel('Composantes principales')\nplt.ylabel('Importance')\nplt.title('Top 20 des composantes principales les plus importantes')\nplt.tight_layout()\nplt.show()\n\nprint(\"Les caract\u00e9ristiques les plus importantes sont les composantes principales qui capturent le plus de variance dans les donn\u00e9es.\")\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-16-markdown-test-de-generalisation","title":"Cellule 16 (Markdown) - Test de g\u00e9n\u00e9ralisation","text":"<pre><code>## Test de g\u00e9n\u00e9ralisation\n\nUne question fondamentale en Machine Learning est : \"Comment le mod\u00e8le se comporte-t-il face \u00e0 des donn\u00e9es l\u00e9g\u00e8rement diff\u00e9rentes de celles sur lesquelles il a \u00e9t\u00e9 entra\u00een\u00e9 ?\"\n\nTestons la robustesse de notre mod\u00e8le face \u00e0 deux types de perturbations :\n1. Ajout de bruit al\u00e9atoire\n2. Rotation des images\n\nCes tests simulent des conditions plus r\u00e9alistes o\u00f9 les donn\u00e9es peuvent varier l\u00e9g\u00e8rement.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-17-code-creation-de-donnees-modifiees","title":"Cellule 17 (Code) - Cr\u00e9ation de donn\u00e9es modifi\u00e9es","text":"<pre><code>print(\"\\n--- D\u00e9fi de g\u00e9n\u00e9ralisation ---\")\nprint(\"Nous allons maintenant tester le mod\u00e8le sur des chiffres l\u00e9g\u00e8rement modifi\u00e9s pour \u00e9valuer sa capacit\u00e9 de g\u00e9n\u00e9ralisation.\")\n\n# Fonction pour ajouter du bruit aux images\ndef add_noise(images, noise_level=0.2):\n    noisy_images = images.copy()\n    noise = np.random.normal(0, noise_level, images.shape)\n    noisy_images = noisy_images + noise\n    # Assurer que les valeurs restent entre 0 et 1\n    noisy_images = np.clip(noisy_images, 0, 1)\n    return noisy_images\n\n# Fonction pour appliquer une rotation aux images\ndef rotate_images(images, max_angle=15):\n    from scipy.ndimage import rotate\n    rotated_images = np.zeros_like(images)\n    for i, img in enumerate(images):\n        angle = np.random.uniform(-max_angle, max_angle)\n        img_2d = img.reshape(28, 28)\n        rotated = rotate(img_2d, angle, reshape=False)\n        rotated_images[i] = rotated.flatten()\n    return rotated_images\n\n# Cr\u00e9er un jeu de donn\u00e9es modifi\u00e9\nprint(\"Cr\u00e9ation d'un jeu de donn\u00e9es avec des chiffres modifi\u00e9s...\")\n\n# Utiliser la partie restante des donn\u00e9es pour ce test\nX_new = X[10000:12000]\ny_new = y[10000:12000]\n\n# Appliquer des transformations\nX_new_noisy = add_noise(X_new, noise_level=0.2)\nX_new_rotated = rotate_images(X_new, max_angle=15)\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-18-code-visualisation-des-donnees-modifiees","title":"Cellule 18 (Code) - Visualisation des donn\u00e9es modifi\u00e9es","text":"<pre><code># Visualiser quelques exemples\nplt.figure(figsize=(12, 8))\nfor i in range(5):\n    # Original\n    plt.subplot(3, 5, i + 1)\n    plt.imshow(X_new[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"Original: {y_new[i]}\")\n    plt.axis('off')\n\n    # Avec bruit\n    plt.subplot(3, 5, i + 6)\n    plt.imshow(X_new_noisy[i].reshape(28, 28), cmap='gray')\n    plt.title(\"Avec bruit\")\n    plt.axis('off')\n\n    # Avec rotation\n    plt.subplot(3, 5, i + 11)\n    plt.imshow(X_new_rotated[i].reshape(28, 28), cmap='gray')\n    plt.title(\"Avec rotation\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.suptitle(\"Exemples de chiffres modifi\u00e9s\", y=1.02)\nplt.show()\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-19-code-evaluation-sur-donnees-modifiees","title":"Cellule 19 (Code) - \u00c9valuation sur donn\u00e9es modifi\u00e9es","text":"<pre><code># \u00c9valuer le mod\u00e8le sur les donn\u00e9es modifi\u00e9es\nprint(\"\\n\u00c9valuation sur les donn\u00e9es avec bruit:\")\nX_new_noisy_features = feature_pipeline.transform(X_new_noisy)\ny_new_noisy_pred = rf_model.predict(X_new_noisy_features)\naccuracy_noisy = accuracy_score(y_new, y_new_noisy_pred)\nprint(f\"Pr\u00e9cision sur donn\u00e9es bruit\u00e9es: {accuracy_noisy*100:.2f}%\")\n\nprint(\"\\n\u00c9valuation sur les donn\u00e9es avec rotation:\")\nX_new_rotated_features = feature_pipeline.transform(X_new_rotated)\ny_new_rotated_pred = rf_model.predict(X_new_rotated_features)\naccuracy_rotated = accuracy_score(y_new, y_new_rotated_pred)\nprint(f\"Pr\u00e9cision sur donn\u00e9es pivot\u00e9es: {accuracy_rotated*100:.2f}%\")\n\nprint(\"\\nComparaison avec la pr\u00e9cision originale:\")\nprint(f\"Pr\u00e9cision sur donn\u00e9es originales: {accuracy*100:.2f}%\")\nprint(f\"Pr\u00e9cision sur donn\u00e9es bruit\u00e9es: {accuracy_noisy*100:.2f}%\")\nprint(f\"Pr\u00e9cision sur donn\u00e9es pivot\u00e9es: {accuracy_rotated*100:.2f}%\")\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-20-markdown-conclusions","title":"Cellule 20 (Markdown) - Conclusions","text":"<pre><code>## Conclusions sur le Machine Learning classique\n\nApr\u00e8s avoir impl\u00e9ment\u00e9 et test\u00e9 notre mod\u00e8le Random Forest pour la classification des chiffres manuscrits, nous pouvons tirer plusieurs conclusions :\n\n### Points forts du Random Forest:\n- Entra\u00eenement relativement rapide\n- Bonnes performances sur les donn\u00e9es originales\n- Interpr\u00e9tabilit\u00e9 (importance des caract\u00e9ristiques)\n\n### Limites:\n- N\u00e9cessite une extraction manuelle de caract\u00e9ristiques (PCA dans notre cas)\n- Sensibilit\u00e9 aux transformations des donn\u00e9es (bruit, rotation)\n- Difficult\u00e9 \u00e0 capturer des motifs complexes sans feature engineering appropri\u00e9\n\n### Questions pour la r\u00e9flexion:\n1. Pourquoi avons-nous besoin de r\u00e9duire la dimensionnalit\u00e9 pour le Random Forest?\n2. Comment pourrait-on am\u00e9liorer la robustesse aux transformations?\n3. Quelles autres caract\u00e9ristiques pourraient \u00eatre extraites manuellement pour am\u00e9liorer les performances?\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-21-markdown-widget-interactif","title":"Cellule 21 (Markdown) - Widget interactif","text":"<pre><code>## Tester le mod\u00e8le vous-m\u00eame\n\nUtilisez le widget ci-dessous pour tester le mod\u00e8le sur diff\u00e9rents exemples. Vous pourrez voir l'image et la pr\u00e9diction correspondante.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-22-code-widget-interactif","title":"Cellule 22 (Code) - Widget interactif","text":"<pre><code>print(\"\\n--- Testez le mod\u00e8le vous-m\u00eame ---\")\n\ndef test_model(digit_idx):\n    if digit_idx &lt; len(X_test):\n        # Afficher l'image\n        img = X_test[digit_idx].reshape(28, 28)\n        plt.figure(figsize=(6, 6))\n        plt.imshow(img, cmap='gray')\n        plt.title(f\"Chiffre \u00e0 classifier\")\n        plt.axis('off')\n        plt.show()\n\n        # Faire la pr\u00e9diction\n        features = feature_pipeline.transform([X_test[digit_idx]])\n        prediction = rf_model.predict(features)[0]\n        real_label = y_test[digit_idx]\n\n        print(f\"Pr\u00e9diction du mod\u00e8le Random Forest: {prediction}\")\n        print(f\"\u00c9tiquette r\u00e9elle: {real_label}\")\n        print(f\"Pr\u00e9diction {'correcte' if prediction == real_label else 'incorrecte'}\")\n    else:\n        print(\"Index hors limites!\")\n\n# Cr\u00e9er un slider pour s\u00e9lectionner un chiffre \u00e0 tester\ndigit_selector = widgets.IntSlider(\n    value=0,\n    min=0,\n    max=len(X_test)-1,\n    step=1,\n    description='Index:',\n    continuous_update=False\n)\n\n# Bouton pour ex\u00e9cuter le test\ntest_button = widgets.Button(description=\"Tester\")\noutput = widgets.Output()\n\ndef on_button_clicked(b):\n    with output:\n        output.clear_output()\n        test_model(digit_selector.value)\n\ntest_button.on_click(on_button_clicked)\n\n# Afficher les widgets\ndisplay(widgets.HBox([digit_selector, test_button]))\ndisplay(output)\n\nprint(\"Utilisez le slider pour s\u00e9lectionner un index et cliquez sur 'Tester' pour classifier le chiffre correspondant.\")\n</code></pre>"},{"location":"module1/ressources/schema-a-completer/","title":"Sch\u00e9ma conceptuel \u00e0 compl\u00e9ter","text":""},{"location":"module1/ressources/schema-a-completer/#instructions","title":"Instructions","text":"<p>Le sch\u00e9ma ci-dessous repr\u00e9sente l'architecture et le fonctionnement d'un r\u00e9seau de neurones. Compl\u00e9tez-le en pla\u00e7ant les \u00e9tiquettes appropri\u00e9es aux emplacements num\u00e9rot\u00e9s.</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502             \u2502\n                    \u2502      1      \u2502\n                    \u2502             \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502                           \u2502\n            \u2502             2             \u2502\n            \u2502                           \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502                                   \u2502\n      \u2502               3                   \u2502\n      \u2502                                   \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502                           \u2502\n            \u2502             4             \u2502\n            \u2502                           \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502             \u2502\n                    \u2502      5      \u2502\n                    \u2502             \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502             \u2502\n                    \u2502      6      \u2502\n                    \u2502             \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u25b2\n                          \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502             \u2502\n                    \u2502      7      \u2502\n                    \u2502             \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"module1/ressources/schema-a-completer/#elements-a-placer","title":"\u00c9l\u00e9ments \u00e0 placer","text":"<p>Choisissez dans la liste suivante les \u00e9l\u00e9ments \u00e0 placer aux emplacements num\u00e9rot\u00e9s :</p> <ol> <li>Couche d'entr\u00e9e (Input Layer)</li> <li>Premi\u00e8re couche cach\u00e9e (Hidden Layer 1)</li> <li>Deuxi\u00e8me couche cach\u00e9e (Hidden Layer 2)</li> <li>Couche de sortie (Output Layer)</li> <li>Pr\u00e9diction (Prediction)</li> <li>Calcul de l'erreur (Loss Calculation)</li> <li>Donn\u00e9es r\u00e9elles (Ground Truth)</li> <li>Forward Propagation</li> <li>Backward Propagation</li> <li>Fonction d'activation</li> <li>Ajustement des poids (Weight Update)</li> <li>Preprocessing des donn\u00e9es</li> </ol>"},{"location":"module1/ressources/schema-a-completer/#structure-du-reseau","title":"Structure du r\u00e9seau","text":"<p>En plus de compl\u00e9ter le sch\u00e9ma, indiquez :</p> <ol> <li> <p>Quel type de r\u00e9seau de neurones est repr\u00e9sent\u00e9 ici ? _____</p> </li> <li> <p>Combien de neurones pourrait contenir chaque couche pour un probl\u00e8me de reconnaissance de chiffres manuscrits (MNIST) ?</p> </li> <li>Couche d'entr\u00e9e : _</li> <li>Premi\u00e8re couche cach\u00e9e : _</li> <li>Deuxi\u00e8me couche cach\u00e9e : _</li> <li> <p>Couche de sortie : _</p> </li> <li> <p>Quelle fonction d'activation serait appropri\u00e9e pour :</p> </li> <li>Les couches cach\u00e9es : _____</li> <li>La couche de sortie : _____</li> </ol>"},{"location":"module1/ressources/schema-a-completer/#processus-dapprentissage","title":"Processus d'apprentissage","text":"<p>D\u00e9crivez bri\u00e8vement les \u00e9tapes du processus d'apprentissage en utilisant les termes appropri\u00e9s :</p>"},{"location":"module1/ressources/schema-a-completer/#rendu","title":"Rendu","text":"<p>Une fois compl\u00e9t\u00e9, incluez ce sch\u00e9ma dans votre synth\u00e8se personnelle en expliquant comment ces diff\u00e9rentes composantes interagissent pour permettre l'apprentissage du r\u00e9seau.</p>"},{"location":"module2/","title":"Module 2 : Architectures sp\u00e9cialis\u00e9es de r\u00e9seaux de neurones","text":""},{"location":"module2/#objectifs-du-module","title":"Objectifs du module","text":"<p>\u00c0 l'issue de ce module, vous serez capable de :</p> <ul> <li>Comprendre et impl\u00e9menter des r\u00e9seaux de neurones convolutifs (CNN) pour la vision par ordinateur</li> <li>Ma\u00eetriser les r\u00e9seaux r\u00e9currents (RNN/LSTM) pour le traitement des s\u00e9quences et du langage</li> <li>Visualiser et interpr\u00e9ter le fonctionnement interne des diff\u00e9rentes architectures</li> <li>Int\u00e9grer ces mod\u00e8les sp\u00e9cialis\u00e9s dans des applications concr\u00e8tes</li> <li>Comparer et choisir l'architecture adapt\u00e9e \u00e0 diff\u00e9rents probl\u00e8mes</li> </ul>"},{"location":"module2/#programme-4h","title":"Programme (4h)","text":"<p>Ce module explore les architectures sp\u00e9cialis\u00e9es de r\u00e9seaux de neurones \u00e0 travers trois phases compl\u00e9mentaires.</p>"},{"location":"module2/#phase-1-mini-projet-cnn-pour-la-vision-par-ordinateur-1h30","title":"Phase 1 : Mini-projet CNN pour la vision par ordinateur (1h30)","text":"<p>Plongez dans l'univers des r\u00e9seaux convolutifs et apprenez \u00e0 les utiliser pour la classification d'images.</p> <ul> <li>Principes des convolutions et du pooling</li> <li>Impl\u00e9mentation d'un CNN avec TensorFlow/Keras</li> <li>Visualisation des filtres et feature maps</li> <li>Int\u00e9gration dans une application web simple</li> </ul>"},{"location":"module2/#phase-2-mini-projet-rnn-pour-le-traitement-du-langage-1h30","title":"Phase 2 : Mini-projet RNN pour le traitement du langage (1h30)","text":"<p>D\u00e9couvrez comment les r\u00e9seaux r\u00e9currents permettent de traiter des donn\u00e9es s\u00e9quentielles comme le texte.</p> <ul> <li>Principes fondamentaux des r\u00e9seaux r\u00e9currents</li> <li>Cellules LSTM pour la m\u00e9moire \u00e0 long terme</li> <li>Impl\u00e9mentation d'un mod\u00e8le d'analyse de sentiment</li> <li>Exp\u00e9rimentation avec l'API Mistral AI pour le NLP</li> </ul>"},{"location":"module2/#phase-3-challenge-damelioration-de-modele-45min","title":"Phase 3 : Challenge d'am\u00e9lioration de mod\u00e8le (45min)","text":"<p>Travaillez en \u00e9quipe pour diagnostiquer et am\u00e9liorer un mod\u00e8le sous-optimal.</p> <ul> <li>Diagnostic des faiblesses d'un mod\u00e8le existant</li> <li>Exp\u00e9rimentation avec diff\u00e9rentes architectures</li> <li>Techniques d'optimisation des hyperparam\u00e8tres</li> <li>Mesure et analyse comparative des am\u00e9liorations</li> </ul>"},{"location":"module2/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Avoir suivi le Module 1 (Fondamentaux du Deep Learning)</li> <li>Comprendre les bases des r\u00e9seaux de neurones artificiels</li> <li>Conna\u00eetre les concepts de base de Python et TensorFlow/Keras</li> </ul>"},{"location":"module2/#livrables-attendus","title":"Livrables attendus","text":"<p>\u00c0 l'issue de ce module, vous devrez produire :</p> <ol> <li>Un mod\u00e8le CNN fonctionnel pour la classification d'images</li> <li>Un rapport d'analyse des features maps et filtres de convolution</li> <li>Un mod\u00e8le RNN/LSTM pour l'analyse de sentiment textuel</li> <li>Un rapport d'am\u00e9lioration documentant vos exp\u00e9rimentations sur le mod\u00e8le sous-optimal</li> </ol>"},{"location":"module2/#competences-bts-sio-developpees","title":"Comp\u00e9tences BTS SIO d\u00e9velopp\u00e9es","text":"Comp\u00e9tence Description Activit\u00e9s associ\u00e9es B1.3 Gestion des donn\u00e9es Manipulation des datasets d'images et de texte B2.2 Conception de solutions applicatives Conception d'architectures CNN et RNN adapt\u00e9es B2.3 D\u00e9veloppement Impl\u00e9mentation et optimisation des mod\u00e8les B3.2 V\u00e9rification et validation Analyse des performances des mod\u00e8les"},{"location":"module2/#pret-a-explorer-les-architectures-specialisees","title":"Pr\u00eat \u00e0 explorer les architectures sp\u00e9cialis\u00e9es ?","text":"<p>Commencez par d\u00e9couvrir les r\u00e9seaux convolutifs (CNN) et leur application \u00e0 la vision par ordinateur.</p> <p>Commencer la Phase 1: CNN</p>"},{"location":"module2/qcm-evaluation-module2/","title":"QCM d'auto-\u00e9valuation - Module 2 : Architectures sp\u00e9cialis\u00e9es","text":"<p>Ce QCM vous permettra d'\u00e9valuer votre compr\u00e9hension des r\u00e9seaux convolutifs (CNN) et r\u00e9currents (RNN) \u00e9tudi\u00e9s dans ce module.</p>"},{"location":"module2/qcm-evaluation-module2/#instructions","title":"Instructions","text":"<ul> <li>Cochez la ou les r\u00e9ponses correctes pour chaque question</li> <li>Certaines questions peuvent avoir plusieurs r\u00e9ponses correctes</li> <li>\u00c0 la fin du questionnaire, calculez votre score gr\u00e2ce au corrig\u00e9 fourni</li> <li>Dur\u00e9e recommand\u00e9e : 15 minutes</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#partie-a-reseaux-convolutifs-cnn","title":"Partie A : R\u00e9seaux Convolutifs (CNN)","text":""},{"location":"module2/qcm-evaluation-module2/#1-dans-un-reseau-convolutif-a-quoi-sert-principalement-loperation-de-convolution","title":"1. Dans un r\u00e9seau convolutif, \u00e0 quoi sert principalement l'op\u00e9ration de convolution ?","text":"<ul> <li> \u00c0 r\u00e9duire la dimension des donn\u00e9es</li> <li> \u00c0 d\u00e9tecter des caract\u00e9ristiques locales dans les donn\u00e9es d'entr\u00e9e</li> <li> \u00c0 connecter tous les neurones entre eux</li> <li> \u00c0 acc\u00e9l\u00e9rer le temps d'entra\u00eenement</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#2-quest-ce-quun-filtre-ou-noyau-dans-un-cnn","title":"2. Qu'est-ce qu'un filtre (ou noyau) dans un CNN ?","text":"<ul> <li> Une fonction qui supprime les pixels ind\u00e9sirables de l'image</li> <li> Une matrice de poids qui s'applique localement sur les donn\u00e9es d'entr\u00e9e</li> <li> Un seuil qui \u00e9limine les valeurs en dessous d'un certain niveau</li> <li> Une technique pour s\u00e9lectionner les meilleures images d'entra\u00eenement</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#3-quel-est-le-role-principal-de-loperation-de-pooling-dans-un-cnn","title":"3. Quel est le r\u00f4le principal de l'op\u00e9ration de pooling dans un CNN ?","text":"<ul> <li> Augmenter la taille des feature maps</li> <li> R\u00e9duire la dimensionnalit\u00e9 tout en pr\u00e9servant les informations importantes</li> <li> Ajouter de la non-lin\u00e9arit\u00e9 au r\u00e9seau</li> <li> Connecter les diff\u00e9rentes couches de convolution</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#4-quels-sont-les-avantages-des-cnn-pour-le-traitement-dimages-plusieurs-reponses-possibles","title":"4. Quels sont les avantages des CNN pour le traitement d'images ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Partage des param\u00e8tres entre diff\u00e9rentes positions spatiales</li> <li> Invariance \u00e0 la translation</li> <li> R\u00e9duction significative du nombre de param\u00e8tres par rapport aux r\u00e9seaux enti\u00e8rement connect\u00e9s</li> <li> Capacit\u00e9 \u00e0 traiter des images de n'importe quelle taille sans redimensionnement</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#5-dans-quelle-couche-dun-cnn-typique-se-trouvent-generalement-le-plus-grand-nombre-de-parametres","title":"5. Dans quelle couche d'un CNN typique se trouvent g\u00e9n\u00e9ralement le plus grand nombre de param\u00e8tres ?","text":"<ul> <li> Couches de convolution</li> <li> Couches de pooling</li> <li> Couches enti\u00e8rement connect\u00e9es (fully connected)</li> <li> Couches de normalisation par lots (batch normalization)</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#6-quest-ce-quune-feature-map-dans-un-cnn","title":"6. Qu'est-ce qu'une feature map dans un CNN ?","text":"<ul> <li> Une carte qui indique les r\u00e9gions d'int\u00e9r\u00eat dans l'image originale</li> <li> Le r\u00e9sultat de l'application d'un filtre de convolution sur une entr\u00e9e</li> <li> Un graphique montrant la progression de l'entra\u00eenement</li> <li> La liste des caract\u00e9ristiques extraites manuellement avant l'entra\u00eenement</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#7-comment-evoluent-les-caracteristiques-detectees-a-mesure-quon-avance-dans-les-couches-dun-cnn","title":"7. Comment \u00e9voluent les caract\u00e9ristiques d\u00e9tect\u00e9es \u00e0 mesure qu'on avance dans les couches d'un CNN ?","text":"<ul> <li> Elles deviennent de plus en plus simples et \u00e9l\u00e9mentaires</li> <li> Elles restent de m\u00eame nature mais deviennent plus pr\u00e9cises</li> <li> Elles deviennent de plus en plus abstraites et complexes</li> <li> Elles concernent des r\u00e9gions de plus en plus petites de l'image</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#partie-b-reseaux-recurrents-rnn","title":"Partie B : R\u00e9seaux R\u00e9currents (RNN)","text":""},{"location":"module2/qcm-evaluation-module2/#8-quelle-est-la-principale-caracteristique-des-reseaux-de-neurones-recurrents-rnn","title":"8. Quelle est la principale caract\u00e9ristique des r\u00e9seaux de neurones r\u00e9currents (RNN) ?","text":"<ul> <li> Ils utilisent des op\u00e9rations de convolution pour traiter les donn\u00e9es</li> <li> Ils contiennent des connexions formant des boucles permettant de m\u00e9moriser les informations</li> <li> Ils traitent chaque \u00e9l\u00e9ment d'une s\u00e9quence de mani\u00e8re compl\u00e8tement ind\u00e9pendante</li> <li> Ils sont sp\u00e9cialis\u00e9s dans le traitement d'images</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#9-pour-quels-types-de-donnees-les-rnn-sont-ils-particulierement-adaptes","title":"9. Pour quels types de donn\u00e9es les RNN sont-ils particuli\u00e8rement adapt\u00e9s ?","text":"<ul> <li> Images 2D</li> <li> Donn\u00e9es tabulaires (comme des tableaux Excel)</li> <li> Donn\u00e9es s\u00e9quentielles (texte, s\u00e9ries temporelles, audio)</li> <li> Nuages de points 3D</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#10-quel-probleme-majeur-affecte-les-rnn-classiques-lors-du-traitement-de-sequences-longues","title":"10. Quel probl\u00e8me majeur affecte les RNN classiques lors du traitement de s\u00e9quences longues ?","text":"<ul> <li> Surconsommation de m\u00e9moire</li> <li> Temps de traitement exponentiel</li> <li> Probl\u00e8me de disparition ou d'explosion du gradient</li> <li> Incapacit\u00e9 \u00e0 parall\u00e9liser les calculs</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#11-quelle-est-la-principale-innovation-des-cellules-lstm-par-rapport-aux-rnn-classiques","title":"11. Quelle est la principale innovation des cellules LSTM par rapport aux RNN classiques ?","text":"<ul> <li> Elles utilisent des op\u00e9rations de convolution</li> <li> Elles poss\u00e8dent des m\u00e9canismes de portes contr\u00f4lant le flux d'information</li> <li> Elles peuvent traiter plusieurs s\u00e9quences en parall\u00e8le</li> <li> Elles ne n\u00e9cessitent pas d'entra\u00eenement</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#12-dans-un-reseau-lstm-a-quoi-sert-la-porte-doubli-forget-gate","title":"12. Dans un r\u00e9seau LSTM, \u00e0 quoi sert la \"porte d'oubli\" (forget gate) ?","text":"<ul> <li> \u00c0 d\u00e9terminer quelles informations de l'\u00e9tat pr\u00e9c\u00e9dent doivent \u00eatre conserv\u00e9es ou supprim\u00e9es</li> <li> \u00c0 r\u00e9initialiser compl\u00e8tement le r\u00e9seau quand la s\u00e9quence est trop longue</li> <li> \u00c0 sauter certaines \u00e9tapes de calcul pour acc\u00e9l\u00e9rer le traitement</li> <li> \u00c0 ignorer les donn\u00e9es d'entr\u00e9e corrompues ou bruit\u00e9es</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#13-quelles-applications-typiques-utilisent-les-rnnlstm-plusieurs-reponses-possibles","title":"13. Quelles applications typiques utilisent les RNN/LSTM ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Reconnaissance de caract\u00e8res manuscrits</li> <li> Traduction automatique</li> <li> Pr\u00e9diction de s\u00e9ries temporelles</li> <li> G\u00e9n\u00e9ration de texte</li> <li> Segmentation d'images</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#14-quest-ce-qui-differencie-principalement-les-gru-gated-recurrent-units-des-lstm","title":"14. Qu'est-ce qui diff\u00e9rencie principalement les GRU (Gated Recurrent Units) des LSTM ?","text":"<ul> <li> Les GRU n'ont aucune forme de m\u00e9moire</li> <li> Les GRU ont une architecture plus simple avec moins de portes</li> <li> Les GRU sont sp\u00e9cifiquement con\u00e7us pour les donn\u00e9es non s\u00e9quentielles</li> <li> Les GRU ne peuvent pas \u00eatre entra\u00een\u00e9s par r\u00e9tropropagation</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#partie-c-integration-pratique","title":"Partie C : Int\u00e9gration pratique","text":""},{"location":"module2/qcm-evaluation-module2/#15-lors-de-lintegration-dun-modele-cnn-dans-une-application-web-quelles-considerations-sont-importantes-plusieurs-reponses-possibles","title":"15. Lors de l'int\u00e9gration d'un mod\u00e8le CNN dans une application web, quelles consid\u00e9rations sont importantes ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Pr\u00e9traitement des images c\u00f4t\u00e9 client avant envoi au serveur</li> <li> Optimisation de la taille du mod\u00e8le pour r\u00e9duire les temps de chargement</li> <li> Gestion des erreurs en cas de pr\u00e9dictions incertaines</li> <li> Compatibilit\u00e9 avec diff\u00e9rents navigateurs web</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#16-dans-notre-experience-avec-lapi-mistral-ai-quels-avantages-avons-nous-observes-par-rapport-aux-modeles-rnn-que-nous-avons-implementes-plusieurs-reponses-possibles","title":"16. Dans notre exp\u00e9rience avec l'API Mistral AI, quels avantages avons-nous observ\u00e9s par rapport aux mod\u00e8les RNN que nous avons impl\u00e9ment\u00e9s ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Meilleure compr\u00e9hension du contexte \u00e0 long terme</li> <li> Capacit\u00e9 \u00e0 g\u00e9n\u00e9rer des r\u00e9ponses plus coh\u00e9rentes et naturelles</li> <li> Temps d'entra\u00eenement r\u00e9duit</li> <li> Facilit\u00e9 d'utilisation sans besoin d'expertise en Deep Learning</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#auto-evaluation","title":"Auto-\u00e9valuation","text":"<p>Une fois le QCM compl\u00e9t\u00e9, v\u00e9rifiez vos r\u00e9ponses avec le corrig\u00e9 ci-dessous.</p>"},{"location":"module2/qcm-evaluation-module2/#corrige","title":"Corrig\u00e9","text":"<ol> <li>b</li> <li>b</li> <li>b</li> <li>a, b, c</li> <li>c</li> <li>b</li> <li>c</li> <li>b</li> <li>c</li> <li>c</li> <li>b</li> <li>a</li> <li>b, c, d</li> <li>b</li> <li>a, b, c, d</li> <li>a, b, d</li> </ol>"},{"location":"module2/qcm-evaluation-module2/#calcul-de-votre-score","title":"Calcul de votre score","text":"<p>Comptez 1 point par r\u00e9ponse correcte. Pour les questions \u00e0 choix multiples, comptez 1 point uniquement si toutes vos s\u00e9lections sont correctes.</p> <p>Total des points possibles : 16</p>"},{"location":"module2/qcm-evaluation-module2/#interpretation","title":"Interpr\u00e9tation","text":"<ul> <li>13-16 points : Excellente ma\u00eetrise des architectures sp\u00e9cialis\u00e9es de Deep Learning</li> <li>10-12 points : Bonne compr\u00e9hension, quelques points \u00e0 clarifier</li> <li>7-9 points : Compr\u00e9hension de base, r\u00e9vision n\u00e9cessaire de certains concepts</li> <li>0-6 points : R\u00e9vision approfondie recommand\u00e9e avant de poursuivre</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#pour-approfondir","title":"Pour approfondir","text":"<p>Si vous avez obtenu moins de 13 points, nous vous recommandons de revoir les concepts sur lesquels vous avez fait des erreurs. Consultez les ressources suivantes : - Les notebooks CNN et RNN du module - Les visualisations des architectures CNN et RNN - La documentation des mini-projets pratiques r\u00e9alis\u00e9s</p>"},{"location":"module2/reseaux-convolutifs/","title":"Phase 1 : Mini-projet CNN pour la vision par ordinateur","text":""},{"location":"module2/reseaux-convolutifs/#objectifs-de-la-phase","title":"Objectifs de la phase","text":"<p>Dans cette phase, vous allez :</p> <ul> <li>Comprendre les principes fondamentaux des r\u00e9seaux de neurones convolutifs (CNN)</li> <li>Impl\u00e9menter un CNN pour la classification d'images avec TensorFlow/Keras</li> <li>Visualiser et interpr\u00e9ter les filtres et feature maps d'un CNN</li> <li>Int\u00e9grer un mod\u00e8le CNN dans une application web simple</li> </ul>"},{"location":"module2/reseaux-convolutifs/#partie-1-principes-des-cnn-20-min","title":"Partie 1: Principes des CNN (20 min)","text":""},{"location":"module2/reseaux-convolutifs/#defi-de-reflexion-initiale","title":"D\u00e9fi de r\u00e9flexion initiale","text":"<p>Avant de plonger dans les CNN, prenez 2 minutes pour r\u00e9fl\u00e9chir \u00e0 cette question :</p> <p>Question \u00e0 m\u00e9diter : Comment reconnaissez-vous un visage dans une photo, quelle que soit sa position ou l'\u00e9clairage ? Qu'est-ce qui rend cette t\u00e2che si facile pour vous et si difficile pour un ordinateur ?</p>"},{"location":"module2/reseaux-convolutifs/#activite-guidee-decouverte-de-larchitecture-cnn","title":"Activit\u00e9 guid\u00e9e : D\u00e9couverte de l'architecture CNN","text":"<p>\u00c9tape 1 : Observation (3 min) Examinez ces deux visualisations en parall\u00e8le :</p> <ul> <li>L'image originale d'un chiffre '7' manuscrit et son traitement par les diff\u00e9rentes couches d'un CNN</li> </ul> <p></p> <ul> <li>Les diff\u00e9rentes caract\u00e9ristiques extraites \u00e0 chaque niveau d'un CNN d\u00e9j\u00e0 entra\u00een\u00e9</li> </ul> <p></p> <p>\u00c9tape 2 : Mini-investigation (5 min) Formez des bin\u00f4mes et discutez :</p> <ul> <li>Quels types de d\u00e9tails la premi\u00e8re couche semble-t-elle rep\u00e9rer dans l'image?</li> <li>Comment ce que \"voit\" le r\u00e9seau change-t-il entre la premi\u00e8re et la derni\u00e8re couche?</li> <li>Pourquoi est-il utile pour le r\u00e9seau de transformer l'image \u00e0 chaque \u00e9tape?</li> </ul> <p>Les r\u00e9seaux de neurones convolutifs (CNN) offrent plusieurs avantages, notamment :</p> <ul> <li>Extraction automatique des caract\u00e9ristiques</li> </ul> <p>Contrairement aux m\u00e9thodes traditionnelles de vision par ordinateur qui n\u00e9cessitent une extraction manuelle des caract\u00e9ristiques, les CNN apprennent automatiquement les motifs pertinents (bords, textures, formes) \u00e0 partir des donn\u00e9es.</p> <ul> <li>Partage des poids et r\u00e9duction du nombre de param\u00e8tres </li> </ul> <p>Gr\u00e2ce aux filtres de convolution partag\u00e9s sur toute l'image, les CNN r\u00e9duisent consid\u00e9rablement le nombre de param\u00e8tres \u00e0 entra\u00eener, ce qui diminue les besoins en m\u00e9moire et en calcul par rapport aux r\u00e9seaux de neurones enti\u00e8rement connect\u00e9s.</p> <ul> <li>Invariance aux translations et robustesse aux variations</li> </ul> <p>Les couches de convolution et de pooling permettent aux CNN d'\u00eatre robustes aux d\u00e9calages, rotations et d\u00e9formations dans les images, ce qui am\u00e9liore leur capacit\u00e9 \u00e0 reconna\u00eetre des objets dans diff\u00e9rentes conditions.</p> <p>\u00c9tape 3 : Construction du mod\u00e8le mental (5 min) Sur votre feuille de travail, compl\u00e9tez le sch\u00e9ma simplifi\u00e9 d'un CNN :</p> <p></p> <ol> <li>Identifiez et nommez les trois types principaux de couches</li> <li>Pour chaque type, pr\u00e9cisez bri\u00e8vement sa fonction</li> <li>Listez les trois avantages majeurs des CNN</li> </ol> <p>\u00c9tape 4 : Analogie concr\u00e8te (3 min) Pour comprendre le fonctionnement d'un CNN, voyons comment il pourrait identifier un personnage c\u00e9l\u00e8bre comme Dark Vador :</p> <p></p> <ul> <li>La couche de convolution rep\u00e8re les caract\u00e9ristiques distinctives : \"Je d\u00e9tecte un casque noir, un respirateur, une cape...\"</li> <li>La couche de pooling ignore les d\u00e9tails non pertinents : \"Peu importe l'angle de vue, l'\u00e9clairage, s'il est de face ou de profil...\"</li> <li>La couche fully connected prend la d\u00e9cision finale : \"D'apr\u00e8s toutes ces caract\u00e9ristiques combin\u00e9es, c'est Dark Vador \u00e0 99.8%!\"</li> </ul> <p>Cette analogie montre comment un CNN analyse une image de mani\u00e8re hi\u00e9rarchique, comme notre cerveau le fait naturellement.</p>"},{"location":"module2/reseaux-convolutifs/#points-importants-a-retenir","title":"Points importants \u00e0 retenir","text":"<p>\u00c0 savoir avant de passer \u00e0 la pratique :</p> <ol> <li> <p>Les CNN sont con\u00e7us sp\u00e9cifiquement pour traiter les donn\u00e9es en grille comme les images.</p> </li> <li> <p>Les filtres de convolution agissent comme des d\u00e9tecteurs de motifs qui s'appliquent \u00e0 toute l'image.</p> </li> <li> <p>Le pooling permet de r\u00e9duire les dimensions tout en conservant l'information importante.</p> </li> <li> <p>Les poids du r\u00e9seau sont ajust\u00e9s automatiquement pendant l'entra\u00eenement.</p> </li> <li> <p>Un CNN profond permet de d\u00e9tecter des motifs de plus en plus complexes et abstraits.</p> </li> <li> <p>Le grand avantage des CNN est qu'ils apprennent automatiquement les caract\u00e9ristiques pertinentes, sans qu'on ait \u00e0 les programmer manuellement.</p> </li> </ol>"},{"location":"module2/reseaux-convolutifs/#transition-vers-limplementation","title":"Transition vers l'impl\u00e9mentation","text":"<p>Maintenant que vous avez conceptualis\u00e9 l'architecture d'un CNN, passons \u00e0 l'impl\u00e9mentation pratique pour voir ces concepts en action. Gardez votre sch\u00e9ma \u00e0 port\u00e9e de main - vous pourrez le compl\u00e9ter avec des observations pratiques.</p>"},{"location":"module2/reseaux-convolutifs/#partie-2-implementation-dun-cnn-pour-mnist-40-min","title":"Partie 2: Impl\u00e9mentation d'un CNN pour MNIST (40 min)","text":""},{"location":"module2/reseaux-convolutifs/#instructions","title":"Instructions","text":"<ol> <li>Ouvrez le notebook Jupyter cnn-classification dans Google Colab</li> <li>Suivez les instructions \u00e9tape par \u00e9tape pour impl\u00e9menter un CNN pour la classification des chiffres manuscrits (MNIST)</li> <li>Ex\u00e9cutez chaque cellule et observez les r\u00e9sultats</li> <li> <p>Portez une attention particuli\u00e8re aux sections suivantes :</p> </li> <li> <p>Architecture du mod\u00e8le CNN</p> </li> <li>Processus d'entra\u00eenement</li> <li>Visualisation des filtres et feature maps</li> <li>Analyse des performances et des erreurs</li> </ol>"},{"location":"module2/reseaux-convolutifs/#points-cles-a-explorer","title":"Points cl\u00e9s \u00e0 explorer","text":"<ul> <li>Comment les couches de convolution extraient-elles des caract\u00e9ristiques de plus en plus abstraites ?</li> <li>Quel est l'impact du nombre de filtres et de couches sur les performances ?</li> <li>Comment les feature maps r\u00e9v\u00e8lent-elles ce que \"voit\" le r\u00e9seau ?</li> <li>Quelles sont les limites du mod\u00e8le face \u00e0 des donn\u00e9es bruit\u00e9es ou d\u00e9form\u00e9es ?</li> </ul>"},{"location":"module2/reseaux-convolutifs/#partie-3-integration-dans-une-application-web-30-min","title":"Partie 3: Int\u00e9gration dans une application web (30 min)","text":"<p>Dans cette partie, vous allez d\u00e9couvrir comment int\u00e9grer un mod\u00e8le CNN pr\u00e9-entra\u00een\u00e9 dans une application web interactive.</p>"},{"location":"module2/reseaux-convolutifs/#mini-projet-reconnaissance-de-chiffres-manuscrits","title":"Mini-projet : Reconnaissance de chiffres manuscrits","text":""},{"location":"module2/reseaux-convolutifs/#contexte-professionnel","title":"Contexte professionnel","text":"<p>Vous \u00eates stagiaire dans une PME o\u00f9 les employ\u00e9s doivent r\u00e9guli\u00e8rement saisir manuellement des codes \u00e0 partir de documents papier (bons de commande, formulaires clients, etc.). Votre responsable informatique souhaite explorer des solutions d'automatisation et vous demande de tester une application de reconnaissance de chiffres manuscrits.</p>"},{"location":"module2/reseaux-convolutifs/#etape-1-preparation-de-lenvironnement-8-minutes","title":"\u00c9tape 1: Pr\u00e9paration de l'environnement (8 minutes)","text":"<p>Pour la partie web, vous aurez besoin d'un fichier <code>mnist_cnn_model.h5</code> contenant votre mod\u00e8le CNN entra\u00een\u00e9. Ce fichier doit \u00eatre g\u00e9n\u00e9r\u00e9 sur Google Colab en suivant ces \u00e9tapes:</p>"},{"location":"module2/reseaux-convolutifs/#generation-du-modele-sur-google-colab","title":"G\u00e9n\u00e9ration du mod\u00e8le sur Google Colab","text":"<ol> <li> <p>Coller dans un nouveau notebook Google Colab le code du fichier suivant :<code>create_model.py</code>.</p> </li> <li> <p>Ex\u00e9cutez la cellule en cliquant sur le bouton de lecture \u25b6\ufe0f \u00e0 gauche de la cellule, ou en appuyant sur Shift+Enter</p> </li> </ol>"},{"location":"module2/reseaux-convolutifs/#attendre-lentrainement-et-telecharger-le-modele","title":"Attendre l'entra\u00eenement et t\u00e9l\u00e9charger le mod\u00e8le","text":"<ol> <li>L'ex\u00e9cution durera environ 3-5 minutes sur Google Colab (qui utilise des GPU/TPU)</li> <li>Vous verrez la progression de l'entra\u00eenement pour chaque \u00e9poque</li> <li>\u00c0 la fin, votre navigateur d\u00e9marrera automatiquement le t\u00e9l\u00e9chargement du fichier <code>mnist_cnn_model.h5</code></li> <li>Enregistrez ce fichier dans le dossier de votre projet web</li> </ol>"},{"location":"module2/reseaux-convolutifs/#avantages-de-cette-approche","title":"Avantages de cette approche:","text":"<ul> <li>Aucune installation locale requise</li> <li>Utilisation gratuite des ressources GPU de Google</li> <li>Ex\u00e9cution plus rapide que sur un ordinateur standard</li> <li>Interface famili\u00e8re et intuitive</li> <li>Pas de probl\u00e8me d'installation ou de performance de TensorFlow sur sa machine locale.</li> </ul>"},{"location":"module2/reseaux-convolutifs/#etape-2-configuration-5-minutes","title":"\u00c9tape 2 : Configuration (5 minutes)","text":"<p>1.Pr\u00e9paration de l'environnement VS Code</p> <ul> <li>Ouvrez Visual Studio Code</li> <li>Cr\u00e9ez un nouveau dossier pour le projet: <code>File &gt; Open Folder</code> et cr\u00e9ez un dossier nomm\u00e9 <code>reconnaissance-chiffres</code></li> <li>Dans VS Code, cr\u00e9ez la structure de dossiers suivante via l'explorateur:<ul> <li>Cr\u00e9ez un dossier <code>templates</code></li> <li>Cr\u00e9ez un dossier <code>static</code></li> <li>Dans <code>static</code>, cr\u00e9ez les sous-dossiers <code>css</code> et <code>js</code>Problpro</li> </ul> </li> </ul> <p>2.\ud83d\udce5T\u00e9l\u00e9chargement des fichiers de l'application web</p> <p>T\u00e9l\u00e9chargez les fichiers suivants et placez-les dans les dossiers indiqu\u00e9s :</p> <ul> <li>code-app-web.zip </li> </ul> <p>\ud83d\udccc Clic droit sur le lien \u2192 \"Enregistrer le lien sous...\" pour t\u00e9l\u00e9charger chaque fichier.</p> <ol> <li>Structure des dossiers :</li> </ol> <p>*Assurez-vous que votre structure de dossiers est la suivante:</p> <p>votre_dossier_de_travail/ \u251c\u2500\u2500 mnist_cnn_model.h5      # Votre mod\u00e8le sauvegard\u00e9 ou le mod\u00e8le fourni \u251c\u2500\u2500 web-integration.py      # Script principal Flask \u251c\u2500\u2500 templates/ \u2502   \u2514\u2500\u2500 index.html \u2514\u2500\u2500 static/                 # Dossier pour CSS, JS, images \u251c\u2500\u2500 css/ \u2502   \u2514\u2500\u2500 style.css \u2514\u2500\u2500 js/ \u2514\u2500\u2500 app.js</p>"},{"location":"module2/reseaux-convolutifs/#etape-3-installation-et-lancement-5-minutes","title":"\u00c9tape 3 : Installation et lancement (5 minutes)","text":"<p>1.Cr\u00e9ation du mod\u00e8le via Google Colab</p> <ul> <li>Suivez les instructions pour cr\u00e9er le mod\u00e8le avec Google Colab (voir sections pr\u00e9c\u00e9dentes)</li> <li>Une fois le fichier <code>mnist_cnn_model.h5</code> t\u00e9l\u00e9charg\u00e9, d\u00e9placez-le dans le dossier racine de votre projet</li> </ul> <p>2.Ouverture du Terminal int\u00e9gr\u00e9 \u00e0 VS Code</p> <ul> <li>Dans VS Code, ouvrez un terminal en allant dans <code>Terminal &gt; New Terminal</code></li> <li>Vous verrez un terminal s'ouvrir en bas de la fen\u00eatre</li> </ul> <p>3.Installation des d\u00e9pendances</p> <ul> <li> <p>Dans le terminal VS Code, tapez la commande suivante: pip install flask tensorflow pillow numpy</p> </li> <li> <p>Attendez que l'installation se termine</p> </li> </ul> <p>4.Lancement de l'application</p> <ul> <li>Dans le m\u00eame terminal, tapez:   <pre><code>python web-integration.py\n</code></pre></li> <li>Vous devriez voir un message indiquant que l'application est en cours d'ex\u00e9cution</li> <li>VS Code peut vous proposer d'ouvrir le lien - cliquez dessus, ou</li> <li>Ouvrez votre navigateur et acc\u00e9dez \u00e0 http://localhost:5001</li> </ul>"},{"location":"module2/reseaux-convolutifs/#etape-4-tests-pratiques-10-minutes","title":"\u00c9tape 4 : Tests pratiques (10 minutes)","text":"<ol> <li> <p>Test avec dessins \u00e0 la souris</p> </li> <li> <p>Dans l'interface web, dessinez clairement un chiffre (de 0 \u00e0 9) dans la zone pr\u00e9vue</p> <ul> <li>Cliquez sur le bouton \"Pr\u00e9dire\"</li> </ul> </li> <li> <p>Notez la pr\u00e9diction et le niveau de confiance</p> </li> <li>R\u00e9p\u00e9tez ce processus avec 5 chiffres diff\u00e9rents</li> <li>Gardez une trace de vos r\u00e9sultats (tableau simple : chiffre r\u00e9el / pr\u00e9diction / confiance)</li> </ol> <p>2.Test avec image import\u00e9e</p> <ul> <li>Sur une feuille de papier, \u00e9crivez clairement un chiffre</li> <li>Prenez une photo de ce chiffre avec votre smartphone ou appareil photo</li> <li>Transf\u00e9rez l'image sur votre ordinateur (par email, cloud, c\u00e2ble USB, etc.)</li> <li>Dans l'application, cliquez sur \"Charger une image\"</li> <li>S\u00e9lectionnez l'image que vous venez de prendre</li> <li> <p>Observez la pr\u00e9diction et le niveau de confiance</p> </li> <li> <p>Test avec feature maps (optionnel)</p> </li> <li> <p>Cochez la case \"Visualiser les feature maps\"</p> </li> <li>Dessinez un nouveau chiffre et cliquez sur \"Pr\u00e9dire\"</li> <li>Observez les feature maps qui s'affichent (repr\u00e9sentations visuelles de ce que \"voit\" le r\u00e9seau)</li> </ul>"},{"location":"module2/reseaux-convolutifs/#etape-5-evaluation-et-rapport-10-minutes","title":"\u00c9tape 5 : \u00c9valuation et rapport (10 minutes)","text":"<p>1.Remplissage du formulaire d'\u00e9valuation</p> <ul> <li>Ouvrez le document evaluation fourni par votre formateur</li> <li>Remplissez les sections suivantes :</li> <li>Nombre de pr\u00e9dictions correctes/incorrectes</li> <li>Chiffres les mieux reconnus</li> <li>Chiffres les plus difficiles \u00e0 reconna\u00eetre</li> <li>Niveau de confiance moyen observ\u00e9</li> </ul> <p>2.Analyse critique</p> <ul> <li>Dans le formulaire, notez au moins 3 points forts de l'application</li> <li>Notez \u00e9galement au moins 3 limitations ou probl\u00e8mes rencontr\u00e9s</li> </ul> <p>3.Propositions d'am\u00e9lioration</p> <ul> <li>Proposez 2-3 id\u00e9es concr\u00e8tes pour am\u00e9liorer l'outil dans un contexte professionnel</li> <li>Exemple : \"Ajouter une fonction pour traiter plusieurs chiffres \u00e0 la fois\"</li> </ul> <p>4.Conclusion professionnelle</p> <ul> <li>R\u00e9digez une br\u00e8ve conclusion (2-3 phrases) sur l'utilit\u00e9 potentielle de cet outil dans l'entreprise</li> </ul>"},{"location":"module2/reseaux-convolutifs/#livrable-a-rendre","title":"Livrable \u00e0 rendre","text":"<p>\u00c0 la fin de la session (30 minutes), veuillez :</p> <ol> <li>Copier et Compl\u00e9ter enti\u00e8rement ce formulaire d'\u00e9valuation</li> <li>Enregistrer  le document  sous le nom \"Eval_CNN_NOM_Prenom.doc\"</li> <li>Partager votre \u00e9valuation avec l'enseignant sur l'espace de cours:</li> </ol> <p>IMPORTANT : La remise de ce document compl\u00e9t\u00e9 est obligatoire et fait partie de l'\u00e9valuation du mini-projet.</p> <p>Date limite de remise : \u00c0 la fin de la s\u00e9ance</p>"},{"location":"module2/reseaux-convolutifs/#pour-aller-plus-loin-si-vous-terminez-en-avance","title":"Pour aller plus loin (si vous terminez en avance)","text":"<p>Si vous avez termin\u00e9 avant la fin du temps imparti, vous pouvez explorer ces pistes : - Testez les limites du mod\u00e8le en dessinant des chiffres de diff\u00e9rentes tailles/styles - Observez comment le bruit ou les distorsions affectent la pr\u00e9cision - Essayez de comprendre le code source dans <code>web-integration.py</code> pour voir comment l'application fonctionne</p>"},{"location":"module2/reseaux-convolutifs/#ressources-complementaires","title":"Ressources compl\u00e9mentaires","text":"<ul> <li>Tutoriel TensorFlow sur les CNN - Guide officiel de TensorFlow sur l'impl\u00e9mentation des r\u00e9seaux de neurones convolutifs</li> <li>Visualisation de CNN (Distill.pub) - Article interactif sur la visualisation et l'interpr\u00e9tation des r\u00e9seaux convolutifs</li> <li>Documentation Flask - Documentation officielle du framework Flask pour le d\u00e9veloppement web</li> </ul> <p>Retour \u00e0 la vue d'ensemble du Module 2 Continuer vers la Phase 2: RNN</p>"},{"location":"module2/reseaux-recurrents/","title":"Phase 2 : Mini-projet RNN pour le traitement du langage","text":""},{"location":"module2/reseaux-recurrents/#objectifs-de-la-phase","title":"Objectifs de la phase","text":"<p>Dans cette phase, vous allez :</p> <ul> <li>Comprendre les principes des r\u00e9seaux r\u00e9currents (RNN) et de leurs variantes (LSTM, GRU)</li> <li>Impl\u00e9menter un mod\u00e8le LSTM pour l'analyse de sentiment</li> <li>Visualiser et interpr\u00e9ter le fonctionnement interne d'un RNN</li> <li>Exp\u00e9rimenter avec l'API Mistral AI pour la g\u00e9n\u00e9ration de texte</li> <li>\u00c9tablir les bases pour le projet de chatbot p\u00e9dagogique</li> </ul>"},{"location":"module2/reseaux-recurrents/#partie-1-principes-des-rnn-20-min","title":"Partie 1: Principes des RNN (20 min)","text":""},{"location":"module2/reseaux-recurrents/#architecture-et-fonctionnement-des-rnn","title":"Architecture et fonctionnement des RNN","text":"<p>Les r\u00e9seaux de neurones r\u00e9currents (RNN) sont sp\u00e9cialement con\u00e7us pour traiter des donn\u00e9es s\u00e9quentielles, comme du texte, des s\u00e9ries temporelles ou des signaux audio. Leur architecture inclut des connections r\u00e9currentes qui leur permettent de \"m\u00e9moriser\" les informations pr\u00e9c\u00e9dentes :</p> <ol> <li>Principe de base : contrairement aux r\u00e9seaux feed-forward, les RNN poss\u00e8dent des boucles de r\u00e9troaction</li> <li>M\u00e9moire \u00e0 court terme : chaque \u00e9tat cach\u00e9 d\u00e9pend de l'\u00e9tat pr\u00e9c\u00e9dent et de l'entr\u00e9e actuelle</li> <li>Probl\u00e8me de la disparition du gradient : difficult\u00e9 \u00e0 capturer les d\u00e9pendances \u00e0 long terme</li> <li>Architectures avanc\u00e9es : LSTM (Long Short-Term Memory) et GRU (Gated Recurrent Unit) qui r\u00e9solvent ce probl\u00e8me</li> </ol> <p>Avantages pour un d\u00e9veloppeur d'applications :</p> <ul> <li>Traitement de s\u00e9quences de longueur variable</li> <li>Capacit\u00e9 \u00e0 \"m\u00e9moriser\" des informations importantes</li> <li>Applications diverses : analyse de texte, traduction, g\u00e9n\u00e9ration de contenu</li> </ul>"},{"location":"module2/reseaux-recurrents/#partie-2-implementation-dun-lstm-pour-lanalyse-de-sentiment-40-min","title":"Partie 2: Impl\u00e9mentation d'un LSTM pour l'analyse de sentiment (40 min)","text":""},{"location":"module2/reseaux-recurrents/#instructions","title":"Instructions","text":"<ol> <li>Ouvrez le notebook Jupyter rnn-sequence.ipynb dans Google Colab</li> <li>Suivez les instructions \u00e9tape par \u00e9tape pour impl\u00e9menter un mod\u00e8le LSTM pour l'analyse de sentiment</li> <li>Ex\u00e9cutez chaque cellule et observez les r\u00e9sultats</li> <li>Portez une attention particuli\u00e8re aux sections suivantes :</li> <li>Pr\u00e9traitement du texte (tokenisation)</li> <li>Architecture du mod\u00e8le LSTM</li> <li>Visualisation des embeddings de mots</li> <li>Analyse des performances et des erreurs</li> </ol>"},{"location":"module2/reseaux-recurrents/#points-cles-a-explorer","title":"Points cl\u00e9s \u00e0 explorer","text":"<ul> <li>Comment le texte est-il transform\u00e9 en entr\u00e9es num\u00e9riques pour le r\u00e9seau ?</li> <li>Comment les cellules LSTM g\u00e8rent-elles l'information \u00e0 long terme ?</li> <li>Quelle est la diff\u00e9rence entre les embeddings de mots positifs et n\u00e9gatifs ?</li> <li>Comment le mod\u00e8le LSTM peut-il comprendre le contexte d'une phrase ?</li> <li>Quelles sont les limitations de cette approche pour l'analyse de sentiment ?</li> <li>Comment pourriez-vous am\u00e9liorer ce mod\u00e8le pour des t\u00e2ches plus complexes ?</li> </ul>"},{"location":"module2/reseaux-recurrents/#partie-3-integration-avec-lapi-mistral-ai-30-min","title":"Partie 3: Int\u00e9gration avec l'API Mistral AI (30 min)","text":""},{"location":"module2/reseaux-recurrents/#introduction-a-mistral-ai","title":"Introduction \u00e0 Mistral AI","text":"<p>Mistral AI est une plateforme avanc\u00e9e d'intelligence artificielle sp\u00e9cialis\u00e9e dans le traitement du langage naturel (NLP). Contrairement \u00e0 nos mod\u00e8les LSTM simples, Mistral utilise des architectures de type transformer, beaucoup plus puissantes pour comprendre et g\u00e9n\u00e9rer du texte.</p> <p>Avantages de l'API Mistral AI: - Mod\u00e8les pr\u00e9-entra\u00een\u00e9s sur d'immenses corpus de texte - Compr\u00e9hension contextuelle profonde - Capacit\u00e9s multilingues - Flexibilit\u00e9 pour diff\u00e9rents cas d'usage NLP</p>"},{"location":"module2/reseaux-recurrents/#instructions_1","title":"Instructions","text":"<ol> <li>Ouvrez le script mistral-integration.py</li> <li>Examinez comment l'API Mistral AI est int\u00e9gr\u00e9e pour am\u00e9liorer les capacit\u00e9s de traitement du langage</li> <li>Vous aurez besoin d'une cl\u00e9 API (une cl\u00e9 de d\u00e9monstration sera fournie pendant la s\u00e9ance)</li> <li>Pour ex\u00e9cuter le script:</li> </ol> <p>```bash</p>"},{"location":"module2/reseaux-recurrents/#installer-les-dependances","title":"Installer les d\u00e9pendances","text":"<p>pip install requests pandas matplotlib </p>"},{"location":"module2/reseaux-recurrents/#configurer-votre-cle-api-sur-windows","title":"Configurer votre cl\u00e9 API (sur Windows)","text":"<p>set MISTRAL_API_KEY=votre_cl\u00e9_api_ici</p>"},{"location":"module2/reseaux-recurrents/#executer-le-script","title":"Ex\u00e9cuter le script","text":"<p>python mistral-integration.py</p> <p>Structure du script Mistral AI Le script est organis\u00e9 en plusieurs sections:</p>"},{"location":"module2/reseaux-recurrents/#structure-du-script-mistral-integrationpy","title":"Structure du script mistral-integration.py","text":""},{"location":"module2/reseaux-recurrents/#1-configuration-et-imports","title":"1. Configuration et imports","text":"<p>import requests import json import os import pandas as pd import matplotlib.pyplot as plt</p>"},{"location":"module2/reseaux-recurrents/#2-configuration-de-lapi","title":"2. Configuration de l'API","text":"<p>API_KEY = os.environ.get(\"MISTRAL_API_KEY\") API_URL = \"https://api.mistral.ai/v1/chat/completions\"</p>"},{"location":"module2/reseaux-recurrents/#3-fonction-pour-envoyer-des-requetes-a-lapi","title":"3. Fonction pour envoyer des requ\u00eates \u00e0 l'API","text":"<p>def query_mistral(prompt, system_message=None, temperature=0.7, max_tokens=256):     \"\"\"     Envoie une requ\u00eate \u00e0 l'API Mistral et retourne la r\u00e9ponse.</p> <pre><code>Args:\n    prompt (str): Le message utilisateur\n    system_message (str): Instructions syst\u00e8me pour guider le mod\u00e8le\n    temperature (float): Contr\u00f4le la cr\u00e9ativit\u00e9 (0.0-1.0)\n    max_tokens (int): Limite de tokens pour la r\u00e9ponse\n\nReturns:\n    str: R\u00e9ponse du mod\u00e8le\n\"\"\"\n# Construction des messages\nmessages = []\nif system_message:\n    messages.append({\"role\": \"system\", \"content\": system_message})\nmessages.append({\"role\": \"user\", \"content\": prompt})\n\n# Configuration de la requ\u00eate\nheaders = {\n    \"Authorization\": f\"Bearer {API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\npayload = {\n    \"model\": \"mistral-small\",\n    \"messages\": messages,\n    \"temperature\": temperature,\n    \"max_tokens\": max_tokens\n}\n\n# Envoi de la requ\u00eate\ntry:\n    response = requests.post(API_URL, headers=headers, json=payload)\n    response.raise_for_status()\n    return response.json()[\"choices\"][0][\"message\"][\"content\"]\nexcept Exception as e:\n    print(f\"Erreur lors de la requ\u00eate \u00e0 l'API: {e}\")\n    return None\n</code></pre>"},{"location":"module2/reseaux-recurrents/#4-demonstrations-dapplications","title":"4. D\u00e9monstrations d'applications","text":""},{"location":"module2/reseaux-recurrents/#41-analyse-de-sentiment-avancee","title":"4.1 Analyse de sentiment avanc\u00e9e","text":"<p>def analyze_sentiment_mistral(text):     \"\"\"Analyse le sentiment d'un texte avec Mistral AI\"\"\"     system_message = \"\"\"     Vous \u00eates un expert en analyse de sentiment.     Analysez le sentiment du texte fourni et r\u00e9pondez uniquement par:     POSITIF, N\u00c9GATIF ou NEUTRE, suivi d'un score de -1.0 \u00e0 1.0 entre parenth\u00e8ses.     Exemple: POSITIF (0.8) ou N\u00c9GATIF (-0.5)     \"\"\"     prompt = f\"Analysez le sentiment du texte suivant: '{text}'\"     return query_mistral(prompt, system_message, temperature=0.3)</p>"},{"location":"module2/reseaux-recurrents/#42-generation-de-texte-controlee","title":"4.2 G\u00e9n\u00e9ration de texte contr\u00f4l\u00e9e","text":"<p>def generate_continuation(text, style=\"informatif\"):     \"\"\"G\u00e9n\u00e8re une continuation de texte dans un style sp\u00e9cifi\u00e9\"\"\"     style_instructions = {         \"informatif\": \"Continuez ce texte dans un style informatif et factuel.\",         \"persuasif\": \"Continuez ce texte dans un style persuasif et convaincant.\",         \"narratif\": \"Continuez ce texte dans un style narratif engageant.\"     }</p> <pre><code>system_message = f\"\"\"\nVous \u00eates un expert en r\u00e9daction et g\u00e9n\u00e9ration de texte.\n{style_instructions.get(style, style_instructions['informatif'])}\n\"\"\"\n\nprompt = f\"Voici le d\u00e9but d'un texte. Continuez-le de mani\u00e8re coh\u00e9rente:\\n\\n{text}\"\nreturn query_mistral(prompt, system_message, temperature=0.7, max_tokens=150)\n</code></pre>"},{"location":"module2/reseaux-recurrents/#43-question-reponse-contextuelle","title":"4.3 Question-r\u00e9ponse contextuelle","text":"<p>def answer_question(context, question):     \"\"\"R\u00e9pond \u00e0 une question bas\u00e9e sur un contexte donn\u00e9\"\"\"     system_message = \"\"\"     Vous \u00eates un assistant sp\u00e9cialis\u00e9 en compr\u00e9hension de texte et question-r\u00e9ponse.     R\u00e9pondez \u00e0 la question uniquement \u00e0 partir des informations fournies dans le contexte.     Si la r\u00e9ponse n'est pas dans le contexte, r\u00e9pondez \"Je ne peux pas r\u00e9pondre \u00e0 cette question d'apr\u00e8s le contexte fourni.\"     \"\"\"</p> <pre><code>prompt = f\"\"\"\nContexte: {context}\n\nQuestion: {question}\n\"\"\"\n\nreturn query_mistral(prompt, system_message, temperature=0.3)\n</code></pre>"},{"location":"module2/reseaux-recurrents/#5-demonstration-interactive","title":"5. D\u00e9monstration interactive","text":"<p>def run_demos():     print(\"=== D\u00e9monstration d'int\u00e9gration de Mistral AI ===\\n\")</p> <pre><code># Test d'analyse de sentiment\nprint(\"1. Analyse de sentiment avanc\u00e9e\")\nsample_texts = [\n    \"J'ai ador\u00e9 ce film, c'\u00e9tait vraiment captivant !\",\n    \"Le service \u00e9tait m\u00e9diocre et la nourriture froide.\",\n    \"Cette application semble int\u00e9ressante mais manque de fonctionnalit\u00e9s.\"\n]\n\nresults = []\nfor text in sample_texts:\n    sentiment = analyze_sentiment_mistral(text)\n    print(f\"Texte: '{text}'\")\n    print(f\"Sentiment: {sentiment}\\n\")\n\n    # Extraction du score pour visualisation\n    if sentiment:\n        score_str = sentiment.split('(')[1].split(')')[0]\n        try:\n            score = float(score_str)\n            results.append({\"text\": text, \"score\": score})\n        except:\n            pass\n\n# Visualisation des scores de sentiment\nif results:\n    df = pd.DataFrame(results)\n    plt.figure(figsize=(10, 6))\n    bars = plt.barh(df['text'], df['score'], color=['green' if x &gt; 0 else 'red' for x in df['score']])\n    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n    plt.xlim(-1, 1)\n    plt.xlabel('Score de sentiment')\n    plt.title('Analyse de sentiment avec Mistral AI')\n    plt.tight_layout()\n    plt.savefig('sentiment_analysis.png')\n    print(\"Visualisation sauvegard\u00e9e dans 'sentiment_analysis.png'\\n\")\n\n# Test de g\u00e9n\u00e9ration de texte\nprint(\"2. G\u00e9n\u00e9ration de texte contr\u00f4l\u00e9e\")\nsample_text = \"L'intelligence artificielle transforme notre mani\u00e8re de travailler.\"\nfor style in [\"informatif\", \"persuasif\", \"narratif\"]:\n    continuation = generate_continuation(sample_text, style)\n    print(f\"Style: {style\n</code></pre>"},{"location":"module2/ressources/cnn-classification/","title":"CNN pour la classification d'images - MNIST","text":"<p>Ce document contient le code et les explications pour le notebook de classification d'images MNIST avec un CNN. Vous pouvez copier-coller chaque section dans une cellule Google Colab.</p>"},{"location":"module2/ressources/cnn-classification/#cellule-1-markdown-introduction","title":"Cellule 1 (Markdown) - Introduction","text":"<pre><code># CNN pour la classification d'images - MNIST\n\n## BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux de neurones\n\nCe notebook vous guidera \u00e0 travers l'impl\u00e9mentation et l'utilisation d'un r\u00e9seau de neurones convolutif (CNN) pour la classification d'images, en utilisant le c\u00e9l\u00e8bre dataset MNIST des chiffres manuscrits.\n\n### Objectifs d'apprentissage:\n- Comprendre l'architecture d'un r\u00e9seau convolutif (CNN)\n- Impl\u00e9menter un CNN avec TensorFlow/Keras\n- Visualiser les filtres et feature maps\n- Analyser les performances du mod\u00e8le\n\n### Pr\u00e9requis:\n- Connaissances de base en Python\n- Avoir suivi la s\u00e9ance 1 d'introduction au Deep Learning\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#1-configuration-et-imports","title":"1. Configuration et imports","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Reproductibilit\u00e9\nnp.random.seed(42)\ntf.random.set_seed(42)\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#2-chargement-et-preparation-des-donnees","title":"2. Chargement et pr\u00e9paration des donn\u00e9es","text":"<pre><code># Charger MNIST\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Redimensionner et normaliser\nX_train = X_train.reshape(-1, 28, 28, 1) / 255.0\nX_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n\n# Convertir les \u00e9tiquettes en one-hot\ny_train_onehot = to_categorical(y_train, 10)\ny_test_onehot = to_categorical(y_test, 10)\n\n# Visualiser quelques exemples\nplt.figure(figsize=(10, 4))\nfor i in range(10):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"Chiffre: {y_train[i]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#3-creation-du-modele-cnn","title":"3. Cr\u00e9ation du mod\u00e8le CNN","text":"<pre><code># Cr\u00e9er un mod\u00e8le CNN\nmodel = Sequential([\n    # Premi\u00e8re couche de convolution\n    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n    MaxPooling2D((2, 2), name='pool1'),\n\n    # Deuxi\u00e8me couche de convolution\n    Conv2D(64, (3, 3), activation='relu', name='conv2'),\n    MaxPooling2D((2, 2), name='pool2'),\n\n    # Aplatissement et couches denses\n    Flatten(name='flatten'),\n    Dense(128, activation='relu', name='dense1'),\n    Dropout(0.5, name='dropout1'),\n    Dense(10, activation='softmax', name='output')\n])\n\n# Compiler le mod\u00e8le\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Afficher le r\u00e9sum\u00e9\nmodel.summary()\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#4-entrainement-du-modele","title":"4. Entra\u00eenement du mod\u00e8le","text":"<pre><code># Entra\u00eener le mod\u00e8le\nhistory = model.fit(\n    X_train, y_train_onehot, \n    batch_size=128, \n    epochs=5,\n    validation_split=0.2,\n    verbose=1\n)\n\n# Visualiser les courbes d'apprentissage\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train')\nplt.plot(history.history['val_accuracy'], label='Validation')\nplt.title('Pr\u00e9cision')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.title('Perte')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#5-evaluation-du-modele","title":"5. \u00c9valuation du mod\u00e8le","text":"<pre><code># \u00c9valuer sur le jeu de test\ntest_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)\nprint(f\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\")\n\n# Pr\u00e9dictions et matrice de confusion\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\n\n# Matrice de confusion\nplt.figure(figsize=(8, 6))\nsns.heatmap(confusion_matrix(y_test, y_pred_classes), annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Pr\u00e9dit')\nplt.ylabel('R\u00e9el')\nplt.title('Matrice de confusion')\nplt.show()\n\n# Afficher des erreurs\nmisclassified = np.where(y_pred_classes != y_test)[0]\nplt.figure(figsize=(10, 4))\nfor i, idx in enumerate(misclassified[:10]):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n    plt.title(f\"R:{y_test[idx]} P:{y_pred_classes[idx]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#6-visualisation-des-filtres-et-feature-maps","title":"6. Visualisation des filtres et feature maps","text":"<pre><code># Visualiser les filtres de la premi\u00e8re couche\n# Approche alternative compl\u00e8te pour la visualisation\nprint(\"Initialisation et visualisation avec une approche alternative...\")\n\n# 1. R\u00e9initialiser le mod\u00e8le pour s'assurer qu'il est correctement d\u00e9fini\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n    MaxPooling2D((2, 2), name='pool1'),\n    Conv2D(64, (3, 3), activation='relu', name='conv2'),\n    MaxPooling2D((2, 2), name='pool2'),\n    Flatten(name='flatten'),\n    Dense(128, activation='relu', name='dense1'),\n    Dropout(0.5, name='dropout1'),\n    Dense(10, activation='softmax', name='output')\n])\n\n# 2. Compiler le mod\u00e8le\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# 3. Forcer l'initialisation avec build ET un forward pass\nmodel.build(input_shape=(None, 28, 28, 1))\ndummy_input = np.zeros((1, 28, 28, 1))\n_ = model(dummy_input)\n\n# 4. V\u00e9rifier que les couches sont accessibles\nprint(f\"Couches dans le mod\u00e8le: {[layer.name for layer in model.layers]}\")\n\n# 5. Cr\u00e9er et visualiser des poids al\u00e9atoires puisque le mod\u00e8le n'est pas entra\u00een\u00e9\nfilters = np.random.normal(size=(3, 3, 1, 8))  # Simuler 8 filtres 3x3\nf_min, f_max = filters.min(), filters.max()\nfilters = (filters - f_min) / (f_max - f_min)\n\nplt.figure(figsize=(10, 4))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(filters[:, :, 0, i], cmap='viridis')\n    plt.title(f'Filtre {i+1}')\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n\n# 6. Simuler des feature maps al\u00e9atoires\nsample_idx = 12\nsample_image = X_test[sample_idx]\nplt.figure(figsize=(3, 3))\nplt.imshow(sample_image.reshape(28, 28), cmap='gray')\nplt.title(f\"Chiffre: {y_test[sample_idx]}\")\nplt.axis('off')\nplt.show()\n\n# 7. G\u00e9n\u00e9rer des feature maps simul\u00e9es\nfeature_maps = np.random.rand(1, 26, 26, 8)  # Taille typique apr\u00e8s convolution 3x3\n\nplt.figure(figsize=(10, 4))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n    plt.axis('off')\nplt.suptitle('Feature Maps - Couche 1 (Simul\u00e9es)')\nplt.tight_layout()\nplt.show()\n\nprint(\"Visualisation termin\u00e9e avec des donn\u00e9es simul\u00e9es.\")\nprint(\"Note: Pour voir les vrais filtres et feature maps, le mod\u00e8le doit \u00eatre entra\u00een\u00e9.\")\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#7-test-avec-des-images-bruitees","title":"7. Test avec des images bruit\u00e9es","text":"<pre><code># Ajouter du bruit\ndef add_noise(images, noise_level=0.2):\n    noisy_images = images.copy()\n    noise = np.random.normal(0, noise_level, images.shape)\n    return np.clip(noisy_images + noise, 0, 1)\n\n# Tester avec quelques images bruit\u00e9es\ntest_samples = X_test[:5]\nnoisy_samples = add_noise(test_samples, noise_level=0.3)\n\n# Afficher les images originales et bruit\u00e9es\nplt.figure(figsize=(10, 4))\nfor i in range(5):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(test_samples[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"Original: {y_test[i]}\")\n    plt.axis('off')\n\n    plt.subplot(2, 5, i+6)\n    plt.imshow(noisy_samples[i].reshape(28, 28), cmap='gray')\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n\n# Pr\u00e9dictions sur les images bruit\u00e9es\npredictions = model.predict(noisy_samples)\npred_classes = np.argmax(predictions, axis=1)\n\nprint(\"R\u00e9sultats sur les images bruit\u00e9es:\")\nfor i in range(5):\n    print(f\"Image {i+1} - R\u00e9el: {y_test[i]}, Pr\u00e9dit: {pred_classes[i]}\")\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#8-exercice-ameliorez-le-modele","title":"8. Exercice: Am\u00e9liorez le mod\u00e8le","text":"<p>Modifiez l'architecture pour am\u00e9liorer les performances:</p> <ol> <li>Essayez d'ajouter une couche de convolution suppl\u00e9mentaire</li> <li>Modifiez le nombre de filtres ou leur taille</li> <li>Ajustez les param\u00e8tres d'entra\u00eenement (epochs, batch_size)</li> </ol> <pre><code># VOTRE CODE ICI - Cr\u00e9ez un mod\u00e8le am\u00e9lior\u00e9\nimproved_model = Sequential([\n    # Ajoutez votre architecture am\u00e9lior\u00e9e ici\n])\n\n# Compiler et entra\u00eener votre mod\u00e8le\n# ...\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#9-sauvegarde-du-modele-pour-lapplication-web","title":"9. Sauvegarde du mod\u00e8le pour l'application web","text":"<pre><code># Sauvegarder le mod\u00e8le pour l'int\u00e9gration web\nmodel.save('mnist_cnn_model.h5')\nprint(\"Mod\u00e8le sauvegard\u00e9 avec succ\u00e8s!\")\n\n# Si vous utilisez Google Colab, t\u00e9l\u00e9chargez le fichier\ntry:\n    from google.colab import files\n    files.download('mnist_cnn_model.h5')\n    print(\"T\u00e9l\u00e9chargement du fichier initi\u00e9...\")\nexcept:\n    print(\"Vous n'\u00eates pas sur Google Colab. Le mod\u00e8le est sauvegard\u00e9 localement.\")\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#questions-de-reflexion","title":"Questions de r\u00e9flexion","text":"<ol> <li>Qu'est-ce qui rend les CNNs plus efficaces que les r\u00e9seaux denses pour les images?</li> <li>Comment les couches de convolution extraient-elles les caract\u00e9ristiques des images?</li> <li>Pourquoi utilisons-nous le pooling dans les CNNs?</li> <li>Quelles am\u00e9liorations pourriez-vous apporter pour rendre le mod\u00e8le plus robuste au bruit?</li> </ol>"},{"location":"module2/ressources/create_model/","title":"Create model","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\n</pre> import numpy as np import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout from tensorflow.keras.utils import to_categorical In\u00a0[\u00a0]: Copied! <pre>print(\"TensorFlow version:\", tf.__version__)\n</pre> print(\"TensorFlow version:\", tf.__version__) In\u00a0[\u00a0]: Copied! <pre># Pour la reproductibilit\u00e9\nnp.random.seed(42)\ntf.random.set_seed(42)\n</pre> # Pour la reproductibilit\u00e9 np.random.seed(42) tf.random.set_seed(42) In\u00a0[\u00a0]: Copied! <pre># Charger les donn\u00e9es MNIST\nprint(\"Chargement des donn\u00e9es MNIST...\")\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n</pre> # Charger les donn\u00e9es MNIST print(\"Chargement des donn\u00e9es MNIST...\") (X_train, y_train), (X_test, y_test) = mnist.load_data() In\u00a0[\u00a0]: Copied! <pre># Pr\u00e9traitement des donn\u00e9es\nX_train = X_train.reshape(-1, 28, 28, 1) / 255.0\nX_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n</pre> # Pr\u00e9traitement des donn\u00e9es X_train = X_train.reshape(-1, 28, 28, 1) / 255.0 X_test = X_test.reshape(-1, 28, 28, 1) / 255.0 In\u00a0[\u00a0]: Copied! <pre># Conversion des \u00e9tiquettes en format one-hot\ny_train_onehot = to_categorical(y_train, 10)\ny_test_onehot = to_categorical(y_test, 10)\n</pre> # Conversion des \u00e9tiquettes en format one-hot y_train_onehot = to_categorical(y_train, 10) y_test_onehot = to_categorical(y_test, 10) In\u00a0[\u00a0]: Copied! <pre># Cr\u00e9ation du mod\u00e8le CNN\nmodel = Sequential([\n    # Premi\u00e8re couche de convolution\n    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n    MaxPooling2D((2, 2), name='pool1'),\n    \n    # Deuxi\u00e8me couche de convolution\n    Conv2D(64, (3, 3), activation='relu', name='conv2'),\n    MaxPooling2D((2, 2), name='pool2'),\n    \n    # Aplatissement pour passer aux couches denses\n    Flatten(name='flatten'),\n    \n    # Couches denses (fully connected)\n    Dense(128, activation='relu', name='dense1'),\n    Dropout(0.5, name='dropout1'),  # \u00c9vite le surapprentissage\n    Dense(10, activation='softmax', name='output')  # 10 classes (chiffres 0-9)\n])\n</pre> # Cr\u00e9ation du mod\u00e8le CNN model = Sequential([     # Premi\u00e8re couche de convolution     Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),     MaxPooling2D((2, 2), name='pool1'),          # Deuxi\u00e8me couche de convolution     Conv2D(64, (3, 3), activation='relu', name='conv2'),     MaxPooling2D((2, 2), name='pool2'),          # Aplatissement pour passer aux couches denses     Flatten(name='flatten'),          # Couches denses (fully connected)     Dense(128, activation='relu', name='dense1'),     Dropout(0.5, name='dropout1'),  # \u00c9vite le surapprentissage     Dense(10, activation='softmax', name='output')  # 10 classes (chiffres 0-9) ]) In\u00a0[\u00a0]: Copied! <pre># Compiler le mod\u00e8le\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n</pre> # Compiler le mod\u00e8le model.compile(     optimizer='adam',     loss='categorical_crossentropy',     metrics=['accuracy'] ) In\u00a0[\u00a0]: Copied! <pre># Afficher le r\u00e9sum\u00e9 de l'architecture\nmodel.summary()\n</pre> # Afficher le r\u00e9sum\u00e9 de l'architecture model.summary() In\u00a0[\u00a0]: Copied! <pre># Entra\u00eenement du mod\u00e8le\nprint(\"Entra\u00eenement du mod\u00e8le...\")\nhistory = model.fit(\n    X_train, \n    y_train_onehot, \n    batch_size=128, \n    epochs=5,  # Entra\u00eenement court pour l'exemple\n    validation_split=0.2,\n    verbose=1\n)\n</pre> # Entra\u00eenement du mod\u00e8le print(\"Entra\u00eenement du mod\u00e8le...\") history = model.fit(     X_train,      y_train_onehot,      batch_size=128,      epochs=5,  # Entra\u00eenement court pour l'exemple     validation_split=0.2,     verbose=1 ) In\u00a0[\u00a0]: Copied! <pre># \u00c9valuer le mod\u00e8le\ntest_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)\nprint(f\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\")\n</pre> # \u00c9valuer le mod\u00e8le test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1) print(f\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\") In\u00a0[\u00a0]: Copied! <pre># Sauvegarder le mod\u00e8le\nmodel.save('mnist_cnn_model.h5')\nprint(\"Mod\u00e8le sauvegard\u00e9 avec succ\u00e8s sous 'mnist_cnn_model.h5'\")\n</pre> # Sauvegarder le mod\u00e8le model.save('mnist_cnn_model.h5') print(\"Mod\u00e8le sauvegard\u00e9 avec succ\u00e8s sous 'mnist_cnn_model.h5'\")"},{"location":"module3/","title":"Module 3 : D\u00e9veloppement d'applications pratiques","text":""},{"location":"module3/#objectifs-du-module","title":"Objectifs du module","text":"<p>\u00c0 l'issue de ce module, vous serez capable de :</p> <ul> <li>Utiliser efficacement les frameworks de Deep Learning (TensorFlow/Keras)</li> <li>Int\u00e9grer des mod\u00e8les pr\u00e9-entra\u00een\u00e9s dans des applications concr\u00e8tes</li> <li>Optimiser les performances de vos mod\u00e8les pour des environnements \u00e0 ressources limit\u00e9es</li> <li>Concevoir et pr\u00e9parer le d\u00e9veloppement d'un chatbot p\u00e9dagogique</li> <li>Explorer l'API Mistral AI pour d\u00e9velopper des applications d'IA conversationnelle</li> </ul>"},{"location":"module3/#programme-4h","title":"Programme (4h)","text":"<p>Ce module se concentre sur les aspects pratiques du d\u00e9ploiement et de l'int\u00e9gration de mod\u00e8les de Deep Learning dans des applications r\u00e9elles.</p>"},{"location":"module3/#phase-1-frameworks-de-deep-learning-1h30","title":"Phase 1 : Frameworks de Deep Learning (1h30)","text":"<p>D\u00e9couvrez comment utiliser efficacement les frameworks de Deep Learning sans complexit\u00e9 excessive.</p> <ul> <li>Installation et configuration de TensorFlow/Keras</li> <li>Utilisation optimale des API de haut niveau</li> <li>Mod\u00e8les pr\u00e9-entra\u00een\u00e9s pour des t\u00e2ches courantes</li> <li>D\u00e9veloppement d'une API simple de reconnaissance d'images</li> </ul>"},{"location":"module3/#phase-2-amelioration-des-performances-1h30","title":"Phase 2 : Am\u00e9lioration des performances (1h30)","text":"<p>Apprenez \u00e0 optimiser vos mod\u00e8les pour les rendre plus rapides et plus efficaces.</p> <ul> <li>Techniques d'optimisation des performances</li> <li>Bonnes pratiques pour les mod\u00e8les de production</li> <li>Quantification et compression de mod\u00e8les</li> <li>TP pratique : am\u00e9lioration d'un mod\u00e8le pour une application web</li> </ul>"},{"location":"module3/#phase-3-preparation-au-projet-final-45min","title":"Phase 3 : Pr\u00e9paration au projet final (45min)","text":"<p>Pr\u00e9parez-vous au d\u00e9veloppement du chatbot p\u00e9dagogique qui constituera le projet final.</p> <ul> <li>Pr\u00e9sentation d\u00e9taill\u00e9e du cahier des charges</li> <li>\u00c9tude de cas r\u00e9els d'entreprises utilisant des chatbots</li> <li>Exploration de l'API Mistral AI pour le traitement du langage naturel</li> <li>Premiers pas vers un prototype fonctionnel</li> </ul>"},{"location":"module3/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Avoir suivi les Modules 1 et 2</li> <li>Comprendre les architectures CNN et RNN</li> <li>Ma\u00eetriser les bases de Python</li> <li>Un compte Google pour l'acc\u00e8s \u00e0 Colab</li> </ul>"},{"location":"module3/#livrables-attendus","title":"Livrables attendus","text":"<p>\u00c0 l'issue de ce module, vous devrez produire :</p> <ol> <li>Une API de reconnaissance d'images fonctionnelle</li> <li>Un mod\u00e8le optimis\u00e9 avec mesures de performances avant/apr\u00e8s</li> <li>Un document de conception pour votre chatbot p\u00e9dagogique</li> <li>Un premier prototype d'int\u00e9gration avec l'API Mistral</li> </ol>"},{"location":"module3/#competences-bts-sio-developpees","title":"Comp\u00e9tences BTS SIO d\u00e9velopp\u00e9es","text":"Comp\u00e9tence Description Activit\u00e9s associ\u00e9es B1.1 Gestion du patrimoine informatique Manipulation des outils et API professionnels B1.2 R\u00e9ponse aux incidents Gestion des erreurs API et cas limites B1.3 D\u00e9veloppement de la pr\u00e9sence en ligne Cr\u00e9ation d'applications web avec IA B2.2 Conception de solutions Architecture d'applications IA B2.3 D\u00e9veloppement d'applications Int\u00e9gration et optimisation de mod\u00e8les B3.1 Test et d\u00e9ploiement Mesure de performances et optimisation"},{"location":"module3/#ressources-fournies","title":"Ressources fournies","text":"<ul> <li>Templates de code pour l'API Flask/FastAPI</li> <li>Guide d'utilisation de l'API Mistral AI</li> <li>Mod\u00e8les pr\u00e9-entra\u00een\u00e9s pour d\u00e9monstration</li> <li>Documentation des bonnes pratiques d'optimisation</li> </ul>"},{"location":"module3/#pret-pour-la-partie-pratique","title":"Pr\u00eat pour la partie pratique ?","text":"<p>D\u00e9couvrez comment les frameworks modernes facilitent le d\u00e9veloppement d'applications de Deep Learning.</p> <p>Commencer par les Frameworks</p>"},{"location":"module3/frameworks/","title":"Phase 1 : Frameworks de Deep Learning (1h30)","text":""},{"location":"module3/frameworks/#introduction-aux-frameworks-dans-un-contexte-professionnel-15-min","title":"Introduction aux frameworks dans un contexte professionnel (15 min)","text":"<p>Objectif: Comprendre l'utilit\u00e9 des frameworks de Deep Learning pour un d\u00e9veloppeur en entreprise et identifier ceux qui sont r\u00e9ellement utilis\u00e9s sur le terrain.</p>"},{"location":"module3/frameworks/#les-frameworks-en-entreprise","title":"Les frameworks en entreprise","text":"<p>Avant de plonger dans le code, prenons un moment pour comprendre pourquoi les frameworks de Deep Learning sont si importants en contexte professionnel:</p> <ul> <li>Productivit\u00e9: Ils permettent de d\u00e9velopper des applications d'IA sans repartir de z\u00e9ro</li> <li>Maintenabilit\u00e9: Code plus standard, plus facile \u00e0 comprendre par d'autres d\u00e9veloppeurs</li> <li>Performances: Optimisations int\u00e9gr\u00e9es qui seraient complexes \u00e0 d\u00e9velopper soi-m\u00eame</li> <li>D\u00e9ploiement: Outils int\u00e9gr\u00e9s pour mettre en production les mod\u00e8les</li> </ul> <p>Dans le monde professionnel actuel, plusieurs frameworks de Deep Learning sont couramment utilis\u00e9s:</p> Framework Principaux cas d'usage TensorFlow/Keras Applications web/mobile, syst\u00e8mes en production PyTorch Recherche, prototypage, startups Hugging Face NLP, chatbots, traitement de texte Scikit-learn Pr\u00e9traitement, ML classique, pipeline de donn\u00e9es <p>\"Pour un stage, la capacit\u00e9 \u00e0 utiliser efficacement des frameworks existants est recherch\u00e9e davantage que l'expertise th\u00e9orique approfondie en Deep Learning.\"  </p>"},{"location":"module3/frameworks/#tensorflowkeras-la-solution-pragmatique","title":"TensorFlow/Keras: la solution pragmatique","text":"<p>Pour cette s\u00e9ance, nous allons nous concentrer sur TensorFlow/Keras pour plusieurs raisons:</p> <ol> <li>Interface simple: Keras offre une API haut niveau, parfaite pour d\u00e9buter</li> <li>D\u00e9ploiement facile: Solutions int\u00e9gr\u00e9es pour mettre en production (TF Serving, TFLite)</li> <li>Documentation riche: Ressources abondantes en fran\u00e7ais</li> <li>Mod\u00e8les pr\u00e9-entra\u00een\u00e9s: Large biblioth\u00e8que de mod\u00e8les pr\u00eats \u00e0 l'emploi</li> <li>Demande professionnelle: Le plus mentionn\u00e9 dans les offres de stage</li> </ol>"},{"location":"module3/frameworks/#demonstration-applications-reelles-en-entreprise","title":"D\u00e9monstration: Applications r\u00e9elles en entreprise","text":"<p>Voici quelques exemples concrets d\u00e9velopp\u00e9s par des entreprises locales employant des anciens \u00e9tudiants:</p> <ul> <li>PME de logistique: Application de reconnaissance de documents (bons de livraison, factures) permettant d'automatiser la saisie \u2192 \u00c9conomie de 15h/semaine</li> <li>Agence web: Syst\u00e8me de d\u00e9tection de contenu inappropri\u00e9 dans les commentaires de sites e-commerce</li> <li>Cabinet m\u00e9dical: Application de classification d'images pour le tri pr\u00e9liminaire de photos de l\u00e9sions cutan\u00e9es</li> </ul>"},{"location":"module3/frameworks/#atelier-pratique-prise-en-main-de-tensorflowkeras-30-min","title":"Atelier pratique : Prise en main de TensorFlow/Keras (30 min)","text":""},{"location":"module3/frameworks/#objectif","title":"Objectif","text":"<p>D\u00e9velopper une premi\u00e8re application de reconnaissance d'images simple en utilisant TensorFlow/Keras et en suivant les bonnes pratiques de l'industrie.</p>"},{"location":"module3/frameworks/#instructions","title":"Instructions","text":"<ol> <li>Configuration de l'environnement (5 min)</li> </ol> <p>Ouvrez Google Colab et cr\u00e9ez un nouveau notebook. Commencez par installer et importer les biblioth\u00e8ques n\u00e9cessaires:</p> <pre><code># V\u00e9rification de la version de TensorFlow\nimport tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)\n\n# Importation des biblioth\u00e8ques essentielles\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre> <ol> <li>Utilisation d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (10 min)</li> </ol> <p>Au lieu de cr\u00e9er un mod\u00e8le \u00e0 partir de z\u00e9ro, nous allons utiliser un mod\u00e8le pr\u00e9-entra\u00een\u00e9, comme le font la plupart des professionnels:</p> <pre><code># Chargement d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (sans les couches de classification)\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Gel des couches pr\u00e9-entra\u00een\u00e9es pour \u00e9viter leur modification\nbase_model.trainable = False\n\n# Cr\u00e9ation de notre propre mod\u00e8le en ajoutant des couches de classification\nmodel = models.Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1000, activation='softmax')  # 1000 classes ImageNet\n])\n\n# R\u00e9sum\u00e9 du mod\u00e8le pour comprendre son architecture\nmodel.summary()\n</code></pre> <ol> <li>Pr\u00e9paration d'une image de test (5 min)</li> </ol> <p>T\u00e9l\u00e9chargez une image de test et pr\u00e9parez-la pour le mod\u00e8le:</p> <pre><code># T\u00e9l\u00e9chargement d'une image d'exemple\n!wget -q -O test_image.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Pug_600.jpg/280px-Pug_600.jpg\n\n# Pr\u00e9paration de l'image pour le mod\u00e8le\ndef preprocess_image(img_path):\n    # Chargement et redimensionnement\n    img = image.load_img(img_path, target_size=(224, 224))\n\n    # Conversion en tableau\n    img_array = image.img_to_array(img)\n\n    # Ajout de la dimension de batch (1 seule image)\n    img_array = np.expand_dims(img_array, axis=0)\n\n    # Pr\u00e9traitement sp\u00e9cifique au mod\u00e8le utilis\u00e9\n    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n\n    return img_array\n\n# Affichage de l'image\nplt.figure(figsize=(4, 4))\nplt.imshow(image.load_img('test_image.jpg'))\nplt.axis('off')\nplt.show()\n\n# Pr\u00e9traitement\nprocessed_image = preprocess_image('test_image.jpg')\n</code></pre> <ol> <li>Pr\u00e9diction et interpr\u00e9tation des r\u00e9sultats (10 min)</li> </ol> <p>Utilisez le mod\u00e8le pour faire une pr\u00e9diction et visualisez les r\u00e9sultats:</p> <pre><code># Pr\u00e9diction\npredictions = model.predict(processed_image)\n\n# Interpr\u00e9tation des r\u00e9sultats (top 5)\nfrom tensorflow.keras.applications.mobilenet_v2 import decode_predictions\n\n# D\u00e9codage des pr\u00e9dictions (conversion des indices en labels)\ndecoded_predictions = decode_predictions(predictions, top=5)[0]\n\n# Affichage des r\u00e9sultats\nplt.figure(figsize=(10, 3))\nlabels = [pred[1] for pred in decoded_predictions]\nscores = [pred[2] for pred in decoded_predictions]\n\nplt.barh(labels, scores)\nplt.xlabel('Probabilit\u00e9')\nplt.title('Top 5 des pr\u00e9dictions')\nplt.xlim(0, 1.0)\nplt.gca().invert_yaxis()  # Pour que le plus probable soit en haut\nplt.show()\n\nprint(\"Pr\u00e9dictions:\")\nfor i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n    print(f\"{i+1}. {label} ({score:.2f})\")\n</code></pre>"},{"location":"module3/frameworks/#mini-projet-developpement-dune-api-de-reconnaissance-dimages-45-min","title":"Mini-projet : D\u00e9veloppement d'une API de reconnaissance d'images (45 min)","text":""},{"location":"module3/frameworks/#objectif_1","title":"Objectif","text":"<p>Cr\u00e9er une API REST simple qui permette \u00e0 d'autres applications d'utiliser votre mod\u00e8le de reconnaissance d'images.</p>"},{"location":"module3/frameworks/#instructions_1","title":"Instructions","text":"<ol> <li>Pr\u00e9paration de l'environnement pour l'API (5 min)</li> </ol> <pre><code># Installation de Flask pour l'API\n!pip install flask flask-cors\n\n# Cr\u00e9ation du fichier principal de l'API\n%%writefile app.py\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2, ResNet50\nfrom tensorflow.keras.applications.mobilenet_v2 import decode_predictions, preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\n\napp = Flask(__name__)\nCORS(app)  # Permet les requ\u00eates cross-origin\n\n# Chargement du mod\u00e8le pr\u00e9-entra\u00een\u00e9 (fait une seule fois au d\u00e9marrage)\nprint(\"Chargement du mod\u00e8le...\")\nmodel = MobileNetV2(weights='imagenet')\nprint(\"Mod\u00e8le charg\u00e9!\")\n\n# Fonction de pr\u00e9traitement\ndef preprocess_image(img):\n    img = img.resize((224, 224))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    return preprocess_input(img_array)\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    # V\u00e9rification qu'une image a \u00e9t\u00e9 envoy\u00e9e\n    if 'image' not in request.files:\n        # V\u00e9rifier si l'image est envoy\u00e9e en base64\n        if request.json and 'image' in request.json:\n            # D\u00e9codage de l'image base64\n            image_data = base64.b64decode(request.json['image'].split(',')[1])\n            img = Image.open(BytesIO(image_data))\n        else:\n            return jsonify({'error': 'Aucune image fournie'}), 400\n    else:\n        # Lecture de l'image depuis les fichiers\n        file = request.files['image']\n        img = Image.open(file.stream)\n\n    # Pr\u00e9traitement\n    processed_img = preprocess_image(img)\n\n    # Pr\u00e9diction\n    predictions = model.predict(processed_img)\n\n    # D\u00e9codage des pr\u00e9dictions (top 5)\n    decoded = decode_predictions(predictions, top=5)[0]\n\n    # Formatage de la r\u00e9ponse\n    results = []\n    for _, label, score in decoded:\n        results.append({\n            'label': label,\n            'probability': float(score)\n        })\n\n    return jsonify({\n        'predictions': results\n    })\n\nif __name__ == '__main__':\n    # D\u00e9marrer le serveur sur le port 5000\n    app.run(host='0.0.0.0', port=5000)\n</code></pre> <ol> <li>Interface utilisateur simple (15 min)</li> </ol> <p>Cr\u00e9ez une interface web simple pour tester votre API:</p> <pre><code>%%writefile index.html\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Reconnaissance d'images&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #f5f5f5;\n        }\n        h1 {\n            color: #333;\n            text-align: center;\n        }\n        .container {\n            background-color: white;\n            padding: 20px;\n            border-radius: 8px;\n            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);\n        }\n        .upload-section {\n            margin: 20px 0;\n            text-align: center;\n        }\n        #preview {\n            max-width: 100%;\n            max-height: 300px;\n            display: block;\n            margin: 20px auto;\n            border-radius: 8px;\n            display: none;\n        }\n        #results {\n            margin-top: 20px;\n            display: none;\n        }\n        .result-item {\n            display: flex;\n            justify-content: space-between;\n            padding: 8px 0;\n            border-bottom: 1px solid #eee;\n        }\n        .progress-bar {\n            height: 20px;\n            background-color: #4CAF50;\n            border-radius: 4px;\n        }\n        button {\n            background-color: #4CAF50;\n            color: white;\n            border: none;\n            padding: 10px 15px;\n            border-radius: 4px;\n            cursor: pointer;\n            font-size: 16px;\n        }\n        button:hover {\n            background-color: #45a049;\n        }\n        #loading {\n            text-align: center;\n            display: none;\n            margin: 20px 0;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;h1&gt;Reconnaissance d'Images&lt;/h1&gt;\n        &lt;p&gt;T\u00e9l\u00e9chargez une image pour savoir ce qu'elle contient gr\u00e2ce \u00e0 notre API de reconnaissance d'images bas\u00e9e sur MobileNetV2.&lt;/p&gt;\n\n        &lt;div class=\"upload-section\"&gt;\n            &lt;input type=\"file\" id=\"image-upload\" accept=\"image/*\"&gt;\n            &lt;button id=\"predict-button\"&gt;Analyser l'image&lt;/button&gt;\n        &lt;/div&gt;\n\n        &lt;img id=\"preview\" src=\"#\" alt=\"Aper\u00e7u de l'image\"&gt;\n\n        &lt;div id=\"loading\"&gt;\n            Analyse en cours...\n        &lt;/div&gt;\n\n        &lt;div id=\"results\"&gt;\n            &lt;h2&gt;R\u00e9sultats&lt;/h2&gt;\n            &lt;div id=\"predictions-container\"&gt;&lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n        document.addEventListener('DOMContentLoaded', function() {\n            const imageUpload = document.getElementById('image-upload');\n            const previewImage = document.getElementById('preview');\n            const predictButton = document.getElementById('predict-button');\n            const resultsDiv = document.getElementById('results');\n            const predictionsContainer = document.getElementById('predictions-container');\n            const loadingDiv = document.getElementById('loading');\n\n            // Afficher l'aper\u00e7u de l'image s\u00e9lectionn\u00e9e\n            imageUpload.addEventListener('change', function() {\n                if (this.files &amp;&amp; this.files[0]) {\n                    const reader = new FileReader();\n\n                    reader.onload = function(e) {\n                        previewImage.src = e.target.result;\n                        previewImage.style.display = 'block';\n                        resultsDiv.style.display = 'none';\n                    }\n\n                    reader.readAsDataURL(this.files[0]);\n                }\n            });\n\n            // Envoyer l'image \u00e0 l'API pour pr\u00e9diction\n            predictButton.addEventListener('click', function() {\n                if (!imageUpload.files || !imageUpload.files[0]) {\n                    alert('Veuillez s\u00e9lectionner une image.');\n                    return;\n                }\n\n                // Afficher le chargement\n                loadingDiv.style.display = 'block';\n                resultsDiv.style.display = 'none';\n\n                const formData = new FormData();\n                formData.append('image', imageUpload.files[0]);\n\n                fetch('http://localhost:5000/predict', {\n                    method: 'POST',\n                    body: formData\n                })\n                .then(response =&gt; response.json())\n                .then(data =&gt; {\n                    // Cacher le chargement\n                    loadingDiv.style.display = 'none';\n\n                    // Afficher les r\u00e9sultats\n                    predictionsContainer.innerHTML = '';\n\n                    if (data.error) {\n                        predictionsContainer.innerHTML = `&lt;p class=\"error\"&gt;${data.error}&lt;/p&gt;`;\n                    } else {\n                        data.predictions.forEach(pred =&gt; {\n                            const resultItem = document.createElement('div');\n                            resultItem.className = 'result-item';\n\n                            const label = document.createElement('div');\n                            label.textContent = pred.label.replace('_', ' ');\n\n                            const probability = document.createElement('div');\n                            probability.textContent = `${(pred.probability * 100).toFixed(2)}%`;\n\n                            const progressContainer = document.createElement('div');\n                            progressContainer.style.width = '60%';\n                            progressContainer.style.backgroundColor = '#f1f1f1';\n                            progressContainer.style.borderRadius = '4px';\n\n                            const progressBar = document.createElement('div');\n                            progressBar.className = 'progress-bar';\n                            progressBar.style.width = `${pred.probability * 100}%`;\n\n                            progressContainer.appendChild(progressBar);\n\n                            resultItem.appendChild(label);\n                            resultItem.appendChild(progressContainer);\n                            resultItem.appendChild(probability);\n\n                            predictionsContainer.appendChild(resultItem);\n                        });\n                    }\n\n                    resultsDiv.style.display = 'block';\n                })\n                .catch(error =&gt; {\n                    loadingDiv.style.display = 'none';\n                    alert('Erreur lors de la communication avec l\\'API: ' + error);\n                });\n            });\n        });\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <ol> <li>Lancement et test de l'API (15 min)</li> </ol> <p>Utilisez ngrok pour exposer votre API en ligne (facultatif):</p> <pre><code># Installation de pyngrok pour exposer l'API sur internet (facultatif)\n!pip install pyngrok\n\n# \u00c9crire un script pour lancer l'API avec ngrok\n%%writefile launch_api.py\nfrom flask import Flask, send_file\nimport threading\nimport os\nfrom pyngrok import ngrok\nimport time\n\n# Chemin vers le fichier app.py\napp_path = 'app.py'\n\n# Cr\u00e9er un serveur Flask pour servir l'interface utilisateur\nui_app = Flask(__name__)\n\n@ui_app.route('/')\ndef index():\n    return send_file('index.html')\n\n# D\u00e9marrer l'API dans un thread s\u00e9par\u00e9\ndef run_api():\n    os.system(f'python {app_path}')\n\napi_thread = threading.Thread(target=run_api)\napi_thread.daemon = True  # Pour que le thread s'arr\u00eate quand le programme principal s'arr\u00eate\napi_thread.start()\n\n# D\u00e9marrer le tunnel ngrok pour l'API\napi_url = ngrok.connect(5000)\nprint(f\"API accessible \u00e0: {api_url}\")\n\n# D\u00e9marrer le tunnel ngrok pour l'interface utilisateur\nui_url = ngrok.connect(5001)\nprint(f\"Interface accessible \u00e0: {ui_url}\")\n\n# D\u00e9marrer le serveur de l'interface utilisateur\nui_app.run(host='0.0.0.0', port=5001)\n</code></pre> <p>Ex\u00e9cutez le script:</p> <pre><code>!python launch_api.py\n</code></pre> <ol> <li>Documentation de l'API (10 min)</li> </ol> <p>Cr\u00e9ez une documentation simple pour votre API:</p> <pre><code>%%writefile api_documentation.md\n# Documentation de l'API de reconnaissance d'images\n\nCette API permet d'identifier le contenu d'une image en utilisant un mod\u00e8le de Deep Learning pr\u00e9-entra\u00een\u00e9 (MobileNetV2).\n\n## Endpoint\n\n### POST /predict\n\nAnalyse une image et retourne les pr\u00e9dictions.\n\n#### Param\u00e8tres\n\nL'API accepte deux formats d'envoi d'image:\n\n1. **Multipart Form Data**:\n   - `image`: Fichier image (JPG, PNG, etc.)\n\n2. **JSON avec Base64**:\n   ```json\n   {\n       \"image\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEASABIAAD/...\"\n   }\n   ```\n\n#### R\u00e9ponse\n\n```json\n{\n    \"predictions\": [\n        {\n            \"label\": \"dog\",\n            \"probability\": 0.95\n        },\n        {\n            \"label\": \"golden_retriever\",\n            \"probability\": 0.03\n        },\n        ...\n    ]\n}\n</code></pre> <p>#### Codes de retour</p> <ul> <li><code>200 OK</code>: Succ\u00e8s</li> <li><code>400 Bad Request</code>: Aucune image fournie ou format invalide</li> <li><code>500 Internal Server Error</code>: Erreur lors du traitement</li> </ul> <p>## Exemples d'utilisation</p> <p>### Python</p> <pre><code>import requests\n\n# Avec un fichier\nwith open('image.jpg', 'rb') as img:\n    response = requests.post('http://localhost:5000/predict', \n                            files={'image': img})\n\n# Avec base64\nimport base64\nwith open('image.jpg', 'rb') as img:\n    img_base64 = base64.b64encode(img.read()).decode('utf-8')\n    payload = {'image': f'data:image/jpeg;base64,{img_base64}'}\n    response = requests.post('http://localhost:5000/predict', \n                            json=payload)\n\nprint(response.json())\n</code></pre> <p>### JavaScript</p> <p><pre><code>// Avec un fichier\nconst formData = new FormData();\nformData.append('image', fileInput.files[0]);\n\nfetch('http://localhost:5000/predict', {\n    method: 'POST',\n    body: formData\n})\n.then(response =&gt; response.json())\n.then(data =&gt; console.log(data));\n\n// Avec base64\nconst reader = new FileReader();\nreader.onload = function(e) {\n    fetch('http://localhost:5000/predict', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n            image: e.target.result\n        })\n    })\n    .then(response =&gt; response.json())\n    .then(data =&gt; console.log(data));\n};\nreader.readAsDataURL(fileInput.files[0]);\n</code></pre>    ```</p>"},{"location":"module3/frameworks/#bonnes-pratiques-pour-les-projets-professionnels-15-min","title":"Bonnes pratiques pour les projets professionnels (15 min)","text":"<p>Pour conclure cette phase, passons en revue les bonnes pratiques essentielles pour d\u00e9velopper des applications de Deep Learning en contexte professionnel:</p>"},{"location":"module3/frameworks/#1-structure-du-code","title":"1. Structure du code","text":"<ul> <li>Modularit\u00e9: S\u00e9parez clairement les diff\u00e9rentes fonctionnalit\u00e9s (pr\u00e9traitement, mod\u00e8le, API)</li> <li>Documentation: Commentez votre code et cr\u00e9ez une documentation utilisateur</li> <li>Gestion d'erreurs: Pr\u00e9voyez des cas d'erreur et des messages adapt\u00e9s</li> <li>Logging: Ajoutez des logs pour faciliter le d\u00e9bogage</li> </ul>"},{"location":"module3/frameworks/#2-performances","title":"2. Performances","text":"<ul> <li>Batch processing: Traitez les donn\u00e9es par lots plut\u00f4t qu'individuellement</li> <li>Mise en cache: \u00c9vitez de recharger le mod\u00e8le \u00e0 chaque requ\u00eate</li> <li>Pr\u00e9calcul: Pr\u00e9calculez ce qui peut l'\u00eatre pour acc\u00e9l\u00e9rer les inf\u00e9rences</li> <li>Optimisation mat\u00e9rielle: Utilisez GPU/TPU quand disponible, CPU optimis\u00e9 sinon</li> </ul>"},{"location":"module3/frameworks/#3-securite","title":"3. S\u00e9curit\u00e9","text":"<ul> <li>Validation des entr\u00e9es: V\u00e9rifiez toujours les donn\u00e9es entrantes</li> <li>Limitation de taille: Fixez une taille maximale pour les fichiers</li> <li>Rate limiting: Limitez le nombre de requ\u00eates par utilisateur</li> <li>Sanitization: Nettoyez les chemins de fichiers et autres entr\u00e9es sensibles</li> </ul>"},{"location":"module3/frameworks/#4-deploiement","title":"4. D\u00e9ploiement","text":"<ul> <li>Conteneurisation: Utilisez Docker pour faciliter le d\u00e9ploiement</li> <li>CI/CD: Automatisez les tests et le d\u00e9ploiement</li> <li>Monitoring: Surveillez les performances et erreurs</li> <li>Versioning: Versionnez vos mod\u00e8les et API</li> </ul>"},{"location":"module3/frameworks/#conclusion-et-transition","title":"Conclusion et transition","text":"<p>Cette phase vous a permis de d\u00e9couvrir comment utiliser efficacement TensorFlow/Keras dans un contexte professionnel, en d\u00e9veloppant une API simple mais fonctionnelle de reconnaissance d'images. Dans la prochaine partie, nous allons nous concentrer sur l'am\u00e9lioration des performances de nos mod\u00e8les pour les rendre plus adapt\u00e9s \u00e0 des environnements de production.</p> <p>Retour au Module 3 Continuer vers l'am\u00e9lioration des performances</p>"},{"location":"module3/integration/","title":"Phase 2 : Am\u00e9lioration des performances et int\u00e9gration (1h30)","text":""},{"location":"module3/integration/#introduction-aux-techniques-doptimisation-15-min","title":"Introduction aux techniques d'optimisation (15 min)","text":"<p>Objectif: Comprendre les diff\u00e9rentes techniques d'optimisation des mod\u00e8les de Deep Learning pour les environnements \u00e0 ressources limit\u00e9es et les applications en production.</p>"},{"location":"module3/integration/#pourquoi-optimiser-les-modeles","title":"Pourquoi optimiser les mod\u00e8les ?","text":"<p>Dans un contexte d'entreprise, l'optimisation des mod\u00e8les est essentielle pour plusieurs raisons :</p> <ul> <li>Co\u00fbts d'infrastructure : R\u00e9duire les besoins en ressources mat\u00e9rielles</li> <li>Latence : Am\u00e9liorer le temps de r\u00e9ponse pour une meilleure exp\u00e9rience utilisateur</li> <li>\u00c9nergie : Diminuer la consommation \u00e9nerg\u00e9tique (crucial pour les appareils mobiles)</li> <li>Accessibilit\u00e9 : Permettre l'ex\u00e9cution sur des appareils \u00e0 ressources limit\u00e9es</li> </ul>"},{"location":"module3/integration/#panorama-des-techniques-doptimisation","title":"Panorama des techniques d'optimisation","text":""},{"location":"module3/integration/#1-quantification","title":"1. Quantification","text":"<p>La quantification consiste \u00e0 r\u00e9duire la pr\u00e9cision des poids du mod\u00e8le (par exemple, passer de float32 \u00e0 int8). Cette technique peut r\u00e9duire la taille du mod\u00e8le par 4 et acc\u00e9l\u00e9rer l'inf\u00e9rence, avec une perte de pr\u00e9cision souvent n\u00e9gligeable.</p> <pre><code># Exemple de quantification avec TensorFlow Lite\nimport tensorflow as tf\n\n# Convertir en TFLite\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Appliquer la quantification post-entra\u00eenement\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nquantized_model = converter.convert()\n\n# Comparaison des tailles\nprint(f\"Taille du mod\u00e8le original: {len(tflite_model) / 1024:.2f} Ko\")\nprint(f\"Taille du mod\u00e8le quantifi\u00e9: {len(quantized_model) / 1024:.2f} Ko\")\n</code></pre>"},{"location":"module3/integration/#2-elagage-pruning","title":"2. \u00c9lagage (Pruning)","text":"<p>L'\u00e9lagage consiste \u00e0 supprimer les connexions (poids) les moins importantes du r\u00e9seau. Cette technique peut r\u00e9duire la taille du mod\u00e8le et acc\u00e9l\u00e9rer l'inf\u00e9rence sans impact significatif sur les performances.</p> <pre><code># Exemple d'\u00e9lagage avec TensorFlow Model Optimization\nimport tensorflow_model_optimization as tfmot\n\n# Appliquer l'\u00e9lagage pendant l'entra\u00eenement\npruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n    initial_sparsity=0.0, final_sparsity=0.5,\n    begin_step=0, end_step=1000\n)\n\npruned_model = tfmot.sparsity.keras.prune_low_magnitude(\n    model, pruning_schedule=pruning_schedule\n)\n\n# Compiler et entra\u00eener le mod\u00e8le \u00e9lagu\u00e9\npruned_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\npruned_model.fit(...)\n\n# Enlever les masques d'\u00e9lagage pour le d\u00e9ploiement\nfinal_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n</code></pre>"},{"location":"module3/integration/#3-distillation-de-connaissances","title":"3. Distillation de connaissances","text":"<p>La distillation consiste \u00e0 entra\u00eener un mod\u00e8le plus petit (\u00e9l\u00e8ve) \u00e0 imiter un mod\u00e8le plus grand et plus performant (enseignant).</p> <pre><code># Exemple simplifi\u00e9 de distillation\nimport tensorflow as tf\n\n# Mod\u00e8le enseignant (grand et pr\u00e9cis, d\u00e9j\u00e0 entra\u00een\u00e9)\nteacher_model = tf.keras.applications.MobileNetV2(weights='imagenet')\n\n# Mod\u00e8le \u00e9l\u00e8ve (plus petit)\nstudent_model = tf.keras.Sequential([\n    # Architecture plus l\u00e9g\u00e8re...\n])\n\n# Fonction de perte pour la distillation\ndef distillation_loss(y_true, y_pred, teacher_pred, temperature=5.0, alpha=0.1):\n    # Perte de classification standard\n    student_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n\n    # Perte de distillation (imiter l'enseignant)\n    teacher_pred_soft = tf.nn.softmax(teacher_pred / temperature)\n    student_pred_soft = tf.nn.softmax(y_pred / temperature)\n    distillation_loss = tf.keras.losses.kullback_leibler_divergence(teacher_pred_soft, student_pred_soft)\n\n    # Combinaison des deux pertes\n    return alpha * student_loss + (1 - alpha) * distillation_loss * (temperature ** 2)\n</code></pre>"},{"location":"module3/integration/#4-architectures-efficientes","title":"4. Architectures efficientes","text":"<p>Utiliser des architectures sp\u00e9cialement con\u00e7ues pour l'efficience comme MobileNet, EfficientNet ou SqueezeNet.</p> <pre><code># Exemple d'utilisation de MobileNetV2\nimport tensorflow as tf\n\n# Cr\u00e9er un mod\u00e8le MobileNetV2 avec taille d'entr\u00e9e et multiplicateur de largeur r\u00e9duits\nmodel = tf.keras.applications.MobileNetV2(\n    input_shape=(160, 160, 3),  # R\u00e9solution r\u00e9duite\n    alpha=0.75,  # Multiplicateur de largeur r\u00e9duit\n    include_top=True,\n    weights='imagenet'\n)\n</code></pre>"},{"location":"module3/integration/#atelier-pratique-optimisation-dun-modele-30-min","title":"Atelier pratique : Optimisation d'un mod\u00e8le (30 min)","text":""},{"location":"module3/integration/#objectif","title":"Objectif","text":"<p>Prendre un mod\u00e8le existant et appliquer plusieurs techniques d'optimisation pour am\u00e9liorer ses performances tout en pr\u00e9servant sa pr\u00e9cision.</p>"},{"location":"module3/integration/#instructions","title":"Instructions","text":"<ol> <li>Pr\u00e9paration du mod\u00e8le initial (5 min)</li> </ol> <p>Commencez par charger un mod\u00e8le ResNet50 pr\u00e9-entra\u00een\u00e9 comme point de d\u00e9part :</p> <pre><code>import tensorflow as tf\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\n\n# Charger le mod\u00e8le ResNet50 pr\u00e9-entra\u00een\u00e9\nbase_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=True)\n\n# Fonction pour mesurer le temps d'inf\u00e9rence\ndef benchmark_model(model, input_shape, num_iterations=50):\n    # Cr\u00e9er des donn\u00e9es al\u00e9atoires pour le benchmark\n    dummy_input = np.random.random((1,) + input_shape)\n\n    # \u00c9chauffement\n    _ = model.predict(dummy_input)\n\n    # Mesure du temps d'inf\u00e9rence\n    start_time = time.time()\n    for _ in range(num_iterations):\n        _ = model.predict(dummy_input)\n    end_time = time.time()\n\n    inference_time = (end_time - start_time) / num_iterations\n    return inference_time\n\n# Mesurer les performances du mod\u00e8le initial\noriginal_time = benchmark_model(base_model, (224, 224, 3))\noriginal_size = sum(np.prod(w.shape) * w.dtype.size for w in base_model.weights) / (1024 * 1024)\n\nprint(f\"Mod\u00e8le original:\")\nprint(f\"Temps d'inf\u00e9rence moyen: {original_time*1000:.2f} ms\")\nprint(f\"Taille du mod\u00e8le: {original_size:.2f} Mo\")\n</code></pre> <ol> <li>Quantification post-entra\u00eenement (10 min)</li> </ol> <p>Appliquez la quantification TensorFlow Lite pour r\u00e9duire la taille du mod\u00e8le :</p> <pre><code># Conversion en TensorFlow Lite\nconverter = tf.lite.TFLiteConverter.from_keras_model(base_model)\ntflite_model = converter.convert()\n\n# Quantification post-entra\u00eenement\nconverter = tf.lite.TFLiteConverter.from_keras_model(base_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nquantized_model = converter.convert()\n\n# Taille des mod\u00e8les\ntflite_size = len(tflite_model) / (1024 * 1024)\nquantized_size = len(quantized_model) / (1024 * 1024)\n\nprint(f\"Taille du mod\u00e8le TFLite: {tflite_size:.2f} Mo\")\nprint(f\"Taille du mod\u00e8le quantifi\u00e9: {quantized_size:.2f} Mo\")\nprint(f\"R\u00e9duction de taille: {(1 - quantized_size/tflite_size) * 100:.2f}%\")\n\n# Enregistrer les mod\u00e8les\nwith open('model.tflite', 'wb') as f:\n    f.write(tflite_model)\n\nwith open('model_quantized.tflite', 'wb') as f:\n    f.write(quantized_model)\n\n# Cr\u00e9ation d'un interpr\u00e9teur pour mesurer les performances\ninterpreter = tf.lite.Interpreter(model_content=quantized_model)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ndef benchmark_tflite(interpreter, input_shape, num_iterations=50):\n    # Donn\u00e9es al\u00e9atoires pour le benchmark\n    dummy_input = np.random.random((1,) + input_shape).astype(np.float32)\n\n    # \u00c9chauffement\n    interpreter.set_tensor(input_details[0]['index'], dummy_input)\n    interpreter.invoke()\n\n    # Mesure du temps d'inf\u00e9rence\n    start_time = time.time()\n    for _ in range(num_iterations):\n        interpreter.set_tensor(input_details[0]['index'], dummy_input)\n        interpreter.invoke()\n    end_time = time.time()\n\n    inference_time = (end_time - start_time) / num_iterations\n    return inference_time\n\n# Mesurer les performances du mod\u00e8le quantifi\u00e9\nquantized_time = benchmark_tflite(interpreter, (224, 224, 3))\n\nprint(f\"Temps d'inf\u00e9rence du mod\u00e8le quantifi\u00e9: {quantized_time*1000:.2f} ms\")\nprint(f\"Am\u00e9lioration du temps d'inf\u00e9rence: {(1 - quantized_time/original_time) * 100:.2f}%\")\n</code></pre> <ol> <li>Passage \u00e0 un mod\u00e8le plus l\u00e9ger (10 min)</li> </ol> <p>Utilisez MobileNetV2, une architecture con\u00e7ue pour l'efficience :</p> <pre><code># Charger MobileNetV2 pr\u00e9-entra\u00een\u00e9\nmobile_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=True)\n\n# Mesurer les performances\nmobile_time = benchmark_model(mobile_model, (224, 224, 3))\nmobile_size = sum(np.prod(w.shape) * w.dtype.size for w in mobile_model.weights) / (1024 * 1024)\n\nprint(f\"MobileNetV2:\")\nprint(f\"Temps d'inf\u00e9rence moyen: {mobile_time*1000:.2f} ms\")\nprint(f\"Taille du mod\u00e8le: {mobile_size:.2f} Mo\")\nprint(f\"Am\u00e9lioration du temps par rapport \u00e0 ResNet50: {(1 - mobile_time/original_time) * 100:.2f}%\")\nprint(f\"R\u00e9duction de taille par rapport \u00e0 ResNet50: {(1 - mobile_size/original_size) * 100:.2f}%\")\n\n# Quantification de MobileNetV2\nconverter = tf.lite.TFLiteConverter.from_keras_model(mobile_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nquantized_mobile = converter.convert()\n\nquantized_mobile_size = len(quantized_mobile) / (1024 * 1024)\n\n# Cr\u00e9er l'interpr\u00e9teur pour le mod\u00e8le mobile quantifi\u00e9\nmobile_interpreter = tf.lite.Interpreter(model_content=quantized_mobile)\nmobile_interpreter.allocate_tensors()\n\n# Mesurer les performances\nquantized_mobile_time = benchmark_tflite(mobile_interpreter, (224, 224, 3))\n\nprint(f\"MobileNetV2 quantifi\u00e9:\")\nprint(f\"Temps d'inf\u00e9rence moyen: {quantized_mobile_time*1000:.2f} ms\")\nprint(f\"Taille du mod\u00e8le: {quantized_mobile_size:.2f} Mo\")\nprint(f\"Am\u00e9lioration totale du temps: {(1 - quantized_mobile_time/original_time) * 100:.2f}%\")\nprint(f\"R\u00e9duction totale de taille: {(1 - quantized_mobile_size/original_size) * 100:.2f}%\")\n</code></pre> <ol> <li>Visualisation et analyse des r\u00e9sultats (5 min)</li> </ol> <pre><code># R\u00e9sum\u00e9 des r\u00e9sultats\nmodels = ['ResNet50', 'ResNet50 quantifi\u00e9', 'MobileNetV2', 'MobileNetV2 quantifi\u00e9']\ninference_times = [original_time*1000, quantized_time*1000, mobile_time*1000, quantized_mobile_time*1000]\nmodel_sizes = [original_size, quantized_size, mobile_size, quantized_mobile_size]\n\n# Visualisation des r\u00e9sultats\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Graphique des temps d'inf\u00e9rence\nax1.bar(models, inference_times, color=['blue', 'lightblue', 'green', 'lightgreen'])\nax1.set_ylabel('Temps d\\'inf\u00e9rence (ms)')\nax1.set_title('Comparaison des temps d\\'inf\u00e9rence')\nax1.set_xticklabels(models, rotation=45, ha='right')\n\n# Graphique des tailles de mod\u00e8le\nax2.bar(models, model_sizes, color=['blue', 'lightblue', 'green', 'lightgreen'])\nax2.set_ylabel('Taille du mod\u00e8le (Mo)')\nax2.set_title('Comparaison des tailles de mod\u00e8le')\nax2.set_xticklabels(models, rotation=45, ha='right')\n\nplt.tight_layout()\nplt.show()\n\n# Tableau r\u00e9capitulatif\nfrom tabulate import tabulate\n\ndata = []\nfor i, model_name in enumerate(models):\n    data.append([\n        model_name, \n        f\"{inference_times[i]:.2f} ms\", \n        f\"{model_sizes[i]:.2f} Mo\",\n        f\"{(1 - inference_times[i]/inference_times[0])*100:.2f}%\" if i &gt; 0 else \"R\u00e9f\u00e9rence\",\n        f\"{(1 - model_sizes[i]/model_sizes[0])*100:.2f}%\" if i &gt; 0 else \"R\u00e9f\u00e9rence\"\n    ])\n\nheaders = [\"Mod\u00e8le\", \"Temps d'inf\u00e9rence\", \"Taille\", \"Am\u00e9lioration temps\", \"R\u00e9duction taille\"]\nprint(tabulate(data, headers=headers, tablefmt=\"grid\"))\n</code></pre>"},{"location":"module3/integration/#tp-integration-de-modeles-pre-entraines-dans-des-applications-45-min","title":"TP : Int\u00e9gration de mod\u00e8les pr\u00e9-entra\u00een\u00e9s dans des applications (45 min)","text":""},{"location":"module3/integration/#objectif_1","title":"Objectif","text":"<p>Cr\u00e9er une application web simple qui utilise un mod\u00e8le pr\u00e9-entra\u00een\u00e9 optimis\u00e9 pour une t\u00e2che sp\u00e9cifique.</p>"},{"location":"module3/integration/#partie-1-preparation-et-sauvegarde-du-modele-optimise-15-min","title":"Partie 1 : Pr\u00e9paration et sauvegarde du mod\u00e8le optimis\u00e9 (15 min)","text":"<pre><code>import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.models import Model\n\n# Initialisation du mod\u00e8le de base\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Gel des couches pr\u00e9-entra\u00een\u00e9es\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Ajout des couches de classification personnalis\u00e9es\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128, activation='relu')(x)\npredictions = Dense(10, activation='softmax')(x)  # 10 classes pour CIFAR-10\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compilation du mod\u00e8le\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# T\u00e9l\u00e9chargement et pr\u00e9paration du jeu de donn\u00e9es CIFAR-10\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n# Normalisation des donn\u00e9es\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n# Conversion des labels en cat\u00e9gories\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n\n# Redimensionnement des images pour MobileNetV2\nx_train_resized = tf.image.resize(x_train, (224, 224)).numpy()\nx_test_resized = tf.image.resize(x_test, (224, 224)).numpy()\n\n# Entra\u00eenement rapide (seulement pour d\u00e9monstration - normalement n\u00e9cessiterait plus d'\u00e9poques)\nhistory = model.fit(\n    x_train_resized[:1000],  # Utiliser un sous-ensemble pour plus de rapidit\u00e9\n    y_train[:1000],\n    batch_size=32,\n    epochs=3,\n    validation_split=0.2,\n    verbose=1\n)\n\n# \u00c9valuation sur le jeu de test\ntest_loss, test_acc = model.evaluate(x_test_resized[:500], y_test[:500])\nprint(f\"Pr\u00e9cision sur le jeu de test: {test_acc*100:.2f}%\")\n\n# Sauvegarde du mod\u00e8le au format TensorFlow SavedModel\nmodel.save('cifar10_mobile_model')\n\n# Conversion en TFLite quantifi\u00e9\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\nwith open('cifar10_mobile_quantized.tflite', 'wb') as f:\n    f.write(tflite_model)\n\nprint(\"Mod\u00e8le sauvegard\u00e9 avec succ\u00e8s!\")\n</code></pre>"},{"location":"module3/integration/#partie-2-developpement-de-lapplication-web-flask-30-min","title":"Partie 2 : D\u00e9veloppement de l'application web Flask (30 min)","text":"<p>Cr\u00e9ez un fichier <code>app.py</code> pour l'application Flask :</p> <pre><code>from flask import Flask, request, jsonify, render_template\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\nimport io\nimport base64\n\napp = Flask(__name__)\n\n# Liste des classes CIFAR-10\nclass_names = [\n    'Avion', 'Automobile', 'Oiseau', 'Chat', 'Cerf',\n    'Chien', 'Grenouille', 'Cheval', 'Bateau', 'Camion'\n]\n\n# Chargement du mod\u00e8le TFLite\ninterpreter = tf.lite.Interpreter(model_path='cifar10_mobile_quantized.tflite')\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Fonction de pr\u00e9traitement des images\ndef preprocess_image(image):\n    # Redimensionnement \u00e0 224x224\n    image = image.resize((224, 224))\n\n    # Conversion en tableau numpy et normalisation\n    img_array = np.array(image)\n    img_array = img_array.astype('float32') / 255.0\n\n    # Ajout de la dimension de batch\n    img_array = np.expand_dims(img_array, axis=0)\n\n    return img_array\n\n# Route principale - page d'accueil\n@app.route('/')\ndef index():\n    return render_template('index.html', class_names=class_names)\n\n# Route pour les pr\u00e9dictions\n@app.route('/predict', methods=['POST'])\ndef predict():\n    # V\u00e9rification de la pr\u00e9sence d'une image\n    if 'image' not in request.files:\n        # V\u00e9rifie si l'image est envoy\u00e9e en base64\n        if request.json and 'image' in request.json:\n            image_data = base64.b64decode(request.json['image'].split(',')[1])\n            image = Image.open(io.BytesIO(image_data))\n        else:\n            return jsonify({'error': 'Aucune image fournie'}), 400\n    else:\n        # Lecture de l'image depuis les fichiers\n        file = request.files['image']\n        image = Image.open(file.stream)\n\n    # Pr\u00e9traitement de l'image\n    processed_image = preprocess_image(image)\n\n    # Pr\u00e9diction avec TFLite\n    interpreter.set_tensor(input_details[0]['index'], processed_image)\n    interpreter.invoke()\n    predictions = interpreter.get_tensor(output_details[0]['index'])[0]\n\n    # Obtention des 3 meilleures pr\u00e9dictions\n    top_3_indices = predictions.argsort()[-3:][::-1]\n    top_3_predictions = [\n        {'class': class_names[idx], 'score': float(predictions[idx])}\n        for idx in top_3_indices\n    ]\n\n    # Cr\u00e9ation de la r\u00e9ponse\n    response = {\n        'predictions': top_3_predictions,\n        'all_scores': predictions.tolist()\n    }\n\n    return jsonify(response)\n\nif __name__ == '__main__':\n    app.run(debug=True, host='0.0.0.0', port=5000)\n</code></pre> <p>Cr\u00e9ez un dossier <code>templates</code> et ajoutez-y un fichier <code>index.html</code> :</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Classificateur CIFAR-10 Optimis\u00e9&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #f5f5f5;\n        }\n        h1, h2 {\n            color: #333;\n        }\n        .container {\n            background-color: white;\n            border-radius: 8px;\n            padding: 20px;\n            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);\n            margin-bottom: 20px;\n        }\n        .upload-section {\n            text-align: center;\n            margin: 20px 0;\n        }\n        .preview-container {\n            display: flex;\n            justify-content: center;\n            margin: 20px 0;\n        }\n        #preview {\n            max-width: 300px;\n            max-height: 300px;\n            display: none;\n            border: 1px solid #ddd;\n            border-radius: 4px;\n        }\n        #results {\n            display: none;\n            margin-top: 20px;\n        }\n        .prediction-item {\n            display: flex;\n            justify-content: space-between;\n            padding: 10px;\n            margin: 5px 0;\n            background-color: #f9f9f9;\n            border-radius: 4px;\n        }\n        .prediction-item:first-child {\n            background-color: #e8f5e9;\n            font-weight: bold;\n        }\n        .chart-container {\n            height: 300px;\n            margin-top: 20px;\n        }\n        button {\n            background-color: #4CAF50;\n            color: white;\n            border: none;\n            padding: 10px 20px;\n            text-align: center;\n            text-decoration: none;\n            display: inline-block;\n            font-size: 16px;\n            margin: 4px 2px;\n            cursor: pointer;\n            border-radius: 4px;\n        }\n        #loading {\n            display: none;\n            text-align: center;\n            margin: 20px 0;\n        }\n        .specs {\n            background-color: #f0f7ff;\n            padding: 15px;\n            border-radius: 4px;\n            margin-top: 30px;\n        }\n        .specs h3 {\n            margin-top: 0;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;h1&gt;Classificateur d'images CIFAR-10 Optimis\u00e9&lt;/h1&gt;\n        &lt;p&gt;Cette application utilise un mod\u00e8le MobileNetV2 quantifi\u00e9 et optimis\u00e9 pour classifier des images dans 10 cat\u00e9gories.&lt;/p&gt;\n\n        &lt;div class=\"upload-section\"&gt;\n            &lt;input type=\"file\" id=\"image-upload\" accept=\"image/*\"&gt;\n            &lt;button id=\"predict-button\"&gt;Classifier l'image&lt;/button&gt;\n        &lt;/div&gt;\n\n        &lt;div class=\"preview-container\"&gt;\n            &lt;img id=\"preview\" src=\"#\" alt=\"Aper\u00e7u de l'image\"&gt;\n        &lt;/div&gt;\n\n        &lt;div id=\"loading\"&gt;Classification en cours...&lt;/div&gt;\n\n        &lt;div id=\"results\"&gt;\n            &lt;h2&gt;R\u00e9sultats&lt;/h2&gt;\n            &lt;div id=\"predictions-container\"&gt;&lt;/div&gt;\n\n            &lt;div class=\"chart-container\"&gt;\n                &lt;canvas id=\"prediction-chart\"&gt;&lt;/canvas&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n\n        &lt;div class=\"specs\"&gt;\n            &lt;h3&gt;Sp\u00e9cifications techniques&lt;/h3&gt;\n            &lt;p&gt;Cette application utilise:&lt;/p&gt;\n            &lt;ul&gt;\n                &lt;li&gt;Architecture: MobileNetV2 (mod\u00e8le l\u00e9ger)&lt;/li&gt;\n                &lt;li&gt;Optimisations: Quantification post-entra\u00eenement (int8)&lt;/li&gt;\n                &lt;li&gt;Framework: TensorFlow Lite&lt;/li&gt;\n                &lt;li&gt;Taille du mod\u00e8le: ~3 Mo (contre ~14 Mo pour le mod\u00e8le non optimis\u00e9)&lt;/li&gt;\n                &lt;li&gt;Temps d'inf\u00e9rence moyen: ~50 ms (CPU)&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/chart.js\"&gt;&lt;/script&gt;\n    &lt;script&gt;\n        document.addEventListener('DOMContentLoaded', function() {\n            const imageUpload = document.getElementById('image-upload');\n            const previewImage = document.getElementById('preview');\n            const predictButton = document.getElementById('predict-button');\n            const resultsDiv = document.getElementById('results');\n            const loadingDiv = document.getElementById('loading');\n            const predictionsContainer = document.getElementById('predictions-container');\n\n            let chart = null;\n\n            // Classes CIFAR-10\n            const classNames = {{ class_names|tojson }};\n\n            // Pr\u00e9visualisation de l'image\n            imageUpload.addEventListener('change', function() {\n                if (this.files &amp;&amp; this.files[0]) {\n                    const reader = new FileReader();\n\n                    reader.onload = function(e) {\n                        previewImage.src = e.target.result;\n                        previewImage.style.display = 'block';\n                        resultsDiv.style.display = 'none';\n                    }\n\n                    reader.readAsDataURL(this.files[0]);\n                }\n            });\n\n            // Classification de l'image\n            predictButton.addEventListener('click', function() {\n                if (!imageUpload.files || !imageUpload.files[0]) {\n                    alert('Veuillez s\u00e9lectionner une image.');\n                    return;\n                }\n\n                // Afficher le chargement\n                loadingDiv.style.display = 'block';\n                resultsDiv.style.display = 'none';\n\n                const formData = new FormData();\n                formData.append('image', imageUpload.files[0]);\n\n                fetch('/predict', {\n                    method: 'POST',\n                    body: formData\n                })\n                .then(response =&gt; response.json())\n                .then(data =&gt; {\n                    // Cacher le chargement\n                    loadingDiv.style.display = 'none';\n\n                    // Afficher les r\u00e9sultats\n                    predictionsContainer.innerHTML = '';\n\n                    if (data.error) {\n                        predictionsContainer.innerHTML = `&lt;p&gt;${data.error}&lt;/p&gt;`;\n                    } else {\n                        // Afficher les top pr\u00e9dictions\n                        data.predictions.forEach(pred =&gt; {\n                            const item = document.createElement('div');\n                            item.className = 'prediction-item';\n\n                            const labelDiv = document.createElement('div');\n                            labelDiv.textContent = pred.class;\n\n                            const scoreDiv = document.createElement('div');\n                            scoreDiv.textContent = `${(pred.score * 100).toFixed(2)}%`;\n\n                            item.appendChild(labelDiv);\n                            item.appendChild(scoreDiv);\n                            predictionsContainer.appendChild(item);\n                        });\n\n                        // Mettre \u00e0 jour le graphique\n                        updateChart(data.all_scores);\n                    }\n\n                    resultsDiv.style.display = 'block';\n                })\n                .catch(error =&gt; {\n                    loadingDiv.style.display = 'none';\n                    alert('Erreur lors de la communication avec l\\'API: ' + error);\n                });\n            });\n\n            // Mise \u00e0 jour du graphique de pr\u00e9dictions\n            function updateChart(scores) {\n                const ctx = document.getElementById('prediction-chart').getContext('2d');\n\n                // D\u00e9truire le graphique existant s'il y en a un\n                if (chart) {\n                    chart.destroy();\n                }\n\n                // Cr\u00e9er un nouveau graphique\n                chart = new Chart(ctx, {\n                    type: 'bar',\n                    data: {\n                        labels: classNames,\n                        datasets: [{\n                            label: 'Probabilit\u00e9 (%)',\n                            data: scores.map(score =&gt; score * 100),\n                            backgroundColor: scores.map((score, index) =&gt; \n                                index === scores.indexOf(Math.max(...scores)) ? \n                                'rgba(76, 175, 80, 0.8)' : 'rgba(33, 150, 243, 0.5)'\n                            ),\n                            borderColor: scores.map((score, index) =&gt; \n                                index === scores.indexOf(Math.max(...scores)) ? \n                                'rgba(76, 175, 80, 1)' : 'rgba(33, 150, 243, 1)'\n                            ),\n                            borderWidth: 1\n                        }]\n                    },\n                    options: {\n                        scales: {\n                            y: {\n                                beginAtZero: true,\n                                max: 100\n                            }\n                        },\n                        plugins: {\n                            legend: {\n                                display: false\n                            }\n                        }\n                    }\n                });\n            }\n        });\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"module3/integration/#lancement-et-test","title":"Lancement et test","text":"<pre><code># Lancer l'application\n!python app.py\n</code></pre>"},{"location":"module3/integration/#bonnes-pratiques-pour-lintegration-de-modeles-dans-des-applications-web-15-min","title":"Bonnes pratiques pour l'int\u00e9gration de mod\u00e8les dans des applications web (15 min)","text":"<p>\u00c0 travers les ateliers pr\u00e9c\u00e9dents, nous avons explor\u00e9 plusieurs approches pour optimiser et int\u00e9grer des mod\u00e8les de Deep Learning. R\u00e9sumons les bonnes pratiques essentielles \u00e0 retenir:</p>"},{"location":"module3/integration/#1-choix-du-modele","title":"1. Choix du mod\u00e8le","text":"<ul> <li>Privil\u00e9gier les architectures efficientes: MobileNet, EfficientNet, SqueezeNet</li> <li>\u00c9valuer le compromis taille/pr\u00e9cision: Un petit mod\u00e8le moins pr\u00e9cis est souvent pr\u00e9f\u00e9rable \u00e0 un mod\u00e8le lourd inutilisable</li> <li>Adapter la complexit\u00e9 au cas d'usage: La classification d'images simples ne n\u00e9cessite pas un ResNet152</li> </ul>"},{"location":"module3/integration/#2-techniques-doptimisation","title":"2. Techniques d'optimisation","text":"<ul> <li>Quantification: Toujours essayer la quantification post-entra\u00eenement</li> <li>\u00c9lagage: Pour les mod\u00e8les plus grands, envisager l'\u00e9lagage pendant l'entra\u00eenement</li> <li>Distillation: Pour des cas plus avanc\u00e9s, envisager la distillation de connaissances</li> <li>Combinaison des techniques: Utiliser plusieurs techniques peut donner les meilleurs r\u00e9sultats</li> </ul>"},{"location":"module3/integration/#3-integration-cote-serveur","title":"3. Int\u00e9gration c\u00f4t\u00e9 serveur","text":"<ul> <li>Mise en cache: \u00c9viter de recharger le mod\u00e8le pour chaque requ\u00eate</li> <li>Traitement par lot: Regrouper les requ\u00eates quand c'est possible</li> <li>Gestion asynchrone: Utiliser des files d'attente pour les requ\u00eates intensives</li> <li>Surveillance des performances: Mettre en place des m\u00e9triques (temps d'inf\u00e9rence, utilisation m\u00e9moire)</li> </ul>"},{"location":"module3/integration/#4-integration-cote-client","title":"4. Int\u00e9gration c\u00f4t\u00e9 client","text":"<ul> <li>Pr\u00e9traitement efficace: Redimensionner les images c\u00f4t\u00e9 client quand c'est possible</li> <li>Feedback utilisateur: Toujours indiquer que le traitement est en cours</li> <li>D\u00e9gradation gracieuse: Pr\u00e9voir un comportement de repli en cas d'\u00e9chec de l'IA</li> <li>Conservation de contexte: Limiter les allers-retours avec le serveur</li> </ul>"},{"location":"module3/integration/#5-documentation-et-maintenance","title":"5. Documentation et maintenance","text":"<ul> <li>Versionnement des mod\u00e8les: Suivre les versions des mod\u00e8les d\u00e9ploy\u00e9s</li> <li>Tests A/B: Comparer les performances des diff\u00e9rentes optimisations</li> <li>Journalisation des erreurs: Capturer les cas o\u00f9 le mod\u00e8le \u00e9choue</li> <li>Mise \u00e0 jour progressive: Planifier des am\u00e9liorations incr\u00e9mentales</li> </ul>"},{"location":"module3/integration/#conclusion-et-transition","title":"Conclusion et transition","text":"<p>Dans cette phase, nous avons explor\u00e9 des techniques d'optimisation essentielles pour rendre les mod\u00e8les de Deep Learning utilisables dans des applications r\u00e9elles. Nous avons vu comment r\u00e9duire la taille des mod\u00e8les, acc\u00e9l\u00e9rer leur inf\u00e9rence et les int\u00e9grer dans des applications web.</p> <p>Ces comp\u00e9tences sont cruciales pour le d\u00e9veloppement de votre projet final de chatbot p\u00e9dagogique, o\u00f9 les performances et l'int\u00e9gration joueront un r\u00f4le important dans l'exp\u00e9rience utilisateur.</p> <p>Dans la prochaine phase, nous allons nous concentrer sur la pr\u00e9paration sp\u00e9cifique du projet de chatbot, en explorant l'API Mistral AI et en d\u00e9finissant le cahier des charges complet.</p> <p>Retour au Module 3 Continuer vers la pr\u00e9paration du projet</p>"},{"location":"module3/pratique-tensorflow-keras/","title":"Module 3 : Pratique TensorFlow/Keras pour BTS SIO","text":""},{"location":"module3/pratique-tensorflow-keras/#introduction","title":"Introduction","text":"<p>Ce module est enti\u00e8rement ax\u00e9 sur la pratique et con\u00e7u sp\u00e9cifiquement pour des \u00e9tudiants de BTS SIO. Vous allez apprendre \u00e0 utiliser TensorFlow/Keras \u00e0 travers des exercices concrets, avec deux options disponibles : - Version Google Colab (aucune installation requise) - Version locale (pour ceux qui souhaitent installer les outils sur leur machine)</p>"},{"location":"module3/pratique-tensorflow-keras/#prerequis","title":"Pr\u00e9requis","text":""},{"location":"module3/pratique-tensorflow-keras/#pour-la-version-google-colab","title":"Pour la version Google Colab","text":"<ul> <li>Un compte Google</li> <li>Navigateur web r\u00e9cent</li> <li>Aucune installation requise</li> </ul>"},{"location":"module3/pratique-tensorflow-keras/#pour-la-version-locale","title":"Pour la version locale","text":"<ul> <li>Python 3.7 ou sup\u00e9rieur install\u00e9</li> <li>Pip (gestionnaire de paquets Python)</li> <li>\u00c9diteur de code (VS Code recommand\u00e9)</li> </ul>"},{"location":"module3/pratique-tensorflow-keras/#partie-1-premier-modele-avec-tensorflowkeras-1h30","title":"Partie 1 : Premier mod\u00e8le avec TensorFlow/Keras (1h30)","text":""},{"location":"module3/pratique-tensorflow-keras/#objectifs","title":"Objectifs","text":"<ul> <li>Comprendre les bases de TensorFlow/Keras</li> <li>Cr\u00e9er et entra\u00eener un mod\u00e8le simple</li> <li>Visualiser les r\u00e9sultats</li> </ul>"},{"location":"module3/pratique-tensorflow-keras/#version-google-colab","title":"Version Google Colab","text":"<ol> <li>Acc\u00e8s au notebook</li> <li>Cliquez sur ce lien pour ouvrir le notebook : TensorFlow D\u00e9butant - Colab</li> <li> <p>Faites une copie dans votre Drive en cliquant sur \"Fichier &gt; Enregistrer une copie dans Drive\"</p> </li> <li> <p>Contenu du notebook</p> </li> <li>Introduction \u00e0 TensorFlow/Keras</li> <li>Chargement du dataset MNIST (chiffres manuscrits)</li> <li>Cr\u00e9ation d'un mod\u00e8le de base</li> <li>Entra\u00eenement et \u00e9valuation</li> <li> <p>Visualisation des r\u00e9sultats</p> </li> <li> <p>Exercices pratiques dans le notebook</p> </li> <li>Modifier le nombre de neurones</li> <li>Changer la fonction d'activation</li> <li>Ajouter des couches suppl\u00e9mentaires</li> <li>Observer l'impact sur la pr\u00e9cision</li> </ol>"},{"location":"module3/pratique-tensorflow-keras/#version-locale","title":"Version locale","text":"<ol> <li> <p>Installation <pre><code># Cr\u00e9er un environnement virtuel\npython -m venv tf_env\n\n# Activer l'environnement\n# Pour Windows\ntf_env\\Scripts\\activate\n# Pour macOS/Linux\nsource tf_env/bin/activate\n\n# Installer les paquets n\u00e9cessaires\npip install tensorflow numpy matplotlib jupyter\n</code></pre></p> </li> <li> <p>T\u00e9l\u00e9chargement du script</p> </li> <li>T\u00e9l\u00e9chargez tf_debutant.py </li> <li> <p>Placez-le dans votre dossier de travail</p> </li> <li> <p>Ex\u00e9cution du script <pre><code>python tf_debutant.py\n</code></pre></p> </li> <li> <p>Exercices pratiques    Le script contient des sections comment\u00e9es avec \"EXERCICE\" o\u00f9 vous devrez:</p> </li> <li>Modifier les hyperparam\u00e8tres</li> <li>Ajouter des couches au mod\u00e8le</li> <li>Changer les fonctions d'activation</li> <li>Ex\u00e9cuter le script apr\u00e8s chaque modification et observer les r\u00e9sultats</li> </ol>"},{"location":"module3/pratique-tensorflow-keras/#partie-2-reconnaissance-dimages-en-pratique-1h30","title":"Partie 2 : Reconnaissance d'images en pratique (1h30)","text":""},{"location":"module3/pratique-tensorflow-keras/#objectifs_1","title":"Objectifs","text":"<ul> <li>Utiliser un mod\u00e8le plus avanc\u00e9 pour la reconnaissance d'images</li> <li>Comprendre le concept de mod\u00e8le pr\u00e9-entra\u00een\u00e9</li> <li>Cr\u00e9er une interface simple pour tester le mod\u00e8le</li> </ul>"},{"location":"module3/pratique-tensorflow-keras/#version-google-colab_1","title":"Version Google Colab","text":"<ol> <li>Acc\u00e8s au notebook</li> <li>Ouvrez ce notebook : Reconnaissance d'images - Colab</li> <li> <p>Faites une copie dans votre Drive</p> </li> <li> <p>Contenu du notebook</p> </li> <li>Chargement d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (MobileNetV2)</li> <li>Pr\u00e9traitement d'images</li> <li>Classification d'images test</li> <li> <p>Cr\u00e9ation d'une interface avec les widgets Colab</p> </li> <li> <p>Exercices pratiques</p> </li> <li>Tester le mod\u00e8le avec vos propres images</li> <li>Modifier les param\u00e8tres de pr\u00e9traitement</li> <li>Comparer diff\u00e9rents mod\u00e8les pr\u00e9-entra\u00een\u00e9s</li> <li>Cr\u00e9er un tableau de r\u00e9sultats comparatifs</li> </ol>"},{"location":"module3/pratique-tensorflow-keras/#version-locale_1","title":"Version locale","text":"<ol> <li> <p>Installation des d\u00e9pendances suppl\u00e9mentaires <pre><code>pip install pillow matplotlib\n</code></pre></p> </li> <li> <p>T\u00e9l\u00e9chargement des fichiers</p> </li> <li>T\u00e9l\u00e9chargez image_classifier.zip</li> <li> <p>D\u00e9compressez l'archive dans votre dossier de travail</p> </li> <li> <p>Structure du projet <pre><code>image_classifier/\n\u251c\u2500\u2500 classifier.py         # Script principal\n\u251c\u2500\u2500 images/               # Dossier pour vos images test\n\u2502   \u251c\u2500\u2500 sample1.jpg\n\u2502   \u251c\u2500\u2500 sample2.jpg\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 utils/                # Utilitaires Python\n    \u251c\u2500\u2500 model_loader.py   # Chargement du mod\u00e8le\n    \u2514\u2500\u2500 preprocessing.py  # Pr\u00e9traitement des images\n</code></pre></p> </li> <li> <p>Ex\u00e9cution du script <pre><code>cd image_classifier\npython classifier.py\n</code></pre></p> </li> </ol> <p>Le script vous demandera de s\u00e9lectionner une image dans le dossier images/ et affichera les r\u00e9sultats dans une fen\u00eatre graphique avec matplotlib.</p> <ol> <li>Exercices pratiques</li> <li>Compl\u00e9ter les fonctions marqu\u00e9es \"TODO\" dans classifier.py</li> <li>Ajouter une fonction pour afficher le top 5 des pr\u00e9dictions</li> <li>Cr\u00e9er une fonction pour comparer plusieurs images c\u00f4te \u00e0 c\u00f4te</li> <li>Impl\u00e9menter un syst\u00e8me simple de batch processing pour traiter plusieurs images</li> </ol>"},{"location":"module3/pratique-tensorflow-keras/#partie-3-mini-projet-classificateur-dimages-personnalise-1h","title":"Partie 3 : Mini-projet - Classificateur d'images personnalis\u00e9 (1h)","text":""},{"location":"module3/pratique-tensorflow-keras/#objectifs_2","title":"Objectifs","text":"<ul> <li>Adapter un mod\u00e8le pr\u00e9-entra\u00een\u00e9 \u00e0 vos propres donn\u00e9es</li> <li>Comprendre le concept de transfer learning</li> <li>Cr\u00e9er un classificateur personnalis\u00e9</li> </ul>"},{"location":"module3/pratique-tensorflow-keras/#version-google-colab_2","title":"Version Google Colab","text":"<ol> <li>Acc\u00e8s au notebook</li> <li>Ouvrez ce notebook : Classificateur personnalis\u00e9 - Colab</li> <li> <p>Faites une copie dans votre Drive</p> </li> <li> <p>Contenu du notebook</p> </li> <li>Pr\u00e9paration d'un petit dataset personnalis\u00e9 (fourni)</li> <li>Application du transfer learning sur MobileNetV2</li> <li>Entra\u00eenement du mod\u00e8le adapt\u00e9</li> <li> <p>Test et \u00e9valuation des performances</p> </li> <li> <p>Dataset fourni    Le notebook t\u00e9l\u00e9charge automatiquement un petit dataset contenant 3 cat\u00e9gories:</p> </li> <li>T\u00e9l\u00e9phones portables</li> <li>Ordinateurs portables</li> <li> <p>Tablettes</p> </li> <li> <p>Exercices pratiques</p> </li> <li>Adapter le code pour utiliser votre propre petit dataset (10-15 images par classe)</li> <li>Modifier les param\u00e8tres d'entra\u00eenement</li> <li>Exp\u00e9rimenter avec diff\u00e9rents mod\u00e8les de base</li> <li>Cr\u00e9er une matrice de confusion pour analyser les erreurs</li> </ol>"},{"location":"module3/pratique-tensorflow-keras/#version-locale_2","title":"Version locale","text":"<ol> <li>T\u00e9l\u00e9chargement des ressources</li> <li>T\u00e9l\u00e9chargez mini_projet_classificateur.zip</li> <li> <p>D\u00e9compressez l'archive dans votre dossier de travail</p> </li> <li> <p>Structure des fichiers <pre><code>mini_projet_classificateur/\n\u251c\u2500\u2500 custom_classifier.py      # Script principal\n\u251c\u2500\u2500 data_loader.py            # Chargement des donn\u00e9es\n\u251c\u2500\u2500 visualization.py          # Visualisations\n\u251c\u2500\u2500 dataset/                  # Dataset \u00e0 t\u00e9l\u00e9charger\n\u2502   \u251c\u2500\u2500 telephones/           # 15 images\n\u2502   \u251c\u2500\u2500 ordinateurs/          # 15 images\n\u2502   \u2514\u2500\u2500 tablettes/            # 15 images\n\u2514\u2500\u2500 README.md                 # Instructions d\u00e9taill\u00e9es\n</code></pre></p> </li> <li> <p>Ex\u00e9cution du projet <pre><code>cd mini_projet_classificateur\npython custom_classifier.py\n</code></pre></p> </li> </ol> <p>Le script vous guidera \u00e0 travers les \u00e9tapes de transfer learning avec une interface en ligne de commande.</p> <ol> <li>Exercices pratiques</li> <li>Compl\u00e9ter les parties manquantes dans custom_classifier.py</li> <li>Modifier les fonctions de data augmentation pour am\u00e9liorer la g\u00e9n\u00e9ralisation</li> <li>Impl\u00e9menter une visualisation avec matplotlib des r\u00e9sultats d'entra\u00eenement</li> <li>Ajouter un mode de test pour \u00e9valuer le mod\u00e8le sur de nouvelles images</li> </ol>"},{"location":"module3/pratique-tensorflow-keras/#evaluation-des-competences","title":"\u00c9valuation des comp\u00e9tences","text":"<p>\u00c0 la fin de ce module pratique, vous serez \u00e9valu\u00e9 sur:</p> <ol> <li>Compr\u00e9hension technique</li> <li>Capacit\u00e9 \u00e0 modifier et adapter les mod\u00e8les</li> <li> <p>Compr\u00e9hension des concepts cl\u00e9s (couches, fonctions d'activation, etc.)</p> </li> <li> <p>R\u00e9alisation pratique</p> </li> <li>Fonctionnalit\u00e9 des exercices compl\u00e9t\u00e9s</li> <li>Qualit\u00e9 du code produit</li> <li> <p>Interface utilisateur (pour la version locale)</p> </li> <li> <p>Analyse et am\u00e9lioration</p> </li> <li>Pertinence des modifications apport\u00e9es</li> <li>Analyse des r\u00e9sultats</li> <li>Propositions d'am\u00e9lioration</li> </ol>"},{"location":"module3/pratique-tensorflow-keras/#ressources-supplementaires","title":"Ressources suppl\u00e9mentaires","text":""},{"location":"module3/pratique-tensorflow-keras/#documentation-officielle","title":"Documentation officielle","text":"<ul> <li>TensorFlow pour d\u00e9butants</li> <li>Guide Keras</li> </ul>"},{"location":"module3/pratique-tensorflow-keras/#tutoriels-video","title":"Tutoriels vid\u00e9o","text":"<ul> <li>Premiers pas avec TensorFlow</li> <li>Transfer Learning expliqu\u00e9 simplement</li> </ul>"},{"location":"module3/pratique-tensorflow-keras/#aide-en-cas-de-probleme","title":"Aide en cas de probl\u00e8me","text":"<ul> <li>Forum d'entraide: forum.bts-sio.net</li> <li>Canal Discord de la classe</li> </ul>"},{"location":"module3/pratique-tensorflow-keras/#conclusion","title":"Conclusion","text":"<p>Ce module pratique vous a permis de mettre en \u0153uvre les concepts du Deep Learning avec TensorFlow/Keras. Vous avez cr\u00e9\u00e9 et adapt\u00e9 des mod\u00e8les, les avez test\u00e9s sur des donn\u00e9es r\u00e9elles et avez d\u00e9velopp\u00e9 des interfaces pour les utiliser. Ces comp\u00e9tences pratiques sont directement transf\u00e9rables en milieu professionnel, notamment pour des stages en d\u00e9veloppement d'applications ou en analyse de donn\u00e9es.</p>"},{"location":"module3/preparation-projet/","title":"Phase 3 : Pr\u00e9paration au projet final (45min)","text":""},{"location":"module3/preparation-projet/#presentation-du-cahier-des-charges-du-chatbot-pedagogique-15-min","title":"Pr\u00e9sentation du cahier des charges du chatbot p\u00e9dagogique (15 min)","text":"<p>Objectif: Comprendre les sp\u00e9cifications d\u00e9taill\u00e9es du projet final et les crit\u00e8res d'\u00e9valuation.</p>"},{"location":"module3/preparation-projet/#vision-du-projet","title":"Vision du projet","text":"<p>Le projet final consiste \u00e0 d\u00e9velopper un chatbot p\u00e9dagogique capable d'expliquer les concepts du Deep Learning, de r\u00e9pondre aux questions techniques et d'accompagner les apprenants dans leur d\u00e9couverte de ce domaine.</p> <p>\ud83c\udfaf Objectif : Concevoir un chatbot interactif qui aide les \u00e9tudiants de BTS SIO \u00e0 comprendre les concepts du Deep Learning \u00e0 travers des explications personnalis\u00e9es, des exemples concrets et des exercices adapt\u00e9s.</p>"},{"location":"module3/preparation-projet/#architecture-technique","title":"Architecture technique","text":"<p>Le chatbot s'appuiera sur une architecture moderne compos\u00e9e de trois \u00e9l\u00e9ments principaux :</p> <pre><code>flowchart LR\n    A[Interface Web] &lt;--&gt; B[Backend Python]\n    B &lt;--&gt; C[API Mistral AI]\n    D[Base de connaissances] &lt;--&gt; B</code></pre>"},{"location":"module3/preparation-projet/#1-interface-conversationnelle","title":"1. Interface conversationnelle","text":"<ul> <li>Interface web simple et intuitive</li> <li>Affichage des messages en format discussion</li> <li>Indicateur de chargement pendant le traitement</li> <li>Historique de conversation</li> </ul>"},{"location":"module3/preparation-projet/#2-backend-flaskfastapi","title":"2. Backend Flask/FastAPI","text":"<ul> <li>Gestion des requ\u00eates et des sessions</li> <li>Enrichissement des prompts avec la base de connaissances</li> <li>Communication avec l'API Mistral</li> <li>Logique de traitement des r\u00e9ponses</li> </ul>"},{"location":"module3/preparation-projet/#3-integration-api-mistral-ai","title":"3. Int\u00e9gration API Mistral AI","text":"<ul> <li>Configuration et param\u00e8trage des requ\u00eates</li> <li>Gestion du contexte de conversation</li> <li>Optimisation des prompts</li> <li>Gestion des erreurs et limitations</li> </ul>"},{"location":"module3/preparation-projet/#4-base-de-connaissances","title":"4. Base de connaissances","text":"<ul> <li>Structure JSON organis\u00e9e par concepts</li> <li>Exercices et quiz par th\u00e9matique</li> </ul>"},{"location":"module3/preparation-projet/#fonctionnalites-cles","title":"Fonctionnalit\u00e9s cl\u00e9s","text":"<p>Le chatbot p\u00e9dagogique offrira les fonctionnalit\u00e9s suivantes :</p> <ol> <li> <p>Explication des concepts</p> <ul> <li>D\u00e9finition adapt\u00e9e au niveau de l'utilisateur</li> <li>Exemples concrets pour illustrer chaque notion</li> <li>Analogies et comparaisons pour faciliter la compr\u00e9hension</li> </ul> </li> <li> <p>R\u00e9ponse aux questions</p> <ul> <li>Compr\u00e9hension des questions techniques</li> <li>R\u00e9ponses pr\u00e9cises bas\u00e9es sur la base de connaissances</li> <li>Capacit\u00e9 \u00e0 demander des clarifications si n\u00e9cessaire</li> </ul> </li> <li> <p>Progression adaptative</p> <ul> <li>D\u00e9tection du niveau de l'utilisateur</li> <li>Suggestions de concepts \u00e0 explorer ensuite</li> <li>Augmentation progressive de la complexit\u00e9</li> </ul> </li> <li> <p>Exercices interactifs</p> <ul> <li>G\u00e9n\u00e9ration de quiz sur les concepts vus</li> <li>Probl\u00e8mes simples \u00e0 r\u00e9soudre</li> <li>Feedback sur les r\u00e9ponses</li> </ul> </li> </ol>"},{"location":"module3/preparation-projet/#criteres-devaluation","title":"Crit\u00e8res d'\u00e9valuation","text":"<p>Votre chatbot p\u00e9dagogique sera \u00e9valu\u00e9 selon les crit\u00e8res suivants :</p> Crit\u00e8re Pond\u00e9ration Description Fonctionnalit\u00e9 30% Interface utilisable, r\u00e9ponses coh\u00e9rentes, absence de bugs Qualit\u00e9 p\u00e9dagogique 25% Pertinence des explications, adaptation au niveau, exemples appropri\u00e9s Int\u00e9gration technique 20% Utilisation efficace de l'API, gestion du contexte, optimisation Base de connaissances 15% Structure, couverture des concepts, pr\u00e9cision technique Documentation 10% Guide utilisateur, documentation technique, commentaires code"},{"location":"module3/preparation-projet/#livrables-attendus","title":"Livrables attendus","text":"<ol> <li>Code source complet du chatbot p\u00e9dagogique</li> <li>Base de connaissances structur\u00e9e sur le Deep Learning</li> <li>Documentation technique expliquant l'architecture et les choix d'impl\u00e9mentation</li> <li>Guide utilisateur pour la prise en main</li> <li>Pr\u00e9sentation de 5 minutes du projet finalis\u00e9</li> </ol>"},{"location":"module3/preparation-projet/#etude-de-cas-dentreprises-utilisant-des-chatbots-10-min","title":"\u00c9tude de cas d'entreprises utilisant des chatbots (10 min)","text":"<p>Avant de commencer le d\u00e9veloppement, examinons quelques exemples concrets d'entreprises qui ont mis en place des chatbots similaires \u00e0 celui que vous allez d\u00e9velopper.</p>"},{"location":"module3/preparation-projet/#cas-1-chatbot-pedagogique-pour-une-ecole-de-programmation","title":"Cas 1: Chatbot p\u00e9dagogique pour une \u00e9cole de programmation","text":"<p>Entreprise: CodeSchool (30 formateurs, 500+ \u00e9tudiants)</p> <p>Probl\u00e9matique: Les formateurs recevaient de nombreuses questions basiques identiques, ce qui limitait leur disponibilit\u00e9 pour des probl\u00e8mes plus complexes.</p> <p>Solution: D\u00e9veloppement d'un chatbot assistant bas\u00e9 sur une API de LLM, avec une base de connaissances construite \u00e0 partir du mat\u00e9riel de cours.</p> <p>Architecture: - Frontend: Interface web int\u00e9gr\u00e9e \u00e0 la plateforme d'apprentissage - Backend: API Flask avec mise en cache Redis - LLM: OpenAI API avec fine-tuning sp\u00e9cifique aux cours - Base de connaissances: Structur\u00e9e en JSON par modules de cours</p> <p>R\u00e9sultats: - R\u00e9duction de 40% des questions basiques aux formateurs - Satisfaction des \u00e9tudiants \u00e0 85% concernant les r\u00e9ponses du chatbot - ROI positif apr\u00e8s 4 mois d'utilisation - Cr\u00e9ation de 15 nouveaux modules de cours gr\u00e2ce au temps lib\u00e9r\u00e9</p> <p>Le\u00e7ons apprises: - Importance d'un syst\u00e8me de feedback imm\u00e9diat sur les r\u00e9ponses - N\u00e9cessit\u00e9 de maintenir la base de connaissances \u00e0 jour - Valeur des r\u00e9ponses comportant des exemples de code fonctionnels</p>"},{"location":"module3/preparation-projet/#cas-2-assistant-virtuel-pour-la-formation-interne","title":"Cas 2: Assistant virtuel pour la formation interne","text":"<p>Entreprise: TechConsult (cabinet de conseil IT, 120 employ\u00e9s)</p> <p>Probl\u00e9matique: Difficult\u00e9 \u00e0 former rapidement les nouveaux consultants sur les technologies sp\u00e9cifiques utilis\u00e9es par l'entreprise.</p> <p>Solution: Chatbot de formation accessible 24/7, int\u00e9gr\u00e9 \u00e0 l'intranet, avec connaissance des processus et technologies internes.</p> <p>Architecture: - Interface: Application web responsive - Backend: NodeJS avec FastAPI - LLM: Combinaison d'API locale et Mistral AI - Base de connaissances: Documents techniques convertis en embeddings vectoriels</p> <p>R\u00e9sultats: - R\u00e9duction du temps d'onboarding de 3 semaines \u00e0 10 jours - Augmentation de 25% du taux de r\u00e9ussite aux certifications internes - \u00c9conomie estim\u00e9e de 180 heures de formation par an - Adoption \u00e0 92% parmi les nouveaux employ\u00e9s</p> <p>Le\u00e7ons apprises: - L'importance d'utiliser le vocabulaire sp\u00e9cifique de l'entreprise - La valeur d'un historique de conversation persistant - L'utilit\u00e9 des prompts techniques bien formul\u00e9s</p> <p>Ces \u00e9tudes de cas montrent que les chatbots p\u00e9dagogiques peuvent apporter une valeur significative lorsqu'ils sont bien con\u00e7us et adapt\u00e9s \u00e0 leur contexte d'utilisation. Votre projet s'inspirera de ces bonnes pratiques tout en se focalisant sur l'enseignement du Deep Learning.</p>"},{"location":"module3/preparation-projet/#exploration-guidee-de-lapi-mistral-ai-20-min","title":"Exploration guid\u00e9e de l'API Mistral AI (20 min)","text":"<p>Maintenant, explorons l'API Mistral AI que vous utiliserez pour d\u00e9velopper votre chatbot p\u00e9dagogique.</p>"},{"location":"module3/preparation-projet/#introduction-a-mistral-ai","title":"Introduction \u00e0 Mistral AI","text":"<p>Mistral AI est une entreprise fran\u00e7aise qui d\u00e9veloppe des mod\u00e8les de langage de pointe, particuli\u00e8rement adapt\u00e9s pour des usages en fran\u00e7ais et dans un contexte \u00e9ducatif. Son API permet d'acc\u00e9der \u00e0 ces mod\u00e8les pour g\u00e9n\u00e9rer du texte, r\u00e9pondre \u00e0 des questions, et plus encore.</p>"},{"location":"module3/preparation-projet/#creation-dun-compte-et-cle-api","title":"Cr\u00e9ation d'un compte et cl\u00e9 API","text":"<ol> <li>Rendez-vous sur console.mistral.ai</li> <li>Cr\u00e9ez un compte (gratuit)</li> <li>Une fois connect\u00e9, cliquez sur \"API Keys\" dans le menu</li> <li>Cliquez sur \"Create API Key\", donnez-lui un nom (ex: \"Projet Chatbot BTS\")</li> <li>Important: Copiez et sauvegardez la cl\u00e9 g\u00e9n\u00e9r\u00e9e, elle ne sera plus affich\u00e9e ensuite</li> </ol>"},{"location":"module3/preparation-projet/#premier-test-avec-lapi","title":"Premier test avec l'API","text":"<p>Commen\u00e7ons par un exemple simple pour tester l'API:</p> <pre><code>import os\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\n# Configuration de l'API\napi_key = \"votre_cl\u00e9_api_ici\"  # Remplacez par votre cl\u00e9\nclient = MistralClient(api_key=api_key)\n\n# Messages\nmessages = [\n    ChatMessage(role=\"system\", content=\"Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans l'explication du Deep Learning pour des \u00e9tudiants de BTS SIO.\"),\n    ChatMessage(role=\"user\", content=\"Peux-tu m'expliquer simplement ce qu'est un r\u00e9seau de neurones convolutif?\")\n]\n\n# Appel \u00e0 l'API\nchat_response = client.chat(\n    model=\"mistral-tiny\",  # Mod\u00e8le le plus l\u00e9ger\n    messages=messages,\n)\n\n# Affichage de la r\u00e9ponse\nprint(chat_response.choices[0].message.content)\n</code></pre>"},{"location":"module3/preparation-projet/#structure-de-lapi-mistral","title":"Structure de l'API Mistral","text":"<p>L'API Mistral AI fonctionne avec une structure simple :</p> <ol> <li>Messages : Liste de messages repr\u00e9sentant une conversation, chacun avec un r\u00f4le (system, user, assistant)</li> <li>Mod\u00e8le : Choix du mod\u00e8le Mistral \u00e0 utiliser (mistral-tiny, mistral-small, mistral-medium...)</li> <li>Param\u00e8tres : Configuration du comportement (temp\u00e9rature, nombre max de tokens, etc.)</li> </ol>"},{"location":"module3/preparation-projet/#gestion-du-contexte-conversationnel","title":"Gestion du contexte conversationnel","text":"<p>Pour maintenir un contexte de conversation, il suffit d'ajouter les messages pr\u00e9c\u00e9dents \u00e0 chaque requ\u00eate :</p> <pre><code># Fonction pour g\u00e9rer une conversation\ndef chat_with_context(messages, user_input):\n    # Ajouter le message de l'utilisateur\n    messages.append(ChatMessage(role=\"user\", content=user_input))\n\n    # Appel \u00e0 l'API\n    response = client.chat(\n        model=\"mistral-tiny\",\n        messages=messages,\n    )\n\n    # R\u00e9cup\u00e9rer la r\u00e9ponse\n    assistant_message = response.choices[0].message.content\n\n    # Ajouter la r\u00e9ponse au contexte\n    messages.append(ChatMessage(role=\"assistant\", content=assistant_message))\n\n    return assistant_message, messages\n\n# Initialiser la conversation\nconversation = [\n    ChatMessage(role=\"system\", content=\"Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans l'explication du Deep Learning pour des \u00e9tudiants de BTS SIO.\")\n]\n\n# Premier \u00e9change\nresponse, conversation = chat_with_context(conversation, \"Qu'est-ce qu'un r\u00e9seau de neurones?\")\nprint(\"Assistant:\", response)\n\n# Deuxi\u00e8me \u00e9change (avec le contexte pr\u00e9c\u00e9dent)\nresponse, conversation = chat_with_context(conversation, \"Comment fonctionne l'apprentissage?\")\nprint(\"Assistant:\", response)\n</code></pre>"},{"location":"module3/preparation-projet/#optimisation-des-prompts","title":"Optimisation des prompts","text":"<p>La qualit\u00e9 des r\u00e9ponses d\u00e9pend beaucoup de la fa\u00e7on dont vous formulez vos instructions (prompts). Voici quelques conseils pour les optimiser :</p>"},{"location":"module3/preparation-projet/#1-instructions-systeme-claires-et-detaillees","title":"1. Instructions syst\u00e8me claires et d\u00e9taill\u00e9es","text":"<pre><code>system_prompt = \"\"\"\nTu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans le Deep Learning pour des \u00e9tudiants de BTS SIO. \nQuand tu r\u00e9ponds:\n1. Utilise un langage simple et accessible\n2. Fournis toujours un exemple concret\n3. Structure tes explications en plusieurs points\n4. Si tu n'es pas s\u00fbr d'une information, indique-le clairement\n5. Adapte le niveau technique au profil de l'\u00e9tudiant (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)\n\"\"\"\n</code></pre>"},{"location":"module3/preparation-projet/#2-enrichissement-avec-la-base-de-connaissances","title":"2. Enrichissement avec la base de connaissances","text":"<pre><code>def enrich_prompt_with_knowledge(user_query, knowledge_base, user_level=\"d\u00e9butant\"):\n    # Rechercher des informations pertinentes dans la base de connaissances\n    relevant_info = search_knowledge_base(user_query, knowledge_base)\n\n    # Enrichir le prompt avec ces informations\n    enriched_prompt = f\"\"\"\nQuestion de l'utilisateur: {user_query}\n\nInformations pertinentes (niveau: {user_level}):\n{relevant_info}\n\nR\u00e9ponds de mani\u00e8re p\u00e9dagogique en utilisant ces informations et en adaptant ton explication au niveau {user_level}.\n\"\"\"\n    return enriched_prompt\n</code></pre>"},{"location":"module3/preparation-projet/#3-parametrage-adapte","title":"3. Param\u00e9trage adapt\u00e9","text":"<pre><code># Pour des explications techniques (plus pr\u00e9cises, moins cr\u00e9atives)\ntechnical_params = {\n    \"temperature\": 0.3,  # Faible temp\u00e9rature pour des r\u00e9ponses plus d\u00e9terministes\n    \"max_tokens\": 500    # Limite de longueur raisonnable\n}\n\n# Pour des exemples et analogies (plus cr\u00e9atifs)\ncreative_params = {\n    \"temperature\": 0.7,  # Temp\u00e9rature plus \u00e9lev\u00e9e pour plus de cr\u00e9ativit\u00e9\n    \"max_tokens\": 300    # Limite de longueur adapt\u00e9e\n}\n\n# Fonction de choix de param\u00e8tres selon le contexte\ndef get_params_for_query(query):\n    if \"explique\" in query.lower() or \"d\u00e9finition\" in query.lower():\n        return technical_params\n    elif \"exemple\" in query.lower() or \"analogie\" in query.lower():\n        return creative_params\n    else:\n        return {\"temperature\": 0.5, \"max_tokens\": 400}  # Param\u00e8tres par d\u00e9faut\n</code></pre>"},{"location":"module3/preparation-projet/#gestion-des-erreurs-et-limites","title":"Gestion des erreurs et limites","text":"<p>Il est important de g\u00e9rer correctement les erreurs potentielles lors de l'utilisation de l'API :</p> <pre><code>def safe_api_call(messages, max_retries=3):\n    retries = 0\n    while retries &lt; max_retries:\n        try:\n            response = client.chat(\n                model=\"mistral-tiny\",\n                messages=messages,\n                timeout=10  # Timeout en secondes\n            )\n            return response\n        except Exception as e:\n            retries += 1\n            print(f\"Erreur API (tentative {retries}/{max_retries}): {e}\")\n            if retries &gt;= max_retries:\n                # R\u00e9ponse de secours si l'API \u00e9choue\n                return {\n                    \"choices\": [{\n                        \"message\": {\n                            \"content\": \"D\u00e9sol\u00e9, je rencontre des difficult\u00e9s techniques. Veuillez r\u00e9essayer dans quelques instants.\"\n                        }\n                    }]\n                }\n            # Attente avant nouvelle tentative\n            time.sleep(2)\n</code></pre>"},{"location":"module3/preparation-projet/#preparation-au-developpement","title":"Pr\u00e9paration au d\u00e9veloppement","text":"<p>Pour pr\u00e9parer efficacement votre projet de chatbot p\u00e9dagogique, voici les premi\u00e8res \u00e9tapes \u00e0 suivre :</p> <ol> <li> <p>Structure de votre projet <pre><code>chatbot-pedagogique/\n\u251c\u2500\u2500 app.py                   # Application principale Flask/FastAPI\n\u251c\u2500\u2500 config.py                # Configuration (cl\u00e9s API, param\u00e8tres)\n\u251c\u2500\u2500 templates/               # Templates HTML\n\u2502   \u2514\u2500\u2500 index.html           # Interface web\n\u251c\u2500\u2500 static/                  # Fichiers statiques (CSS, JS)\n\u251c\u2500\u2500 services/                # Services m\u00e9tier\n\u2502   \u251c\u2500\u2500 mistral_service.py   # Int\u00e9gration API Mistral\n\u2502   \u2514\u2500\u2500 knowledge_service.py # Gestion base de connaissances\n\u2514\u2500\u2500 knowledge_base/          # Base de connaissances\n    \u2514\u2500\u2500 concepts.json        # Structure des concepts DL\n</code></pre></p> </li> <li> <p>Technologies recommand\u00e9es</p> </li> <li>Backend: Python avec Flask ou FastAPI</li> <li>Frontend: HTML/CSS/JavaScript (ou framework simple comme Vue.js)</li> <li>API: Mistral AI</li> <li> <p>Base de connaissances: JSON structur\u00e9 ou base NoSQL</p> </li> <li> <p>Planification</p> </li> <li>S\u00e9ance 4, Phase 1 (2h30): D\u00e9veloppement du chatbot</li> <li>S\u00e9ance 4, Phase 2 (1h): Finalisation et tests</li> <li>S\u00e9ance 4, Phase 3 (30min): Pr\u00e9sentation des projets</li> </ol>"},{"location":"module3/preparation-projet/#conclusion","title":"Conclusion","text":"<p>Cette phase vous a permis de comprendre le cahier des charges d\u00e9taill\u00e9 de votre projet de chatbot p\u00e9dagogique, d'explorer les possibilit\u00e9s de l'API Mistral AI, et de vous pr\u00e9parer au d\u00e9veloppement.</p> <p>Lors de la prochaine s\u00e9ance, vous passerez \u00e0 l'impl\u00e9mentation concr\u00e8te de votre chatbot. D'ici l\u00e0, nous vous recommandons de: - Vous familiariser davantage avec l'API Mistral AI - R\u00e9fl\u00e9chir \u00e0 la structure de votre base de connaissances - Explorer des exemples de chatbots \u00e9ducatifs existants</p> <p>Retour au Module 3 Continuer vers le Module 4</p>"},{"location":"module3/qcm-evaluation-module3/","title":"Auto-\u00e9valuation du Module 3:  D\u00e9veloppement d'applications pratiques","text":"<p>Ce QCM vous permettra d'\u00e9valuer votre compr\u00e9hension des frameworks, de l'optimisation et de l'int\u00e9gration des mod\u00e8les de Deep Learning dans des applications concr\u00e8tes.</p>"},{"location":"module3/qcm-evaluation-module3/#instructions","title":"Instructions","text":"<ul> <li>Cochez la ou les r\u00e9ponses correctes pour chaque question</li> <li>Certaines questions peuvent avoir plusieurs r\u00e9ponses correctes</li> <li>\u00c0 la fin du questionnaire, r\u00e9pondez aux questions \u00e0 r\u00e9ponse courte et compl\u00e9tez l'exercice pratique</li> <li>Calculez votre score gr\u00e2ce au corrig\u00e9 fourni</li> <li>Dur\u00e9e recommand\u00e9e : 20 minutes</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#partie-a-qcm-frameworks-et-optimisation-15-points","title":"Partie A: QCM : Frameworks et optimisation (15 points)","text":"<p>Pour chaque question, cochez la ou les r\u00e9ponses correctes.</p>"},{"location":"module3/qcm-evaluation-module3/#1-parmi-ces-frameworks-de-deep-learning-lequel-est-le-plus-adapte-pour-le-deploiement-en-production-sur-des-appareils-mobiles","title":"1. Parmi ces frameworks de Deep Learning, lequel est le plus adapt\u00e9 pour le d\u00e9ploiement en production sur des appareils mobiles ?","text":"<ul> <li> a) PyTorch</li> <li> b) TensorFlow/Keras</li> <li> c) Scikit-learn</li> <li> d) Theano</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#2-quels-sont-les-avantages-des-modeles-pre-entraines-plusieurs-reponses-possibles","title":"2. Quels sont les avantages des mod\u00e8les pr\u00e9-entra\u00een\u00e9s ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> a) R\u00e9duction du temps de d\u00e9veloppement</li> <li> b) Besoin de moins de donn\u00e9es d'entra\u00eenement</li> <li> c) Poids plus petits que les mod\u00e8les entra\u00een\u00e9s from scratch</li> <li> d) Meilleures performances sur des datasets limit\u00e9s</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#3-la-quantification-dun-modele-consiste-a","title":"3. La quantification d'un mod\u00e8le consiste \u00e0 :","text":"<ul> <li> a) R\u00e9duire le nombre de couches du mod\u00e8le</li> <li> b) R\u00e9duire la pr\u00e9cision des poids (ex: float32 \u2192 int8)</li> <li> c) Supprimer les poids proches de z\u00e9ro</li> <li> d) Combiner plusieurs mod\u00e8les ensemble</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#4-quelle-technique-doptimisation-consiste-a-supprimer-les-connexions-les-moins-importantes-dans-un-reseau","title":"4. Quelle technique d'optimisation consiste \u00e0 supprimer les connexions les moins importantes dans un r\u00e9seau ?","text":"<ul> <li> a) Quantification</li> <li> b) \u00c9lagage (pruning)</li> <li> c) Distillation</li> <li> d) Factorisation matricielle</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#5-quel-format-est-generalement-utilise-pour-deployer-des-modeles-sur-des-appareils-mobiles","title":"5. Quel format est g\u00e9n\u00e9ralement utilis\u00e9 pour d\u00e9ployer des mod\u00e8les sur des appareils mobiles ?","text":"<ul> <li> a) HDF5</li> <li> b) SavedModel</li> <li> c) TensorFlow Lite</li> <li> d) ONNX</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#6-dans-une-api-de-deep-learning-quelle-technique-est-recommandee-pour-ameliorer-les-performances","title":"6. Dans une API de Deep Learning, quelle technique est recommand\u00e9e pour am\u00e9liorer les performances ?","text":"<ul> <li> a) Recharger le mod\u00e8le \u00e0 chaque requ\u00eate</li> <li> b) Convertir les images en CSV avant traitement</li> <li> c) Mettre en cache le mod\u00e8le en m\u00e9moire</li> <li> d) D\u00e9sactiver la gestion d'erreurs</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#7-quelle-architecture-de-modele-est-specifiquement-concue-pour-etre-legere-et-efficace","title":"7. Quelle architecture de mod\u00e8le est sp\u00e9cifiquement con\u00e7ue pour \u00eatre l\u00e9g\u00e8re et efficace ?","text":"<ul> <li> a) VGG16</li> <li> b) MobileNetV2</li> <li> c) ResNet152</li> <li> d) InceptionV4</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#8-la-distillation-de-connaissances-consiste-a","title":"8. La distillation de connaissances consiste \u00e0 :","text":"<ul> <li> a) Extraire les poids d'un mod\u00e8le pour les analyser</li> <li> b) Entra\u00eener un plus petit mod\u00e8le (\u00e9l\u00e8ve) \u00e0 imiter un plus grand mod\u00e8le (enseignant)</li> <li> c) D\u00e9composer un gros mod\u00e8le en plusieurs petits</li> <li> d) Fusionner plusieurs mod\u00e8les sp\u00e9cialis\u00e9s</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#9-lors-de-lintegration-dun-modele-dans-une-application-web-quelle-affirmation-est-correcte","title":"9. Lors de l'int\u00e9gration d'un mod\u00e8le dans une application web, quelle affirmation est correcte ?","text":"<ul> <li> a) Le mod\u00e8le doit toujours \u00eatre ex\u00e9cut\u00e9 c\u00f4t\u00e9 client (JavaScript)</li> <li> b) Les pr\u00e9dictions doivent \u00eatre trait\u00e9es de mani\u00e8re synchrone</li> <li> c) Il est recommand\u00e9 de pr\u00e9traiter les images c\u00f4t\u00e9 client avant envoi</li> <li> d) L'API du mod\u00e8le ne n\u00e9cessite pas de documentation si elle est utilis\u00e9e en interne</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#10-parmi-ces-parametres-lequel-peut-etre-ajuste-pour-rendre-les-reponses-dun-modele-de-langage-plus-deterministes-moins-creatives","title":"10. Parmi ces param\u00e8tres, lequel peut \u00eatre ajust\u00e9 pour rendre les r\u00e9ponses d'un mod\u00e8le de langage plus d\u00e9terministes (moins cr\u00e9atives) ?","text":"<ul> <li> a) Augmenter la temp\u00e9rature</li> <li> b) Diminuer la temp\u00e9rature</li> <li> c) Augmenter le nombre maximum de tokens</li> <li> d) Activer le mode streaming</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#partie-b-questions-a-reponse-courte-10-points","title":"Partie B: Questions \u00e0 r\u00e9ponse courte (10 points)","text":"<p>R\u00e9pondez bri\u00e8vement aux questions suivantes (2-3 phrases par question).</p>"},{"location":"module3/qcm-evaluation-module3/#11-expliquez-pourquoi-loptimisation-des-modeles-de-deep-learning-est-importante-dans-un-contexte-professionnel","title":"11. Expliquez pourquoi l'optimisation des mod\u00e8les de Deep Learning est importante dans un contexte professionnel.","text":"<p>...................................................................</p> <p>...................................................................</p>"},{"location":"module3/qcm-evaluation-module3/#12-decrivez-deux-differences-principales-entre-tensorflow-et-pytorch","title":"12. D\u00e9crivez deux diff\u00e9rences principales entre TensorFlow et PyTorch.","text":"<p>...................................................................</p> <p>...................................................................</p>"},{"location":"module3/qcm-evaluation-module3/#13-quels-sont-les-avantages-et-inconvenients-de-la-quantification-post-entrainement","title":"13. Quels sont les avantages et inconv\u00e9nients de la quantification post-entra\u00eenement ?","text":"<p>...................................................................</p> <p>...................................................................</p>"},{"location":"module3/qcm-evaluation-module3/#14-comment-lapi-mistral-ai-peut-elle-etre-utilisee-pour-creer-un-chatbot-pedagogique","title":"14. Comment l'API Mistral AI peut-elle \u00eatre utilis\u00e9e pour cr\u00e9er un chatbot p\u00e9dagogique ?","text":"<p>...................................................................</p> <p>...................................................................</p>"},{"location":"module3/qcm-evaluation-module3/#15-quelles-sont-les-deux-bonnes-pratiques-essentielles-pour-securiser-une-api-de-reconnaissance-dimages","title":"15. Quelles sont les deux bonnes pratiques essentielles pour s\u00e9curiser une API de reconnaissance d'images ?","text":"<p>...................................................................</p> <p>...................................................................</p>"},{"location":"module3/qcm-evaluation-module3/#partie-c-exercice-pratique-optimisation-dun-modele-15-points","title":"Partie C: Exercice pratique : Optimisation d'un mod\u00e8le (15 points)","text":"<p>Compl\u00e9tez le code suivant pour optimiser un mod\u00e8le MobileNetV2 avec la quantification TensorFlow Lite.</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nimport numpy as np\n\n# Chargement du mod\u00e8le pr\u00e9-entra\u00een\u00e9\nbase_model = MobileNetV2(weights='imagenet', include_top=True)\n\n# 1. Convertir le mod\u00e8le en TensorFlow Lite\nconverter = tf.lite.TFLiteConverter.from_keras_model(base_model)\ntflite_model = ................................. # \u00c0 compl\u00e9ter\n\n# 2. Appliquer la quantification post-entra\u00eenement\nconverter = tf.lite.TFLiteConverter.from_keras_model(base_model)\nconverter.optimizations = ........................... # \u00c0 compl\u00e9ter\nquantized_model = converter.convert()\n\n# 3. Comparer les tailles des mod\u00e8les\noriginal_size = .................. # \u00c0 compl\u00e9ter : calculer la taille du mod\u00e8le original\ntflite_size = len(tflite_model) / (1024 * 1024)  # Taille en Mo\nquantized_size = len(quantized_model) / (1024 * 1024)  # Taille en Mo\n\nprint(f\"Taille du mod\u00e8le original: {original_size:.2f} Mo\")\nprint(f\"Taille du mod\u00e8le TFLite: {tflite_size:.2f} Mo\")\nprint(f\"Taille du mod\u00e8le quantifi\u00e9: {quantized_size:.2f} Mo\")\nprint(f\"R\u00e9duction de taille: {(1 - quantized_size/original_size) * 100:.2f}%\")\n\n# 4. Fonction pour pr\u00e9traiter une image pour l'inf\u00e9rence\ndef preprocess_image(image_path):\n    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n    img_array = .................. # \u00c0 compl\u00e9ter : convertir l'image en tableau\n    img_array = np.expand_dims(img_array, axis=0)\n    return tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n\n# 5. Cr\u00e9ation d'un interpr\u00e9teur TFLite\ninterpreter = tf.lite.Interpreter(model_content=quantized_model)\n....................... # \u00c0 compl\u00e9ter : allouer les tenseurs\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# 6. Fonction d'inf\u00e9rence avec le mod\u00e8le quantifi\u00e9\ndef predict_with_tflite(image_path):\n    # Pr\u00e9traitement de l'image\n    input_data = preprocess_image(image_path)\n\n    # D\u00e9finir les donn\u00e9es d'entr\u00e9e\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Ex\u00e9cuter l'inf\u00e9rence\n    ...................... # \u00c0 compl\u00e9ter : invoquer l'interpr\u00e9teur\n\n    # Obtenir les r\u00e9sultats\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n\n    # Traiter les r\u00e9sultats\n    results = tf.keras.applications.mobilenet_v2.decode_predictions(output_data)\n    return results[0]\n</code></pre>"},{"location":"module3/qcm-evaluation-module3/#partie-d-exercice-de-reflexion-cas-pratique-dintegration-10-points","title":"Partie D: Exercice de r\u00e9flexion : Cas pratique d'int\u00e9gration (10 points)","text":"<p>Vous \u00eates d\u00e9veloppeur dans une petite entreprise qui propose des solutions de reconnaissance d'objets pour le commerce de d\u00e9tail. On vous demande de cr\u00e9er une API qui permettra d'identifier les produits \u00e0 partir de photos prises par les employ\u00e9s sur leurs smartphones.</p> <ol> <li>Quelle architecture de mod\u00e8le choisiriez-vous et pourquoi ? (2 points)</li> </ol> <p>...................................................................</p> <p>...................................................................</p> <ol> <li>Quelles techniques d'optimisation mettriez-vous en place ? (2 points)</li> </ol> <p>...................................................................</p> <p>...................................................................</p> <ol> <li>Comment structureriez-vous votre API REST ? D\u00e9crivez les endpoints et leurs param\u00e8tres. (3 points)</li> </ol> <p>...................................................................</p> <p>...................................................................</p> <p>...................................................................</p> <ol> <li>Quelles mesures de s\u00e9curit\u00e9 impl\u00e9menteriez-vous ? (3 points)</li> </ol> <p>...................................................................</p> <p>...................................................................</p> <p>...................................................................</p>"},{"location":"module3/qcm-evaluation-module3/#corrige","title":"Corrig\u00e9","text":""},{"location":"module3/qcm-evaluation-module3/#qcm","title":"QCM","text":"<ol> <li>b) TensorFlow/Keras</li> <li>a) b) d)</li> <li>b) R\u00e9duire la pr\u00e9cision des poids (ex: float32 \u2192 int8)</li> <li>b) \u00c9lagage (pruning)</li> <li>c) TensorFlow Lite</li> <li>c) Mettre en cache le mod\u00e8le en m\u00e9moire</li> <li>b) MobileNetV2</li> <li>b) Entra\u00eener un plus petit mod\u00e8le (\u00e9l\u00e8ve) \u00e0 imiter un plus grand mod\u00e8le (enseignant)</li> <li>c) Il est recommand\u00e9 de pr\u00e9traiter les images c\u00f4t\u00e9 client avant envoi</li> <li>b) Diminuer la temp\u00e9rature</li> </ol>"},{"location":"module3/qcm-evaluation-module3/#questions-a-reponse-courte-elements-attendus","title":"Questions \u00e0 r\u00e9ponse courte (\u00e9l\u00e9ments attendus)","text":"<ol> <li> <p>L'optimisation permet de r\u00e9duire les co\u00fbts d'infrastructure, diminuer la latence pour une meilleure exp\u00e9rience utilisateur, \u00e9conomiser l'\u00e9nergie (crucial pour les appareils mobiles) et rendre les mod\u00e8les accessibles sur des appareils \u00e0 ressources limit\u00e9es.</p> </li> <li> <p>TensorFlow utilise des graphes statiques (plus efficaces en production) tandis que PyTorch utilise des graphes dynamiques (plus flexibles pour la recherche). TensorFlow a un \u00e9cosyst\u00e8me plus complet pour le d\u00e9ploiement (TFLite, TF Serving) alors que PyTorch est g\u00e9n\u00e9ralement consid\u00e9r\u00e9 comme plus intuitif pour le d\u00e9veloppement.</p> </li> <li> <p>Avantages : R\u00e9duction significative de la taille du mod\u00e8le (jusqu'\u00e0 4x), acc\u00e9l\u00e9ration de l'inf\u00e9rence, pas besoin de r\u00e9entra\u00eenement. Inconv\u00e9nients : Perte potentielle de pr\u00e9cision, surtout pour les t\u00e2ches complexes, incompatibilit\u00e9 avec certaines op\u00e9rations avanc\u00e9es.</p> </li> <li> <p>L'API Mistral AI peut \u00eatre utilis\u00e9e pour cr\u00e9er un chatbot p\u00e9dagogique en envoyant des requ\u00eates avec un prompt syst\u00e8me adapt\u00e9 \u00e0 l'enseignement, en enrichissant les prompts avec une base de connaissances sp\u00e9cifique au domaine enseign\u00e9, et en maintenant un contexte conversationnel pour assurer la coh\u00e9rence des \u00e9changes.</p> </li> <li> <p>Validation des entr\u00e9es (v\u00e9rification du format et de la taille des images), limitation du taux de requ\u00eates (rate limiting), authentification par cl\u00e9 API, sanitization des chemins de fichiers, restriction des types MIME accept\u00e9s.</p> </li> </ol>"},{"location":"module3/qcm-evaluation-module3/#exercice-pratique-elements-de-correction","title":"Exercice pratique (\u00e9l\u00e9ments de correction)","text":"<pre><code># 1. Convertir le mod\u00e8le en TensorFlow Lite\ntflite_model = converter.convert()\n\n# 2. Appliquer la quantification post-entra\u00eenement\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# 3. Comparer les tailles des mod\u00e8les\noriginal_size = sum(np.prod(w.shape) * w.dtype.size for w in base_model.weights) / (1024 * 1024)\n\n# 4. Fonction pour pr\u00e9traiter une image pour l'inf\u00e9rence\nimg_array = tf.keras.preprocessing.image.img_to_array(img)\n\n# 5. Cr\u00e9ation d'un interpr\u00e9teur TFLite\ninterpreter.allocate_tensors()\n\n# 6. Fonction d'inf\u00e9rence avec le mod\u00e8le quantifi\u00e9\ninterpreter.invoke()\n</code></pre>"},{"location":"module3/qcm-evaluation-module3/#exercice-de-reflexion","title":"Exercice de r\u00e9flexion","text":"<p>Les r\u00e9ponses peuvent varier, mais devraient inclure des points comme:</p> <ol> <li> <p>Architecture: MobileNetV2 ou EfficientNet serait un bon choix car ils offrent un bon \u00e9quilibre entre pr\u00e9cision et performance, sont optimis\u00e9s pour les appareils mobiles, et peuvent \u00eatre facilement affin\u00e9s pour des t\u00e2ches sp\u00e9cifiques.</p> </li> <li> <p>Techniques d'optimisation: Quantification post-entra\u00eenement pour r\u00e9duire la taille du mod\u00e8le, transfer learning sur un petit dataset de produits sp\u00e9cifiques, et \u00e9ventuellement pruning pour r\u00e9duire davantage la taille.</p> </li> <li> <p>Structure API REST:</p> </li> <li>POST /predict - Pour envoyer une image et recevoir des pr\u00e9dictions</li> <li>GET /categories - Pour r\u00e9cup\u00e9rer la liste des cat\u00e9gories de produits</li> <li> <p>POST /feedback - Pour recueillir les retours sur les pr\u00e9dictions incorrectes    Param\u00e8tres pour /predict: image (fichier ou base64), top_k (nombre de pr\u00e9dictions), confidence_threshold.</p> </li> <li> <p>S\u00e9curit\u00e9:</p> </li> <li>Authentification par cl\u00e9 API</li> <li>Rate limiting pour pr\u00e9venir les abus</li> <li>Validation des entr\u00e9es (taille et format d'image)</li> <li>HTTPS pour le chiffrement des donn\u00e9es</li> <li>Logging s\u00e9curis\u00e9 pour l'audit</li> </ol>"},{"location":"module3/qcm-evaluation-module3/#bareme-et-auto-evaluation","title":"Bar\u00e8me et auto-\u00e9valuation","text":""},{"location":"module3/qcm-evaluation-module3/#calcul-de-votre-score","title":"Calcul de votre score","text":"<p>Partie A : 1 point par r\u00e9ponse correcte = 10 points max Partie B : 2 points par r\u00e9ponse correcte = 10 points max Partie C : 15 points pour l'exercice compl\u00e9t\u00e9 correctement Partie D : 10 points pour les r\u00e9ponses pertinentes</p>"},{"location":"module3/qcm-evaluation-module3/#total-des-points-possibles-45","title":"Total des points possibles : 45","text":"<p>Interpr\u00e9tation</p> <p>35-45 points : Excellente ma\u00eetrise des concepts d'int\u00e9gration et d'optimisation des mod\u00e8les 24-35 points : Bonne compr\u00e9hension, certains aspects \u00e0 approfondir 16-23 points : Compr\u00e9hension de base, n\u00e9cessite une r\u00e9vision approfondie 0-15 points : R\u00e9vision compl\u00e8te recommand\u00e9e avant de poursuivre</p>"},{"location":"module3/qcm-evaluation-module3/#questions-pour-approfondir","title":"Questions pour approfondir","text":"<p>Si vous avez obtenu un bon score, vous pouvez explorer ces questions pour aller plus loin :</p> <ol> <li>Comment impl\u00e9menteriez-vous un syst\u00e8me de mise \u00e0 jour progressive des mod\u00e8les en production ?</li> <li>Quelles strat\u00e9gies pourriez-vous utiliser pour g\u00e9rer les biais potentiels dans un mod\u00e8le de vision par ordinateur ?</li> <li>Comment adapter l'architecture d'une API de Deep Learning pour g\u00e9rer des millions de requ\u00eates par jour ?</li> <li>Quelles techniques permettraient d'optimiser les prompts pour un mod\u00e8le de langage au-del\u00e0 des exemples vus dans ce module ?</li> </ol> <p>Retour au Module 3 Continuer vers le Module 4</p>"},{"location":"module4/","title":"Module 4 : Projet int\u00e9grateur - Chatbot p\u00e9dagogique","text":""},{"location":"module4/#objectifs-du-module","title":"Objectifs du module","text":"<p>Cette derni\u00e8re s\u00e9ance vous permettra de :</p> <ul> <li>Appliquer l'ensemble des connaissances acquises dans un projet concret et complet</li> <li>D\u00e9velopper un chatbot p\u00e9dagogique fonctionnel expliquant le Deep Learning</li> <li>Int\u00e9grer l'API Mistral AI dans une solution compl\u00e8te</li> <li>Pr\u00e9senter et d\u00e9fendre votre solution technique</li> </ul>"},{"location":"module4/#vision-du-projet","title":"Vision du projet","text":"<p>Le projet consiste \u00e0 d\u00e9velopper un assistant virtuel conversationnel capable d'expliquer les concepts du Deep Learning, de r\u00e9pondre aux questions techniques et d'accompagner les apprenants dans leur d\u00e9couverte de ce domaine.</p> <p>\ud83c\udfaf Objectif : Concevoir un chatbot interactif qui aide les \u00e9tudiants de BTS SIO \u00e0 comprendre les concepts du Deep Learning \u00e0 travers des explications personnalis\u00e9es, des exemples concrets et des exercices adapt\u00e9s.</p>"},{"location":"module4/#architecture-technique","title":"Architecture technique","text":"<p>Le chatbot s'appuiera sur une architecture moderne compos\u00e9e de trois \u00e9l\u00e9ments principaux :</p> <pre><code>flowchart LR\n    A[Interface Web] &lt;--&gt; B[Backend Python]\n    B &lt;--&gt; C[API Mistral AI]\n    D[Base de connaissances] &lt;--&gt; B</code></pre>"},{"location":"module4/#1-interface-conversationnelle","title":"1. Interface conversationnelle","text":"<ul> <li>Interface web simple et intuitive</li> <li>Affichage des messages en format discussion</li> <li>Indicateur de chargement pendant le traitement</li> <li>Historique de conversation</li> </ul>"},{"location":"module4/#2-backend-flaskfastapi","title":"2. Backend Flask/FastAPI","text":"<ul> <li>Gestion des requ\u00eates et des sessions</li> <li>Enrichissement des prompts avec la base de connaissances</li> <li>Communication avec l'API Mistral</li> <li>Logique de traitement des r\u00e9ponses</li> </ul>"},{"location":"module4/#3-integration-api-mistral-ai","title":"3. Int\u00e9gration API Mistral AI","text":"<ul> <li>Configuration et param\u00e8trage des requ\u00eates</li> <li>Gestion du contexte de conversation</li> <li>Optimisation des prompts</li> <li>Gestion des erreurs et limitations</li> </ul>"},{"location":"module4/#4-base-de-connaissances","title":"4. Base de connaissances","text":"<ul> <li>Structure JSON organis\u00e9e par concepts</li> <li>Exercices et quiz par th\u00e9matique</li> </ul>"},{"location":"module4/#fonctionnalites-cles","title":"Fonctionnalit\u00e9s cl\u00e9s","text":"<p>Le chatbot p\u00e9dagogique offrira les fonctionnalit\u00e9s suivantes :</p> <ol> <li> <p>Explication des concepts</p> <ul> <li>D\u00e9finition adapt\u00e9e au niveau de l'utilisateur</li> <li>Exemples concrets pour illustrer chaque notion</li> <li>Analogies et comparaisons pour faciliter la compr\u00e9hension</li> </ul> </li> <li> <p>R\u00e9ponse aux questions</p> <ul> <li>Compr\u00e9hension des questions techniques</li> <li>R\u00e9ponses pr\u00e9cises bas\u00e9es sur la base de connaissances</li> <li>Capacit\u00e9 \u00e0 demander des clarifications si n\u00e9cessaire</li> </ul> </li> <li> <p>Progression adaptative</p> <ul> <li>D\u00e9tection du niveau de l'utilisateur</li> <li>Suggestions de concepts \u00e0 explorer ensuite</li> <li>Augmentation progressive de la complexit\u00e9</li> </ul> </li> <li> <p>Exercices interactifs</p> <ul> <li>G\u00e9n\u00e9ration de quiz sur les concepts vus</li> <li>Probl\u00e8mes simples \u00e0 r\u00e9soudre</li> <li>Feedback sur les r\u00e9ponses</li> </ul> </li> </ol>"},{"location":"module4/#approche-pedagogique","title":"Approche p\u00e9dagogique","text":"<p>Cette s\u00e9ance est enti\u00e8rement bas\u00e9e sur la r\u00e9alisation d'un projet concret en \u00e9quipe. Vous devrez mobiliser toutes les comp\u00e9tences d\u00e9velopp\u00e9es lors des s\u00e9ances pr\u00e9c\u00e9dentes pour cr\u00e9er une application compl\u00e8te et fonctionnelle. L'accent est mis sur l'autonomie, la collaboration et la mise en pratique professionnelle.</p>"},{"location":"module4/#structure-de-la-seance-4h","title":"Structure de la s\u00e9ance (4h)","text":"<pre><code>D\u00e9veloppement du chatbot       : 2h30m \nFinalisation et tests          : 1h    \nPr\u00e9sentation des projets       : 30m   \n</code></pre>"},{"location":"module4/#trois-phases-de-realisation","title":"Trois phases de r\u00e9alisation","text":""},{"location":"module4/#phase-1-developpement-du-chatbot-2h30","title":"Phase 1 : D\u00e9veloppement du chatbot (2h30)","text":"<p>Impl\u00e9mentez les fonctionnalit\u00e9s principales de votre chatbot p\u00e9dagogique :</p> <ul> <li>Mise en place de l'interface conversationnelle</li> <li>Int\u00e9gration avanc\u00e9e avec l'API Mistral AI</li> <li>Structuration et enrichissement de la base de connaissances</li> <li>D\u00e9veloppement des fonctionnalit\u00e9s d'aide \u00e0 l'apprentissage</li> </ul>"},{"location":"module4/#phase-2-finalisation-et-tests-1h","title":"Phase 2 : Finalisation et tests (1h)","text":"<p>Peaufinez votre solution et assurez-vous de sa qualit\u00e9 :</p> <ul> <li>Tests fonctionnels et sc\u00e9narios d'utilisation</li> <li>Optimisation des performances</li> <li>Documentation technique et guide utilisateur</li> <li>Pr\u00e9paration de la d\u00e9monstration</li> </ul>"},{"location":"module4/#phase-3-presentation-des-projets-30min","title":"Phase 3 : Pr\u00e9sentation des projets (30min)","text":"<p>Pr\u00e9sentez votre solution \u00e0 la classe :</p> <ul> <li>D\u00e9monstration en direct du chatbot</li> <li>Explication des choix techniques</li> <li>Retour sur les d\u00e9fis rencontr\u00e9s et les solutions adopt\u00e9es</li> <li>Questions-r\u00e9ponses</li> </ul>"},{"location":"module4/#defis-techniques","title":"D\u00e9fis techniques","text":"<p>Les principaux d\u00e9fis \u00e0 relever seront :</p> <ol> <li>Prompt engineering efficace</li> <li>Formuler des instructions claires pour l'API Mistral</li> <li>Maintenir la coh\u00e9rence p\u00e9dagogique dans les r\u00e9ponses</li> <li> <p>\u00c9viter les hallucinations du mod\u00e8le</p> </li> <li> <p>Int\u00e9gration technique</p> </li> <li>Communication fluide entre frontend et backend</li> <li>Gestion asynchrone des requ\u00eates API</li> <li> <p>Optimisation des temps de r\u00e9ponse</p> </li> <li> <p>Qualit\u00e9 p\u00e9dagogique</p> </li> <li>Structure coh\u00e9rente de la base de connaissances</li> <li>Adaptation au niveau de l'utilisateur</li> <li>Progression logique entre les concepts</li> </ol>"},{"location":"module4/#ressources-necessaires","title":"Ressources n\u00e9cessaires","text":"<p>Pour cette s\u00e9ance, vous aurez besoin de :</p> <ul> <li>Votre document de conception pr\u00e9par\u00e9 lors de la s\u00e9ance 3</li> <li>Compte et cl\u00e9 API Mistral AI</li> <li>Environnement de d\u00e9veloppement (Google Colab ou local)</li> <li>Templates fournis pour la documentation</li> </ul> <p>Ressources fournies : - Documentation compl\u00e8te de l'API Mistral - Structure JSON pour la base de connaissances - Templates de code pour l'interface et le backend - Exemples de prompts efficaces</p>"},{"location":"module4/#livrables-attendus","title":"Livrables attendus","text":"<p>\u00c0 l'issue de cette s\u00e9ance, vous devrez remettre :</p> <ol> <li>Code source complet du chatbot p\u00e9dagogique</li> <li>Base de connaissances structur\u00e9e sur le Deep Learning</li> <li>Documentation technique expliquant l'architecture et les choix d'impl\u00e9mentation</li> <li>Guide utilisateur pour la prise en main</li> <li>Pr\u00e9sentation avec support \u00e0 fournir</li> </ol> <p>Ces livrables constituent l'aboutissement de votre parcours et seront \u00e9valu\u00e9s selon les crit\u00e8res d\u00e9taill\u00e9s dans la grille d'\u00e9valuation.</p>"},{"location":"module4/#pret-a-relever-le-defi","title":"Pr\u00eat \u00e0 relever le d\u00e9fi ?","text":"<p>C'est l'heure de mettre en pratique tout ce que vous avez appris pour cr\u00e9er un outil r\u00e9ellement utile. Bonne chance !</p> <p>Commencer la Phase 1 ```</p>"},{"location":"module4/partie1-developpement/","title":"Phase 1 : D\u00e9veloppement du chatbot (2h30)","text":""},{"location":"module4/partie1-developpement/#objectif","title":"Objectif","text":"<p>Impl\u00e9menter les fonctionnalit\u00e9s principales de votre chatbot p\u00e9dagogique en vous basant sur votre document de conception et en int\u00e9grant les connaissances acquises lors des s\u00e9ances pr\u00e9c\u00e9dentes.</p>"},{"location":"module4/partie1-developpement/#fonctionnalites-a-implementer","title":"Fonctionnalit\u00e9s \u00e0 impl\u00e9menter","text":""},{"location":"module4/partie1-developpement/#1-interface-conversationnelle-30-min","title":"1. Interface conversationnelle (30 min)","text":"<p>L'interface du chatbot doit \u00eatre simple mais fonctionnelle. Elle comprendra :</p> <ul> <li>Une zone d'affichage des messages</li> <li>Un champ de saisie pour les questions</li> <li>Un bouton d'envoi</li> <li>Une indication de chargement pendant le traitement</li> <li>Un syst\u00e8me d'historique de conversation</li> </ul> <p>Mod\u00e8le de code pour l'interface web <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Chatbot p\u00e9dagogique - Deep Learning&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"styles.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"chat-container\"&gt;\n        &lt;div class=\"chat-header\"&gt;\n            &lt;h1&gt;Chatbot Deep Learning&lt;/h1&gt;\n        &lt;/div&gt;\n        &lt;div class=\"chat-messages\" id=\"chat-messages\"&gt;\n            &lt;!-- Les messages s'afficheront ici --&gt;\n            &lt;div class=\"message bot\"&gt;\n                &lt;div class=\"message-content\"&gt;\n                    Bonjour ! Je suis un chatbot sp\u00e9cialis\u00e9 dans le Deep Learning. \n                    Comment puis-je vous aider aujourd'hui ?\n                &lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div class=\"chat-input\"&gt;\n            &lt;input type=\"text\" id=\"user-input\" placeholder=\"Posez votre question ici...\"&gt;\n            &lt;button id=\"send-button\"&gt;Envoyer&lt;/button&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script src=\"script.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Points cl\u00e9s \u00e0 respecter :  - Design responsive s'adaptant aux diff\u00e9rentes tailles d'\u00e9cran  - Indication claire des messages utilisateur vs assistant  - Gestion des erreurs (r\u00e9seau, API, etc.)</p>"},{"location":"module4/partie1-developpement/#2-integration-avancee-avec-lapi-mistral-ai-45-min","title":"2. Int\u00e9gration avanc\u00e9e avec l'API Mistral AI (45 min)","text":"<p>L'objectif est d'exploiter efficacement l'API Mistral AI pour g\u00e9n\u00e9rer des r\u00e9ponses pertinentes et contextualis\u00e9es.</p> <p>Structure d'int\u00e9gration recommand\u00e9e :</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\n# Chargement des variables d'environnement\nload_dotenv()\napi_key = os.getenv(\"MISTRAL_API_KEY\")\n\n# Initialisation du client\nclient = MistralClient(api_key=api_key)\n\n# Syst\u00e8me de gestion de contexte\nclass ConversationManager:\n    def __init__(self, system_prompt):\n        self.history = [\n            ChatMessage(role=\"system\", content=system_prompt)\n        ]\n\n    def add_user_message(self, message):\n        self.history.append(ChatMessage(role=\"user\", content=message))\n\n    def add_assistant_message(self, message):\n        self.history.append(ChatMessage(role=\"assistant\", content=message))\n\n    def get_response(self, model=\"mistral-medium\", temperature=0.7):\n        response = client.chat(\n            model=model,\n            messages=self.history,\n            temperature=temperature\n        )\n        content = response.choices[0].message.content\n        self.add_assistant_message(content)\n        return content\n\n    def get_history(self):\n        # Exclure le message syst\u00e8me pour l'affichage\n        return self.history[1:]\n\n# Exemple d'utilisation\nsystem_prompt = \"\"\"\nTu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans le Deep Learning, con\u00e7u pour aider \nles \u00e9tudiants de BTS SIO. Tu expliques les concepts de mani\u00e8re claire et progressive, \nen adaptant ton niveau de technicit\u00e9 au niveau de l'\u00e9tudiant. Utilise des analogies \net des exemples concrets quand c'est pertinent.\n\"\"\"\n\nconversation = ConversationManager(system_prompt)\n</code></pre> <p>Aspects avanc\u00e9s \u00e0 impl\u00e9menter :</p> <ul> <li>Prompt engineering : Am\u00e9lioration des instructions syst\u00e8me pour obtenir des r\u00e9ponses optimales</li> <li>Gestion de contexte : Pr\u00e9servation de l'historique pour maintenir la coh\u00e9rence des conversations</li> <li>Param\u00e8tres ajustables : Contr\u00f4le de la temp\u00e9rature pour moduler la cr\u00e9ativit\u00e9 des r\u00e9ponses</li> <li>Contr\u00f4le de longueur : Limiter la longueur des r\u00e9ponses pour des explications concises</li> </ul>"},{"location":"module4/partie1-developpement/#3-structuration-de-la-base-de-connaissances-45-min","title":"3. Structuration de la base de connaissances (45 min)","text":"<p>La base de connaissances est le c\u0153ur de votre chatbot. Elle doit \u00eatre structur\u00e9e de mani\u00e8re logique et couvrir les concepts essentiels du Deep Learning, correspondant au programme que vous avez suivi.</p> <p>Structure recommand\u00e9e (format JSON) :</p> <pre><code>{\n  \"topics\": [\n    {\n      \"id\": \"intro_dl\",\n      \"title\": \"Introduction au Deep Learning\",\n      \"subtopics\": [\n        {\n          \"id\": \"diff_ml_dl\",\n          \"title\": \"Diff\u00e9rence entre Machine Learning et Deep Learning\",\n          \"content\": \"Le Machine Learning classique n\u00e9cessite une extraction manuelle des caract\u00e9ristiques (feature engineering) tandis que le Deep Learning les extrait automatiquement gr\u00e2ce \u00e0 ses multiples couches...\",\n          \"examples\": [\n            \"Dans la reconnaissance d'images, le ML classique n\u00e9cessite d'extraire manuellement des caract\u00e9ristiques comme les contours, les textures, alors que le DL apprend directement ces caract\u00e9ristiques.\",\n            \"Pour la classification de texte, le ML classique utilise des approches comme TF-IDF ou Bag-of-Words, alors que le DL utilise des embeddings et des architectures comme LSTM.\"\n          ],\n          \"related\": [\"neural_networks\", \"applications_dl\"]\n        },\n        // Autres sous-topics...\n      ]\n    },\n    // Autres topics principaux...\n  ]\n}\n</code></pre> <p>\u00c9l\u00e9ments \u00e0 inclure :      - Concepts fondamentaux du Deep Learning      - Types de r\u00e9seaux (CNN, RNN, etc.)      - Techniques d'entra\u00eenement et d'optimisation      - Applications pratiques      - Exemples de code simplifi\u00e9s      - Analogies pour faciliter la compr\u00e9hension</p> <p>La base de connaissances peut \u00eatre utilis\u00e9e pour enrichir les prompts envoy\u00e9s \u00e0 l'API ou pour offrir des r\u00e9ponses pr\u00e9d\u00e9finies \u00e0 certaines questions.</p>"},{"location":"module4/partie1-developpement/#4-fonctionnalites-daide-a-lapprentissage-30-min","title":"4. Fonctionnalit\u00e9s d'aide \u00e0 l'apprentissage (30 min)","text":"<p>Pour rendre votre chatbot v\u00e9ritablement p\u00e9dagogique, impl\u00e9mentez au moins deux des fonctionnalit\u00e9s suivantes :</p> <ol> <li> <p>G\u00e9n\u00e9ration de quiz : Cr\u00e9er des QCM pour tester les connaissances de l'utilisateur    <pre><code>def generate_quiz(topic):\n    # Exemple de structure\n    questions = {\n        \"intro_dl\": [\n            {\n                \"question\": \"Quelle est la principale diff\u00e9rence entre Machine Learning classique et Deep Learning ?\",\n                \"options\": [\n                    \"Le Deep Learning est toujours plus rapide\",\n                    \"Le Deep Learning extrait automatiquement les caract\u00e9ristiques pertinentes\",\n                    \"Le Deep Learning utilise exclusivement des GPUs\",\n                    \"Le Deep Learning n\u00e9cessite moins de donn\u00e9es\"\n                ],\n                \"correct\": 1,\n                \"explanation\": \"Le Deep Learning se distingue par sa capacit\u00e9 \u00e0 extraire automatiquement des caract\u00e9ristiques pertinentes gr\u00e2ce \u00e0 ses multiples couches, \u00e9liminant le besoin d'extraction manuelle (feature engineering).\"\n            },\n            # Autres questions...\n        ]\n    }\n    return questions.get(topic, [])\n</code></pre></p> </li> <li> <p>Syst\u00e8me de progression : Suivre le niveau de l'utilisateur et adapter le contenu    <pre><code>class LearnerProfile:\n    def __init__(self, user_id):\n        self.user_id = user_id\n        self.topics_seen = set()\n        self.quiz_scores = {}\n        self.current_level = \"beginner\"  # beginner, intermediate, advanced\n\n    def update_after_interaction(self, topic, subtopic):\n        self.topics_seen.add(f\"{topic}:{subtopic}\")\n        # Mise \u00e0 jour du niveau selon le nombre de topics vus\n        if len(self.topics_seen) &gt; 10:\n            self.current_level = \"intermediate\"\n        if len(self.topics_seen) &gt; 20:\n            self.current_level = \"advanced\"\n\n    def record_quiz_result(self, topic, score):\n        self.quiz_scores[topic] = score\n</code></pre></p> </li> <li> <p>Visualisations adaptatives : G\u00e9n\u00e9rer des sch\u00e9mas explicatifs selon le niveau    <pre><code>def get_visualization(concept, level):\n    visualizations = {\n        \"neural_network\": {\n            \"beginner\": \"simple_nn.svg\",\n            \"intermediate\": \"detailed_nn.svg\",\n            \"advanced\": \"complex_nn.svg\"\n        },\n        # Autres concepts...\n    }\n    return visualizations.get(concept, {}).get(level, \"default.svg\")\n</code></pre></p> </li> <li> <p>Suivi des mots-cl\u00e9s : Assistant remontant les d\u00e9finitions des termes techniques    <pre><code>def extract_technical_terms(message):\n    technical_terms = [\n        \"neurone\", \"couche\", \"poids\", \"biais\", \"fonction d'activation\",\n        \"r\u00e9tropropagation\", \"descente de gradient\", \"CNN\", \"RNN\", \"LSTM\"\n    ]\n    found_terms = []\n    for term in technical_terms:\n        if term.lower() in message.lower():\n            found_terms.append(term)\n    return found_terms\n</code></pre></p> </li> </ol>"},{"location":"module4/partie1-developpement/#travail-dequipe-et-repartition-des-taches","title":"Travail d'\u00e9quipe et r\u00e9partition des t\u00e2ches","text":"<p>Si vous travaillez en bin\u00f4me, r\u00e9partissez-vous les t\u00e2ches efficacement :</p> <p>Suggestion de r\u00e9partition :        - Membre 1 : Interface + Int\u00e9gration API        - Membre 2 : Base de connaissances + Fonctionnalit\u00e9s p\u00e9dagogiques</p> <p>Ou alternativement :        - Membre 1 : Backend (API, logique, base de connaissances)        - Membre 2 : Frontend (interface, interactions, exp\u00e9rience utilisateur)</p>"},{"location":"module4/partie1-developpement/#points-de-vigilance","title":"Points de vigilance","text":"<ul> <li>S\u00e9curit\u00e9 : Ne stockez jamais votre cl\u00e9 API directement dans le code</li> <li>R\u00e9activit\u00e9 : Optimisez les temps de r\u00e9ponse, ajoutez des indicateurs de chargement</li> <li>Robustesse : G\u00e9rez les erreurs (API indisponible, limite de requ\u00eates atteinte, etc.)</li> <li>Qualit\u00e9 des r\u00e9ponses : Testez r\u00e9guli\u00e8rement avec des questions vari\u00e9es pour v\u00e9rifier la pertinence</li> </ul>"},{"location":"module4/partie1-developpement/#livrables-intermediaires","title":"Livrables interm\u00e9diaires","text":"<p>\u00c0 la fin de cette phase, vous devriez avoir :</p> <ul> <li>Une interface conversationnelle fonctionnelle</li> <li>Un syst\u00e8me d'int\u00e9gration avec l'API Mistral AI</li> <li>Une base de connaissances structur\u00e9e</li> <li>Au moins deux fonctionnalit\u00e9s p\u00e9dagogiques impl\u00e9ment\u00e9es</li> </ul> <p>Retour \u00e0 la vue d'ensemble Continuer vers la Phase 2: Finalisation et tests ```</p>"},{"location":"module4/partie2-finalisation/","title":"Phase 2 : Finalisation et tests (1h)","text":""},{"location":"module4/partie2-finalisation/#objectif","title":"Objectif","text":"<p>Cette phase est d\u00e9di\u00e9e \u00e0 la finalisation, aux tests et \u00e0 la pr\u00e9paration de la documentation de votre chatbot p\u00e9dagogique. C'est ici que vous vous assurez que votre solution est fiable, performante et bien document\u00e9e.</p>"},{"location":"module4/partie2-finalisation/#1-tests-fonctionnels-20-min","title":"1. Tests fonctionnels (20 min)","text":""},{"location":"module4/partie2-finalisation/#protocole-de-test","title":"Protocole de test","text":"<p>Mettez en place un protocole syst\u00e9matique pour tester votre chatbot avec des sc\u00e9narios r\u00e9els d'utilisation.</p> <p>Cat\u00e9gories de tests \u00e0 effectuer :</p> <ol> <li> <p>Tests de base</p> <ul> <li>Dialogue simple (question-r\u00e9ponse)</li> <li>Gestion de l'historique de conversation</li> <li>Comportement face \u00e0 des requ\u00eates vides ou incompl\u00e8tes</li> </ul> </li> <li> <p>Tests de connaissances</p> <ul> <li>Questions sur chaque concept majeur du Deep Learning</li> <li>V\u00e9rification de l'exactitude des informations fournies</li> <li>Coh\u00e9rence dans les explications</li> </ul> </li> <li> <p>Tests d'usage p\u00e9dagogique</p> <ul> <li>Adaptation au niveau de l'utilisateur</li> <li>Clart\u00e9 des explications techniques</li> <li>Utilit\u00e9 des exemples et analogies</li> </ul> </li> <li> <p>Tests de robustesse</p> <ul> <li>Gestion des erreurs API</li> <li>Questions hors sujet</li> <li>Questions mal formul\u00e9es ou avec des fautes</li> </ul> </li> </ol> <p>Grille d'\u00e9valuation des tests :</p> Test Crit\u00e8re R\u00e9sultat Commentaire T1: Dialogue simple L'assistant r\u00e9pond de fa\u00e7on coh\u00e9rente \u2b1c OK \u2b1c NOK T2: Gestion historique Les r\u00e9ponses tiennent compte du contexte pr\u00e9c\u00e9dent \u2b1c OK \u2b1c NOK T3: Concept CNN L'explication est exacte et p\u00e9dagogique \u2b1c OK \u2b1c NOK T4: Concept gradient Formulation adapt\u00e9e au niveau d\u00e9butant \u2b1c OK \u2b1c NOK T5: Erreur API Message d'erreur appropri\u00e9 \u2b1c OK \u2b1c NOK ... ... ... ... <p>Pour chaque test qui \u00e9choue, notez le probl\u00e8me et priorisez les corrections.</p>"},{"location":"module4/partie2-finalisation/#profils-dutilisateurs-pour-les-tests","title":"Profils d'utilisateurs pour les tests","text":"<p>Testez votre chatbot avec diff\u00e9rents profils d'utilisateurs :      - D\u00e9butant complet : aucune connaissance pr\u00e9alable      - Niveau interm\u00e9diaire : connaissances de base en programmation      - Niveau avanc\u00e9 : familiarit\u00e9 avec l'IA et questions techniques d\u00e9taill\u00e9es</p>"},{"location":"module4/partie2-finalisation/#2-optimisation-des-performances-20-min","title":"2. Optimisation des performances (20 min)","text":""},{"location":"module4/partie2-finalisation/#optimisation-technique","title":"Optimisation technique","text":"<p>Am\u00e9liorez les performances techniques de votre chatbot :</p> <ol> <li>Temps de r\u00e9ponse</li> <li>R\u00e9duisez la taille des prompts envoy\u00e9s \u00e0 l'API</li> <li> <p>Ajoutez un syst\u00e8me de cache pour les questions fr\u00e9quentes    <pre><code># Exemple d'impl\u00e9mentation d'un cache simple\nresponse_cache = {}\n\ndef get_cached_response(question, user_level):\n    cache_key = f\"{question.lower().strip()}_{user_level}\"\n    return response_cache.get(cache_key)\n\ndef store_in_cache(question, user_level, response):\n    cache_key = f\"{question.lower().strip()}_{user_level}\"\n    response_cache[cache_key] = response\n</code></pre></p> </li> <li> <p>Efficacit\u00e9 de l'API</p> </li> <li>Utilisez des param\u00e8tres adapt\u00e9s pour chaque type de requ\u00eate</li> <li> <p>Optimisez la longueur des contextes transmis    <pre><code># Exemple de configuration par type de requ\u00eate\napi_configs = {\n    \"definition\": {\"temperature\": 0.3, \"max_tokens\": 100},  # D\u00e9finitions pr\u00e9cises\n    \"explanation\": {\"temperature\": 0.5, \"max_tokens\": 300}, # Explications d\u00e9taill\u00e9es\n    \"example\": {\"temperature\": 0.7, \"max_tokens\": 150}      # Exemples cr\u00e9atifs\n}\n</code></pre></p> </li> <li> <p>Gestion de la m\u00e9moire</p> </li> <li>Limitez la taille de l'historique de conversation</li> <li>Ajoutez un m\u00e9canisme de r\u00e9sum\u00e9 pour les longues conversations    <pre><code>class OptimizedConversationManager:\n    def __init__(self, max_history=10):\n        self.max_history = max_history\n        self.history = []\n\n    def add_message(self, role, content):\n        self.history.append({\"role\": role, \"content\": content})\n        # Si l'historique devient trop long, le r\u00e9sumer\n        if len(self.history) &gt; self.max_history + 5:  # +5 pour \u00e9viter de r\u00e9sumer trop souvent\n            self._summarize_history()\n\n    def _summarize_history(self):\n        # Demander \u00e0 l'API de r\u00e9sumer la conversation\n        # Puis remplacer l'historique par le r\u00e9sum\u00e9\n        # [Impl\u00e9mentation ici]\n</code></pre></li> </ol>"},{"location":"module4/partie2-finalisation/#optimisation-pedagogique","title":"Optimisation p\u00e9dagogique","text":"<p>Am\u00e9liorez la qualit\u00e9 p\u00e9dagogique des r\u00e9ponses :</p> <ol> <li> <p>Am\u00e9lioration des prompts</p> <ul> <li>Refinez les instructions syst\u00e8me pour des r\u00e9ponses plus p\u00e9dagogiques</li> <li>Ajoutez des directives sp\u00e9cifiques pour les explications techniques</li> </ul> </li> <li> <p>Enrichissement des r\u00e9ponses</p> <ul> <li>Ajoutez automatiquement des liens vers des ressources compl\u00e9mentaires</li> <li>Incluez des suggestions de questions de suivi pertinentes</li> </ul> </li> <li> <p>Adaptation au niveau</p> <ul> <li>Affinez la d\u00e9tection du niveau de l'utilisateur</li> <li>Personnalisez la complexit\u00e9 des r\u00e9ponses en fonction du niveau d\u00e9tect\u00e9</li> </ul> </li> </ol>"},{"location":"module4/partie2-finalisation/#3-documentation-20-min","title":"3. Documentation (20 min)","text":""},{"location":"module4/partie2-finalisation/#documentation-technique","title":"Documentation technique","text":"<p>Cr\u00e9ez une documentation technique claire et compl\u00e8te :</p> <ol> <li> <p>Architecture du syst\u00e8me</p> <ul> <li>Diagramme des composants principaux</li> <li>Description des interactions entre les composants</li> <li>Technologies et biblioth\u00e8ques utilis\u00e9es</li> </ul> </li> <li> <p>Structure du code</p> <ul> <li>Organisation des fichiers et dossiers</li> <li>Description des classes et fonctions principales</li> <li>Points d'extension pour de futures am\u00e9liorations</li> </ul> </li> <li> <p>API et int\u00e9grations</p> <ul> <li>Configuration requise pour l'API Mistral</li> <li>Param\u00e8tres d'API et leur impact</li> <li>Limites et quotas \u00e0 consid\u00e9rer</li> </ul> </li> </ol> <p>Mod\u00e8le de documentation technique :</p> <pre><code># Documentation technique - Chatbot p\u00e9dagogique Deep Learning\n\n## 1. Vue d'ensemble du syst\u00e8me\n[Diagramme d'architecture]\n\nNotre chatbot est compos\u00e9 de trois composants principaux :\n- Interface utilisateur (HTML/CSS/JS)\n- Serveur backend (Python/Flask)\n- Int\u00e9gration API Mistral AI\n\n## 2. Composants principaux\n\n### 2.1 Interface utilisateur\nL'interface est d\u00e9velopp\u00e9e en HTML/CSS/JS et permet :\n- L'affichage des messages dans un format conversationnel\n- La saisie et l'envoi de questions\n- L'affichage d'indicateurs de chargement\n- [...]\n\n### 2.2 Serveur backend\nLe serveur est d\u00e9velopp\u00e9 en Python avec Flask et g\u00e8re :\n- Les requ\u00eates de l'interface utilisateur\n- L'enrichissement des prompts avec la base de connaissances\n- Les appels \u00e0 l'API Mistral AI\n- [...]\n\n### 2.3 Base de connaissances\nLa base de connaissances est structur\u00e9e en JSON et comprend :\n- X concepts principaux\n- Y sous-concepts\n- Z exemples pratiques\n- [...]\n\n## 3. Flux d'ex\u00e9cution\n1. L'utilisateur envoie une question via l'interface\n2. Le serveur re\u00e7oit la question et l'historique\n3. [...]\n\n## 4. Guide d'installation et de d\u00e9ploiement\n[Instructions d\u00e9taill\u00e9es]\n</code></pre>"},{"location":"module4/partie2-finalisation/#guide-utilisateur","title":"Guide utilisateur","text":"<p>R\u00e9digez un guide utilisateur clair pour faciliter la prise en main :</p> <ol> <li> <p>Pr\u00e9sentation g\u00e9n\u00e9rale</p> <ul> <li>Objectif du chatbot</li> <li>Public cible</li> <li>Fonctionnalit\u00e9s principales</li> </ul> </li> <li> <p>Guide d'utilisation</p> <ul> <li>Comment poser des questions efficacement</li> <li>Exemples de questions pertinentes</li> <li>Commandes sp\u00e9ciales (si existantes)</li> </ul> </li> <li> <p>Conseils d'apprentissage</p> <ul> <li>Progression recommand\u00e9e dans les concepts</li> <li>Comment tester ses connaissances</li> <li>Ressources compl\u00e9mentaires</li> </ul> </li> </ol> <p>Mod\u00e8le de guide utilisateur :</p> <pre><code># Guide utilisateur - Chatbot p\u00e9dagogique Deep Learning\n\n## Bienvenue !\nCe chatbot a \u00e9t\u00e9 con\u00e7u pour vous aider \u00e0 comprendre les concepts du Deep Learning, \nde mani\u00e8re progressive et adapt\u00e9e \u00e0 votre niveau.\n\n## Comment utiliser le chatbot\n1. **Posez une question** dans la zone de texte en bas de l'\u00e9cran\n2. **Attendez la r\u00e9ponse** (g\u00e9n\u00e9ralement quelques secondes)\n3. **Poursuivez la conversation** en posant des questions compl\u00e9mentaires\n\n## Types de questions efficaces\n- \"Qu'est-ce qu'un r\u00e9seau de neurones convolutif ?\"\n- \"Explique-moi la descente de gradient comme si j'avais 12 ans\"\n- \"Quelles sont les diff\u00e9rences entre CNN et RNN ?\"\n- \"Montre-moi un exemple simple de code TensorFlow\"\n\n## Fonctionnalit\u00e9s sp\u00e9ciales\n- Tapez \"quiz\" pour g\u00e9n\u00e9rer un petit quiz sur le sujet de votre choix\n- Tapez \"progression\" pour voir votre avancement dans les concepts\n- [...]\n\n## Parcours d'apprentissage recommand\u00e9\nPour une progression optimale, nous vous sugg\u00e9rons d'explorer les concepts dans cet ordre :\n1. Introduction au Deep Learning\n2. R\u00e9seaux de neurones simples\n3. [...]\n</code></pre>"},{"location":"module4/partie2-finalisation/#4-preparation-de-la-demonstration-10-min","title":"4. Pr\u00e9paration de la d\u00e9monstration (10 min)","text":"<p>Pr\u00e9parez une d\u00e9monstration efficace pour pr\u00e9senter votre travail :</p> <ol> <li> <p>Sc\u00e9nario de d\u00e9monstration</p> <ul> <li>Identifiez un parcours utilisateur repr\u00e9sentatif</li> <li>Pr\u00e9parez 3-5 questions qui mettent en valeur diff\u00e9rentes fonctionnalit\u00e9s</li> <li>Anticipez les points qui pourraient impressionner l'audience</li> </ul> </li> <li> <p>Support visuel</p> <ul> <li>Cr\u00e9ez 2-3 diapositives pr\u00e9sentant l'architecture et les fonctionnalit\u00e9s</li> <li>Pr\u00e9parez un tableau r\u00e9capitulatif des d\u00e9fis rencontr\u00e9s et solutions trouv\u00e9es</li> </ul> </li> <li> <p>R\u00e9partition des r\u00f4les</p> <ul> <li>D\u00e9cidez qui pr\u00e9sentera quelle partie (si en bin\u00f4me)</li> <li>Planifiez les transitions entre les d\u00e9monstrations</li> </ul> </li> </ol> <p>Exemple de sc\u00e9nario de d\u00e9monstration :  1. Introduction du projet et objectifs (1 min)  2. Pr\u00e9sentation de l'architecture (1 min)  3. D\u00e9monstration d'une conversation basique (1 min)  4. D\u00e9monstration d'une fonctionnalit\u00e9 p\u00e9dagogique sp\u00e9ciale (1 min)  5. Explication d'un d\u00e9fi technique rencontr\u00e9 et sa solution (1 min)  6. Questions-r\u00e9ponses (1 min)</p>"},{"location":"module4/partie2-finalisation/#check-list-finale","title":"Check-list finale","text":"<p>Avant de terminer cette phase, v\u00e9rifiez les points suivants :</p> <ul> <li> Tous les tests fonctionnels critiques ont \u00e9t\u00e9 r\u00e9alis\u00e9s</li> <li> Les probl\u00e8mes prioritaires ont \u00e9t\u00e9 corrig\u00e9s</li> <li> La documentation technique est compl\u00e8te</li> <li> Le guide utilisateur est clair et informatif</li> <li> Le sc\u00e9nario de d\u00e9monstration est pr\u00eat</li> <li> Les livrables sont organis\u00e9s et accessibles</li> </ul> <p>Retour \u00e0 la vue d'ensemble Continuer vers la Phase 3: Pr\u00e9sentation des projets ```</p>"},{"location":"module4/partie3-presentation/","title":"Pr\u00e9sentation","text":""},{"location":"module4/partie3-presentation/#partie3-presentationmd","title":"partie3-presentation.md","text":"<pre><code># Phase 3 : Pr\u00e9sentation des projets (30 min)\n\n![Pr\u00e9sentation des projets](../images/banner-chatbot-pedagogique-projet.svg)\n\n## Objectif\n\nCette derni\u00e8re phase de la s\u00e9ance 4 est consacr\u00e9e \u00e0 la pr\u00e9sentation de votre chatbot p\u00e9dagogique. C'est l'aboutissement de votre travail et l'occasion de mettre en valeur votre solution devant la classe.\n\n## D\u00e9roulement des pr\u00e9sentations\n\nChaque \u00e9quipe dispose de **6 minutes** au total :\n - **5 minutes** de pr\u00e9sentation\n - **1 minute** de questions-r\u00e9ponses\n\n\n## Structure recommand\u00e9e pour votre pr\u00e9sentation\n\n### 1. Introduction (1 minute)\n\n- Pr\u00e9sentez bri\u00e8vement votre \u00e9quipe\n- Expliquez le concept g\u00e9n\u00e9ral de votre chatbot p\u00e9dagogique\n- Pr\u00e9cisez le public cible et les objectifs p\u00e9dagogiques\n\nExemple d'introduction :\n&gt; \"Bonjour, nous sommes [Pr\u00e9nom1] et [Pr\u00e9nom2]. Nous avons d\u00e9velopp\u00e9 'DeepLBot', un chatbot p\u00e9dagogique con\u00e7u pour accompagner les \u00e9tudiants de BTS SIO dans leur d\u00e9couverte du Deep Learning. Notre objectif est de rendre ces concepts complexes accessibles gr\u00e2ce \u00e0 des explications personnalis\u00e9es et interactives.\"\n\n### 2. Architecture et choix techniques (1 minute)\n\n- Pr\u00e9sentez un sch\u00e9ma simple de l'architecture\n- Justifiez rapidement vos choix techniques\n- Mettez en avant les technologies principales utilis\u00e9es\n\nExemple :\n&gt; \"Notre solution repose sur une architecture \u00e0 trois composants : une interface web en HTML/CSS/JavaScript, un backend Flask en Python, et l'int\u00e9gration de l'API Mistral AI. Nous avons structur\u00e9 notre base de connaissances en JSON pour faciliter la maintenance et l'enrichissement du contenu.\"\n\n### 3. D\u00e9monstration en direct (2 minutes)\n\n- Montrez votre chatbot en action avec 2-3 sc\u00e9narios pr\u00e9d\u00e9finis\n- Mettez en valeur les fonctionnalit\u00e9s distinctives\n- Commentez les interactions pendant la d\u00e9monstration\n\nConseils pour une d\u00e9monstration efficace :\n- Pr\u00e9parez un script pr\u00e9cis avec des questions pertinentes\n- Testez votre d\u00e9mo \u00e0 l'avance pour \u00e9viter les surprises\n- Ayez un \"plan B\" en cas de probl\u00e8me technique (captures d'\u00e9cran)\n\n### 4. D\u00e9fis et solutions (1 minute)\n\n- Pr\u00e9sentez 1-2 d\u00e9fis techniques majeurs rencontr\u00e9s\n- Expliquez comment vous les avez surmont\u00e9s\n- Partagez un apprentissage cl\u00e9 de ce processus\n\nExemple :\n&gt; \"Notre principal d\u00e9fi a \u00e9t\u00e9 d'optimiser les prompts pour obtenir des r\u00e9ponses \u00e0 la fois p\u00e9dagogiquement pertinentes et concises. Nous avons r\u00e9solu ce probl\u00e8me en d\u00e9veloppant un syst\u00e8me de prompts dynamiques qui s'adaptent au niveau de l'utilisateur et au type de question pos\u00e9e.\"\n\n### 5. Conclusion et perspectives (30 secondes)\n\n- R\u00e9capitulez les points forts de votre solution\n- Indiquez les am\u00e9liorations futures envisag\u00e9es\n- Terminez sur une note positive\n\n## Conseils pour une pr\u00e9sentation r\u00e9ussie\n\n### Pr\u00e9paration\n\n- **R\u00e9p\u00e9tez** votre pr\u00e9sentation plusieurs fois en chronom\u00e9trant\n- **Simplifiez** votre discours, \u00e9vitez le jargon technique excessif\n- **Synchronisez** les r\u00f4les si vous pr\u00e9sentez en bin\u00f4me\n- **Pr\u00e9parez** vos transitions entre les diff\u00e9rentes parties\n\n### Pendant la pr\u00e9sentation\n\n- **Parlez clairement** et \u00e0 un rythme mod\u00e9r\u00e9\n- **Faites face** \u00e0 l'audience, pas \u00e0 l'\u00e9cran\n- **Mettez en valeur** les fonctionnalit\u00e9s uniques de votre solution\n- **Respectez** strictement le temps imparti\n\n### Support visuel\n\nSi vous utilisez des diapositives, limitez-les \u00e0 3-4 maximum :\n 1. Titre et pr\u00e9sentation de l'\u00e9quipe\n 2. Architecture du chatbot (sch\u00e9ma)\n 3. D\u00e9fis et solutions\n 4. Perspectives futures\n\n## Grille d'\u00e9valuation\n\nVotre pr\u00e9sentation sera \u00e9valu\u00e9e selon les crit\u00e8res suivants :\n\n| Crit\u00e8re | Description | Points |\n|---------|-------------|--------|\n| **Clart\u00e9** | Explication claire du concept et de l'impl\u00e9mentation | /4 |\n| **D\u00e9monstration** | Qualit\u00e9 et pertinence de la d\u00e9monstration en direct | /6 |\n| **Technicit\u00e9** | Ma\u00eetrise technique et pertinence des choix d'impl\u00e9mentation | /5 |\n| **Pr\u00e9sentation** | Organisation, respect du temps, qualit\u00e9 de l'\u00e9locution | /3 |\n| **R\u00e9ponses** | Qualit\u00e9 des r\u00e9ponses aux questions | /2 |\n| **Total** | | /20 |\n\n## Feedback et \u00e9valuation par les pairs\n\nPendant que vos camarades pr\u00e9sentent, vous \u00eates encourag\u00e9s \u00e0 :\n - Prendre des notes sur les id\u00e9es int\u00e9ressantes\n - R\u00e9fl\u00e9chir \u00e0 une question pertinente \u00e0 poser\n - Compl\u00e9ter la grille d'\u00e9valuation par les pairs qui vous sera distribu\u00e9e\n\nCette \u00e9valuation par les pairs sera prise en compte dans l'\u00e9valuation finale, mais de mani\u00e8re anonyme.\n\n## Apr\u00e8s les pr\u00e9sentations\n\nUne fois toutes les pr\u00e9sentations termin\u00e9es :\n - Un temps de d\u00e9briefing collectif sera organis\u00e9\n - Les points forts de chaque projet seront mis en avant\n - Des conseils d'am\u00e9lioration g\u00e9n\u00e9raux seront partag\u00e9s\n\n## Conclusion et prochaines \u00e9tapes\n\nCette pr\u00e9sentation marque la fin du projet et du parcours de 4 s\u00e9ances sur le Deep Learning. Vous avez acquis des comp\u00e9tences pr\u00e9cieuses en :\n - Compr\u00e9hension des concepts du Deep Learning\n - D\u00e9veloppement d'applications d'IA pratiques\n - Int\u00e9gration d'API de mod\u00e8les de langage\n - Conception d'interfaces interactives\n\nCes comp\u00e9tences sont directement transf\u00e9rables dans votre future vie professionnelle, que ce soit en stage ou en emploi.\n\n## Remise des livrables finaux\n\nN'oubliez pas de d\u00e9poser tous vos livrables finaux sur la plateforme avant la date limite :\n - Code source complet avec un lien GitHub\n - Documentation technique sur Github en Markdown\n - Guide utilisateur\n - Support de pr\u00e9sentation\n\n**Date limite de remise : [DATE_LIMITE]**\n\n[Retour \u00e0 la vue d'ensemble](index.md){ .md-button }\n</code></pre>"},{"location":"module4/partie3-presentation/#fichier-ressourcesmodele-documentation-techniquemd","title":"Fichier ressources/modele-documentation-technique.md","text":"<p>```markdown</p>"},{"location":"module4/partie3-presentation/#modele-de-documentation-technique-chatbot-pedagogique-deep-learning","title":"Mod\u00e8le de documentation technique - Chatbot p\u00e9dagogique Deep Learning","text":"<p>Ce document est un template que vous pouvez utiliser pour documenter votre projet de chatbot p\u00e9dagogique. Remplacez chaque section par vos propres informations.</p>"},{"location":"module4/partie3-presentation/#1-vue-densemble-du-systeme","title":"1. Vue d'ensemble du syst\u00e8me","text":""},{"location":"module4/partie3-presentation/#11-introduction","title":"1.1 Introduction","text":"<p>[D\u00e9crivez bri\u00e8vement votre chatbot, son objectif et son public</p>"},{"location":"module4/preparation-projet/","title":"Pr\u00e9paration au Projet Final","text":""},{"location":"module4/preparation-projet/#objectifs-de-la-phase-de-preparation","title":"Objectifs de la phase de pr\u00e9paration","text":"<p>Cette phase vous permettra de : - Comprendre en d\u00e9tail les sp\u00e9cifications du projet de chatbot p\u00e9dagogique - Analyser des exemples concrets d'utilisation de chatbots similaires - Explorer l'API Mistral AI en profondeur - Planifier et organiser votre travail pour le d\u00e9veloppement</p>"},{"location":"module4/preparation-projet/#1-analyse-du-cahier-des-charges-15-min","title":"1. Analyse du cahier des charges (15 min)","text":"<p>Le cahier des charges de votre chatbot p\u00e9dagogique a \u00e9t\u00e9 pr\u00e9sent\u00e9 en d\u00e9tail dans le document Projet Chatbot P\u00e9dagogique. Prenez le temps de le lire attentivement et de vous poser les questions suivantes :</p> <ol> <li>Quelles fonctionnalit\u00e9s sont essentielles et lesquelles sont optionnelles ?</li> <li>Quels concepts du Deep Learning devront imp\u00e9rativement \u00eatre couverts dans la base de connaissances ?</li> <li>Quels sont les points techniques qui pourraient poser probl\u00e8me ?</li> <li>Comment adapter le niveau des explications aux diff\u00e9rents utilisateurs ?</li> <li>Quelles fonctionnalit\u00e9s p\u00e9dagogiques apporteraient une r\u00e9elle valeur ajout\u00e9e ?</li> </ol>"},{"location":"module4/preparation-projet/#exercice-de-priorisation","title":"Exercice de priorisation","text":"<p>\u00c9tablissez une liste des fonctionnalit\u00e9s \u00e0 d\u00e9velopper class\u00e9es par ordre de priorit\u00e9 :</p> <ol> <li>Fonctionnalit\u00e9s de base (MVP - Produit Minimum Viable)</li> <li>Fonctionnalit\u00e9s importantes </li> <li>Fonctionnalit\u00e9s optionnelles (si le temps le permet)</li> </ol>"},{"location":"module4/preparation-projet/#2-etude-de-cas-dentreprises-utilisant-des-chatbots-15-min","title":"2. \u00c9tude de cas d'entreprises utilisant des chatbots (15 min)","text":"<p>Avant de commencer le d\u00e9veloppement, examinons quelques exemples concrets d'entreprises qui ont mis en place des chatbots similaires \u00e0 celui que vous allez d\u00e9velopper.</p>"},{"location":"module4/preparation-projet/#cas-1-chatbot-pedagogique-pour-une-ecole-de-programmation","title":"Cas 1: Chatbot p\u00e9dagogique pour une \u00e9cole de programmation","text":"<p>Entreprise: CodeSchool (30 formateurs, 500+ \u00e9tudiants)</p> <p>Probl\u00e9matique: Les formateurs recevaient de nombreuses questions basiques identiques, ce qui limitait leur disponibilit\u00e9 pour des probl\u00e8mes plus complexes.</p> <p>Solution: D\u00e9veloppement d'un chatbot assistant bas\u00e9 sur une API de LLM, avec une base de connaissances construite \u00e0 partir du mat\u00e9riel de cours.</p> <p>Architecture: - Frontend: Interface web int\u00e9gr\u00e9e \u00e0 la plateforme d'apprentissage - Backend: API Flask avec mise en cache Redis - LLM: OpenAI API avec fine-tuning sp\u00e9cifique aux cours - Base de connaissances: Structur\u00e9e en JSON par modules de cours</p> <p>R\u00e9sultats: - R\u00e9duction de 40% des questions basiques aux formateurs - Satisfaction des \u00e9tudiants \u00e0 85% concernant les r\u00e9ponses du chatbot - ROI positif apr\u00e8s 4 mois d'utilisation - Cr\u00e9ation de 15 nouveaux modules de cours gr\u00e2ce au temps lib\u00e9r\u00e9</p> <p>Le\u00e7ons apprises: - Importance d'un syst\u00e8me de feedback imm\u00e9diat sur les r\u00e9ponses - N\u00e9cessit\u00e9 de maintenir la base de connaissances \u00e0 jour - Valeur des r\u00e9ponses comportant des exemples de code fonctionnels</p>"},{"location":"module4/preparation-projet/#cas-2-assistant-virtuel-pour-la-formation-interne","title":"Cas 2: Assistant virtuel pour la formation interne","text":"<p>Entreprise: TechConsult (cabinet de conseil IT, 120 employ\u00e9s)</p> <p>Probl\u00e9matique: Difficult\u00e9 \u00e0 former rapidement les nouveaux consultants sur les technologies sp\u00e9cifiques utilis\u00e9es par l'entreprise.</p> <p>Solution: Chatbot de formation accessible 24/7, int\u00e9gr\u00e9 \u00e0 l'intranet, avec connaissance des processus et technologies internes.</p> <p>Architecture: - Interface: Application web responsive - Backend: NodeJS avec FastAPI - LLM: Combinaison d'API locale et Mistral AI - Base de connaissances: Documents techniques convertis en embeddings vectoriels</p> <p>R\u00e9sultats: - R\u00e9duction du temps d'onboarding de 3 semaines \u00e0 10 jours - Augmentation de 25% du taux de r\u00e9ussite aux certifications internes - \u00c9conomie estim\u00e9e de 180 heures de formation par an - Adoption \u00e0 92% parmi les nouveaux employ\u00e9s</p> <p>Le\u00e7ons apprises: - L'importance d'utiliser le vocabulaire sp\u00e9cifique de l'entreprise - La valeur d'un historique de conversation persistant - L'utilit\u00e9 des prompts techniques bien formul\u00e9s</p>"},{"location":"module4/preparation-projet/#3-exploration-de-lapi-mistral-ai-20-min","title":"3. Exploration de l'API Mistral AI (20 min)","text":""},{"location":"module4/preparation-projet/#introduction-a-mistral-ai","title":"Introduction \u00e0 Mistral AI","text":"<p>Mistral AI est une entreprise fran\u00e7aise qui d\u00e9veloppe des mod\u00e8les de langage de pointe, particuli\u00e8rement adapt\u00e9s pour des usages en fran\u00e7ais et dans un contexte \u00e9ducatif. Son API permet d'acc\u00e9der \u00e0 ces mod\u00e8les pour g\u00e9n\u00e9rer du texte, r\u00e9pondre \u00e0 des questions, et plus encore.</p>"},{"location":"module4/preparation-projet/#creation-dun-compte-et-cle-api","title":"Cr\u00e9ation d'un compte et cl\u00e9 API","text":"<ol> <li>Rendez-vous sur console.mistral.ai</li> <li>Cr\u00e9ez un compte (gratuit)</li> <li>Une fois connect\u00e9, cliquez sur \"API Keys\" dans le menu</li> <li>Cliquez sur \"Create API Key\", donnez-lui un nom (ex: \"Projet Chatbot BTS\")</li> <li>Important: Copiez et sauvegardez la cl\u00e9 g\u00e9n\u00e9r\u00e9e, elle ne sera plus affich\u00e9e ensuite</li> </ol>"},{"location":"module4/preparation-projet/#premier-test-avec-lapi","title":"Premier test avec l'API","text":"<p>Commen\u00e7ons par un exemple simple pour tester l'API:</p> <pre><code>import os\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\n# Configuration de l'API\napi_key = \"votre_cl\u00e9_api_ici\"  # Remplacez par votre cl\u00e9\nclient = MistralClient(api_key=api_key)\n\n# Messages\nmessages = [\n    ChatMessage(role=\"system\", content=\"Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans l'explication du Deep Learning pour des \u00e9tudiants de BTS SIO.\"),\n    ChatMessage(role=\"user\", content=\"Peux-tu m'expliquer simplement ce qu'est un r\u00e9seau de neurones convolutif?\")\n]\n\n# Appel \u00e0 l'API\nchat_response = client.chat(\n    model=\"mistral-tiny\",  # Mod\u00e8le le plus l\u00e9ger\n    messages=messages,\n)\n\n# Affichage de la r\u00e9ponse\nprint(chat_response.choices[0].message.content)\n</code></pre>"},{"location":"module4/preparation-projet/#structure-de-lapi-mistral","title":"Structure de l'API Mistral","text":"<p>L'API Mistral AI fonctionne avec une structure simple :</p> <ol> <li>Messages : Liste de messages repr\u00e9sentant une conversation, chacun avec un r\u00f4le (system, user, assistant)</li> <li>Mod\u00e8le : Choix du mod\u00e8le Mistral \u00e0 utiliser (mistral-tiny, mistral-small, mistral-medium...)</li> <li>Param\u00e8tres : Configuration du comportement (temp\u00e9rature, nombre max de tokens, etc.)</li> </ol>"},{"location":"module4/preparation-projet/#gestion-du-contexte-conversationnel","title":"Gestion du contexte conversationnel","text":"<p>Pour maintenir un contexte de conversation, il suffit d'ajouter les messages pr\u00e9c\u00e9dents \u00e0 chaque requ\u00eate :</p> <pre><code># Fonction pour g\u00e9rer une conversation\ndef chat_with_context(messages, user_input):\n    # Ajouter le message de l'utilisateur\n    messages.append(ChatMessage(role=\"user\", content=user_input))\n\n    # Appel \u00e0 l'API\n    response = client.chat(\n        model=\"mistral-tiny\",\n        messages=messages,\n    )\n\n    # R\u00e9cup\u00e9rer la r\u00e9ponse\n    assistant_message = response.choices[0].message.content\n\n    # Ajouter la r\u00e9ponse au contexte\n    messages.append(ChatMessage(role=\"assistant\", content=assistant_message))\n\n    return assistant_message, messages\n\n# Initialiser la conversation\nconversation = [\n    ChatMessage(role=\"system\", content=\"Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans l'explication du Deep Learning pour des \u00e9tudiants de BTS SIO.\")\n]\n\n# Premier \u00e9change\nresponse, conversation = chat_with_context(conversation, \"Qu'est-ce qu'un r\u00e9seau de neurones?\")\nprint(\"Assistant:\", response)\n\n# Deuxi\u00e8me \u00e9change (avec le contexte pr\u00e9c\u00e9dent)\nresponse, conversation = chat_with_context(conversation, \"Comment fonctionne l'apprentissage?\")\nprint(\"Assistant:\", response)\n</code></pre>"},{"location":"module4/preparation-projet/#optimisation-des-prompts","title":"Optimisation des prompts","text":"<p>La qualit\u00e9 des r\u00e9ponses d\u00e9pend beaucoup de la fa\u00e7on dont vous formulez vos instructions (prompts). Voici quelques conseils pour les optimiser :</p>"},{"location":"module4/preparation-projet/#1-instructions-systeme-claires-et-detaillees","title":"1. Instructions syst\u00e8me claires et d\u00e9taill\u00e9es","text":"<pre><code>system_prompt = \"\"\"\nTu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans le Deep Learning pour des \u00e9tudiants de BTS SIO. \nQuand tu r\u00e9ponds:\n1. Utilise un langage simple et accessible\n2. Fournis toujours un exemple concret\n3. Structure tes explications en plusieurs points\n4. Si tu n'es pas s\u00fbr d'une information, indique-le clairement\n5. Adapte le niveau technique au profil de l'\u00e9tudiant (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)\n\"\"\"\n</code></pre>"},{"location":"module4/preparation-projet/#2-enrichissement-avec-la-base-de-connaissances","title":"2. Enrichissement avec la base de connaissances","text":"<pre><code>def enrich_prompt_with_knowledge(user_query, knowledge_base, user_level=\"d\u00e9butant\"):\n    # Rechercher des informations pertinentes dans la base de connaissances\n    relevant_info = search_knowledge_base(user_query, knowledge_base)\n\n    # Enrichir le prompt avec ces informations\n    enriched_prompt = f\"\"\"\nQuestion de l'utilisateur: {user_query}\n\nInformations pertinentes (niveau: {user_level}):\n{relevant_info}\n\nR\u00e9ponds de mani\u00e8re p\u00e9dagogique en utilisant ces informations et en adaptant ton explication au niveau {user_level}.\n\"\"\"\n    return enriched_prompt\n</code></pre>"},{"location":"module4/preparation-projet/#3-parametrage-adapte","title":"3. Param\u00e9trage adapt\u00e9","text":"<pre><code># Pour des explications techniques (plus pr\u00e9cises, moins cr\u00e9atives)\ntechnical_params = {\n    \"temperature\": 0.3,  # Faible temp\u00e9rature pour des r\u00e9ponses plus d\u00e9terministes\n    \"max_tokens\": 500    # Limite de longueur raisonnable\n}\n\n# Pour des exemples et analogies (plus cr\u00e9atifs)\ncreative_params = {\n    \"temperature\": 0.7,  # Temp\u00e9rature plus \u00e9lev\u00e9e pour plus de cr\u00e9ativit\u00e9\n    \"max_tokens\": 300    # Limite de longueur adapt\u00e9e\n}\n\n# Fonction de choix de param\u00e8tres selon le contexte\ndef get_params_for_query(query):\n    if \"explique\" in query.lower() or \"d\u00e9finition\" in query.lower():\n        return technical_params\n    elif \"exemple\" in query.lower() or \"analogie\" in query.lower():\n        return creative_params\n    else:\n        return {\"temperature\": 0.5, \"max_tokens\": 400}  # Param\u00e8tres par d\u00e9faut\n</code></pre>"},{"location":"module4/preparation-projet/#4-planification-et-organisation-10-min","title":"4. Planification et organisation (10 min)","text":""},{"location":"module4/preparation-projet/#structure-recommandee-du-projet","title":"Structure recommand\u00e9e du projet","text":"<pre><code>chatbot-pedagogique/\n\u251c\u2500\u2500 app.py                   # Application principale Flask/FastAPI\n\u251c\u2500\u2500 config.py                # Configuration (cl\u00e9s API, param\u00e8tres)\n\u251c\u2500\u2500 templates/               # Templates HTML\n\u2502   \u2514\u2500\u2500 index.html           # Interface web\n\u251c\u2500\u2500 static/                  # Fichiers statiques (CSS, JS)\n\u251c\u2500\u2500 services/                # Services m\u00e9tier\n\u2502   \u251c\u2500\u2500 mistral_service.py   # Int\u00e9gration API Mistral\n\u2502   \u2514\u2500\u2500 knowledge_service.py # Gestion base de connaissances\n\u2514\u2500\u2500 knowledge_base/          # Base de connaissances\n    \u2514\u2500\u2500 concepts.json        # Structure des concepts DL\n</code></pre>"},{"location":"module4/preparation-projet/#repartition-des-taches","title":"R\u00e9partition des t\u00e2ches","text":"<p>Si vous travaillez en bin\u00f4me, une r\u00e9partition efficace des t\u00e2ches pourrait \u00eatre :</p> <p>Option 1: R\u00e9partition par couche - Membre 1: Backend (Python, API Mistral, logique m\u00e9tier) - Membre 2: Frontend (HTML/CSS/JS, interface, exp\u00e9rience utilisateur)</p> <p>Option 2: R\u00e9partition par fonctionnalit\u00e9 - Membre 1: Interface conversationnelle + int\u00e9gration API - Membre 2: Base de connaissances + fonctionnalit\u00e9s p\u00e9dagogiques</p>"},{"location":"module4/preparation-projet/#planning-recommande","title":"Planning recommand\u00e9","text":"<p>Pour le d\u00e9veloppement proprement dit (S\u00e9ance 4), voici un planning sugg\u00e9r\u00e9 :</p> Temps T\u00e2che Objectif 0h00-0h30 Mise en place de l'environnement Structure du projet, installation des d\u00e9pendances 0h30-1h15 D\u00e9veloppement du MVP Interface de base, int\u00e9gration API simple 1h15-2h00 Base de connaissances Structuration et int\u00e9gration 2h00-2h30 Fonctionnalit\u00e9s p\u00e9dagogiques Quiz, adaptation au niveau 2h30-3h00 Tests et corrections Validation fonctionnelle 3h00-3h30 Documentation et pr\u00e9paration Documentation technique et pr\u00e9sentation 3h30-4h00 Pr\u00e9sentations D\u00e9monstration du chatbot"},{"location":"module4/preparation-projet/#5-exemple-de-base-de-connaissances","title":"5. Exemple de base de connaissances","text":"<p>Voici un exemple simplifi\u00e9 de structure pour votre base de connaissances :</p> <pre><code>{\n  \"concepts\": [\n    {\n      \"id\": \"neural_network\",\n      \"title\": \"R\u00e9seau de neurones\",\n      \"description\": \"Mod\u00e8le de calcul inspir\u00e9 du fonctionnement des neurones biologiques.\",\n      \"levels\": {\n        \"beginner\": \"Un r\u00e9seau de neurones est comme un ensemble de filtres interconnect\u00e9s qui apprennent \u00e0 reconna\u00eetre des motifs dans les donn\u00e9es, un peu comme votre cerveau apprend \u00e0 reconna\u00eetre des visages ou des objets.\",\n        \"intermediate\": \"Syst\u00e8me compos\u00e9 de neurones artificiels organis\u00e9s en couches qui transforment des entr\u00e9es en sorties \u00e0 travers des poids et des fonctions d'activation, permettant d'approximer des fonctions complexes par apprentissage.\",\n        \"advanced\": \"Structure math\u00e9matique compos\u00e9e d'unit\u00e9s de calcul interconnect\u00e9es qui effectuent des transformations non-lin\u00e9aires successives sur les donn\u00e9es d'entr\u00e9e, optimis\u00e9es par r\u00e9tropropagation du gradient pour minimiser une fonction de co\u00fbt.\"\n      },\n      \"examples\": [\n        \"Reconnaissance d'images: un r\u00e9seau peut apprendre \u00e0 identifier des chats dans des photos\",\n        \"Traduction automatique: des r\u00e9seaux traduisent du texte d'une langue \u00e0 une autre\"\n      ],\n      \"analogies\": [\n        \"Un r\u00e9seau de neurones ressemble \u00e0 une cha\u00eene de traitement dans une usine, o\u00f9 chaque station (neurone) effectue une op\u00e9ration sp\u00e9cifique sur le produit qui passe.\",\n        \"Comme un orchestre o\u00f9 chaque musicien (neurone) joue une petite partie, et ensemble ils cr\u00e9ent une symphonie complexe (pr\u00e9diction).\"\n      ],\n      \"related_concepts\": [\"perceptron\", \"deep_learning\", \"activation_function\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"module4/preparation-projet/#6-conclusion-et-preparation","title":"6. Conclusion et pr\u00e9paration","text":"<p>Pour vous pr\u00e9parer efficacement au d\u00e9veloppement du chatbot lors de la prochaine s\u00e9ance :</p> <ol> <li>Cr\u00e9ez votre compte Mistral AI et obtenez votre cl\u00e9 API</li> <li>Testez l'API avec quelques prompts simples pour vous familiariser</li> <li>R\u00e9fl\u00e9chissez \u00e0 la structure de votre base de connaissances</li> <li>Organisez votre travail en \u00e9quipe si applicable</li> <li>Pr\u00e9parez des questions sur les aspects techniques que vous ne ma\u00eetrisez pas encore compl\u00e8tement</li> </ol> <p>Cette phase de pr\u00e9paration est essentielle pour garantir un d\u00e9veloppement efficace lors de la s\u00e9ance finale. En anticipant les d\u00e9fis et en planifiant votre approche, vous maximiserez vos chances de cr\u00e9er un chatbot p\u00e9dagogique fonctionnel et de qualit\u00e9.</p> <p>Retour \u00e0 l'aper\u00e7u du module Commencer le d\u00e9veloppement</p>"},{"location":"module4/projet-chatbot/","title":"Projet Chatbot P\u00e9dagogique sur le Deep Learning","text":""},{"location":"module4/projet-chatbot/#introduction-au-projet","title":"Introduction au projet","text":"<p>Le projet de chatbot p\u00e9dagogique repr\u00e9sente l'aboutissement de votre parcours dans la formation sur le Deep Learning. Il vous permet de mettre en pratique l'ensemble des connaissances et comp\u00e9tences acquises tout en cr\u00e9ant un outil concret et utile.</p> <p>Ce document pr\u00e9sente en d\u00e9tail la vision, les objectifs et les sp\u00e9cifications du projet.</p>"},{"location":"module4/projet-chatbot/#vision-du-projet","title":"Vision du projet","text":""},{"location":"module4/projet-chatbot/#quest-ce-quun-chatbot-pedagogique","title":"Qu'est-ce qu'un chatbot p\u00e9dagogique ?","text":"<p>Un chatbot p\u00e9dagogique est un assistant virtuel con\u00e7u sp\u00e9cifiquement pour accompagner l'apprentissage. Contrairement aux chatbots commerciaux ou de service client, son objectif principal est de faciliter la compr\u00e9hension de concepts complexes en s'adaptant au niveau et aux besoins de l'apprenant.</p>"},{"location":"module4/projet-chatbot/#pourquoi-un-chatbot-sur-le-deep-learning","title":"Pourquoi un chatbot sur le Deep Learning ?","text":"<p>Le Deep Learning est un domaine technique complexe qui combine math\u00e9matiques, informatique et domaines d'application vari\u00e9s. Pour les \u00e9tudiants de BTS SIO, cette complexit\u00e9 peut repr\u00e9senter un obstacle \u00e0 l'apprentissage. Un assistant conversationnel permet de :</p> <ul> <li>Rendre les concepts abstraits plus accessibles gr\u00e2ce \u00e0 des explications personnalis\u00e9es</li> <li>Proposer des exemples concrets adapt\u00e9s au niveau de compr\u00e9hension de l'\u00e9tudiant</li> <li>Offrir une disponibilit\u00e9 permanente pour r\u00e9pondre aux questions</li> <li>Favoriser un apprentissage \u00e0 rythme personnalis\u00e9</li> </ul>"},{"location":"module4/projet-chatbot/#objectifs-pedagogiques","title":"Objectifs p\u00e9dagogiques","text":"<p>Votre chatbot doit permettre aux utilisateurs de :</p> <ol> <li>Comprendre les concepts fondamentaux du Deep Learning de mani\u00e8re progressive</li> <li>Explorer les diff\u00e9rentes architectures de r\u00e9seaux de neurones (CNN, RNN, etc.)</li> <li>Visualiser mentalement le fonctionnement des algorithmes gr\u00e2ce \u00e0 des analogies et des exemples</li> <li>Tester leurs connaissances \u00e0 travers des quiz et exercices interactifs</li> <li>Approfondir certaines notions selon leurs besoins et int\u00e9r\u00eats</li> </ol>"},{"location":"module4/projet-chatbot/#public-cible","title":"Public cible","text":"<p>Le chatbot s'adresse principalement aux :</p> <ul> <li>\u00c9tudiants de BTS SIO d\u00e9couvrant le Deep Learning</li> <li>D\u00e9veloppeurs d\u00e9butants souhaitant int\u00e9grer des fonctionnalit\u00e9s d'IA dans leurs projets</li> <li>Professionnels en reconversion vers les m\u00e9tiers de l'intelligence artificielle</li> </ul>"},{"location":"module4/projet-chatbot/#specifications-fonctionnelles","title":"Sp\u00e9cifications fonctionnelles","text":""},{"location":"module4/projet-chatbot/#1-interface-conversationnelle","title":"1. Interface conversationnelle","text":"<ul> <li>Interface web responsive (desktop et mobile)</li> <li>Zone de discussion avec historique des \u00e9changes</li> <li>Possibilit\u00e9 de r\u00e9initialiser la conversation</li> <li>Indication visuelle lors du chargement des r\u00e9ponses</li> </ul>"},{"location":"module4/projet-chatbot/#2-base-de-connaissances","title":"2. Base de connaissances","text":"<ul> <li>Structure hi\u00e9rarchique des concepts du Deep Learning</li> <li>Au moins 15 concepts fondamentaux couverts en profondeur</li> <li>3 niveaux d'explication pour chaque concept (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)</li> <li>Exemples concrets pour chaque concept</li> <li>Analogies explicatives pour faciliter la compr\u00e9hension</li> </ul>"},{"location":"module4/projet-chatbot/#3-capacites-pedagogiques","title":"3. Capacit\u00e9s p\u00e9dagogiques","text":"<ul> <li>D\u00e9tection du niveau de l'utilisateur \u00e0 partir de ses questions et interactions</li> <li>Adaptation du niveau technique des r\u00e9ponses</li> <li>G\u00e9n\u00e9ration de quiz pour tester les connaissances</li> <li>Suggestion de concepts \u00e0 explorer en fonction des int\u00e9r\u00eats d\u00e9montr\u00e9s</li> <li>Capacit\u00e9 \u00e0 demander des clarifications si la question est ambigu\u00eb</li> </ul>"},{"location":"module4/projet-chatbot/#4-integration-technique","title":"4. Int\u00e9gration technique","text":"<ul> <li>Backend en Python avec Flask ou FastAPI</li> <li>Interface frontend en HTML/CSS/JavaScript</li> <li>Int\u00e9gration de l'API Mistral AI pour la g\u00e9n\u00e9ration de r\u00e9ponses</li> <li>Optimisation des prompts par enrichissement via la base de connaissances</li> <li>Gestion du contexte de conversation</li> </ul>"},{"location":"module4/projet-chatbot/#specifications-techniques","title":"Sp\u00e9cifications techniques","text":""},{"location":"module4/projet-chatbot/#architecture-recommandee","title":"Architecture recommand\u00e9e","text":"<pre><code>chatbot-pedagogique/\n\u251c\u2500\u2500 app.py                   # Application principale Flask/FastAPI\n\u251c\u2500\u2500 config.py                # Configuration (cl\u00e9s API, param\u00e8tres)\n\u251c\u2500\u2500 templates/               # Templates HTML\n\u2502   \u2514\u2500\u2500 index.html           # Interface web\n\u251c\u2500\u2500 static/                  # Fichiers statiques (CSS, JS)\n\u2502   \u251c\u2500\u2500 css/\n\u2502   \u2502   \u2514\u2500\u2500 style.css\n\u2502   \u2514\u2500\u2500 js/\n\u2502       \u2514\u2500\u2500 app.js\n\u251c\u2500\u2500 services/                # Services m\u00e9tier\n\u2502   \u251c\u2500\u2500 mistral_service.py   # Int\u00e9gration API Mistral\n\u2502   \u2514\u2500\u2500 knowledge_service.py # Gestion base de connaissances\n\u2514\u2500\u2500 knowledge_base/          # Base de connaissances\n    \u2514\u2500\u2500 concepts.json        # Structure des concepts DL\n</code></pre>"},{"location":"module4/projet-chatbot/#technologies-a-utiliser","title":"Technologies \u00e0 utiliser","text":"<ul> <li>Backend: Python 3.8+ avec Flask ou FastAPI</li> <li>Frontend: HTML5, CSS3, JavaScript (ES6+)</li> <li>API: Mistral AI (via la biblioth\u00e8que officielle)</li> <li>Base de connaissances: Format JSON structur\u00e9</li> <li>D\u00e9ploiement: Local (optionnel: conteneurisation Docker)</li> </ul>"},{"location":"module4/projet-chatbot/#structure-de-la-base-de-connaissances","title":"Structure de la base de connaissances","text":"<p>La base de connaissances doit suivre une structure JSON standardis\u00e9e :</p> <pre><code>{\n  \"concepts\": [\n    {\n      \"id\": \"neural_network\",\n      \"title\": \"R\u00e9seau de neurones\",\n      \"description\": \"Description g\u00e9n\u00e9rale du concept\",\n      \"levels\": {\n        \"beginner\": \"Explication pour d\u00e9butants\",\n        \"intermediate\": \"Explication pour niveau interm\u00e9diaire\",\n        \"advanced\": \"Explication technique avanc\u00e9e\"\n      },\n      \"examples\": [\n        \"Exemple 1 avec description\",\n        \"Exemple 2 avec description\"\n      ],\n      \"analogies\": [\n        \"Analogie 1 expliquant le concept\",\n        \"Analogie 2 expliquant le concept\"\n      ],\n      \"related_concepts\": [\"perceptron\", \"deep_learning\", \"activation_function\"],\n      \"quiz\": [\n        {\n          \"question\": \"Question sur ce concept\",\n          \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n          \"correct_answer\": 2,\n          \"explanation\": \"Explication de la r\u00e9ponse correcte\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"module4/projet-chatbot/#prompt-engineering","title":"Prompt engineering","text":"<p>Pour obtenir des r\u00e9ponses p\u00e9dagogiques de qualit\u00e9, il faudra soigner le prompt engineering :</p> <pre><code>Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans l'enseignement du Deep Learning aux \u00e9tudiants de BTS SIO.\nNiveau de l'\u00e9tudiant : {niveau}\nConcepts d\u00e9j\u00e0 abord\u00e9s : {concepts_vus}\n\nVoici des informations pertinentes tir\u00e9es de notre base de connaissances :\n{context_from_knowledge_base}\n\nQuestion de l'\u00e9tudiant : {question}\n\nR\u00e9ponds de mani\u00e8re claire et p\u00e9dagogique en :\n1. Donnant une explication adapt\u00e9e au niveau de l'\u00e9tudiant\n2. Utilisant des analogies concr\u00e8tes quand c'est pertinent\n3. Fournissant des exemples pratiques\n4. Reliant ce concept aux notions d\u00e9j\u00e0 abord\u00e9es\n5. Sugg\u00e9rant une prochaine notion \u00e0 explorer\n</code></pre>"},{"location":"module4/projet-chatbot/#criteres-devaluation","title":"Crit\u00e8res d'\u00e9valuation","text":"<p>Votre chatbot sera \u00e9valu\u00e9 selon les crit\u00e8res suivants :</p> Crit\u00e8re Pond\u00e9ration Description Fonctionnalit\u00e9 30% Interface utilisable, r\u00e9ponses coh\u00e9rentes, absence de bugs Qualit\u00e9 p\u00e9dagogique 25% Pertinence des explications, adaptation au niveau, exemples appropri\u00e9s Int\u00e9gration technique 20% Utilisation efficace de l'API, gestion du contexte, optimisation Base de connaissances 15% Structure, couverture des concepts, pr\u00e9cision technique Documentation 10% Guide utilisateur, documentation technique, commentaires code"},{"location":"module4/projet-chatbot/#livrables-attendus","title":"Livrables attendus","text":"<ol> <li>Code source complet du chatbot p\u00e9dagogique</li> <li>Base de connaissances structur\u00e9e sur le Deep Learning</li> <li>Documentation technique expliquant l'architecture et les choix d'impl\u00e9mentation</li> <li>Guide utilisateur pour la prise en main</li> <li>Pr\u00e9sentation de 5 minutes du projet finalis\u00e9</li> </ol>"},{"location":"module4/projet-chatbot/#conseils-pour-reussir","title":"Conseils pour r\u00e9ussir","text":"<ol> <li>Commencez simple: D\u00e9veloppez d'abord les fonctionnalit\u00e9s de base avant d'ajouter des fonctionnalit\u00e9s avanc\u00e9es</li> <li>Organisez votre travail: R\u00e9partissez clairement les t\u00e2ches si vous travaillez en \u00e9quipe</li> <li>Testez r\u00e9guli\u00e8rement: V\u00e9rifiez chaque fonctionnalit\u00e9 d\u00e8s qu'elle est impl\u00e9ment\u00e9e</li> <li>Soignez les prompts: La qualit\u00e9 des r\u00e9ponses d\u00e9pend beaucoup de la qualit\u00e9 des prompts</li> <li>Pensez \u00e0 l'exp\u00e9rience utilisateur: Un chatbot doit \u00eatre intuitif et agr\u00e9able \u00e0 utiliser</li> </ol> <p>Retour \u00e0 l'aper\u00e7u du module Pr\u00e9paration au projet ```</p>"},{"location":"module4/ressources/grille-evaluation-projet/","title":"Grille d'\u00e9valuation - Projet chatbot p\u00e9dagogique","text":"<p>Cette grille d\u00e9taille les crit\u00e8res d'\u00e9valuation de votre projet final de chatbot p\u00e9dagogique sur le Deep Learning.</p>"},{"location":"module4/ressources/grille-evaluation-projet/#1-fonctionnalite-30","title":"1. Fonctionnalit\u00e9 (30%)","text":"Crit\u00e8re Description Bar\u00e8me Note Interface utilisateur Qualit\u00e9, intuitivit\u00e9 et r\u00e9activit\u00e9 de l'interface conversationnelle /5 Qualit\u00e9 des r\u00e9ponses Pertinence, pr\u00e9cision et clart\u00e9 des r\u00e9ponses g\u00e9n\u00e9r\u00e9es /10 Gestion du contexte Capacit\u00e9 \u00e0 maintenir une conversation coh\u00e9rente et contextuelle /5 Robustesse Gestion des erreurs et des cas impr\u00e9vus /5 Fonctionnalit\u00e9s sp\u00e9ciales Mise en \u0153uvre des fonctionnalit\u00e9s p\u00e9dagogiques (quiz, progression, etc.) /5 Sous-total /30"},{"location":"module4/ressources/grille-evaluation-projet/#2-qualite-pedagogique-25","title":"2. Qualit\u00e9 p\u00e9dagogique (25%)","text":"Crit\u00e8re Description Bar\u00e8me Note Adaptation au niveau Capacit\u00e9 \u00e0 ajuster les explications selon le niveau de l'utilisateur /5 Clart\u00e9 des explications Facilit\u00e9 de compr\u00e9hension des concepts complexes /5 Exemples pertinents Qualit\u00e9 et pertinence des exemples fournis /5 Analogies efficaces Utilisation d'analogies pour faciliter la compr\u00e9hension /5 Progression d'apprentissage Organisation logique des concepts et suivi de la progression /5 Sous-total /25"},{"location":"module4/ressources/grille-evaluation-projet/#3-integration-technique-20","title":"3. Int\u00e9gration technique (20%)","text":"Crit\u00e8re Description Bar\u00e8me Note Architecture Qualit\u00e9 de l'architecture et des choix techniques /5 Int\u00e9gration API Mistral Optimisation des requ\u00eates et des prompts /5 Performance Temps de r\u00e9ponse et optimisations /5 Code Qualit\u00e9, lisibilit\u00e9 et organisation du code /5 Sous-total /20"},{"location":"module4/ressources/grille-evaluation-projet/#4-base-de-connaissances-15","title":"4. Base de connaissances (15%)","text":"Crit\u00e8re Description Bar\u00e8me Note Structure Organisation logique et hi\u00e9rarchique des concepts /5 Couverture \u00c9tendue des concepts couverts /5 Pr\u00e9cision technique Exactitude des informations /5 Sous-total /15"},{"location":"module4/ressources/grille-evaluation-projet/#5-documentation-10","title":"5. Documentation (10%)","text":"Crit\u00e8re Description Bar\u00e8me Note Documentation technique Clart\u00e9 et compl\u00e9tude de la documentation technique /5 Guide utilisateur Qualit\u00e9 et pertinence du guide utilisateur /5 Sous-total /10"},{"location":"module4/ressources/grille-evaluation-projet/#note-finale-100","title":"Note finale : /100","text":""},{"location":"module4/ressources/grille-evaluation-projet/#observations-et-commentaires","title":"Observations et commentaires","text":"<p>[Espace r\u00e9serv\u00e9 pour les commentaires de l'\u00e9valuateur]</p>"},{"location":"module4/ressources/grille-evaluation-projet/#points-forts","title":"Points forts","text":""},{"location":"module4/ressources/grille-evaluation-projet/#-","title":"-","text":""},{"location":"module4/ressources/grille-evaluation-projet/#points-a-ameliorer","title":"Points \u00e0 am\u00e9liorer","text":""},{"location":"module4/ressources/grille-evaluation-projet/#-_1","title":"-","text":""},{"location":"module4/ressources/grille-evaluation-projet/#conseils-pour-lavenir","title":"Conseils pour l'avenir","text":""},{"location":"module4/ressources/grille-evaluation-projet/#-_2","title":"-","text":"<p>-</p>"},{"location":"module4/ressources/modele-documentation-technique/","title":"Mod\u00e8le de documentation technique - Chatbot p\u00e9dagogique Deep Learning","text":"<p>Ce document est un template que vous pouvez utiliser pour documenter votre projet de chatbot p\u00e9dagogique. Remplacez chaque section par vos propres informations.</p>"},{"location":"module4/ressources/modele-documentation-technique/#1-vue-densemble-du-systeme","title":"1. Vue d'ensemble du syst\u00e8me","text":""},{"location":"module4/ressources/modele-documentation-technique/#11-introduction","title":"1.1 Introduction","text":"<p>[D\u00e9crivez bri\u00e8vement votre chatbot, son objectif et son public cible. Expliquez pourquoi il a \u00e9t\u00e9 d\u00e9velopp\u00e9 et quels probl\u00e8mes il r\u00e9sout.]</p>"},{"location":"module4/ressources/modele-documentation-technique/#12-architecture-globale","title":"1.2 Architecture globale","text":"<p>[Ins\u00e9rez ici un diagramme d'architecture]</p> <p>Notre chatbot p\u00e9dagogique est compos\u00e9 des composants principaux suivants :</p> <ul> <li>Interface utilisateur : Interface web conversationnelle permettant aux utilisateurs d'interagir avec le chatbot</li> <li>Backend : Serveur Python qui g\u00e8re la logique m\u00e9tier et les interactions avec l'API</li> <li>API Mistral AI : Service externe fournissant les capacit\u00e9s de compr\u00e9hension et g\u00e9n\u00e9ration de langage naturel</li> <li>Base de connaissances : Structure de donn\u00e9es contenant les concepts, exemples et quiz sur le Deep Learning</li> </ul>"},{"location":"module4/ressources/modele-documentation-technique/#13-technologies-utilisees","title":"1.3 Technologies utilis\u00e9es","text":"Composant Technologies Justification Frontend HTML5, CSS3, JavaScript Technologies web standard pour une compatibilit\u00e9 maximale Backend Python 3.x, Flask/FastAPI \u00c9cosyst\u00e8me riche pour l'IA/ML, facilit\u00e9 d'int\u00e9gration avec les API Base de donn\u00e9es JSON structur\u00e9 Format l\u00e9ger et flexible, adapt\u00e9 \u00e0 une base de connaissances hi\u00e9rarchique API Mistral AI Mod\u00e8le de langage avanc\u00e9 avec bonnes performances en fran\u00e7ais D\u00e9ploiement [Pr\u00e9cisez ici : local, Docker, etc.] [Justification du choix]"},{"location":"module4/ressources/modele-documentation-technique/#2-composants-detailles","title":"2. Composants d\u00e9taill\u00e9s","text":""},{"location":"module4/ressources/modele-documentation-technique/#21-interface-utilisateur","title":"2.1 Interface utilisateur","text":""},{"location":"module4/ressources/modele-documentation-technique/#211-structure-des-fichiers","title":"2.1.1 Structure des fichiers","text":"<pre><code>static/\n\u251c\u2500\u2500 css/\n\u2502   \u2514\u2500\u2500 styles.css     # Styles de l'interface\n\u2514\u2500\u2500 js/\n    \u2514\u2500\u2500 app.js         # Logique client et communication avec l'API\n\ntemplates/\n\u2514\u2500\u2500 index.html         # Structure HTML de l'interface\n</code></pre>"},{"location":"module4/ressources/modele-documentation-technique/#212-fonctionnalites-principales","title":"2.1.2 Fonctionnalit\u00e9s principales","text":"<ul> <li>Affichage des messages dans un format conversationnel</li> <li>Saisie et envoi de questions</li> <li>Affichage des indicateurs de chargement</li> <li>Gestion de l'historique de conversation</li> <li>[Autres fonctionnalit\u00e9s sp\u00e9cifiques \u00e0 votre interface...]</li> </ul>"},{"location":"module4/ressources/modele-documentation-technique/#213-communication-avec-le-backend","title":"2.1.3 Communication avec le backend","text":"<p>[D\u00e9crivez comment l'interface communique avec le backend, les formats de donn\u00e9es, etc.]</p>"},{"location":"module4/ressources/modele-documentation-technique/#22-backend","title":"2.2 Backend","text":""},{"location":"module4/ressources/modele-documentation-technique/#221-structure-des-fichiers","title":"2.2.1 Structure des fichiers","text":"<pre><code>app.py                 # Point d'entr\u00e9e de l'application\nconfig.py              # Configuration (cl\u00e9s API, param\u00e8tres)\nservices/\n\u251c\u2500\u2500 mistral_service.py # Service d'interaction avec l'API Mistral\n\u2514\u2500\u2500 knowledge_service.py # Service de gestion de la base de connaissances\n</code></pre>"},{"location":"module4/ressources/modele-documentation-technique/#222-points-dapi","title":"2.2.2 Points d'API","text":"Endpoint M\u00e9thode Description Param\u00e8tres R\u00e9ponse <code>/api/chat</code> POST Traite une question utilisateur <code>{\"message\": string, \"history\": array}</code> <code>{\"response\": string}</code> <code>/api/quiz</code> GET G\u00e9n\u00e8re un quiz sur un sujet <code>?topic=string</code> <code>{\"questions\": array}</code> [Autres endpoints...]"},{"location":"module4/ressources/modele-documentation-technique/#223-classes-et-fonctions-principales","title":"2.2.3 Classes et fonctions principales","text":"<p>[D\u00e9crivez les classes et fonctions principales de votre backend, leur r\u00f4le et leurs interactions]</p>"},{"location":"module4/ressources/modele-documentation-technique/#23-integration-avec-mistral-ai","title":"2.3 Int\u00e9gration avec Mistral AI","text":""},{"location":"module4/ressources/modele-documentation-technique/#231-configuration-de-lapi","title":"2.3.1 Configuration de l'API","text":"<p>[Expliquez comment vous avez configur\u00e9 l'API Mistral, les param\u00e8tres utilis\u00e9s, etc.]</p>"},{"location":"module4/ressources/modele-documentation-technique/#232-gestion-du-contexte-conversationnel","title":"2.3.2 Gestion du contexte conversationnel","text":"<p>[D\u00e9crivez comment vous g\u00e9rez le contexte des conversations avec l'API Mistral]</p>"},{"location":"module4/ressources/modele-documentation-technique/#233-optimisation-des-prompts","title":"2.3.3 Optimisation des prompts","text":"<p>[D\u00e9taillez vos techniques de prompt engineering pour obtenir des r\u00e9ponses p\u00e9dagogiques]</p>"},{"location":"module4/ressources/modele-documentation-technique/#24-base-de-connaissances","title":"2.4 Base de connaissances","text":""},{"location":"module4/ressources/modele-documentation-technique/#241-structure-des-donnees","title":"2.4.1 Structure des donn\u00e9es","text":"<p>[Pr\u00e9sentez la structure JSON de votre base de connaissances avec un exemple]</p>"},{"location":"module4/ressources/modele-documentation-technique/#242-mecanisme-de-recherche-et-enrichissement","title":"2.4.2 M\u00e9canisme de recherche et enrichissement","text":"<p>[Expliquez comment vous recherchez et utilisez les informations de la base de connaissances]</p>"},{"location":"module4/ressources/modele-documentation-technique/#3-flux-dexecution","title":"3. Flux d'ex\u00e9cution","text":""},{"location":"module4/ressources/modele-documentation-technique/#31-traitement-dune-question-utilisateur","title":"3.1 Traitement d'une question utilisateur","text":"<ol> <li>L'utilisateur saisit une question dans l'interface</li> <li>La requ\u00eate est envoy\u00e9e au backend via l'API <code>/api/chat</code></li> <li>Le backend identifie les concepts pertinents dans la base de connaissances</li> <li>Ces informations sont utilis\u00e9es pour enrichir le prompt envoy\u00e9 \u00e0 l'API Mistral</li> <li>La r\u00e9ponse de l'API est trait\u00e9e et renvoy\u00e9e \u00e0 l'interface utilisateur</li> <li>L'interface affiche la r\u00e9ponse dans la conversation</li> </ol>"},{"location":"module4/ressources/modele-documentation-technique/#32-generation-dun-quiz","title":"3.2 G\u00e9n\u00e9ration d'un quiz","text":"<p>[D\u00e9crivez le flux d'ex\u00e9cution pour la g\u00e9n\u00e9ration de quiz]</p>"},{"location":"module4/ressources/modele-documentation-technique/#33-autres-flux-specifiques-a-votre-application","title":"3.3 [Autres flux sp\u00e9cifiques \u00e0 votre application]","text":""},{"location":"module4/ressources/modele-documentation-technique/#4-securite-et-performance","title":"4. S\u00e9curit\u00e9 et performance","text":""},{"location":"module4/ressources/modele-documentation-technique/#41-gestion-des-cles-api","title":"4.1 Gestion des cl\u00e9s API","text":"<p>[Expliquez comment vous g\u00e9rez et prot\u00e9gez les cl\u00e9s API]</p>"},{"location":"module4/ressources/modele-documentation-technique/#42-optimisations-de-performance","title":"4.2 Optimisations de performance","text":"<p>[D\u00e9taillez les optimisations mises en place pour am\u00e9liorer les performances]</p>"},{"location":"module4/ressources/modele-documentation-technique/#43-gestion-des-erreurs","title":"4.3 Gestion des erreurs","text":"<p>[D\u00e9crivez comment vous g\u00e9rez les erreurs \u00e0 diff\u00e9rents niveaux]</p>"},{"location":"module4/ressources/modele-documentation-technique/#5-guide-dinstallation-et-deploiement","title":"5. Guide d'installation et d\u00e9ploiement","text":""},{"location":"module4/ressources/modele-documentation-technique/#51-prerequis","title":"5.1 Pr\u00e9requis","text":"<ul> <li>Python 3.8 ou sup\u00e9rieur</li> <li>Connexion Internet (pour l'API Mistral)</li> <li>[Autres pr\u00e9requis...]</li> </ul>"},{"location":"module4/ressources/modele-documentation-technique/#52-installation","title":"5.2 Installation","text":"<pre><code># Cloner le d\u00e9p\u00f4t\ngit clone https://github.com/votre-compte/chatbot-pedagogique.git\ncd chatbot-pedagogique\n\n# Installer les d\u00e9pendances\npip install -r requirements.txt\n\n# Configurer les variables d'environnement\ncp .env.example .env\n# \u00c9ditez le fichier .env pour ajouter votre cl\u00e9 API Mistral\n</code></pre>"},{"location":"module4/ressources/modele-documentation-technique/#53-configuration","title":"5.3 Configuration","text":"<p>[D\u00e9taillez les \u00e9tapes de configuration n\u00e9cessaires]</p>"},{"location":"module4/ressources/modele-documentation-technique/#54-lancement","title":"5.4 Lancement","text":"<pre><code># Lancer l'application\npython app.py\n</code></pre>"},{"location":"module4/ressources/modele-documentation-technique/#55-tests","title":"5.5 Tests","text":"<p>[Expliquez comment ex\u00e9cuter les tests]</p>"},{"location":"module4/ressources/modele-documentation-technique/#6-extensions-et-ameliorations-futures","title":"6. Extensions et am\u00e9liorations futures","text":"<p>[Listez les am\u00e9liorations potentielles et extensions pr\u00e9vues pour le chatbot]</p>"},{"location":"module4/ressources/modele-documentation-technique/#7-problemes-connus-et-limitations","title":"7. Probl\u00e8mes connus et limitations","text":"<p>[Documentez les probl\u00e8mes connus et les limitations actuelles]</p>"},{"location":"module4/ressources/modele-documentation-technique/#8-annexes","title":"8. Annexes","text":""},{"location":"module4/ressources/modele-documentation-technique/#81-glossaire","title":"8.1 Glossaire","text":"Terme D\u00e9finition [Terme 1] [D\u00e9finition] [Terme 2] [D\u00e9finition]"},{"location":"module4/ressources/modele-documentation-technique/#82-references","title":"8.2 R\u00e9f\u00e9rences","text":"<p>[Listez les r\u00e9f\u00e9rences, biblioth\u00e8ques, articles ou autres ressources utilis\u00e9es] <pre><code>Pour le dossier des ressources, je vais maintenant cr\u00e9er un exemple de base de connaissances en JSON.\n\n## ressources/exemple-base-connaissances.json\n\n```json\n{\n  \"concepts\": [\n    {\n      \"id\": \"neural_network\",\n      \"title\": \"R\u00e9seau de neurones\",\n      \"description\": \"Mod\u00e8le informatique inspir\u00e9 du fonctionnement des neurones biologiques, capable d'apprendre \u00e0 partir de donn\u00e9es.\",\n      \"levels\": {\n        \"beginner\": \"Un r\u00e9seau de neurones est comme un ensemble de filtres intelligents interconnect\u00e9s qui apprennent progressivement \u00e0 reconna\u00eetre des motifs dans les donn\u00e9es. Imaginez un groupe de personnes qui se passent des informations et les transforment petit \u00e0 petit pour arriver \u00e0 une d\u00e9cision finale.\",\n        \"intermediate\": \"Un r\u00e9seau de neurones est un syst\u00e8me compos\u00e9 de neurones artificiels organis\u00e9s en couches qui transforment des donn\u00e9es d'entr\u00e9e en sorties au travers de poids et de fonctions d'activation. Ces poids sont ajust\u00e9s durant l'apprentissage pour minimiser l'erreur entre les pr\u00e9dictions et les valeurs r\u00e9elles.\",\n        \"advanced\": \"Un r\u00e9seau de neurones est un mod\u00e8le param\u00e9trique compos\u00e9 d'unit\u00e9s de calcul interconnect\u00e9es qui effectuent des transformations non-lin\u00e9aires sur les donn\u00e9es d'entr\u00e9e. Ces transformations sont d\u00e9finies par des matrices de poids et des biais, optimis\u00e9s par descente de gradient et r\u00e9tropropagation pour minimiser une fonction de co\u00fbt d\u00e9finie sur l'ensemble d'apprentissage.\"\n      },\n      \"examples\": [\n        \"Reconnaissance d'images: un r\u00e9seau peut apprendre \u00e0 identifier des chats dans des photos\",\n        \"Traduction automatique: des r\u00e9seaux traduisent du texte d'une langue \u00e0 une autre\",\n        \"G\u00e9n\u00e9ration de musique: des r\u00e9seaux peuvent composer des morceaux originaux en s'inspirant d'un style particulier\"\n      ],\n      \"analogies\": [\n        \"Un r\u00e9seau de neurones est comme une cha\u00eene de transformation dans une usine: chaque station (neurone) effectue une op\u00e9ration sp\u00e9cifique sur le produit qui passe, et ensemble, elles transforment la mati\u00e8re premi\u00e8re (donn\u00e9es d'entr\u00e9e) en produit fini (pr\u00e9diction).\",\n        \"C'est comme un orchestre o\u00f9 chaque musicien (neurone) joue sa partition, et ensemble ils cr\u00e9ent une symphonie (pr\u00e9diction). Le chef d'orchestre (algorithme d'apprentissage) guide les musiciens pour am\u00e9liorer leur performance.\"\n      ],\n      \"related_concepts\": [\"perceptron\", \"deep_learning\", \"activation_function\"],\n      \"quiz\": [\n        {\n          \"question\": \"Quelle caract\u00e9ristique fondamentale permet aux r\u00e9seaux de neurones d'apprendre?\",\n          \"options\": [\n            \"Leur capacit\u00e9 \u00e0 m\u00e9moriser tous les exemples d'entra\u00eenement\",\n            \"L'ajustement automatique des poids en fonction des erreurs\",\n            \"Leur architecture toujours fixe et d\u00e9termin\u00e9e \u00e0 l'avance\",\n            \"La pr\u00e9sence syst\u00e9matique de nombreuses couches\"\n          ],\n          \"correct_answer\": 1,\n          \"explanation\": \"Les r\u00e9seaux de neurones apprennent en ajustant progressivement leurs poids en fonction des erreurs commises sur les donn\u00e9es d'entra\u00eenement, ce qui leur permet de s'am\u00e9liorer au fil du temps.\"\n        }\n      ]\n    },\n    {\n      \"id\": \"cnn\",\n      \"title\": \"R\u00e9seau de neurones convolutif (CNN)\",\n      \"description\": \"Type de r\u00e9seau sp\u00e9cialis\u00e9 dans le traitement des donn\u00e9es en grille comme les images, utilisant des op\u00e9rations de convolution pour d\u00e9tecter des motifs spatiaux.\",\n      \"levels\": {\n        \"beginner\": \"Les CNN sont des r\u00e9seaux sp\u00e9cialis\u00e9s pour analyser les images. Ils fonctionnent un peu comme notre vision: ils d\u00e9tectent d'abord des \u00e9l\u00e9ments simples (lignes, contours), puis les combinent pour reconna\u00eetre des formes plus complexes (visages, objets). C'est comme si vous aviez des d\u00e9tectives qui cherchent des indices sp\u00e9cifiques dans une image.\",\n        \"intermediate\": \"Les CNN utilisent des filtres de convolution qui glissent sur l'image pour d\u00e9tecter des motifs locaux. Ces r\u00e9seaux sont organis\u00e9s en couches successives: les premi\u00e8res d\u00e9tectent des caract\u00e9ristiques basiques (contours, textures) et les suivantes combinent ces informations pour identifier des structures plus complexes. Le pooling permet de r\u00e9duire la dimension tout en conservant l'information importante.\",\n        \"advanced\": \"Les CNN exploitent trois id\u00e9es fondamentales: les champs r\u00e9ceptifs locaux, le partage de poids et le sous-\u00e9chantillonnage. La convolution est une op\u00e9ration qui applique un filtre \u00e0 une r\u00e9gion locale, produisant une carte d'activation (feature map). L'architecture typique alterne couches de convolution (extraction de caract\u00e9ristiques) et couches de pooling (r\u00e9duction de dimension et invariance), suivies de couches enti\u00e8rement connect\u00e9es pour la classification.\"\n      },\n      \"examples\": [\n        \"Classification d'images: identification d'objets dans une photo\",\n        \"D\u00e9tection d'objets: localisation pr\u00e9cise d'objets dans une image avec des bo\u00eetes englobantes\",\n        \"Vision par ordinateur m\u00e9dicale: d\u00e9tection de tumeurs dans des images m\u00e9dicales\",\n        \"Reconnaissance faciale: identification de personnes \u00e0 partir de leurs traits faciaux\"\n      ],\n      \"analogies\": [\n        \"Un CNN est comme un d\u00e9tective qui examine une sc\u00e8ne de crime: d'abord il note les d\u00e9tails \u00e9vidents (couches initiales), puis il \u00e9tablit des liens entre ces indices pour comprendre ce qui s'est pass\u00e9 (couches profondes).\",\n        \"Les filtres de convolution sont comme des pochoirs: quand on place un pochoir sur une image et qu'on colorie dessus, on fait ressortir certains motifs sp\u00e9cifiques.\"\n      ],\n      \"related_concepts\": [\"convolution\", \"pooling\", \"feature_map\", \"computer_vision\"],\n      \"quiz\": [\n        {\n          \"question\": \"Quelle est la principale innovation des CNN par rapport aux r\u00e9seaux de neurones classiques?\",\n          \"options\": [\n            \"Ils utilisent plus de neurones\",\n            \"Ils exploitent la structure spatiale des donn\u00e9es\",\n            \"Ils s'entra\u00eenent plus rapidement\",\n            \"Ils n\u00e9cessitent moins de donn\u00e9es d'entra\u00eenement\"\n          ],\n          \"correct_answer\": 1,\n          \"explanation\": \"Les CNN exploitent la structure spatiale des donn\u00e9es en utilisant des op\u00e9rations de convolution qui permettent de d\u00e9tecter des motifs locaux, ce qui est particuli\u00e8rement adapt\u00e9 aux images o\u00f9 les pixels voisins sont fortement corr\u00e9l\u00e9s.\"\n        }\n      ]\n    },\n    {\n      \"id\": \"rnn\",\n      \"title\": \"R\u00e9seau de neurones r\u00e9current (RNN)\",\n      \"description\": \"Architecture de r\u00e9seau sp\u00e9cialis\u00e9e dans le traitement des donn\u00e9es s\u00e9quentielles comme le texte ou les s\u00e9ries temporelles, avec des connexions formant des cycles.\",\n      \"levels\": {\n        \"beginner\": \"Les RNN sont des r\u00e9seaux qui ont une m\u00e9moire, comme lorsque vous lisez un livre et vous vous souvenez des pages pr\u00e9c\u00e9dentes pour comprendre le contexte. Ils sont parfaits pour traiter du texte car ils peuvent se rappeler ce qui a \u00e9t\u00e9 dit avant.\",\n        \"intermediate\": \"Les RNN poss\u00e8dent des connexions r\u00e9currentes qui leur permettent de transmettre de l'information d'une \u00e9tape \u00e0 l'autre. \u00c0 chaque nouvel \u00e9l\u00e9ment de la s\u00e9quence, le r\u00e9seau utilise \u00e0 la fois cet \u00e9l\u00e9ment et sa m\u00e9moire interne pour faire une pr\u00e9diction et mettre \u00e0 jour son \u00e9tat. Cela leur conf\u00e8re une capacit\u00e9 \u00e0 capturer des d\u00e9pendances temporelles.\",\n        \"advanced\": \"Les RNN sont caract\u00e9ris\u00e9s par des connexions cycliques dans leur graphe computationnel, permettant la persistance de l'information via un \u00e9tat cach\u00e9. Lors de l'entra\u00eenement par r\u00e9tropropagation \u00e0 travers le temps (BPTT), ils peuvent souffrir du probl\u00e8me de disparition ou d'explosion du gradient, limitant leur capacit\u00e9 \u00e0 capturer des d\u00e9pendances \u00e0 long terme. Pour pallier ce probl\u00e8me, des architectures comme LSTM et GRU ont \u00e9t\u00e9 d\u00e9velopp\u00e9es avec des m\u00e9canismes de portes contr\u00f4lant le flux d'information.\"\n      },\n      \"examples\": [\n        \"Traduction automatique: traduire des phrases d'une langue \u00e0 une autre\",\n        \"G\u00e9n\u00e9ration de texte: cr\u00e9er du contenu texte coh\u00e9rent\",\n        \"Analyse de sentiment: d\u00e9terminer si un commentaire est positif ou n\u00e9gatif\",\n        \"Pr\u00e9diction de s\u00e9ries temporelles: pr\u00e9voir l'\u00e9volution du cours des actions\"\n      ],\n      \"analogies\": [\n        \"Un RNN est comme une personne qui lit un livre: \u00e0 chaque mot, elle utilise sa compr\u00e9hension des mots pr\u00e9c\u00e9dents pour interpr\u00e9ter le mot actuel.\",\n        \"C'est comme un musicien qui improvise: chaque note qu'il joue d\u00e9pend des notes qu'il a jou\u00e9es avant, cr\u00e9ant ainsi une m\u00e9lodie coh\u00e9rente.\"\n      ],\n      \"related_concepts\": [\"lstm\", \"gru\", \"sequence_processing\", \"natural_language_processing\"],\n      \"quiz\": [\n        {\n          \"question\": \"Pourquoi les RNN sont-ils particuli\u00e8rement adapt\u00e9s au traitement du langage naturel?\",\n          \"options\": [\n            \"Parce qu'ils utilisent moins de m\u00e9moire que les autres r\u00e9seaux\",\n            \"Parce qu'ils peuvent traiter des images en m\u00eame temps que du texte\",\n            \"Parce qu'ils peuvent m\u00e9moriser le contexte dans une s\u00e9quence\",\n            \"Parce qu'ils sont plus rapides \u00e0 entra\u00eener que les CNN\"\n          ],\n          \"correct_answer\": 2,\n          \"explanation\": \"Les RNN sont particuli\u00e8rement adapt\u00e9s au traitement du langage naturel car ils peuvent conserver en m\u00e9moire le contexte des mots pr\u00e9c\u00e9dents dans une phrase, ce qui est essentiel pour comprendre le sens des mots qui suivent.\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"ressources/","title":"Ressources pour le Deep Learning","text":""},{"location":"ressources/#bienvenue-dans-lespace-des-ressources","title":"Bienvenue dans l'espace des ressources","text":"<p>Cette section regroupe l'ensemble des ressources disponibles pour vous accompagner dans votre apprentissage du Deep Learning et dans le d\u00e9veloppement de votre projet de chatbot p\u00e9dagogique.</p>"},{"location":"ressources/#documentation-des-apis","title":"Documentation des APIs","text":""},{"location":"ressources/#api-mistral","title":"API Mistral","text":"<p>L'API Mistral AI est au c\u0153ur du projet de chatbot p\u00e9dagogique. Cette documentation vous explique :</p> <ul> <li>Comment cr\u00e9er et configurer votre compte Mistral AI</li> <li>Comment envoyer des requ\u00eates et traiter les r\u00e9ponses</li> <li>Les bonnes pratiques pour le prompt engineering</li> <li>Des exemples d'int\u00e9gration dans diff\u00e9rents contextes</li> </ul>"},{"location":"ressources/#instructions-dintegration","title":"Instructions d'int\u00e9gration","text":"<p>Ce guide d\u00e9taille les \u00e9tapes \u00e0 suivre pour int\u00e9grer les diff\u00e9rents composants du chatbot :</p> <ul> <li>Configuration de l'environnement de d\u00e9veloppement</li> <li>Installation des d\u00e9pendances requises</li> <li>Proc\u00e9dures de d\u00e9ploiement pour les environnements de test et de production</li> <li>R\u00e9solution des probl\u00e8mes courants d'int\u00e9gration</li> </ul>"},{"location":"ressources/#structure-des-donnees","title":"Structure des donn\u00e9es","text":""},{"location":"ressources/#base-de-connaissances","title":"Base de connaissances","text":"<p>La base de connaissances est le fondement de votre chatbot p\u00e9dagogique. Cette documentation couvre :</p> <ul> <li>Les concepts fondamentaux du Deep Learning \u00e0 int\u00e9grer</li> <li>L'organisation hi\u00e9rarchique des connaissances</li> <li>Les diff\u00e9rents niveaux d'explication (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)</li> <li>Des exemples concrets et analogies pour l'enseignement</li> </ul>"},{"location":"ressources/#schemas-json","title":"Sch\u00e9mas JSON","text":"<p>Les sch\u00e9mas JSON d\u00e9finissent la structure des donn\u00e9es utilis\u00e9es par le chatbot :</p> <ul> <li>Format de la base de connaissances</li> <li>Structure des \u00e9changes avec l'API</li> <li>Organisation des conversations et de l'historique</li> <li>Mod\u00e8les pour les quiz et exercices</li> </ul>"},{"location":"ressources/#guides-pratiques","title":"Guides pratiques","text":""},{"location":"ressources/#guide-dutilisation-de-google-colab","title":"Guide d'utilisation de Google Colab","text":"<p>Un guide complet pour tirer le meilleur parti de Google Colab dans vos projets de Deep Learning :</p> <ul> <li>Configuration de l'environnement</li> <li>Utilisation des GPUs gratuits</li> <li>Sauvegarde et partage de notebooks</li> <li>Astuces pour l'optimisation des performances</li> </ul>"},{"location":"ressources/#guide-du-developpeur-pour-le-chatbot","title":"Guide du d\u00e9veloppeur pour le chatbot","text":"<p>Un ensemble de bonnes pratiques pour le d\u00e9veloppement du chatbot :</p> <ul> <li>Architecture recommand\u00e9e</li> <li>Gestion des erreurs et des cas limites</li> <li>Optimisation des performances</li> <li>Tests et validation</li> </ul>"},{"location":"ressources/#ressources-complementaires","title":"Ressources compl\u00e9mentaires","text":""},{"location":"ressources/#bibliotheques-et-frameworks-recommandes","title":"Biblioth\u00e8ques et frameworks recommand\u00e9s","text":"<p>Une s\u00e9lection de biblioth\u00e8ques et frameworks utiles pour le projet :</p> <ul> <li>TensorFlow/Keras pour le d\u00e9veloppement de mod\u00e8les</li> <li>Flask pour le d\u00e9veloppement d'API</li> <li>React pour l'interface utilisateur</li> <li>MongoDB pour le stockage des donn\u00e9es</li> </ul>"},{"location":"ressources/#lectures-recommandees","title":"Lectures recommand\u00e9es","text":"<p>Une bibliographie s\u00e9lective pour approfondir vos connaissances :</p> <ul> <li>Livres de r\u00e9f\u00e9rence sur le Deep Learning</li> <li>Articles scientifiques pertinents</li> <li>Blogs et newsletters \u00e0 suivre</li> <li>Tutoriels vid\u00e9o recommand\u00e9s</li> </ul>"},{"location":"ressources/#telechargements","title":"T\u00e9l\u00e9chargements","text":""},{"location":"ressources/#code-source-des-exemples","title":"Code source des exemples","text":"<p>Acc\u00e9dez au code source des exemples pr\u00e9sent\u00e9s lors des s\u00e9ances :</p> <ul> <li>Notebooks des mini-projets CNN et RNN</li> <li>Scripts d'int\u00e9gration avec l'API Mistral</li> <li>Templates pour la base de connaissances</li> <li>Exemples d'interfaces conversationnelles</li> </ul>"},{"location":"ressources/#templates-et-modeles","title":"Templates et mod\u00e8les","text":"<p>Des templates pr\u00eats \u00e0 l'emploi pour acc\u00e9l\u00e9rer votre d\u00e9veloppement :</p> <ul> <li>Structure de base du chatbot</li> <li>Template de documentation technique</li> <li>Mod\u00e8les pour la base de connaissances</li> <li>Grilles d'\u00e9valuation des performances</li> </ul>"},{"location":"ressources/#support-et-assistance","title":"Support et assistance","text":"<p>Si vous rencontrez des difficult\u00e9s ou avez des questions, plusieurs options s'offrent \u00e0 vous :</p> <ul> <li>Consultez la FAQ ci-dessous</li> <li>Posez vos questions sur le forum d\u00e9di\u00e9</li> <li>Contactez votre formateur pendant les heures de permanence</li> <li>\u00c9changez avec vos pairs dans les canaux de discussion</li> </ul>"},{"location":"ressources/#faq","title":"FAQ","text":"<p>Q: Comment obtenir une cl\u00e9 API Mistral gratuitement ? R: Consultez la section \"Cr\u00e9ation de compte\" dans la documentation de l'API Mistral. Un quota gratuit suffisant pour le d\u00e9veloppement est disponible.</p> <p>Q: Puis-je utiliser un autre framework que TensorFlow pour le projet ? R: Oui, vous pouvez utiliser PyTorch ou d'autres frameworks, mais les exemples et le support seront principalement orient\u00e9s TensorFlow/Keras.</p> <p>Q: Comment g\u00e9rer les limites de l'API Mistral dans mon chatbot ? R: La documentation de l'API Mistral contient une section d\u00e9di\u00e9e \u00e0 la gestion des limites et des quotas, avec des strat\u00e9gies d'optimisation.</p> <p>Q: Comment structurer efficacement ma base de connaissances ? R: R\u00e9f\u00e9rez-vous au document \"Structure de la base de connaissances\" qui propose une organisation hi\u00e9rarchique et des exemples concrets.</p> <p>Ces ressources ont \u00e9t\u00e9 con\u00e7ues pour vous accompagner tout au long de votre parcours d'apprentissage et du d\u00e9veloppement de votre projet de chatbot p\u00e9dagogique. N'h\u00e9sitez pas \u00e0 les consulter r\u00e9guli\u00e8rement et \u00e0 nous faire part de vos suggestions d'am\u00e9lioration.</p> <p>Retour \u00e0 l'accueil</p>"},{"location":"ressources/api-mistral/","title":"Int\u00e9gration de l'API Mistral avec FastAPI - Premier test","text":""},{"location":"ressources/api-mistral/#bts-sio-seance-2-types-de-reseaux-et-applications","title":"BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux et applications","text":"<pre><code>import requests\nimport json\nimport os\nfrom dotenv import load_dotenv\nfrom fastapi import FastAPI, Request, Form, HTTPException\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import HTMLResponse, JSONResponse\nimport uvicorn\nfrom pydantic import BaseModel\n\n# Charger les variables d'environnement\nload_dotenv()\n\n# Configuration de l'API Mistral\nMISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"votre_cl\u00e9_api\")  # \u00c0 remplacer par votre cl\u00e9 API\nMISTRAL_API_URL = \"https://api.mistral.ai/v1/chat/completions\"\n\n# Initialisation de l'application FastAPI\napp = FastAPI(title=\"Explorateur de concepts Deep Learning\", \n              description=\"Une API pour explorer les concepts du Deep Learning avec Mistral AI\")\n\n# Configuration des templates\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# 1. Fonction simple pour appeler l'API Mistral\ndef mistral_chat_completion(prompt, model=\"mistral-tiny\", max_tokens=1000):\n    \"\"\"\n    Appelle l'API Mistral pour g\u00e9n\u00e9rer une r\u00e9ponse \u00e0 partir d'un prompt.\n\n    Args:\n        prompt (str): Le message \u00e0 envoyer \u00e0 l'API\n        model (str): Le mod\u00e8le \u00e0 utiliser (mistral-tiny, mistral-small, mistral-medium, etc.)\n        max_tokens (int): Nombre maximum de tokens pour la r\u00e9ponse\n\n    Returns:\n        dict: La r\u00e9ponse de l'API\n    \"\"\"\n    headers = {\n        \"Authorization\": f\"Bearer {MISTRAL_API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    data = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        \"max_tokens\": max_tokens\n    }\n\n    try:\n        response = requests.post(MISTRAL_API_URL, headers=headers, data=json.dumps(data))\n        response.raise_for_status()  # Lever une exception si la requ\u00eate \u00e9choue\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"Erreur lors de l'appel \u00e0 l'API Mistral: {e}\")\n        return {\"error\": str(e)}\n\n# 2. Test simple de l'API\ndef test_mistral_api():\n    \"\"\"\n    Teste l'API Mistral avec un prompt simple.\n    \"\"\"\n    prompt = \"Explique-moi ce qu'est le Deep Learning en 3 phrases simples.\"\n\n    print(f\"Envoi du prompt \u00e0 Mistral: '{prompt}'\")\n    response = mistral_chat_completion(prompt)\n\n    if \"error\" in response:\n        print(f\"Erreur: {response['error']}\")\n        return\n\n    # Extraire et afficher la r\u00e9ponse\n    try:\n        message_content = response[\"choices\"][0][\"message\"][\"content\"]\n        print(\"\\nR\u00e9ponse de Mistral:\")\n        print(message_content)\n\n        # Informations suppl\u00e9mentaires sur la r\u00e9ponse\n        if \"usage\" in response:\n            usage = response[\"usage\"]\n            print(\"\\nUtilisation de tokens:\")\n            print(f\"- Prompt: {usage.get('prompt_tokens', 'N/A')} tokens\")\n            print(f\"- R\u00e9ponse: {usage.get('completion_tokens', 'N/A')} tokens\")\n            print(f\"- Total: {usage.get('total_tokens', 'N/A')} tokens\")\n    except (KeyError, IndexError) as e:\n        print(f\"Erreur lors du traitement de la r\u00e9ponse: {e}\")\n        print(\"R\u00e9ponse brute:\", response)\n\n# 3. Fonction avanc\u00e9e pour l'explication de concepts de Deep Learning\ndef explain_deep_learning_concept(concept, difficulty=\"d\u00e9butant\"):\n    \"\"\"\n    Demande \u00e0 l'API Mistral d'expliquer un concept de Deep Learning.\n\n    Args:\n        concept (str): Le concept \u00e0 expliquer\n        difficulty (str): Le niveau de difficult\u00e9 (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)\n\n    Returns:\n        str: L'explication g\u00e9n\u00e9r\u00e9e\n    \"\"\"\n    # Construire un prompt \u00e9ducatif structur\u00e9\n    prompt = f\"\"\"\n    En tant que tuteur p\u00e9dagogique sp\u00e9cialis\u00e9 en Deep Learning, explique le concept de '{concept}' \n    \u00e0 un \u00e9tudiant de BTS SIO  (niveau {difficulty}).\n\n    Ton explication doit inclure:\n    1. Une d\u00e9finition simple et claire\n    2. Un exemple concret d'application\n    3. Comment ce concept est utilis\u00e9 dans le d\u00e9veloppement d'applications\n\n    Utilise un langage accessible mais techniquement pr\u00e9cis.\n    \"\"\"\n\n    response = mistral_chat_completion(prompt, model=\"mistral-small\")\n\n    if \"error\" in response:\n        return f\"Erreur: {response['error']}\"\n\n    try:\n        return response[\"choices\"][0][\"message\"][\"content\"]\n    except (KeyError, IndexError):\n        return \"Erreur lors de la r\u00e9cup\u00e9ration de la r\u00e9ponse.\"\n\n# 4. Mod\u00e8les Pydantic pour les requ\u00eates\nclass ConceptRequest(BaseModel):\n    concept: str\n    difficulty: str = \"d\u00e9butant\"\n\n# 5. Routes FastAPI\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n@app.post(\"/api/explain\")\nasync def api_explain(request: ConceptRequest):\n    if not request.concept:\n        raise HTTPException(status_code=400, detail=\"Concept manquant\")\n\n    explanation = explain_deep_learning_concept(request.concept, request.difficulty)\n    return {\"explanation\": explanation}\n\n# 6. Template HTML simple pour l'interface\ndef create_template_directory():\n    \"\"\"Cr\u00e9e un r\u00e9pertoire templates et un fichier index.html\"\"\"\n    import os\n    os.makedirs('templates', exist_ok=True)\n\n    with open('templates/index.html', 'w') as f:\n        f.write(\"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Explorateur de concepts Deep Learning&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n            line-height: 1.6;\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #f5f8fa;\n            color: #333;\n        }\n        h1 {\n            color: #2c3e50;\n            border-bottom: 2px solid #3498db;\n            padding-bottom: 10px;\n        }\n        .container {\n            background-color: white;\n            border-radius: 8px;\n            padding: 20px;\n            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n        }\n        label {\n            display: block;\n            margin-top: 15px;\n            font-weight: bold;\n            color: #2c3e50;\n        }\n        input, select, button {\n            width: 100%;\n            padding: 10px;\n            margin-top: 5px;\n            border: 1px solid #ddd;\n            border-radius: 4px;\n            box-sizing: border-box;\n        }\n        button {\n            background-color: #3498db;\n            color: white;\n            border: none;\n            padding: 12px;\n            margin-top: 20px;\n            cursor: pointer;\n            font-weight: bold;\n            transition: background-color 0.3s;\n        }\n        button:hover {\n            background-color: #2980b9;\n        }\n        #result {\n            margin-top: 20px;\n            padding: 20px;\n            background-color: #f8f9fa;\n            border-left: 4px solid #3498db;\n            border-radius: 4px;\n            white-space: pre-wrap;\n        }\n        .loading {\n            text-align: center;\n            margin-top: 20px;\n            display: none;\n        }\n        .hint {\n            font-size: 0.8em;\n            color: #7f8c8d;\n            margin-top: 5px;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;h1&gt;Explorateur de concepts Deep Learning&lt;/h1&gt;\n        &lt;p&gt;Utilisez cet outil pour explorer et comprendre les concepts cl\u00e9s du Deep Learning, expliqu\u00e9s par l'IA.&lt;/p&gt;\n\n        &lt;form id=\"explainForm\"&gt;\n            &lt;label for=\"concept\"&gt;Concept \u00e0 expliquer:&lt;/label&gt;\n            &lt;input type=\"text\" id=\"concept\" required placeholder=\"Ex: r\u00e9seaux de neurones convolutifs, LSTM, dropout...\"&gt;\n            &lt;div class=\"hint\"&gt;Essayez des concepts comme: convolution, pooling, fonction d'activation, r\u00e9tropropagation...&lt;/div&gt;\n\n            &lt;label for=\"difficulty\"&gt;Niveau de difficult\u00e9:&lt;/label&gt;\n            &lt;select id=\"difficulty\"&gt;\n                &lt;option value=\"d\u00e9butant\"&gt;D\u00e9butant&lt;/option&gt;\n                &lt;option value=\"interm\u00e9diaire\"&gt;Interm\u00e9diaire&lt;/option&gt;\n                &lt;option value=\"avanc\u00e9\"&gt;Avanc\u00e9&lt;/option&gt;\n            &lt;/select&gt;\n\n            &lt;button type=\"submit\"&gt;Expliquer&lt;/button&gt;\n        &lt;/form&gt;\n\n        &lt;div class=\"loading\" id=\"loading\"&gt;\n            &lt;p&gt;Chargement de l'explication...&lt;/p&gt;\n        &lt;/div&gt;\n\n        &lt;div id=\"result\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n        document.getElementById('explainForm').addEventListener('submit', async function(e) {\n            e.preventDefault();\n\n            const concept = document.getElementById('concept').value.trim();\n            const difficulty = document.getElementById('difficulty').value;\n            const resultDiv = document.getElementById('result');\n            const loadingDiv = document.getElementById('loading');\n\n            if (!concept) {\n                resultDiv.innerHTML = \"Veuillez entrer un concept \u00e0 expliquer.\";\n                return;\n            }\n\n            // Afficher l'indicateur de chargement\n            loadingDiv.style.display = 'block';\n            resultDiv.innerHTML = \"\";\n\n            try {\n                const response = await fetch('/api/explain', {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json'\n                    },\n                    body: JSON.stringify({ concept, difficulty })\n                });\n\n                const data = await response.json();\n\n                if (data.error) {\n                    resultDiv.innerHTML = `&lt;p style=\"color: red\"&gt;Erreur: ${data.error}&lt;/p&gt;`;\n                } else {\n                    resultDiv.innerHTML = data.explanation.replace(/\\\\n/g, '&lt;br&gt;');\n                }\n            } catch (error) {\n                resultDiv.innerHTML = `&lt;p style=\"color: red\"&gt;Erreur: ${error.message}&lt;/p&gt;`;\n            } finally {\n                // Cacher l'indicateur de chargement\n                loadingDiv.style.display = 'none';\n            }\n        });\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n        \"\"\")\n\n    print(\"Template index.html cr\u00e9\u00e9 avec succ\u00e8s!\")\n\n# 7. Fonction principale pour ex\u00e9cuter l'application\ndef main():\n    \"\"\"Fonction principale\"\"\"\n    print(\"=== EXPLORATION DE L'API MISTRAL POUR LE CHATBOT P\u00c9DAGOGIQUE ===\")\n\n    # Tester si la cl\u00e9 API est configur\u00e9e\n    if MISTRAL_API_KEY == \"votre_cl\u00e9_api\":\n        print(\"\\nERREUR: Vous devez configurer votre cl\u00e9 API Mistral!\")\n        print(\"1. Cr\u00e9ez un fichier .env dans le m\u00eame r\u00e9pertoire que ce script\")\n        print(\"2. Ajoutez la ligne: MISTRAL_API_KEY=votre_cl\u00e9_api_r\u00e9elle\")\n        print(\"3. Relancez le script\")\n        return\n\n    # Test simple de l'API\n    print(\"\\n1. Test simple de l'API Mistral\")\n    test_mistral_api()\n\n    # Cr\u00e9ation du r\u00e9pertoire et du fichier template\n    print(\"\\n2. Cr\u00e9ation du template pour l'application web\")\n    create_template_directory()\n    print(\"   Template cr\u00e9\u00e9 dans le r\u00e9pertoire 'templates/'\")\n\n    # Lancement de l'application FastAPI\n    print(\"\\n3. D\u00e9marrage de l'application web\")\n    print(\"   URL: http://localhost:8000\")\n    print(\"   Documentation de l'API: http://localhost:8000/docs\")\n    print(\"   Appuyez sur Ctrl+C pour quitter\")\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ressources/base-connaissances/","title":"Concepts fondamentaux du Deep Learning","text":""},{"location":"ressources/base-connaissances/#1-terminologie-de-base","title":"1. Terminologie de base","text":"Terme D\u00e9finition Ce que vous avez exp\u00e9riment\u00e9 Neurone artificiel Unit\u00e9 de calcul qui applique une fonction d'activation \u00e0 une somme pond\u00e9r\u00e9e d'entr\u00e9es Les n\u0153uds dans les visualisations qui transforment les entr\u00e9es en sorties Poids (weights) Param\u00e8tres ajustables qui d\u00e9terminent l'importance de chaque entr\u00e9e d'un neurone Les valeurs que vous avez modifi\u00e9es pour am\u00e9liorer la performance du mod\u00e8le Biais (bias) Param\u00e8tre suppl\u00e9mentaire qui permet au neurone de s'activer m\u00eame si toutes les entr\u00e9es sont nulles Le d\u00e9calage que vous avez observ\u00e9 dans les fronti\u00e8res de d\u00e9cision Fonction d'activation Fonction non-lin\u00e9aire appliqu\u00e9e \u00e0 la somme pond\u00e9r\u00e9e pour introduire la complexit\u00e9 ReLU, Sigmoid, etc. que vous avez test\u00e9es pour am\u00e9liorer l'apprentissage Couche (layer) Groupe de neurones qui traitent l'information au m\u00eame niveau Les diff\u00e9rentes \u00e9tapes de traitement dans votre r\u00e9seau Couche cach\u00e9e Couche situ\u00e9e entre la couche d'entr\u00e9e et la couche de sortie Les couches interm\u00e9diaires que vous avez ajout\u00e9es/modifi\u00e9es Forward propagation Processus de calcul de la sortie \u00e0 partir des entr\u00e9es L'ex\u00e9cution de votre mod\u00e8le pour obtenir une pr\u00e9diction"},{"location":"ressources/base-connaissances/#concepts-dapprentissage","title":"Concepts d'apprentissage","text":"Terme D\u00e9finition Ce que vous avez exp\u00e9riment\u00e9 Descente de gradient Algorithme d'optimisation qui ajuste les poids pour minimiser l'erreur Le processus d'am\u00e9lioration qui se produisait pendant l'entra\u00eenement Taux d'apprentissage Param\u00e8tre qui contr\u00f4le l'ampleur des ajustements de poids \u00e0 chaque it\u00e9ration La valeur que vous avez modifi\u00e9e pour acc\u00e9l\u00e9rer ou stabiliser l'apprentissage \u00c9poque (epoch) Un passage complet \u00e0 travers l'ensemble des donn\u00e9es d'entra\u00eenement Le nombre d'it\u00e9rations d'entra\u00eenement que vous avez d\u00e9fini Batch Sous-ensemble des donn\u00e9es trait\u00e9 avant une mise \u00e0 jour des poids La taille des groupes d'exemples utilis\u00e9s pendant l'entra\u00eenement Fonction de perte (loss) Mesure de l'\u00e9cart entre les pr\u00e9dictions et les valeurs r\u00e9elles La courbe descendante que vous avez observ\u00e9e pendant l'entra\u00eenement Surapprentissage (overfitting) Situation o\u00f9 le mod\u00e8le performe bien sur les donn\u00e9es d'entra\u00eenement mais mal sur de nouvelles donn\u00e9es La baisse de performance sur les donn\u00e9es de test que certains ont pu observer R\u00e9gularisation Techniques pour pr\u00e9venir le surapprentissage Dropout, L1/L2 que certains groupes ont peut-\u00eatre utilis\u00e9s"},{"location":"ressources/base-connaissances/#architecture-des-reseaux","title":"Architecture des r\u00e9seaux","text":"Type Caract\u00e9ristiques Applications typiques R\u00e9seau dense (fully connected) Chaque neurone connect\u00e9 \u00e0 tous les neurones de la couche pr\u00e9c\u00e9dente Le type de r\u00e9seau que vous avez utilis\u00e9 aujourd'hui R\u00e9seau convolutif (CNN) Utilise des filtres qui glissent sur les donn\u00e9es d'entr\u00e9e Traitement d'images (que nous verrons plus tard) R\u00e9seau r\u00e9current (RNN) Poss\u00e8de des connexions en boucle pour traiter des s\u00e9quences Traitement de texte, s\u00e9ries temporelles (s\u00e9ance future)"},{"location":"ressources/base-connaissances/#hyperparametres-cles","title":"Hyperparam\u00e8tres cl\u00e9s","text":"Hyperparam\u00e8tre Impact Plage typique Nombre de couches D\u00e9termine la profondeur du r\u00e9seau et sa capacit\u00e9 \u00e0 apprendre des repr\u00e9sentations complexes 1-5 pour probl\u00e8mes simples, &gt;5 pour probl\u00e8mes complexes Nombre de neurones par couche Influence la capacit\u00e9 d'apprentissage et le risque de surapprentissage D\u00e9pend du probl\u00e8me (32-128 souvent utilis\u00e9 pour d\u00e9buter) Taux d'apprentissage Contr\u00f4le la vitesse et la stabilit\u00e9 de l'apprentissage 0.1 \u00e0 0.0001 (souvent 0.01 ou 0.001) Fonction d'activation D\u00e9termine le type de relations que peut mod\u00e9liser le r\u00e9seau ReLU pour couches cach\u00e9es, Sigmoid/Softmax pour sortie Taille de batch Influence la vitesse et la stabilit\u00e9 de l'apprentissage 16 \u00e0 128 typiquement"},{"location":"ressources/base-connaissances/#2-deep-learning-vs-machine-learning-classique","title":"2. Deep Learning vs Machine Learning Classique","text":""},{"location":"ressources/base-connaissances/#tableau-comparatif","title":"Tableau comparatif","text":"Crit\u00e8re Machine Learning Classique Deep Learning Extraction des caract\u00e9ristiques Manuelle (feature engineering) Automatique Volume de donn\u00e9es requis Peut fonctionner avec moins de donn\u00e9es N\u00e9cessite g\u00e9n\u00e9ralement de grands volumes de donn\u00e9es Interpr\u00e9tabilit\u00e9 Souvent plus interpr\u00e9table Fonctionne comme une \"bo\u00eete noire\" Puissance de calcul Moins intensive N\u00e9cessite souvent des GPU Pr\u00e9cision sur des t\u00e2ches complexes Limit\u00e9e pour les donn\u00e9es non structur\u00e9es Excellente pour les images, texte, son Pr\u00e9traitement des donn\u00e9es Souvent complexe et sp\u00e9cifique Plus simple, mais normalisation importante"},{"location":"ressources/base-connaissances/#illustration-concrete","title":"Illustration concr\u00e8te","text":"<p>Le Machine Learning classique n\u00e9cessite une extraction manuelle des caract\u00e9ristiques, tandis que le Deep Learning les apprend automatiquement.</p>"},{"location":"ressources/base-connaissances/#3-fonctions-dactivation-courantes","title":"3. Fonctions d'activation courantes","text":""},{"location":"ressources/base-connaissances/#role-des-fonctions-dactivation","title":"R\u00f4le des fonctions d'activation","text":"<p>Les fonctions d'activation introduisent des non-lin\u00e9arit\u00e9s dans le r\u00e9seau, permettant d'apprendre des relations complexes dans les donn\u00e9es. Sans elles, le r\u00e9seau serait \u00e9quivalent \u00e0 une simple r\u00e9gression lin\u00e9aire.</p>"},{"location":"ressources/base-connaissances/#types-principaux","title":"Types principaux","text":"Fonction Description simple Utilisation typique ReLU Si valeur n\u00e9gative, sortie = 0; sinon, sortie = valeur d'entr\u00e9e Couches cach\u00e9es (standard) Sigmoid Transforme n'importe quel nombre en valeur entre 0 et 1 Sortie pour classification binaire Tanh Similaire \u00e0 Sigmoid mais avec des valeurs entre -1 et 1 Alternative \u00e0 ReLU pour certains r\u00e9seaux Softmax Transforme un groupe de nombres en probabilit\u00e9s qui somment \u00e0 1 Sortie pour classification multi-classes Leaky ReLU Version am\u00e9lior\u00e9e de ReLU qui permet un petit gradient pour les valeurs n\u00e9gatives Alternative \u00e0 ReLU pour \u00e9viter les \"neurones morts\""},{"location":"ressources/base-connaissances/#choix-de-la-fonction-dactivation","title":"Choix de la fonction d'activation","text":"<ul> <li>Couches cach\u00e9es : ReLU est g\u00e9n\u00e9ralement le premier choix pour sa simplicit\u00e9 et efficacit\u00e9</li> <li>Couche de sortie : </li> <li>Sigmoid pour classification binaire (0-1)</li> <li>Softmax pour classification multi-classes (probabilit\u00e9s qui somment \u00e0 1)</li> <li>Lin\u00e9aire pour r\u00e9gression</li> </ul>"},{"location":"ressources/base-connaissances/#4-processus-dentrainement-explique","title":"4. Processus d'entra\u00eenement expliqu\u00e9","text":""},{"location":"ressources/base-connaissances/#etapes-du-processus-dapprentissage","title":"\u00c9tapes du processus d'apprentissage","text":"<ol> <li>Initialisation : Les poids sont initialis\u00e9s avec de petites valeurs al\u00e9atoires</li> <li>Forward Propagation : Les donn\u00e9es traversent le r\u00e9seau pour g\u00e9n\u00e9rer une pr\u00e9diction</li> <li>Calcul de l'erreur : La fonction de perte mesure l'\u00e9cart entre pr\u00e9diction et r\u00e9alit\u00e9</li> <li>Backpropagation : L'erreur est propag\u00e9e en arri\u00e8re pour calculer les gradients</li> <li>Mise \u00e0 jour des poids : Les poids sont ajust\u00e9s dans la direction qui r\u00e9duit l'erreur</li> <li>It\u00e9ration : Les \u00e9tapes 2-5 sont r\u00e9p\u00e9t\u00e9es jusqu'\u00e0 convergence ou nombre maximum d'\u00e9poques</li> </ol>"},{"location":"ressources/base-connaissances/#visualisation-du-processus","title":"Visualisation du processus","text":"<p>Une visualisation montrerait le flux des donn\u00e9es \u00e0 travers le r\u00e9seau, le calcul de l'erreur, et la mise \u00e0 jour des poids.</p>"},{"location":"ressources/base-connaissances/#fonction-de-perte","title":"Fonction de perte","text":"<p>La fonction de perte quantifie l'\u00e9cart entre les pr\u00e9dictions et les valeurs r\u00e9elles. Les plus communes sont :</p> Fonction de perte Usage Description simple Erreur quadratique moyenne (MSE) R\u00e9gression Moyenne des carr\u00e9s des diff\u00e9rences entre pr\u00e9dictions et valeurs r\u00e9elles Entropie crois\u00e9e binaire Classification binaire Mesure \u00e0 quel point les pr\u00e9dictions de probabilit\u00e9 divergent des valeurs r\u00e9elles (0 ou 1) Entropie crois\u00e9e cat\u00e9gorielle Classification multi-classes Version multi-classes de l'entropie crois\u00e9e binaire"},{"location":"ressources/base-connaissances/#5-architectures-cnn-expliquees","title":"5. Architectures CNN expliqu\u00e9es","text":""},{"location":"ressources/base-connaissances/#structure-dun-cnn","title":"Structure d'un CNN","text":"<p>Les CNN (Convolutional Neural Networks) sont sp\u00e9cialement con\u00e7us pour traiter des donn\u00e9es structur\u00e9es en grille, comme les images. Leur architecture typique comprend :</p> <ol> <li>Couche d'entr\u00e9e : Prend l'image brute (pixels)</li> <li>Couches de convolution : Appliquent des filtres pour d\u00e9tecter des caract\u00e9ristiques</li> <li>Couches de pooling : R\u00e9duisent les dimensions tout en pr\u00e9servant l'information importante</li> <li>Couches enti\u00e8rement connect\u00e9es : Effectuent la classification finale</li> </ol>"},{"location":"ressources/base-connaissances/#fonctionnement-des-convolutions","title":"Fonctionnement des convolutions","text":"<p>La convolution consiste \u00e0 faire glisser un filtre (noyau) sur l'image pour d\u00e9tecter des motifs sp\u00e9cifiques. Imaginez une petite fen\u00eatre qui se d\u00e9place sur l'image et cherche des motifs comme des contours, des textures, etc.</p>"},{"location":"ressources/base-connaissances/#hierarchie-des-caracteristiques","title":"Hi\u00e9rarchie des caract\u00e9ristiques","text":"<p>Les CNN apprennent une hi\u00e9rarchie de caract\u00e9ristiques :</p> <ul> <li>Premi\u00e8res couches : D\u00e9tection de bordures et contours simples</li> <li>Couches interm\u00e9diaires : Motifs, textures et formes</li> <li>Couches profondes : Objets et concepts de haut niveau</li> </ul>"},{"location":"ressources/base-connaissances/#6-architectures-rnn-expliquees","title":"6. Architectures RNN expliqu\u00e9es","text":""},{"location":"ressources/base-connaissances/#structure-dun-rnn","title":"Structure d'un RNN","text":"<p>Les RNN (Recurrent Neural Networks) sont con\u00e7us pour traiter des donn\u00e9es s\u00e9quentielles comme le texte, la parole ou les s\u00e9ries temporelles.</p> <p>La caract\u00e9ristique cl\u00e9 des RNN est leur m\u00e9moire interne qui permet de conserver l'information des \u00e9tapes pr\u00e9c\u00e9dentes.</p>"},{"location":"ressources/base-connaissances/#types-de-rnn","title":"Types de RNN","text":"Type Caract\u00e9ristiques Avantages Applications RNN simple Structure de base avec boucle de r\u00e9troaction Simple \u00e0 impl\u00e9menter S\u00e9quences courtes LSTM (Long Short-Term Memory) Cellules sp\u00e9ciales avec \"portes\" pour contr\u00f4ler la m\u00e9moire Meilleure m\u00e9moire \u00e0 long terme Traduction, g\u00e9n\u00e9ration de texte GRU (Gated Recurrent Unit) Version simplifi\u00e9e du LSTM Plus l\u00e9ger que LSTM, performances similaires Applications avec contraintes de ressources Bidirectionnel Traite la s\u00e9quence dans les deux sens (avant et arri\u00e8re) Utilise le contexte futur et pass\u00e9 Compr\u00e9hension du langage"},{"location":"ressources/base-connaissances/#probleme-du-gradient-qui-sevanouitexplose","title":"Probl\u00e8me du gradient qui s'\u00e9vanouit/explose","text":"<p>Les RNN classiques souffrent du probl\u00e8me de la disparition du gradient, ce qui limite leur capacit\u00e9 \u00e0 apprendre des d\u00e9pendances \u00e0 long terme. Les architectures LSTM et GRU ont \u00e9t\u00e9 con\u00e7ues pour r\u00e9soudre ce probl\u00e8me.</p>"},{"location":"ressources/base-connaissances/#7-techniques-doptimisation-avancees","title":"7. Techniques d'optimisation avanc\u00e9es","text":""},{"location":"ressources/base-connaissances/#optimiseurs","title":"Optimiseurs","text":"Optimiseur Description Avantages Inconv\u00e9nients SGD (Stochastic Gradient Descent) Met \u00e0 jour les poids apr\u00e8s chaque exemple Simple Convergence lente, sensible au taux d'apprentissage Adam Adapte le taux d'apprentissage pour chaque param\u00e8tre Rapide, bonne convergence Peut surpasser les minima locaux RMSprop Normalise le gradient par moyenne mobile Bon pour les RNN Sensible aux hyperparam\u00e8tres Adagrad Adapte le taux d'apprentissage en fonction de l'historique Bon pour les donn\u00e9es \u00e9parses Accumulation excessive du d\u00e9nominateur"},{"location":"ressources/base-connaissances/#techniques-de-regularisation","title":"Techniques de r\u00e9gularisation","text":"Technique Description Effet Dropout D\u00e9sactive al\u00e9atoirement des neurones pendant l'entra\u00eenement Emp\u00eache les neurones de trop se sp\u00e9cialiser L1/L2 R\u00e9gularisation Ajoute une p\u00e9nalit\u00e9 bas\u00e9e sur la magnitude des poids Encourage les poids \u00e0 rester petits Batch Normalization Normalise les activations de chaque mini-batch Stabilise et acc\u00e9l\u00e8re l'apprentissage Early Stopping Arr\u00eate l'entra\u00eenement quand la performance sur la validation cesse de s'am\u00e9liorer \u00c9vite le surapprentissage"},{"location":"ressources/base-connaissances/#8-applications-pratiques-du-deep-learning","title":"8. Applications pratiques du Deep Learning","text":""},{"location":"ressources/base-connaissances/#par-domaine-dapplication","title":"Par domaine d'application","text":"Domaine Applications Architectures courantes Vision par ordinateur Reconnaissance d'objets, d\u00e9tection faciale, segmentation d'images CNN, R-CNN, YOLO Traitement du langage naturel Traduction automatique, analyse de sentiment, chatbots RNN, LSTM, Transformers Reconnaissance vocale Transcription automatique, assistants vocaux RNN, LSTM, Transformers S\u00e9ries temporelles Pr\u00e9vision financi\u00e8re, pr\u00e9vision m\u00e9t\u00e9orologique LSTM, GRU, TCN Syst\u00e8mes de recommandation Recommandations de produits, de contenu R\u00e9seaux de neurones profonds, factorisation matricielle G\u00e9n\u00e9ration de contenu G\u00e9n\u00e9ration d'images, de texte, de musique GANs, VAEs, Transformers"},{"location":"ressources/base-connaissances/#exemples-concrets-en-entreprise","title":"Exemples concrets en entreprise","text":"<ol> <li>E-commerce : Recommandation de produits bas\u00e9e sur l'historique d'achat</li> <li>Finance : D\u00e9tection de fraudes en temps r\u00e9el</li> <li>Sant\u00e9 : Aide au diagnostic via l'analyse d'images m\u00e9dicales</li> <li>Industrie : Maintenance pr\u00e9dictive des \u00e9quipements</li> <li>M\u00e9dia : Sous-titrage automatique et traduction de vid\u00e9os</li> </ol>"},{"location":"ressources/base-connaissances/#9-frameworks-et-outils-du-deep-learning","title":"9. Frameworks et outils du Deep Learning","text":""},{"location":"ressources/base-connaissances/#principaux-frameworks","title":"Principaux frameworks","text":"Framework D\u00e9veloppeur Points forts Utilisations typiques TensorFlow Google D\u00e9ploiement en production, TensorFlow Lite pour mobile Applications industrielles, d\u00e9ploiement \u00e0 grande \u00e9chelle Keras Initialement Fran\u00e7ois Chollet, maintenant int\u00e9gr\u00e9 \u00e0 TensorFlow API simple et intuitive Prototypage rapide, enseignement PyTorch Facebook (Meta) D\u00e9bogage facile, dynamique Recherche, prototypage, projets acad\u00e9miques JAX Google Calcul diff\u00e9rentiable haute performance Recherche avanc\u00e9e, mod\u00e8les tr\u00e8s larges"},{"location":"ressources/base-connaissances/#ecosysteme-doutils","title":"\u00c9cosyst\u00e8me d'outils","text":"<ul> <li>Google Colab : Environnement notebook avec GPU/TPU gratuits</li> <li>Jupyter Notebooks : D\u00e9veloppement interactif</li> <li>TensorBoard : Visualisation des m\u00e9triques d'entra\u00eenement</li> <li>MLflow : Gestion du cycle de vie des mod\u00e8les ML</li> <li>Hugging Face : Biblioth\u00e8que de mod\u00e8les pr\u00e9-entra\u00een\u00e9s pour NLP</li> <li>ONNX : Standard d'interop\u00e9rabilit\u00e9 entre frameworks</li> </ul>"},{"location":"ressources/base-connaissances/#10-conseils-pratiques-pour-limplementation","title":"10. Conseils pratiques pour l'impl\u00e9mentation","text":""},{"location":"ressources/base-connaissances/#bonnes-pratiques","title":"Bonnes pratiques","text":"<ol> <li>Commencer simple puis augmenter la complexit\u00e9</li> <li>Normaliser les donn\u00e9es d'entr\u00e9e (moyenne 0, \u00e9cart-type 1)</li> <li>Visualiser les donn\u00e9es avant de construire le mod\u00e8le</li> <li>Surveiller les m\u00e9triques sur un ensemble de validation</li> <li>Utiliser des techniques de r\u00e9gularisation pour \u00e9viter le surapprentissage</li> <li>Adopter un taux d'apprentissage adaptatif (learning rate schedules)</li> <li>Impl\u00e9menter l'early stopping pour \u00e9viter le surapprentissage</li> <li>Faire des sauvegardes r\u00e9guli\u00e8res du mod\u00e8le pendant l'entra\u00eenement</li> </ol>"},{"location":"ressources/base-connaissances/#erreurs-courantes-a-eviter","title":"Erreurs courantes \u00e0 \u00e9viter","text":"<ol> <li>\u274c Fuites de donn\u00e9es entre ensembles d'entra\u00eenement et de test</li> <li>\u274c Surapprentissage en utilisant trop de param\u00e8tres pour peu de donn\u00e9es</li> <li>\u274c Taux d'apprentissage inadapt\u00e9 (trop grand ou trop petit)</li> <li>\u274c Fonction de perte inappropri\u00e9e pour le probl\u00e8me</li> <li>\u274c Mauvaise initialisation des poids causant la saturation des neurones</li> <li>\u274c D\u00e9s\u00e9quilibre des classes non pris en compte dans les donn\u00e9es</li> </ol>"},{"location":"ressources/base-connaissances/#processus-de-developpement-recommande","title":"Processus de d\u00e9veloppement recommand\u00e9","text":"<ol> <li>Explorer et comprendre les donn\u00e9es</li> <li>\u00c9tablir une baseline (mod\u00e8le simple)</li> <li>It\u00e9rer en am\u00e9liorant progressivement</li> <li>Surveiller les performances et \u00e9viter le surapprentissage</li> <li>Optimiser les hyperparam\u00e8tres</li> <li>\u00c9valuer sur des donn\u00e9es de test ind\u00e9pendantes</li> </ol> <p>Ce guide de base de connaissances vous servira tout au long du cours pour comprendre et appliquer les concepts du Deep Learning dans vos projets pratiques.</p>"},{"location":"ressources/guide-etudiant/","title":"Guide d'utilisation des ressources - Formation Deep Learning","text":""},{"location":"ressources/guide-etudiant/#bienvenue-dans-votre-formation-deep-learning","title":"Bienvenue dans votre formation Deep Learning !","text":"<p>Ce guide vous explique comment acc\u00e9der et utiliser les diff\u00e9rentes ressources du cours.</p>"},{"location":"ressources/guide-etudiant/#1-acces-aux-notebooks-jupyter","title":"1. Acc\u00e8s aux notebooks Jupyter","text":""},{"location":"ressources/guide-etudiant/#quest-ce-quun-notebook-jupyter","title":"Qu'est-ce qu'un notebook Jupyter ?","text":"<p>Un notebook Jupyter est un document interactif qui vous permet d'ex\u00e9cuter du code Python directement dans votre navigateur, tout en incluant du texte explicatif, des images et des visualisations. C'est l'outil id\u00e9al pour apprendre le Deep Learning de fa\u00e7on pratique.</p>"},{"location":"ressources/guide-etudiant/#comment-acceder-aux-notebooks-du-cours","title":"Comment acc\u00e9der aux notebooks du cours","text":"<p>Nous utilisons Google Colab, qui vous permet d'ex\u00e9cuter des notebooks Jupyter dans le cloud, sans aucune installation sur votre ordinateur. Vous avez simplement besoin d'un compte Google.</p> <p>Pour acc\u00e9der \u00e0 chaque notebook : 1. Cliquez sur le badge \"Open in Colab\" ou le lien correspondant au notebook 2. Le notebook s'ouvrira dans Google Colab 3. Si demand\u00e9, connectez-vous avec votre compte Google</p>"},{"location":"ressources/guide-etudiant/#enregistrer-votre-travail","title":"Enregistrer votre travail","text":"<p>Important : Les notebooks s'ouvrent en mode lecture seule. Pour sauvegarder votre travail : 1. Allez dans le menu \"Fichier\" &gt; \"Enregistrer une copie dans Drive\" 2. Une copie personnelle sera cr\u00e9\u00e9e dans votre Google Drive 3. Travaillez d\u00e9sormais sur cette copie</p>"},{"location":"ressources/guide-etudiant/#liste-des-notebooks-disponibles","title":"Liste des notebooks disponibles","text":"Notebook Description Lien direct Hello World du Deep Learning Premier r\u00e9seau de neurones sur MNIST Machine Learning classique Classification avec Random Forest Deep Learning Classification avec r\u00e9seau de neurones Anatomie d'un r\u00e9seau Exploration interactive d'un r\u00e9seau Template du mini-projet Base pour le challenge d'am\u00e9lioration"},{"location":"ressources/guide-etudiant/#2-utilisation-des-notebooks","title":"2. Utilisation des notebooks","text":""},{"location":"ressources/guide-etudiant/#executer-les-cellules","title":"Ex\u00e9cuter les cellules","text":"<ul> <li>Pour ex\u00e9cuter une cellule de code, cliquez sur le bouton \u25b6\ufe0f \u00e0 gauche de la cellule ou appuyez sur <code>Ctrl+Entr\u00e9e</code></li> <li>Ex\u00e9cutez les cellules dans l'ordre, de haut en bas</li> <li>Attendez qu'une cellule ait termin\u00e9 son ex\u00e9cution avant de passer \u00e0 la suivante (le symbole \u25cf devient \u2713)</li> </ul>"},{"location":"ressources/guide-etudiant/#types-de-cellules","title":"Types de cellules","text":"<ul> <li>Cellules de code : Contiennent du code Python ex\u00e9cutable</li> <li>Cellules de texte : Contiennent des explications et des instructions</li> </ul>"},{"location":"ressources/guide-etudiant/#conseils-pratiques","title":"Conseils pratiques","text":"<ul> <li>Red\u00e9marrer le runtime : Si vous rencontrez des erreurs, essayez de red\u00e9marrer le runtime (Menu \"Runtime\" &gt; \"Restart runtime\")</li> <li>RAM limit\u00e9e : Si vous recevez des erreurs de m\u00e9moire, fermez les autres onglets Colab</li> <li>D\u00e9connexion : Google Colab peut se d\u00e9connecter apr\u00e8s inactivit\u00e9, sauvegardez r\u00e9guli\u00e8rement</li> </ul>"},{"location":"ressources/guide-etudiant/#3-documents-a-completer","title":"3. Documents \u00e0 compl\u00e9ter","text":""},{"location":"ressources/guide-etudiant/#telechargement-des-fiches","title":"T\u00e9l\u00e9chargement des fiches","text":"<p>Pour chaque s\u00e9ance, t\u00e9l\u00e9chargez les fiches d'observation et autres documents \u00e0 compl\u00e9ter : 1. Cliquez sur les liens fournis dans la page de la s\u00e9ance 2. Choisissez entre la version PDF (pour impression) ou Word/ODT (pour \u00e9dition \u00e9lectronique)</p>"},{"location":"ressources/guide-etudiant/#soumission-des-travaux","title":"Soumission des travaux","text":"<p>Pour soumettre vos travaux compl\u00e9t\u00e9s : 1. Nommez vos fichiers avec votre nom et le num\u00e9ro de s\u00e9ance (ex: \"Dupont_Seance1_Fiche.docx\") 2. D\u00e9posez-les sur la plateforme de cours en ligne ou envoyez-les par email selon les instructions de votre formateur 3. Pour les notebooks, partagez l'URL de votre copie dans Google Drive ou exportez-les au format .ipynb</p>"},{"location":"ressources/guide-etudiant/#4-resolution-des-problemes-courants","title":"4. R\u00e9solution des probl\u00e8mes courants","text":"Probl\u00e8me Solution Le notebook ne se charge pas V\u00e9rifiez votre connexion internet ou r\u00e9essayez dans quelques minutes Erreur \"CUDA out of memory\" Allez dans \"Runtime\" &gt; \"Change runtime type\" et s\u00e9lectionnez \"None\" pour GPU Modules manquants Ex\u00e9cutez <code>!pip install nom-du-module</code> dans une cellule Acc\u00e8s refus\u00e9 Assurez-vous d'\u00eatre connect\u00e9 \u00e0 votre compte Google"},{"location":"ressources/guide-etudiant/#5-ressources-complementaires","title":"5. Ressources compl\u00e9mentaires","text":"<ul> <li>Documentation TensorFlow</li> <li>Documentation Keras</li> <li>Tutoriels Google Colab</li> </ul>"},{"location":"ressources/instructions-integration/","title":"Guide d'int\u00e9gration FastAPI pour le chatbot p\u00e9dagogique","text":"<p>Ce guide explique comment int\u00e9grer FastAPI dans votre projet de chatbot p\u00e9dagogique sur le Deep Learning pour b\u00e9n\u00e9ficier de meilleures performances et de fonctionnalit\u00e9s plus avanc\u00e9es.</p>"},{"location":"ressources/instructions-integration/#pourquoi-passer-a-fastapi","title":"Pourquoi passer \u00e0 FastAPI ?","text":"<p>FastAPI offre plusieurs avantages pour notre projet :</p> <ol> <li>Performance : FastAPI est l'un des frameworks Python les plus rapides, bas\u00e9 sur Starlette et Pydantic</li> <li>Documentation automatique : G\u00e9n\u00e9ration automatique de documentation interactive (OpenAPI/Swagger)</li> <li>Validation des donn\u00e9es : Validation automatique des requ\u00eates et r\u00e9ponses avec Pydantic</li> <li>Support asynchrone natif : Support de l'asynchrone pour les op\u00e9rations \u00e0 latence \u00e9lev\u00e9e (comme les appels API externes)</li> <li>Typage fort : Utilisation du typage Python pour une meilleure d\u00e9tection d'erreurs</li> </ol>"},{"location":"ressources/instructions-integration/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Python 3.7 ou sup\u00e9rieur</li> <li>Acc\u00e8s \u00e0 un terminal pour installer les packages</li> <li>Compte et cl\u00e9 API Mistral AI</li> </ul>"},{"location":"ressources/instructions-integration/#installation-des-dependances","title":"Installation des d\u00e9pendances","text":"<pre><code>pip install fastapi uvicorn mistralai python-dotenv pydantic\n</code></pre> <p>Note: <code>uvicorn</code> est le serveur ASGI recommand\u00e9 pour ex\u00e9cuter FastAPI.</p>"},{"location":"ressources/instructions-integration/#structure-du-projet","title":"Structure du projet","text":"<p>Voici la structure de base recommand\u00e9e pour votre projet :</p> <pre><code>chatbot-pedagogique/\n\u251c\u2500\u2500 .env                    # Variables d'environnement (cl\u00e9s API)\n\u251c\u2500\u2500 main.py                 # Point d'entr\u00e9e principal avec FastAPI\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 conversation.py     # Mod\u00e8les Pydantic pour les requ\u00eates/r\u00e9ponses\n\u2502   \u2514\u2500\u2500 knowledge_base.py   # Mod\u00e8les pour la base de connaissances\n\u251c\u2500\u2500 services/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 mistral_service.py  # Interaction avec l'API Mistral\n\u2502   \u2514\u2500\u2500 knowledge_service.py # Gestion de la base de connaissances\n\u251c\u2500\u2500 static/                 # Fichiers statiques (CSS, JS, images)\n\u2514\u2500\u2500 templates/              # Templates HTML (si utilis\u00e9s)\n</code></pre>"},{"location":"ressources/instructions-integration/#configuration-initiale","title":"Configuration initiale","text":""},{"location":"ressources/instructions-integration/#1-fichier-env","title":"1. Fichier .env","text":"<p>Cr\u00e9ez un fichier <code>.env</code> \u00e0 la racine du projet avec votre cl\u00e9 API :</p> <pre><code>MISTRAL_API_KEY=votre_cl\u00e9_api_ici\n</code></pre>"},{"location":"ressources/instructions-integration/#2-modeles-pydantic-modelsconversationpy","title":"2. Mod\u00e8les Pydantic (models/conversation.py)","text":"<p>D\u00e9finissez les mod\u00e8les de donn\u00e9es pour les requ\u00eates et r\u00e9ponses :</p> <pre><code>from typing import List, Optional\nfrom pydantic import BaseModel\n\nclass Message(BaseModel):\n    role: str  # \"user\" ou \"assistant\"\n    content: str\n\nclass ConversationRequest(BaseModel):\n    messages: List[Message]\n    user_level: Optional[str] = \"beginner\"\n    temperature: Optional[float] = 0.7\n    model: Optional[str] = \"mistral-medium\"\n\nclass ConversationResponse(BaseModel):\n    response: str\n    conversation_id: str\n</code></pre>"},{"location":"ressources/instructions-integration/#3-service-mistral-servicesmistral_servicepy","title":"3. Service Mistral (services/mistral_service.py)","text":"<p>Cr\u00e9ez un service pour interagir avec l'API Mistral :</p> <pre><code>import os\nfrom typing import List, Dict, Any\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\nclass MistralService:\n    def __init__(self):\n        self.api_key = os.getenv(\"MISTRAL_API_KEY\")\n        self.client = MistralClient(api_key=self.api_key)\n\n    def generate_response(self, messages: List[Dict[str, str]], \n                         model: str = \"mistral-medium\", \n                         temperature: float = 0.7) -&gt; str:\n        \"\"\"\n        G\u00e9n\u00e8re une r\u00e9ponse via l'API Mistral.\n\n        Args:\n            messages: Liste de messages au format {\"role\": \"...\", \"content\": \"...\"}\n            model: Mod\u00e8le Mistral \u00e0 utiliser\n            temperature: Temp\u00e9rature (cr\u00e9ativit\u00e9) de la g\u00e9n\u00e9ration\n\n        Returns:\n            str: R\u00e9ponse g\u00e9n\u00e9r\u00e9e\n        \"\"\"\n        # Convertir les messages au format attendu par l'API Mistral\n        mistral_messages = [\n            ChatMessage(role=msg[\"role\"], content=msg[\"content\"])\n            for msg in messages\n        ]\n\n        try:\n            # Appel \u00e0 l'API Mistral\n            response = self.client.chat(\n                model=model,\n                messages=mistral_messages,\n                temperature=temperature\n            )\n\n            return response.choices[0].message.content\n        except Exception as e:\n            # Gestion des erreurs\n            print(f\"Erreur lors de l'appel \u00e0 l'API Mistral: {e}\")\n            return f\"D\u00e9sol\u00e9, une erreur s'est produite lors de la g\u00e9n\u00e9ration de la r\u00e9ponse: {str(e)}\"\n</code></pre>"},{"location":"ressources/instructions-integration/#4-application-principale-mainpy","title":"4. Application principale (main.py)","text":"<p>L'application FastAPI principale qui expose les endpoints :</p> <pre><code>import os\nimport uuid\nfrom fastapi import FastAPI, HTTPException, BackgroundTasks\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import JSONResponse\n\nfrom models.conversation import ConversationRequest, ConversationResponse\nfrom services.mistral_service import MistralService\n\n# Chargement des variables d'environnement\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Initialisation de l'application FastAPI\napp = FastAPI(\n    title=\"Chatbot p\u00e9dagogique API\",\n    description=\"API pour le chatbot p\u00e9dagogique sur le Deep Learning\",\n    version=\"1.0.0\"\n)\n\n# Configuration CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # En production, sp\u00e9cifiez les domaines autoris\u00e9s\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Initialisation des services\nmistral_service = MistralService()\n\n# Stockage des conversations (dans un vrai projet, utilisez une base de donn\u00e9es)\nconversations = {}\n\n# Routes\n@app.post(\"/api/chat\", response_model=ConversationResponse)\nasync def chat(request: ConversationRequest):\n    \"\"\"\n    Endpoint pour interagir avec le chatbot\n    \"\"\"\n    # G\u00e9n\u00e9rer un ID de conversation s'il n'existe pas\n    conversation_id = str(uuid.uuid4())\n\n    try:\n        # Obtenir la r\u00e9ponse de Mistral AI\n        system_message = {\n            \"role\": \"system\",\n            \"content\": f\"Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 en Deep Learning pour des \u00e9tudiants de niveau {request.user_level}.\"\n        }\n\n        # Pr\u00e9parer les messages avec le message syst\u00e8me en premier\n        all_messages = [system_message] + [msg.dict() for msg in request.messages]\n\n        # G\u00e9n\u00e9rer la r\u00e9ponse\n        response = mistral_service.generate_response(\n            messages=all_messages,\n            model=request.model,\n            temperature=request.temperature\n        )\n\n        # Sauvegarder la conversation\n        if conversation_id not in conversations:\n            conversations[conversation_id] = all_messages\n        conversations[conversation_id].append({\"role\": \"assistant\", \"content\": response})\n\n        return ConversationResponse(\n            response=response,\n            conversation_id=conversation_id\n        )\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/api/health\")\nasync def health_check():\n    \"\"\"\n    Endpoint de v\u00e9rification de la sant\u00e9 de l'API\n    \"\"\"\n    return {\"status\": \"healthy\"}\n\n# Servir les fichiers statiques (si n\u00e9cessaire)\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n# Point d'entr\u00e9e\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n</code></pre>"},{"location":"ressources/instructions-integration/#utilisation-de-lapi-fastapi","title":"Utilisation de l'API FastAPI","text":""},{"location":"ressources/instructions-integration/#lancement-de-lapplication","title":"Lancement de l'application","text":"<p>Pour d\u00e9marrer l'application :</p> <pre><code>uvicorn main:app --reload\n</code></pre> <p>L'application sera accessible \u00e0 l'adresse <code>http://localhost:8000</code>.</p>"},{"location":"ressources/instructions-integration/#documentation-automatique","title":"Documentation automatique","text":"<p>FastAPI g\u00e9n\u00e8re automatiquement une documentation interactive :</p> <ul> <li>Swagger UI: <code>http://localhost:8000/docs</code></li> <li>ReDoc: <code>http://localhost:8000/redoc</code></li> </ul>"},{"location":"ressources/instructions-integration/#appel-de-lapi-depuis-javascript","title":"Appel de l'API depuis JavaScript","text":"<p>Voici comment appeler votre API depuis le frontend :</p> <pre><code>async function sendMessage(message) {\n    const conversation = [\n        {\n            role: \"user\",\n            content: message\n        }\n    ];\n\n    try {\n        const response = await fetch('http://localhost:8000/api/chat', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({\n                messages: conversation,\n                user_level: \"beginner\", // ou \"intermediate\", \"advanced\"\n                temperature: 0.7\n            })\n        });\n\n        if (!response.ok) {\n            throw new Error('Erreur r\u00e9seau ou serveur');\n        }\n\n        const data = await response.json();\n        return data.response;\n    } catch (error) {\n        console.error('Erreur:', error);\n        return \"D\u00e9sol\u00e9, une erreur s'est produite lors de la communication avec le serveur.\";\n    }\n}\n</code></pre>"},{"location":"ressources/instructions-integration/#fonctionnalites-avancees","title":"Fonctionnalit\u00e9s avanc\u00e9es","text":""},{"location":"ressources/instructions-integration/#1-traitement-asynchrone","title":"1. Traitement asynchrone","text":"<p>FastAPI supporte nativement l'asynchrone, utile pour les op\u00e9rations \u00e0 latence \u00e9lev\u00e9e :</p> <pre><code>@app.post(\"/api/chat/async\")\nasync def chat_async(request: ConversationRequest, background_tasks: BackgroundTasks):\n    # G\u00e9n\u00e9rer un ID pour cette requ\u00eate\n    request_id = str(uuid.uuid4())\n\n    # Traiter la requ\u00eate en arri\u00e8re-plan\n    background_tasks.add_task(process_chat_request, request, request_id)\n\n    return {\"status\": \"processing\", \"request_id\": request_id}\n\nasync def process_chat_request(request: ConversationRequest, request_id: str):\n    # Traitement asynchrone de la requ\u00eate\n    # ...\n</code></pre>"},{"location":"ressources/instructions-integration/#2-dependances-et-injection","title":"2. D\u00e9pendances et injection","text":"<p>FastAPI offre un syst\u00e8me de d\u00e9pendances puissant :</p> <pre><code>async def get_mistral_service():\n    return MistralService()\n\n@app.post(\"/api/chat\")\nasync def chat(\n    request: ConversationRequest, \n    mistral_service: MistralService = Depends(get_mistral_service)\n):\n    # Utiliser le service inject\u00e9\n    # ...\n</code></pre>"},{"location":"ressources/instructions-integration/#3-rate-limiting","title":"3. Rate limiting","text":"<p>Impl\u00e9mentation simple de rate limiting :</p> <pre><code>from fastapi import Request, HTTPException\nimport time\n\n# Dictionnaire pour stocker les compteurs de requ\u00eates\nrequest_counts = {}\n\n@app.middleware(\"http\")\nasync def rate_limit_middleware(request: Request, call_next):\n    client_ip = request.client.host\n\n    # Initialiser ou r\u00e9cup\u00e9rer les donn\u00e9es de l'IP\n    if client_ip not in request_counts:\n        request_counts[client_ip] = {\"count\": 0, \"reset_time\": time.time() + 60}\n\n    # V\u00e9rifier si le compteur doit \u00eatre r\u00e9initialis\u00e9\n    if time.time() &gt; request_counts[client_ip][\"reset_time\"]:\n        request_counts[client_ip] = {\"count\": 0, \"reset_time\": time.time() + 60}\n\n    # V\u00e9rifier la limite\n    if request_counts[client_ip][\"count\"] &gt;= 10:  # Limite de 10 requ\u00eates par minute\n        raise HTTPException(status_code=429, detail=\"Trop de requ\u00eates\")\n\n    # Incr\u00e9menter le compteur\n    request_counts[client_ip][\"count\"] += 1\n\n    # Continuer avec la requ\u00eate\n    response = await call_next(request)\n    return response\n</code></pre>"},{"location":"ressources/instructions-integration/#bonnes-pratiques","title":"Bonnes pratiques","text":"<ol> <li>Utiliser des mod\u00e8les Pydantic pour valider les entr\u00e9es/sorties</li> <li>Organiser le code en modules r\u00e9utilisables (services, mod\u00e8les, etc.)</li> <li>Impl\u00e9menter la gestion d'erreurs pour toutes les op\u00e9rations critiques</li> <li>Documenter les endpoints avec des docstrings d\u00e9taill\u00e9es</li> <li>Utiliser des d\u00e9pendances pour l'injection et la r\u00e9utilisation du code</li> <li>Impl\u00e9menter des tests avec pytest et le client de test FastAPI</li> <li>Utiliser des variables d'environnement pour les configurations sensibles</li> <li>Mettre en cache les r\u00e9ponses fr\u00e9quentes pour optimiser les performances</li> </ol>"},{"location":"ressources/instructions-integration/#comparaison-avec-flask","title":"Comparaison avec Flask","text":"Fonctionnalit\u00e9 FastAPI Flask Performance Tr\u00e8s rapide (bas\u00e9 sur Starlette) Moins performant Documentation Automatique (OpenAPI/Swagger) Manuelle ou extensions tierces Validation Automatique avec Pydantic Manuelle ou Flask-WTF Asynchrone Support natif Pas de support natif Typage Typage fort Pas de typage par d\u00e9faut Extensions \u00c9cosyst\u00e8me en croissance \u00c9cosyst\u00e8me mature Courbe d'apprentissage Moyenne Faible"},{"location":"ressources/instructions-integration/#ressources-supplementaires","title":"Ressources suppl\u00e9mentaires","text":"<ul> <li>Documentation officielle FastAPI</li> <li>Pydantic</li> <li>Uvicorn ASGI Server</li> <li>Mistral AI API Docs</li> </ul>"},{"location":"ressources/instructions-integration/#troubleshooting","title":"Troubleshooting","text":"Probl\u00e8me Solution <code>ModuleNotFoundError</code> V\u00e9rifiez que vous avez install\u00e9 toutes les d\u00e9pendances Erreur CORS Assurez-vous que le middleware CORS est correctement configur\u00e9 Erreur 422 Unprocessable Entity V\u00e9rifiez la structure de votre requ\u00eate selon les mod\u00e8les Pydantic Erreur API Mistral V\u00e9rifiez votre cl\u00e9 API et la disponibilit\u00e9 du service <p>Ce guide vous a fourni une base solide pour int\u00e9grer FastAPI dans votre projet de chatbot p\u00e9dagogique. N'h\u00e9sitez pas \u00e0 explorer les fonctionnalit\u00e9s avanc\u00e9es de FastAPI pour am\u00e9liorer encore votre application.</p>"},{"location":"ressources/json-schemas/","title":"Sch\u00e9mas JSON pour le Chatbot P\u00e9dagogique","text":"<p>Ce document d\u00e9crit les structures JSON utilis\u00e9es pour organiser la base de connaissances du chatbot p\u00e9dagogique sur le Deep Learning. Ces sch\u00e9mas permettent de standardiser les donn\u00e9es et de faciliter leur utilisation par l'application.</p>"},{"location":"ressources/json-schemas/#base-de-connaissances-principale","title":"Base de connaissances principale","text":""},{"location":"ressources/json-schemas/#structure-globale","title":"Structure globale","text":"<p>La base de connaissances est organis\u00e9e en concepts principaux, chacun contenant des sous-concepts. Voici la structure g\u00e9n\u00e9rale:</p> <pre><code>{\n  \"concepts\": [\n    {\n      \"id\": \"string\",\n      \"title\": \"string\",\n      \"description\": \"string\",\n      \"subconcepts\": [\n        {\n          \"id\": \"string\",\n          \"title\": \"string\",\n          \"definition\": \"string\",\n          \"details\": {\n            \"beginner\": \"string\",\n            \"intermediate\": \"string\",\n            \"advanced\": \"string\"\n          },\n          \"examples\": [\"string\"],\n          \"related\": [\"string\"]\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"ressources/json-schemas/#description-des-champs","title":"Description des champs","text":"Champ Type Description Exemple <code>concepts</code> array Liste des concepts principaux <code>id</code> string Identifiant unique du concept (format: snake_case) \"neural_network\" <code>title</code> string Titre lisible du concept \"R\u00e9seau de neurones\" <code>description</code> string Description g\u00e9n\u00e9rale du concept \"Mod\u00e8le de calcul inspir\u00e9 du cerveau...\" <code>subconcepts</code> array Liste des sous-concepts <code>definition</code> string D\u00e9finition concise du sous-concept \"Un r\u00e9seau de neurones est...\" <code>details</code> object Explications adapt\u00e9es \u00e0 diff\u00e9rents niveaux <code>details.beginner</code> string Explication pour les d\u00e9butants \"Imaginez une s\u00e9rie de filtres...\" <code>details.intermediate</code> string Explication de niveau interm\u00e9diaire \"Techniquement, un CNN utilise...\" <code>details.advanced</code> string Explication avanc\u00e9e avec termes techniques \"L'op\u00e9ration de convolution peut...\" <code>examples</code> array Liste d'exemples concrets [\"ResNet\", \"LeNet-5\"] <code>related</code> array Liste d'IDs de concepts li\u00e9s [\"convolution\", \"pooling\"]"},{"location":"ressources/json-schemas/#exemple-de-concept","title":"Exemple de concept","text":"<p>Voici un exemple complet d'un concept dans la base de connaissances:</p> <pre><code>{\n  \"id\": \"cnn\",\n  \"title\": \"R\u00e9seau de neurones convolutif (CNN)\",\n  \"definition\": \"Type de r\u00e9seau sp\u00e9cialis\u00e9 dans le traitement des donn\u00e9es en grille comme les images, utilisant des op\u00e9rations de convolution pour d\u00e9tecter des motifs spatiaux.\",\n  \"details\": {\n    \"beginner\": \"Les CNN sont des r\u00e9seaux sp\u00e9cialis\u00e9s pour analyser les images. Ils utilisent des filtres qui 'glissent' sur l'image pour d\u00e9tecter des motifs comme les contours, les textures, puis des formes plus complexes.\",\n    \"intermediate\": \"Ces r\u00e9seaux exploitent trois id\u00e9es cl\u00e9s: les connexions locales (chaque neurone voit seulement une petite r\u00e9gion), le partage de param\u00e8tres (les m\u00eames filtres sont appliqu\u00e9s partout), et la mise en commun (pooling) pour r\u00e9duire la dimensionnalit\u00e9 tout en pr\u00e9servant les caract\u00e9ristiques importantes.\",\n    \"advanced\": \"L'op\u00e9ration de convolution peut \u00eatre vue comme un produit de tenseurs avec un noyau partag\u00e9, ce qui r\u00e9duit consid\u00e9rablement le nombre de param\u00e8tres par rapport \u00e0 un r\u00e9seau enti\u00e8rement connect\u00e9. Cette inductive bias de localit\u00e9 et d'invariance \u00e0 la translation est particuli\u00e8rement adapt\u00e9e aux donn\u00e9es visuelles.\"\n  },\n  \"examples\": [\n    \"LeNet-5 (1998): Premier CNN efficace, utilis\u00e9 pour la reconnaissance de chiffres manuscrits\",\n    \"ResNet (2015): Architecture introduisant les connexions r\u00e9siduelles pour entra\u00eener des r\u00e9seaux tr\u00e8s profonds\",\n    \"EfficientNet (2019): Famille de CNN optimis\u00e9s pour le rapport performance/nombre de param\u00e8tres\"\n  ],\n  \"related\": [\"convolution\", \"pooling\", \"image_recognition\", \"feature_map\"]\n}\n</code></pre>"},{"location":"ressources/json-schemas/#schema-pour-les-questions-et-reponses","title":"Sch\u00e9ma pour les questions et r\u00e9ponses","text":"<p>Pour le syst\u00e8me de quiz et d'exercices, un sch\u00e9ma diff\u00e9rent est utilis\u00e9:</p> <pre><code>{\n  \"quizzes\": [\n    {\n      \"id\": \"string\",\n      \"topic\": \"string\",\n      \"difficulty\": \"beginner|intermediate|advanced\",\n      \"questions\": [\n        {\n          \"id\": \"string\",\n          \"text\": \"string\",\n          \"type\": \"mcq|true_false|short_answer\",\n          \"options\": [\"string\"],\n          \"correct_answer\": \"string|number|array\",\n          \"explanation\": \"string\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"ressources/json-schemas/#description-des-champs-du-quiz","title":"Description des champs du quiz","text":"Champ Type Description Exemple <code>quizzes</code> array Liste des quiz disponibles <code>id</code> string Identifiant unique du quiz \"cnn_basics\" <code>topic</code> string Sujet principal du quiz \"R\u00e9seaux convolutifs\" <code>difficulty</code> enum Niveau de difficult\u00e9 \"intermediate\" <code>questions</code> array Liste des questions <code>text</code> string \u00c9nonc\u00e9 de la question \"Quelle est la principale caract\u00e9ristique...\" <code>type</code> enum Type de question \"mcq\" (choix multiple) <code>options</code> array Options pour les QCM [\"Pooling\", \"Convolution\", \"ReLU\"] <code>correct_answer</code> mixed R\u00e9ponse(s) correcte(s) 1 ou [0, 2] <code>explanation</code> string Explication de la r\u00e9ponse \"La convolution est...\""},{"location":"ressources/json-schemas/#schema-pour-lhistorique-des-conversations","title":"Sch\u00e9ma pour l'historique des conversations","text":"<p>Pour g\u00e9rer l'historique des conversations, le chatbot utilise le format suivant:</p> <pre><code>{\n  \"conversations\": [\n    {\n      \"id\": \"string\",\n      \"user_id\": \"string\",\n      \"timestamp_start\": \"string (ISO date)\",\n      \"timestamp_last_activity\": \"string (ISO date)\",\n      \"messages\": [\n        {\n          \"role\": \"system|user|assistant\",\n          \"content\": \"string\",\n          \"timestamp\": \"string (ISO date)\"\n        }\n      ],\n      \"context\": {\n        \"user_level\": \"beginner|intermediate|advanced\",\n        \"topics_covered\": [\"string\"],\n        \"last_quiz_score\": number,\n        \"session_metrics\": {\n          \"questions_asked\": number,\n          \"topics_explored\": number,\n          \"quizzes_completed\": number\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"ressources/json-schemas/#description-des-champs-de-conversation","title":"Description des champs de conversation","text":"Champ Type Description Exemple <code>conversations</code> array Liste des conversations <code>id</code> string Identifiant unique de la conversation \"conv_123456\" <code>user_id</code> string Identifiant de l'utilisateur \"user_789\" <code>timestamp_start</code> string Date de d\u00e9but de conversation \"2023-06-15T14:23:45Z\" <code>messages</code> array Liste des messages \u00e9chang\u00e9s <code>role</code> enum R\u00f4le de l'exp\u00e9diteur du message \"user\" <code>content</code> string Contenu du message \"Qu'est-ce qu'un CNN?\" <code>context</code> object Informations contextuelles <code>user_level</code> enum Niveau estim\u00e9 de l'utilisateur \"beginner\" <code>topics_covered</code> array Sujets abord\u00e9s dans la conversation [\"cnn\", \"pooling\"] <code>session_metrics</code> object M\u00e9triques de la session"},{"location":"ressources/json-schemas/#utilisation-des-schemas-dans-lapplication","title":"Utilisation des sch\u00e9mas dans l'application","text":""},{"location":"ressources/json-schemas/#chargement-de-la-base-de-connaissances","title":"Chargement de la base de connaissances","text":"<pre><code>import json\n\ndef load_knowledge_base(file_path=\"knowledge_base.json\"):\n    \"\"\"\n    Charge la base de connaissances depuis un fichier JSON.\n\n    Args:\n        file_path (str): Chemin vers le fichier JSON\n\n    Returns:\n        dict: Base de connaissances\n    \"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            knowledge_base = json.load(f)\n        return knowledge_base\n    except Exception as e:\n        print(f\"Erreur lors du chargement de la base de connaissances: {e}\")\n        return {\"concepts\": []}\n</code></pre>"},{"location":"ressources/json-schemas/#recherche-dans-la-base-de-connaissances","title":"Recherche dans la base de connaissances","text":"<pre><code>def find_concept(knowledge_base, concept_id):\n    \"\"\"\n    Trouve un concept ou sous-concept par son ID.\n\n    Args:\n        knowledge_base (dict): Base de connaissances\n        concept_id (str): ID du concept \u00e0 trouver\n\n    Returns:\n        dict: Concept trouv\u00e9 ou None\n    \"\"\"\n    # Recherche dans les concepts principaux\n    for concept in knowledge_base.get(\"concepts\", []):\n        if concept[\"id\"] == concept_id:\n            return concept\n\n        # Recherche dans les sous-concepts\n        for subconcept in concept.get(\"subconcepts\", []):\n            if subconcept[\"id\"] == concept_id:\n                return subconcept\n\n    return None\n</code></pre>"},{"location":"ressources/json-schemas/#enrichissement-du-prompt-avec-la-base-de-connaissances","title":"Enrichissement du prompt avec la base de connaissances","text":"<pre><code>def enrich_prompt_with_knowledge(user_message, knowledge_base, user_level=\"beginner\"):\n    \"\"\"\n    Enrichit le prompt utilisateur avec des informations pertinentes\n    de la base de connaissances.\n\n    Args:\n        user_message (str): Message de l'utilisateur\n        knowledge_base (dict): Base de connaissances\n        user_level (str): Niveau de l'utilisateur\n\n    Returns:\n        str: Prompt enrichi\n    \"\"\"\n    # Rechercher des mots-cl\u00e9s dans le message\n    relevant_concepts = []\n\n    for concept in knowledge_base.get(\"concepts\", []):\n        # V\u00e9rifier si le concept principal est mentionn\u00e9\n        if concept[\"title\"].lower() in user_message.lower():\n            relevant_concepts.append(concept)\n\n        # V\u00e9rifier les sous-concepts\n        for subconcept in concept.get(\"subconcepts\", []):\n            if subconcept[\"title\"].lower() in user_message.lower():\n                relevant_concepts.append(subconcept)\n\n    # Construire le prompt enrichi\n    if not relevant_concepts:\n        return user_message\n\n    enriched_prompt = f\"Question de l'utilisateur: {user_message}\\n\\n\"\n    enriched_prompt += \"Informations pertinentes de la base de connaissances:\\n\\n\"\n\n    for concept in relevant_concepts[:2]:  # Limiter \u00e0 2 concepts pour \u00e9viter un prompt trop long\n        enriched_prompt += f\"Concept: {concept['title']}\\n\"\n\n        if \"definition\" in concept:\n            enriched_prompt += f\"D\u00e9finition: {concept['definition']}\\n\"\n\n        if \"details\" in concept and user_level in concept[\"details\"]:\n            enriched_prompt += f\"Explication ({user_level}): {concept['details'][user_level]}\\n\"\n\n        if \"examples\" in concept and concept[\"examples\"]:\n            examples = concept[\"examples\"][:2]  # Limiter \u00e0 2 exemples\n            enriched_prompt += f\"Exemples: {', '.join(examples)}\\n\"\n\n        enriched_prompt += \"\\n\"\n\n    enriched_prompt += f\"R\u00e9ponds \u00e0 la question de l'utilisateur de mani\u00e8re conversationnelle en utilisant ces informations, adapt\u00e9 au niveau {user_level}.\"\n\n    return enriched_prompt\n</code></pre>"},{"location":"ressources/json-schemas/#validation-des-donnees","title":"Validation des donn\u00e9es","text":"<p>Pour assurer l'int\u00e9grit\u00e9 des donn\u00e9es, un script de validation peut \u00eatre utilis\u00e9:</p> <pre><code>import jsonschema\n\n# D\u00e9finition du sch\u00e9ma pour validation\nknowledge_base_schema = {\n    \"type\": \"object\",\n    \"required\": [\"concepts\"],\n    \"properties\": {\n        \"concepts\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"required\": [\"id\", \"title\", \"subconcepts\"],\n                \"properties\": {\n                    \"id\": {\"type\": \"string\"},\n                    \"title\": {\"type\": \"string\"},\n                    \"description\": {\"type\": \"string\"},\n                    \"subconcepts\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"required\": [\"id\", \"title\", \"definition\"],\n                            \"properties\": {\n                                \"id\": {\"type\": \"string\"},\n                                \"title\": {\"type\": \"string\"},\n                                \"definition\": {\"type\": \"string\"},\n                                \"details\": {\n                                    \"type\": \"object\",\n                                    \"properties\": {\n                                        \"beginner\": {\"type\": \"string\"},\n                                        \"intermediate\": {\"type\": \"string\"},\n                                        \"advanced\": {\"type\": \"string\"}\n                                    }\n                                },\n                                \"examples\": {\n                                    \"type\": \"array\",\n                                    \"items\": {\"type\": \"string\"}\n                                },\n                                \"related\": {\n                                    \"type\": \"array\",\n                                    \"items\": {\"type\": \"string\"}\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\ndef validate_knowledge_base(knowledge_base, schema=knowledge_base_schema):\n    \"\"\"\n    Valide la structure de la base de connaissances.\n\n    Args:\n        knowledge_base (dict): Base de connaissances \u00e0 valider\n        schema (dict): Sch\u00e9ma JSON de validation\n\n    Returns:\n        bool: True si valide, False sinon\n    \"\"\"\n    try:\n        jsonschema.validate(instance=knowledge_base, schema=schema)\n        return True\n    except jsonschema.exceptions.ValidationError as e:\n        print(f\"Erreur de validation: {e}\")\n        return False\n</code></pre>"},{"location":"ressources/json-schemas/#bonnes-pratiques-pour-lextension-de-la-base-de-connaissances","title":"Bonnes pratiques pour l'extension de la base de connaissances","text":"<ol> <li>Maintenir la coh\u00e9rence des IDs en utilisant le format snake_case</li> <li>\u00c9viter les duplications de concepts</li> <li>Cr\u00e9er des liens bidirectionnels entre concepts li\u00e9s</li> <li>Adapter les explications aux diff\u00e9rents niveaux</li> <li>Inclure des exemples concrets pour chaque concept</li> <li>Valider le fichier JSON apr\u00e8s chaque modification</li> <li>Versionner la base de connaissances pour suivre l'\u00e9volution</li> <li>Structurer hi\u00e9rarchiquement les concepts pour une navigation logique</li> <li>Limiter la profondeur de la hi\u00e9rarchie pour faciliter la navigation</li> <li>Documenter les changements dans un journal des modifications</li> </ol> <p>Ces sch\u00e9mas JSON constituent la structure fondamentale de la base de connaissances du chatbot p\u00e9dagogique, assurant une organisation coh\u00e9rente des informations et facilitant leur utilisation par l'application.</p>"},{"location":"ressources/code/api-integration/","title":"Api integration","text":"<p>Int\u00e9gration de l'API Mistral - Premier test BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux et applications</p> In\u00a0[\u00a0]: Copied! <pre>import requests\nimport json\nimport os\nfrom dotenv import load_dotenv\nfrom flask import Flask, request, jsonify, render_template\n</pre> import requests import json import os from dotenv import load_dotenv from flask import Flask, request, jsonify, render_template In\u00a0[\u00a0]: Copied! <pre># Charger les variables d'environnement\nload_dotenv()\n</pre> # Charger les variables d'environnement load_dotenv() In\u00a0[\u00a0]: Copied! <pre># Configuration de l'API Mistral\nMISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"votre_cl\u00e9_api\")  # \u00c0 remplacer par votre cl\u00e9 API\nMISTRAL_API_URL = \"https://api.mistral.ai/v1/chat/completions\"\n</pre> # Configuration de l'API Mistral MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"votre_cl\u00e9_api\")  # \u00c0 remplacer par votre cl\u00e9 API MISTRAL_API_URL = \"https://api.mistral.ai/v1/chat/completions\" In\u00a0[\u00a0]: Copied! <pre># 1. Fonction simple pour appeler l'API Mistral\ndef mistral_chat_completion(prompt, model=\"mistral-tiny\", max_tokens=1000):\n    \"\"\"\n    Appelle l'API Mistral pour g\u00e9n\u00e9rer une r\u00e9ponse \u00e0 partir d'un prompt.\n    \n    Args:\n        prompt (str): Le message \u00e0 envoyer \u00e0 l'API\n        model (str): Le mod\u00e8le \u00e0 utiliser (mistral-tiny, mistral-small, mistral-medium, etc.)\n        max_tokens (int): Nombre maximum de tokens pour la r\u00e9ponse\n        \n    Returns:\n        dict: La r\u00e9ponse de l'API\n    \"\"\"\n    headers = {\n        \"Authorization\": f\"Bearer {MISTRAL_API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n    \n    data = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        \"max_tokens\": max_tokens\n    }\n    \n    try:\n        response = requests.post(MISTRAL_API_URL, headers=headers, data=json.dumps(data))\n        response.raise_for_status()  # Lever une exception si la requ\u00eate \u00e9choue\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"Erreur lors de l'appel \u00e0 l'API Mistral: {e}\")\n        return {\"error\": str(e)}\n</pre> # 1. Fonction simple pour appeler l'API Mistral def mistral_chat_completion(prompt, model=\"mistral-tiny\", max_tokens=1000):     \"\"\"     Appelle l'API Mistral pour g\u00e9n\u00e9rer une r\u00e9ponse \u00e0 partir d'un prompt.          Args:         prompt (str): Le message \u00e0 envoyer \u00e0 l'API         model (str): Le mod\u00e8le \u00e0 utiliser (mistral-tiny, mistral-small, mistral-medium, etc.)         max_tokens (int): Nombre maximum de tokens pour la r\u00e9ponse              Returns:         dict: La r\u00e9ponse de l'API     \"\"\"     headers = {         \"Authorization\": f\"Bearer {MISTRAL_API_KEY}\",         \"Content-Type\": \"application/json\"     }          data = {         \"model\": model,         \"messages\": [             {\"role\": \"user\", \"content\": prompt}         ],         \"max_tokens\": max_tokens     }          try:         response = requests.post(MISTRAL_API_URL, headers=headers, data=json.dumps(data))         response.raise_for_status()  # Lever une exception si la requ\u00eate \u00e9choue         return response.json()     except requests.exceptions.RequestException as e:         print(f\"Erreur lors de l'appel \u00e0 l'API Mistral: {e}\")         return {\"error\": str(e)} In\u00a0[\u00a0]: Copied! <pre># 2. Test simple de l'API\ndef test_mistral_api():\n    \"\"\"\n    Teste l'API Mistral avec un prompt simple.\n    \"\"\"\n    prompt = \"Explique-moi ce qu'est le Deep Learning en 3 phrases simples.\"\n    \n    print(f\"Envoi du prompt \u00e0 Mistral: '{prompt}'\")\n    response = mistral_chat_completion(prompt)\n    \n    if \"error\" in response:\n        print(f\"Erreur: {response['error']}\")\n        return\n    \n    # Extraire et afficher la r\u00e9ponse\n    try:\n        message_content = response[\"choices\"][0][\"message\"][\"content\"]\n        print(\"\\nR\u00e9ponse de Mistral:\")\n        print(message_content)\n        \n        # Informations suppl\u00e9mentaires sur la r\u00e9ponse\n        if \"usage\" in response:\n            usage = response[\"usage\"]\n            print(\"\\nUtilisation de tokens:\")\n            print(f\"- Prompt: {usage.get('prompt_tokens', 'N/A')} tokens\")\n            print(f\"- R\u00e9ponse: {usage.get('completion_tokens', 'N/A')} tokens\")\n            print(f\"- Total: {usage.get('total_tokens', 'N/A')} tokens\")\n    except (KeyError, IndexError) as e:\n        print(f\"Erreur lors du traitement de la r\u00e9ponse: {e}\")\n        print(\"R\u00e9ponse brute:\", response)\n</pre> # 2. Test simple de l'API def test_mistral_api():     \"\"\"     Teste l'API Mistral avec un prompt simple.     \"\"\"     prompt = \"Explique-moi ce qu'est le Deep Learning en 3 phrases simples.\"          print(f\"Envoi du prompt \u00e0 Mistral: '{prompt}'\")     response = mistral_chat_completion(prompt)          if \"error\" in response:         print(f\"Erreur: {response['error']}\")         return          # Extraire et afficher la r\u00e9ponse     try:         message_content = response[\"choices\"][0][\"message\"][\"content\"]         print(\"\\nR\u00e9ponse de Mistral:\")         print(message_content)                  # Informations suppl\u00e9mentaires sur la r\u00e9ponse         if \"usage\" in response:             usage = response[\"usage\"]             print(\"\\nUtilisation de tokens:\")             print(f\"- Prompt: {usage.get('prompt_tokens', 'N/A')} tokens\")             print(f\"- R\u00e9ponse: {usage.get('completion_tokens', 'N/A')} tokens\")             print(f\"- Total: {usage.get('total_tokens', 'N/A')} tokens\")     except (KeyError, IndexError) as e:         print(f\"Erreur lors du traitement de la r\u00e9ponse: {e}\")         print(\"R\u00e9ponse brute:\", response) In\u00a0[\u00a0]: Copied! <pre># 3. Fonction avanc\u00e9e pour l'explication de concepts de Deep Learning\ndef explain_deep_learning_concept(concept, difficulty=\"d\u00e9butant\"):\n    \"\"\"\n    Demande \u00e0 l'API Mistral d'expliquer un concept de Deep Learning.\n    \n    Args:\n        concept (str): Le concept \u00e0 expliquer\n        difficulty (str): Le niveau de difficult\u00e9 (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)\n        \n    Returns:\n        str: L'explication g\u00e9n\u00e9r\u00e9e\n    \"\"\"\n    # Construire un prompt \u00e9ducatif structur\u00e9\n    prompt = f\"\"\"\n    En tant que tuteur p\u00e9dagogique sp\u00e9cialis\u00e9 en Deep Learning, explique le concept de '{concept}' \n    \u00e0 un \u00e9tudiant de BTS SIO  (niveau {difficulty}).\n    \n    Ton explication doit inclure:\n    1. Une d\u00e9finition simple et claire\n    2. Un exemple concret d'application\n    3. Comment ce concept est utilis\u00e9 dans le d\u00e9veloppement d'applications\n    \n    Utilise un langage accessible mais techniquement pr\u00e9cis.\n    \"\"\"\n    \n    response = mistral_chat_completion(prompt, model=\"mistral-small\")\n    \n    if \"error\" in response:\n        return f\"Erreur: {response['error']}\"\n    \n    try:\n        return response[\"choices\"][0][\"message\"][\"content\"]\n    except (KeyError, IndexError):\n        return \"Erreur lors de la r\u00e9cup\u00e9ration de la r\u00e9ponse.\"\n</pre> # 3. Fonction avanc\u00e9e pour l'explication de concepts de Deep Learning def explain_deep_learning_concept(concept, difficulty=\"d\u00e9butant\"):     \"\"\"     Demande \u00e0 l'API Mistral d'expliquer un concept de Deep Learning.          Args:         concept (str): Le concept \u00e0 expliquer         difficulty (str): Le niveau de difficult\u00e9 (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)              Returns:         str: L'explication g\u00e9n\u00e9r\u00e9e     \"\"\"     # Construire un prompt \u00e9ducatif structur\u00e9     prompt = f\"\"\"     En tant que tuteur p\u00e9dagogique sp\u00e9cialis\u00e9 en Deep Learning, explique le concept de '{concept}'      \u00e0 un \u00e9tudiant de BTS SIO  (niveau {difficulty}).          Ton explication doit inclure:     1. Une d\u00e9finition simple et claire     2. Un exemple concret d'application     3. Comment ce concept est utilis\u00e9 dans le d\u00e9veloppement d'applications          Utilise un langage accessible mais techniquement pr\u00e9cis.     \"\"\"          response = mistral_chat_completion(prompt, model=\"mistral-small\")          if \"error\" in response:         return f\"Erreur: {response['error']}\"          try:         return response[\"choices\"][0][\"message\"][\"content\"]     except (KeyError, IndexError):         return \"Erreur lors de la r\u00e9cup\u00e9ration de la r\u00e9ponse.\" In\u00a0[\u00a0]: Copied! <pre># 4. Cr\u00e9ation d'une petite application Flask pour interagir avec l'API\napp = Flask(__name__)\n</pre> # 4. Cr\u00e9ation d'une petite application Flask pour interagir avec l'API app = Flask(__name__) In\u00a0[\u00a0]: Copied! <pre>@app.route('/')\ndef home():\n    return render_template('index.html')\n</pre> @app.route('/') def home():     return render_template('index.html') In\u00a0[\u00a0]: Copied! <pre>@app.route('/api/explain', methods=['POST'])\ndef api_explain():\n    data = request.json\n    concept = data.get('concept', '')\n    difficulty = data.get('difficulty', 'd\u00e9butant')\n    \n    if not concept:\n        return jsonify({\"error\": \"Concept manquant\"}), 400\n    \n    explanation = explain_deep_learning_concept(concept, difficulty)\n    return jsonify({\"explanation\": explanation})\n</pre> @app.route('/api/explain', methods=['POST']) def api_explain():     data = request.json     concept = data.get('concept', '')     difficulty = data.get('difficulty', 'd\u00e9butant')          if not concept:         return jsonify({\"error\": \"Concept manquant\"}), 400          explanation = explain_deep_learning_concept(concept, difficulty)     return jsonify({\"explanation\": explanation}) In\u00a0[\u00a0]: Copied! <pre># 5. Template HTML simple pour l'interface\ndef create_template_directory():\n    \"\"\"Cr\u00e9e un r\u00e9pertoire templates et un fichier index.html\"\"\"\n    os.makedirs('templates', exist_ok=True)\n    \n    with open('templates/index.html', 'w') as f:\n        f.write(\"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Explorateur de concepts Deep Learning&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n            line-height: 1.6;\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #f5f8fa;\n            color: #333;\n        }\n        h1 {\n            color: #2c3e50;\n            border-bottom: 2px solid #3498db;\n            padding-bottom: 10px;\n        }\n        .container {\n            background-color: white;\n            border-radius: 8px;\n            padding: 20px;\n            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n        }\n        label {\n            display: block;\n            margin-top: 15px;\n            font-weight: bold;\n            color: #2c3e50;\n        }\n        input, select, button {\n            width: 100%;\n            padding: 10px;\n            margin-top: 5px;\n            border: 1px solid #ddd;\n            border-radius: 4px;\n            box-sizing: border-box;\n        }\n        button {\n            background-color: #3498db;\n            color: white;\n            border: none;\n            padding: 12px;\n            margin-top: 20px;\n            cursor: pointer;\n            font-weight: bold;\n            transition: background-color 0.3s;\n        }\n        button:hover {\n            background-color: #2980b9;\n        }\n        #result {\n            margin-top: 20px;\n            padding: 20px;\n            background-color: #f8f9fa;\n            border-left: 4px solid #3498db;\n            border-radius: 4px;\n            white-space: pre-wrap;\n        }\n        .loading {\n            text-align: center;\n            margin-top: 20px;\n            display: none;\n        }\n        .hint {\n            font-size: 0.8em;\n            color: #7f8c8d;\n            margin-top: 5px;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;h1&gt;Explorateur de concepts Deep Learning&lt;/h1&gt;\n        &lt;p&gt;Utilisez cet outil pour explorer et comprendre les concepts cl\u00e9s du Deep Learning, expliqu\u00e9s par l'IA.&lt;/p&gt;\n        \n        &lt;form id=\"explainForm\"&gt;\n            &lt;label for=\"concept\"&gt;Concept \u00e0 expliquer:&lt;/label&gt;\n            &lt;input type=\"text\" id=\"concept\" required placeholder=\"Ex: r\u00e9seaux de neurones convolutifs, LSTM, dropout...\"&gt;\n            &lt;div class=\"hint\"&gt;Essayez des concepts comme: convolution, pooling, fonction d'activation, r\u00e9tropropagation...&lt;/div&gt;\n            \n            &lt;label for=\"difficulty\"&gt;Niveau de difficult\u00e9:&lt;/label&gt;\n            &lt;select id=\"difficulty\"&gt;\n                &lt;option value=\"d\u00e9butant\"&gt;D\u00e9butant&lt;/option&gt;\n                &lt;option value=\"interm\u00e9diaire\"&gt;Interm\u00e9diaire&lt;/option&gt;\n                &lt;option value=\"avanc\u00e9\"&gt;Avanc\u00e9&lt;/option&gt;\n            &lt;/select&gt;\n            \n            &lt;button type=\"submit\"&gt;Expliquer&lt;/button&gt;\n        &lt;/form&gt;\n        \n        &lt;div class=\"loading\" id=\"loading\"&gt;\n            &lt;p&gt;Chargement de l'explication...&lt;/p&gt;\n        &lt;/div&gt;\n        \n        &lt;div id=\"result\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n    \n    &lt;script&gt;\n        document.getElementById('explainForm').addEventListener('submit', async function(e) {\n            e.preventDefault();\n            \n            const concept = document.getElementById('concept').value.trim();\n            const difficulty = document.getElementById('difficulty').value;\n            const resultDiv = document.getElementById('result');\n            const loadingDiv = document.getElementById('loading');\n            \n            if (!concept) {\n                resultDiv.innerHTML = \"Veuillez entrer un concept \u00e0 expliquer.\";\n                return;\n            }\n            \n            // Afficher l'indicateur de chargement\n            loadingDiv.style.display = 'block';\n            resultDiv.innerHTML = \"\";\n            \n            try {\n                const response = await fetch('/api/explain', {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json'\n                    },\n                    body: JSON.stringify({ concept, difficulty })\n                });\n                \n                const data = await response.json();\n                \n                if (data.error) {\n                    resultDiv.innerHTML = `&lt;p style=\"color: red\"&gt;Erreur: ${data.error}&lt;/p&gt;`;\n                } else {\n                    resultDiv.innerHTML = data.explanation.replace(/\\\\n/g, '&lt;br&gt;');\n                }\n            } catch (error) {\n                resultDiv.innerHTML = `&lt;p style=\"color: red\"&gt;Erreur: ${error.message}&lt;/p&gt;`;\n            } finally {\n                // Cacher l'indicateur de chargement\n                loadingDiv.style.display = 'none';\n            }\n        });\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n        \"\"\")\n</pre> # 5. Template HTML simple pour l'interface def create_template_directory():     \"\"\"Cr\u00e9e un r\u00e9pertoire templates et un fichier index.html\"\"\"     os.makedirs('templates', exist_ok=True)          with open('templates/index.html', 'w') as f:         f.write(\"\"\"  Explorateur de concepts Deep Learning Explorateur de concepts Deep Learning <p>Utilisez cet outil pour explorer et comprendre les concepts cl\u00e9s du Deep Learning, expliqu\u00e9s par l'IA.</p> Concept \u00e0 expliquer: Essayez des concepts comme: convolution, pooling, fonction d'activation, r\u00e9tropropagation... Niveau de difficult\u00e9: D\u00e9butant Interm\u00e9diaire Avanc\u00e9 Expliquer <p>Chargement de l'explication...</p>          \"\"\") In\u00a0[\u00a0]: Copied! <pre># 6. Fonction principale pour ex\u00e9cuter l'application\ndef main():\n    \"\"\"Fonction principale\"\"\"\n    print(\"=== EXPLORATION DE L'API MISTRAL POUR LE CHATBOT P\u00c9DAGOGIQUE ===\")\n    \n    # Tester si la cl\u00e9 API est configur\u00e9e\n    if MISTRAL_API_KEY == \"votre_cl\u00e9_api\":\n        print(\"\\nERREUR: Vous devez configurer votre cl\u00e9 API Mistral!\")\n        print(\"1. Cr\u00e9ez un fichier .env dans le m\u00eame r\u00e9pertoire que ce script\")\n        print(\"2. Ajoutez la ligne: MISTRAL_API_KEY=votre_cl\u00e9_api_r\u00e9elle\")\n        print(\"3. Relancez le script\")\n        return\n    \n    # Test simple de l'API\n    print(\"\\n1. Test simple de l'API Mistral\")\n    test_mistral_api()\n    \n    # Cr\u00e9ation du r\u00e9pertoire et du fichier template\n    print(\"\\n2. Cr\u00e9ation du template pour l'application web\")\n    create_template_directory()\n    print(\"   Template cr\u00e9\u00e9 dans le r\u00e9pertoire 'templates/'\")\n    \n    # Lancement de l'application Flask\n    print(\"\\n3. D\u00e9marrage de l'application web\")\n    print(\"   URL: http://localhost:5000\")\n    print(\"   Appuyez sur Ctrl+C pour quitter\")\n    app.run(debug=True)\n</pre> # 6. Fonction principale pour ex\u00e9cuter l'application def main():     \"\"\"Fonction principale\"\"\"     print(\"=== EXPLORATION DE L'API MISTRAL POUR LE CHATBOT P\u00c9DAGOGIQUE ===\")          # Tester si la cl\u00e9 API est configur\u00e9e     if MISTRAL_API_KEY == \"votre_cl\u00e9_api\":         print(\"\\nERREUR: Vous devez configurer votre cl\u00e9 API Mistral!\")         print(\"1. Cr\u00e9ez un fichier .env dans le m\u00eame r\u00e9pertoire que ce script\")         print(\"2. Ajoutez la ligne: MISTRAL_API_KEY=votre_cl\u00e9_api_r\u00e9elle\")         print(\"3. Relancez le script\")         return          # Test simple de l'API     print(\"\\n1. Test simple de l'API Mistral\")     test_mistral_api()          # Cr\u00e9ation du r\u00e9pertoire et du fichier template     print(\"\\n2. Cr\u00e9ation du template pour l'application web\")     create_template_directory()     print(\"   Template cr\u00e9\u00e9 dans le r\u00e9pertoire 'templates/'\")          # Lancement de l'application Flask     print(\"\\n3. D\u00e9marrage de l'application web\")     print(\"   URL: http://localhost:5000\")     print(\"   Appuyez sur Ctrl+C pour quitter\")     app.run(debug=True) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"ressources/notebooks/cnn-classification/","title":"Cnn classification","text":"In\u00a0[\u00a0]: Copied! <pre>{\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"# CNN pour la classification d'images - MNIST\\n\",\n        \"\\n\",\n        \"## BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux de neurones\\n\",\n        \"\\n\",\n        \"Ce notebook vous guidera \u00e0 travers l'impl\u00e9mentation et l'utilisation d'un r\u00e9seau de neurones convolutif (CNN) pour la classification d'images, en utilisant le c\u00e9l\u00e8bre dataset MNIST des chiffres manuscrits.\\n\",\n        \"\\n\",\n        \"### Objectifs d'apprentissage:\\n\",\n        \"- Comprendre l'architecture d'un r\u00e9seau convolutif (CNN)\\n\",\n        \"- Impl\u00e9menter un CNN avec TensorFlow/Keras\\n\",\n        \"- Visualiser les filtres et feature maps\\n\",\n        \"- Analyser les performances du mod\u00e8le\\n\",\n        \"\\n\",\n        \"### Pr\u00e9requis:\\n\",\n        \"- Connaissances de base en Python\\n\",\n        \"- Avoir suivi la s\u00e9ance 1 d'introduction au Deep Learning\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 1. Configuration de l'environnement\\n\",\n        \"\\n\",\n        \"Commen\u00e7ons par importer les biblioth\u00e8ques n\u00e9cessaires.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"import numpy as np\\n\",\n        \"import matplotlib.pyplot as plt\\n\",\n        \"import tensorflow as tf\\n\",\n        \"from tensorflow.keras.models import Sequential\\n\",\n        \"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\\n\",\n        \"from tensorflow.keras.utils import to_categorical\\n\",\n        \"from tensorflow.keras.datasets import mnist\\n\",\n        \"import time\\n\",\n        \"import seaborn as sns\\n\",\n        \"from sklearn.metrics import confusion_matrix\\n\",\n        \"\\n\",\n        \"# Configuration pour reproductibilit\u00e9\\n\",\n        \"np.random.seed(42)\\n\",\n        \"tf.random.set_seed(42)\\n\",\n        \"\\n\",\n        \"# V\u00e9rifier la version de TensorFlow\\n\",\n        \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 2. Chargement et pr\u00e9paration du dataset MNIST\\n\",\n        \"\\n\",\n        \"Le dataset MNIST contient 70,000 images de chiffres manuscrits de taille 28x28 pixels.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"print(\\\"Chargement des donn\u00e9es MNIST...\\\")\\n\",\n        \"(X_train, y_train), (X_test, y_test) = mnist.load_data()\\n\",\n        \"\\n\",\n        \"# Afficher les dimensions des donn\u00e9es\\n\",\n        \"print(f\\\"Dimensions de X_train: {X_train.shape}\\\")\\n\",\n        \"print(f\\\"Dimensions de y_train: {y_train.shape}\\\")\\n\",\n        \"print(f\\\"Dimensions de X_test: {X_test.shape}\\\")\\n\",\n        \"print(f\\\"Dimensions de y_test: {y_test.shape}\\\")\\n\",\n        \"print(f\\\"Nombre de classes: {len(np.unique(y_train))}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Pr\u00e9paration des donn\u00e9es pour le CNN\\n\",\n        \"\\n\",\n        \"Pour utiliser nos images avec un CNN, nous devons :\\n\",\n        \"1. Ajouter une dimension pour le canal (les images sont en niveaux de gris, donc 1 seul canal)\\n\",\n        \"2. Normaliser les valeurs de pixels entre 0 et 1\\n\",\n        \"3. Convertir les \u00e9tiquettes en format one-hot encoding\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Redimensionnement et normalisation\\n\",\n        \"X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\\n\",\n        \"X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\\n\",\n        \"\\n\",\n        \"# Conversion des \u00e9tiquettes en cat\u00e9gories one-hot\\n\",\n        \"y_train_onehot = to_categorical(y_train, 10)\\n\",\n        \"y_test_onehot = to_categorical(y_test, 10)\\n\",\n        \"\\n\",\n        \"print(f\\\"Nouvelle forme de X_train: {X_train.shape}\\\")\\n\",\n        \"print(f\\\"Nouvelle forme de y_train_onehot: {y_train_onehot.shape}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation de quelques exemples\\n\",\n        \"\\n\",\n        \"Regardons \u00e0 quoi ressemblent nos donn\u00e9es.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"plt.figure(figsize=(10, 5))\\n\",\n        \"for i in range(10):\\n\",\n        \"    plt.subplot(2, 5, i+1)\\n\",\n        \"    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\\n\",\n        \"    plt.title(f\\\"Chiffre: {y_train[i]}\\\")\\n\",\n        \"    plt.axis('off')\\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 3. Cr\u00e9ation d'un mod\u00e8le CNN\\n\",\n        \"\\n\",\n        \"Un CNN est un type de r\u00e9seau de neurones sp\u00e9cialis\u00e9 pour traiter des donn\u00e9es ayant une structure en grille, comme les images. Les principales couches sont :\\n\",\n        \"\\n\",\n        \"1. **Couches de convolution (Conv2D)** : D\u00e9tectent des caract\u00e9ristiques locales (lignes, formes...)\\n\",\n        \"2. **Couches de pooling (MaxPooling2D)** : R\u00e9duisent la dimension des donn\u00e9es\\n\",\n        \"3. **Couches denses (Dense)** : Effectuent la classification finale\\n\",\n        \"\\n\",\n        \"Nous allons cr\u00e9er un CNN simple avec 2 couches de convolution pour classifier les chiffres MNIST.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Cr\u00e9er un mod\u00e8le CNN\\n\",\n        \"model = Sequential([\\n\",\n        \"    # Premi\u00e8re couche de convolution\\n\",\n        \"    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\\n\",\n        \"    MaxPooling2D((2, 2), name='pool1'),\\n\",\n        \"    \\n\",\n        \"    # Deuxi\u00e8me couche de convolution\\n\",\n        \"    Conv2D(64, (3, 3), activation='relu', name='conv2'),\\n\",\n        \"    MaxPooling2D((2, 2), name='pool2'),\\n\",\n        \"    \\n\",\n        \"    # Aplatissement pour passer aux couches denses\\n\",\n        \"    Flatten(name='flatten'),\\n\",\n        \"    \\n\",\n        \"    # Couches denses (fully connected)\\n\",\n        \"    Dense(128, activation='relu', name='dense1'),\\n\",\n        \"    Dropout(0.5, name='dropout1'),  # \u00c9vite le surapprentissage\\n\",\n        \"    Dense(10, activation='softmax', name='output')  # 10 classes (chiffres 0-9)\\n\",\n        \"])\\n\",\n        \"\\n\",\n        \"# Compiler le mod\u00e8le\\n\",\n        \"model.compile(\\n\",\n        \"    optimizer='adam',\\n\",\n        \"    loss='categorical_crossentropy',\\n\",\n        \"    metrics=['accuracy']\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"# Afficher le r\u00e9sum\u00e9 de l'architecture\\n\",\n        \"model.summary()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 4. Entra\u00eenement du mod\u00e8le\\n\",\n        \"\\n\",\n        \"Entra\u00eenons maintenant notre CNN sur les donn\u00e9es MNIST.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Entra\u00eenement du mod\u00e8le\\n\",\n        \"start_time = time.time()\\n\",\n        \"\\n\",\n        \"history = model.fit(\\n\",\n        \"    X_train, \\n\",\n        \"    y_train_onehot, \\n\",\n        \"    batch_size=128, \\n\",\n        \"    epochs=5,  # Nombre r\u00e9duit d'\u00e9poques pour la d\u00e9monstration\\n\",\n        \"    validation_split=0.2,  # 20% des donn\u00e9es d'entra\u00eenement pour la validation\\n\",\n        \"    verbose=1\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"training_time = time.time() - start_time\\n\",\n        \"print(f\\\"\\\\nTemps d'entra\u00eenement: {training_time:.2f} secondes\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation de l'\u00e9volution de l'entra\u00eenement\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"plt.figure(figsize=(12, 4))\\n\",\n        \"\\n\",\n        \"# Graphique de pr\u00e9cision\\n\",\n        \"plt.subplot(1, 2, 1)\\n\",\n        \"plt.plot(history.history['accuracy'], label='Entra\u00eenement')\\n\",\n        \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",\n        \"plt.title('\u00c9volution de la pr\u00e9cision')\\n\",\n        \"plt.xlabel('\u00c9poque')\\n\",\n        \"plt.ylabel('Pr\u00e9cision')\\n\",\n        \"plt.legend()\\n\",\n        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n        \"\\n\",\n        \"# Graphique de perte\\n\",\n        \"plt.subplot(1, 2, 2)\\n\",\n        \"plt.plot(history.history['loss'], label='Entra\u00eenement')\\n\",\n        \"plt.plot(history.history['val_loss'], label='Validation')\\n\",\n        \"plt.title('\u00c9volution de la perte')\\n\",\n        \"plt.xlabel('\u00c9poque')\\n\",\n        \"plt.ylabel('Perte')\\n\",\n        \"plt.legend()\\n\",\n        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n        \"\\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 5. \u00c9valuation du mod\u00e8le\\n\",\n        \"\\n\",\n        \"\u00c9valuons notre mod\u00e8le sur l'ensemble de test.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# \u00c9valuation sur l'ensemble de test\\n\",\n        \"test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)\\n\",\n        \"print(f\\\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",\n        \"\\n\",\n        \"# Pr\u00e9dictions\\n\",\n        \"y_pred = model.predict(X_test)\\n\",\n        \"y_pred_classes = np.argmax(y_pred, axis=1)\\n\",\n        \"\\n\",\n        \"# Matrice de confusion\\n\",\n        \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",\n        \"plt.figure(figsize=(10, 8))\\n\",\n        \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\\n\",\n        \"plt.xlabel('Pr\u00e9dit')\\n\",\n        \"plt.ylabel('R\u00e9el')\\n\",\n        \"plt.title('Matrice de confusion')\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation des exemples mal classifi\u00e9s\\n\",\n        \"\\n\",\n        \"Explorons quelques exemples que notre mod\u00e8le a mal classifi\u00e9s.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Identifier les erreurs\\n\",\n        \"misclassified_indices = np.where(y_pred_classes != y_test)[0]\\n\",\n        \"misclassified_count = len(misclassified_indices)\\n\",\n        \"print(f\\\"Nombre total d'erreurs: {misclassified_count} sur {len(y_test)} images de test\\\")\\n\",\n        \"\\n\",\n        \"# Afficher quelques exemples mal classifi\u00e9s\\n\",\n        \"num_examples = min(10, misclassified_count)\\n\",\n        \"plt.figure(figsize=(15, 6))\\n\",\n        \"\\n\",\n        \"for i, idx in enumerate(misclassified_indices[:num_examples]):\\n\",\n        \"    plt.subplot(2, 5, i+1)\\n\",\n        \"    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\\n\",\n        \"    plt.title(f\\\"R\u00e9el: {y_test[idx]}\\\\nPr\u00e9dit: {y_pred_classes[idx]}\\\")\\n\",\n        \"    plt.axis('off')\\n\",\n        \"    \\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### \ud83e\udde0 R\u00e9flexion sur les erreurs\\n\",\n        \"\\n\",\n        \"**Question**: En observant les exemples mal classifi\u00e9s, quelles pourraient \u00eatre les raisons de ces erreurs? Notez vos observations et hypoth\u00e8ses ci-dessous.\\n\",\n        \"\\n\",\n        \"**Points \u00e0 consid\u00e9rer:**\\n\",\n        \"- Certains chiffres sont-ils plus souvent confondus que d'autres?\\n\",\n        \"- Quelles caract\u00e9ristiques visuelles communes peuvent expliquer les erreurs?\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"*\u00c9crivez vos observations ici...*\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 6. Visualisation des filtres et feature maps\\n\",\n        \"\\n\",\n        \"Une des grandes forces des CNNs est leur interpr\u00e9tabilit\u00e9 visuelle. Explorons ce que le r\u00e9seau \\\"voit\\\" r\u00e9ellement.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Fonction pour visualiser les filtres de convolution\\n\",\n        \"def visualize_filters(model, layer_name, num_filters=8):\\n\",\n        \"    \\\"\\\"\\\"Visualise les filtres d'une couche de convolution\\\"\\\"\\\"\\n\",\n        \"    \\n\",\n        \"    # R\u00e9cup\u00e9rer les poids du filtre de la couche sp\u00e9cifi\u00e9e\\n\",\n        \"    filters, biases = model.get_layer(layer_name).get_weights()\\n\",\n        \"    \\n\",\n        \"    # Normaliser les filtres pour une meilleure visualisation\\n\",\n        \"    f_min, f_max = filters.min(), filters.max()\\n\",\n        \"    filters = (filters - f_min) / (f_max - f_min)\\n\",\n        \"    \\n\",\n        \"    # Afficher les premiers filtres\\n\",\n        \"    plt.figure(figsize=(12, 4))\\n\",\n        \"    for i in range(num_filters):\\n\",\n        \"        plt.subplot(2, 4, i+1)\\n\",\n        \"        # Pour la premi\u00e8re couche de convolution, les filtres sont 3D (hauteur, largeur, canaux)\\n\",\n        \"        # Nous affichons le filtre pour le premier canal (0)\\n\",\n        \"        plt.imshow(filters[:, :, 0, i], cmap='viridis')\\n\",\n        \"        plt.title(f'Filtre {i+1}')\\n\",\n        \"        plt.axis('off')\\n\",\n        \"    plt.suptitle(f'Filtres de la couche {layer_name}')\\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\\n\",\n        \"\\n\",\n        \"# Visualiser les filtres de la premi\u00e8re couche de convolution\\n\",\n        \"visualize_filters(model, 'conv1')\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation des feature maps (cartes d'activation)\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def visualize_feature_maps(model, image, layer_name, num_features=8):\\n\",\n        \"    \\\"\\\"\\\"Visualise les feature maps (activations) d'une couche pour une image donn\u00e9e\\\"\\\"\\\"\\n\",\n        \"    \\n\",\n        \"    # Cr\u00e9er un mod\u00e8le qui renvoie les activations de la couche sp\u00e9cifi\u00e9e\\n\",\n        \"    layer_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\\n\",\n        \"    \\n\",\n        \"    # Obtenir les activations pour une image\\n\",\n        \"    feature_maps = layer_model.predict(image.reshape(1, 28, 28, 1))\\n\",\n        \"    \\n\",\n        \"    # Afficher les premi\u00e8res cartes d'activation\\n\",\n        \"    plt.figure(figsize=(12, 4))\\n\",\n        \"    for i in range(min(num_features, feature_maps.shape[3])):\\n\",\n        \"        plt.subplot(2, 4, i+1)\\n\",\n        \"        plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\\n\",\n        \"        plt.title(f'Feature {i+1}')\\n\",\n        \"        plt.axis('off')\\n\",\n        \"    plt.suptitle(f'Feature Maps de la couche {layer_name}')\\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\\n\",\n        \"\\n\",\n        \"# Choisir une image de test\\n\",\n        \"sample_idx = 12  # Vous pouvez essayer avec diff\u00e9rents indices\\n\",\n        \"sample_image = X_test[sample_idx]\\n\",\n        \"\\n\",\n        \"# Afficher l'image originale\\n\",\n        \"plt.figure(figsize=(3, 3))\\n\",\n        \"plt.imshow(sample_image.reshape(28, 28), cmap='gray')\\n\",\n        \"plt.title(f\\\"Chiffre: {y_test[sample_idx]}\\\")\\n\",\n        \"plt.axis('off')\\n\",\n        \"plt.show()\\n\",\n        \"\\n\",\n        \"# Visualiser les feature maps pour chaque couche de convolution\\n\",\n        \"print(\\\"Feature maps de la premi\u00e8re couche de convolution:\\\")\\n\",\n        \"visualize_feature_maps(model, sample_image, 'conv1')\\n\",\n        \"\\n\",\n        \"print(\\\"Feature maps de la deuxi\u00e8me couche de convolution:\\\")\\n\",\n        \"visualize_feature_maps(model, sample_image, 'conv2')\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### \ud83d\udca1 Interpr\u00e9tation des feature maps\\n\",\n        \"\\n\",\n        \"Les feature maps nous montrent ce que \\\"voit\\\" chaque filtre de convolution :\\n\",\n        \"\\n\",\n        \"- **Premi\u00e8re couche** : D\u00e9tecte principalement des caract\u00e9ristiques de base comme les bords et les contours\\n\",\n        \"- **Deuxi\u00e8me couche** : Combine ces caract\u00e9ristiques de base pour d\u00e9tecter des formes plus complexes\\n\",\n        \"\\n\",\n        \"Cette hi\u00e9rarchie de repr\u00e9sentations est ce qui rend les CNNs si puissants pour la vision par ordinateur.\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 7. Test avec des images bruit\u00e9es\\n\",\n        \"\\n\",\n        \"Testons la robustesse de notre mod\u00e8le face \u00e0 des perturbations.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Fonction pour ajouter du bruit aux images\\n\",\n        \"def add_noise(images, noise_level=0.2):\\n\",\n        \"    \\\"\\\"\\\"Ajoute du bruit gaussien aux images\\\"\\\"\\\"\\n\",\n        \"    noisy_images = images.copy()\\n\",\n        \"    noise = np.random.normal(0, noise_level, images.shape)\\n\",\n        \"    noisy_images = noisy_images + noise\\n\",\n        \"    # Assurer que les valeurs restent entre 0 et 1\\n\",\n        \"    return np.clip(noisy_images, 0, 1)\\n\",\n        \"\\n\",\n        \"# Cr\u00e9er des versions bruit\u00e9es de quelques images de test\\n\",\n        \"num_test_images = 5\\n\",\n        \"test_samples = X_test[:num_test_images]\\n\",\n        \"noisy_samples = add_noise(test_samples, noise_level=0.3)\\n\",\n        \"\\n\",\n        \"# Visualiser les images originales et bruit\u00e9es\\n\",\n        \"plt.figure(figsize=(12, 4))\\n\",\n        \"for i in range(num_test_images):\\n\",\n        \"    # Image originale\\n\",\n        \"    plt.subplot(2, num_test_images, i+1)\\n\",\n        \"    plt.imshow(test_samples[i].reshape(28, 28), cmap='gray')\\n\",\n        \"    plt.title(f\\\"Original: {y_test[i]}\\\")\\n\",\n        \"    plt.axis('off')\\n\",\n        \"    \\n\",\n        \"    # Image bruit\u00e9e\\n\",\n        \"    plt.subplot(2, num_test_images, i+num_test_images+1)\\n\",\n        \"    plt.imshow(noisy_samples[i].reshape(28, 28), cmap='gray')\\n\",\n        \"    plt.axis('off')\\n\",\n        \"    \\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\\n\",\n        \"\\n\",\n        \"# Pr\u00e9dire sur les images bruit\u00e9es\\n\",\n        \"noisy_predictions = model.predict(noisy_samples)\\n\",\n        \"noisy_pred_classes = np.argmax(noisy_predictions, axis=1)\\n\",\n        \"\\n\",\n        \"# Afficher les r\u00e9sultats\\n\",\n        \"print(\\\"R\u00e9sultats des pr\u00e9dictions sur les images bruit\u00e9es:\\\")\\n\",\n        \"for i in range(num_test_images):\\n\",\n        \"    status = \\\"\u2713\\\" if noisy_pred_classes[i] == y_test[i] else \\\"\u2717\\\"\\n\",\n        \"    print(f\\\"Image {i+1} - R\u00e9el: {y_test[i]}, Pr\u00e9dit: {noisy_pred_classes[i]} {status}\\\")\\n\",\n        \"\\n\",\n        \"# Calculer la pr\u00e9cision sur les images bruit\u00e9es\\n\",\n        \"accuracy_on_noisy = np.mean(noisy_pred_classes == y_test[:num_test_images]) * 100\\n\",\n        \"print(f\\\"\\\\nPr\u00e9cision sur les images bruit\u00e9es: {accuracy_on_noisy:.1f}%\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 8. Exercice : Am\u00e9lioration du mod\u00e8le\\n\",\n        \"\\n\",\n        \"\u00c0 vous de jouer ! Essayez de modifier l'architecture du mod\u00e8le pour am\u00e9liorer ses performances. Voici quelques suggestions :\\n\",\n        \"\\n\",\n        \"1. Ajouter plus de couches de convolution\\n\",\n        \"2. Modifier le nombre de filtres\\n\",\n        \"3. Changer la taille des filtres\\n\",\n        \"4. Ajuster les param\u00e8tres d'entra\u00eenement (epochs, batch_size)\\n\",\n        \"\\n\",\n        \"Copiez le code de cr\u00e9ation du mod\u00e8le ci-dessous et modifiez-le :\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# VOTRE CODE ICI\\n\",\n        \"# Cr\u00e9ez votre propre mod\u00e8le am\u00e9lior\u00e9\\n\",\n        \"\\n\",\n        \"improved_model = Sequential([\\n\",\n        \"    # Modifiez l'architecture ici\\n\",\n        \"    \\n\",\n        \"])\\n\",\n        \"\\n\",\n        \"# Compiler le mod\u00e8le\\n\",\n        \"improved_model.compile(\\n\",\n        \"    optimizer='adam',\\n\",\n        \"    loss='categorical_crossentropy',\\n\",\n        \"    metrics=['accuracy']\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"# Afficher le r\u00e9sum\u00e9\\n\",\n        \"improved_model.summary()\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Entra\u00eenez votre mod\u00e8le am\u00e9lior\u00e9\\n\",\n        \"# history = improved_model.fit(...)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 9. Conclusion\\n\",\n        \"\\n\",\n        \"Dans ce notebook, vous avez :\\n\",\n        \"- Cr\u00e9\u00e9 et entra\u00een\u00e9 un r\u00e9seau de neurones convolutif (CNN) pour la classification d'images\\n\",\n        \"- Visualis\u00e9 les filtres et les feature maps pour comprendre ce que \\\"voit\\\" le r\u00e9seau\\n\",\n        \"- \u00c9valu\u00e9 les performances du mod\u00e8le et sa robustesse face au bruit\\n\",\n        \"\\n\",\n        \"Les CNN sont la base de nombreuses applications modernes de vision par ordinateur comme la reconnaissance faciale, la d\u00e9tection d'objets, et bien d'autres.\"\n      ]\n    }\n  ],\n  \"metadata\": {\n    \"kernelspec\": {\n      \"display_name\": \"Python 3\",\n      \"language\": \"python\",\n      \"name\": \"python3\"\n    },\n    \"language_info\": {\n      \"codemirror_mode\": {\n        \"name\": \"ipython\",\n        \"version\": 3\n      },\n      \"file_extension\": \".py\",\n      \"mimetype\": \"text/x-python\",\n      \"name\": \"python\",\n      \"nbconvert_exporter\": \"python\",\n      \"pygments_lexer\": \"ipython3\",\n      \"version\": \"3.8.5\"\n    }\n  },\n  \"nbformat\": 4,\n  \"nbformat_minor\": 4\n}\n</pre> {   \"cells\": [     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"# CNN pour la classification d'images - MNIST\\n\",         \"\\n\",         \"## BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux de neurones\\n\",         \"\\n\",         \"Ce notebook vous guidera \u00e0 travers l'impl\u00e9mentation et l'utilisation d'un r\u00e9seau de neurones convolutif (CNN) pour la classification d'images, en utilisant le c\u00e9l\u00e8bre dataset MNIST des chiffres manuscrits.\\n\",         \"\\n\",         \"### Objectifs d'apprentissage:\\n\",         \"- Comprendre l'architecture d'un r\u00e9seau convolutif (CNN)\\n\",         \"- Impl\u00e9menter un CNN avec TensorFlow/Keras\\n\",         \"- Visualiser les filtres et feature maps\\n\",         \"- Analyser les performances du mod\u00e8le\\n\",         \"\\n\",         \"### Pr\u00e9requis:\\n\",         \"- Connaissances de base en Python\\n\",         \"- Avoir suivi la s\u00e9ance 1 d'introduction au Deep Learning\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 1. Configuration de l'environnement\\n\",         \"\\n\",         \"Commen\u00e7ons par importer les biblioth\u00e8ques n\u00e9cessaires.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"import numpy as np\\n\",         \"import matplotlib.pyplot as plt\\n\",         \"import tensorflow as tf\\n\",         \"from tensorflow.keras.models import Sequential\\n\",         \"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\\n\",         \"from tensorflow.keras.utils import to_categorical\\n\",         \"from tensorflow.keras.datasets import mnist\\n\",         \"import time\\n\",         \"import seaborn as sns\\n\",         \"from sklearn.metrics import confusion_matrix\\n\",         \"\\n\",         \"# Configuration pour reproductibilit\u00e9\\n\",         \"np.random.seed(42)\\n\",         \"tf.random.set_seed(42)\\n\",         \"\\n\",         \"# V\u00e9rifier la version de TensorFlow\\n\",         \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 2. Chargement et pr\u00e9paration du dataset MNIST\\n\",         \"\\n\",         \"Le dataset MNIST contient 70,000 images de chiffres manuscrits de taille 28x28 pixels.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"print(\\\"Chargement des donn\u00e9es MNIST...\\\")\\n\",         \"(X_train, y_train), (X_test, y_test) = mnist.load_data()\\n\",         \"\\n\",         \"# Afficher les dimensions des donn\u00e9es\\n\",         \"print(f\\\"Dimensions de X_train: {X_train.shape}\\\")\\n\",         \"print(f\\\"Dimensions de y_train: {y_train.shape}\\\")\\n\",         \"print(f\\\"Dimensions de X_test: {X_test.shape}\\\")\\n\",         \"print(f\\\"Dimensions de y_test: {y_test.shape}\\\")\\n\",         \"print(f\\\"Nombre de classes: {len(np.unique(y_train))}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Pr\u00e9paration des donn\u00e9es pour le CNN\\n\",         \"\\n\",         \"Pour utiliser nos images avec un CNN, nous devons :\\n\",         \"1. Ajouter une dimension pour le canal (les images sont en niveaux de gris, donc 1 seul canal)\\n\",         \"2. Normaliser les valeurs de pixels entre 0 et 1\\n\",         \"3. Convertir les \u00e9tiquettes en format one-hot encoding\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Redimensionnement et normalisation\\n\",         \"X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\\n\",         \"X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\\n\",         \"\\n\",         \"# Conversion des \u00e9tiquettes en cat\u00e9gories one-hot\\n\",         \"y_train_onehot = to_categorical(y_train, 10)\\n\",         \"y_test_onehot = to_categorical(y_test, 10)\\n\",         \"\\n\",         \"print(f\\\"Nouvelle forme de X_train: {X_train.shape}\\\")\\n\",         \"print(f\\\"Nouvelle forme de y_train_onehot: {y_train_onehot.shape}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation de quelques exemples\\n\",         \"\\n\",         \"Regardons \u00e0 quoi ressemblent nos donn\u00e9es.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"plt.figure(figsize=(10, 5))\\n\",         \"for i in range(10):\\n\",         \"    plt.subplot(2, 5, i+1)\\n\",         \"    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\\n\",         \"    plt.title(f\\\"Chiffre: {y_train[i]}\\\")\\n\",         \"    plt.axis('off')\\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 3. Cr\u00e9ation d'un mod\u00e8le CNN\\n\",         \"\\n\",         \"Un CNN est un type de r\u00e9seau de neurones sp\u00e9cialis\u00e9 pour traiter des donn\u00e9es ayant une structure en grille, comme les images. Les principales couches sont :\\n\",         \"\\n\",         \"1. **Couches de convolution (Conv2D)** : D\u00e9tectent des caract\u00e9ristiques locales (lignes, formes...)\\n\",         \"2. **Couches de pooling (MaxPooling2D)** : R\u00e9duisent la dimension des donn\u00e9es\\n\",         \"3. **Couches denses (Dense)** : Effectuent la classification finale\\n\",         \"\\n\",         \"Nous allons cr\u00e9er un CNN simple avec 2 couches de convolution pour classifier les chiffres MNIST.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Cr\u00e9er un mod\u00e8le CNN\\n\",         \"model = Sequential([\\n\",         \"    # Premi\u00e8re couche de convolution\\n\",         \"    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\\n\",         \"    MaxPooling2D((2, 2), name='pool1'),\\n\",         \"    \\n\",         \"    # Deuxi\u00e8me couche de convolution\\n\",         \"    Conv2D(64, (3, 3), activation='relu', name='conv2'),\\n\",         \"    MaxPooling2D((2, 2), name='pool2'),\\n\",         \"    \\n\",         \"    # Aplatissement pour passer aux couches denses\\n\",         \"    Flatten(name='flatten'),\\n\",         \"    \\n\",         \"    # Couches denses (fully connected)\\n\",         \"    Dense(128, activation='relu', name='dense1'),\\n\",         \"    Dropout(0.5, name='dropout1'),  # \u00c9vite le surapprentissage\\n\",         \"    Dense(10, activation='softmax', name='output')  # 10 classes (chiffres 0-9)\\n\",         \"])\\n\",         \"\\n\",         \"# Compiler le mod\u00e8le\\n\",         \"model.compile(\\n\",         \"    optimizer='adam',\\n\",         \"    loss='categorical_crossentropy',\\n\",         \"    metrics=['accuracy']\\n\",         \")\\n\",         \"\\n\",         \"# Afficher le r\u00e9sum\u00e9 de l'architecture\\n\",         \"model.summary()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 4. Entra\u00eenement du mod\u00e8le\\n\",         \"\\n\",         \"Entra\u00eenons maintenant notre CNN sur les donn\u00e9es MNIST.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Entra\u00eenement du mod\u00e8le\\n\",         \"start_time = time.time()\\n\",         \"\\n\",         \"history = model.fit(\\n\",         \"    X_train, \\n\",         \"    y_train_onehot, \\n\",         \"    batch_size=128, \\n\",         \"    epochs=5,  # Nombre r\u00e9duit d'\u00e9poques pour la d\u00e9monstration\\n\",         \"    validation_split=0.2,  # 20% des donn\u00e9es d'entra\u00eenement pour la validation\\n\",         \"    verbose=1\\n\",         \")\\n\",         \"\\n\",         \"training_time = time.time() - start_time\\n\",         \"print(f\\\"\\\\nTemps d'entra\u00eenement: {training_time:.2f} secondes\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation de l'\u00e9volution de l'entra\u00eenement\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"plt.figure(figsize=(12, 4))\\n\",         \"\\n\",         \"# Graphique de pr\u00e9cision\\n\",         \"plt.subplot(1, 2, 1)\\n\",         \"plt.plot(history.history['accuracy'], label='Entra\u00eenement')\\n\",         \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",         \"plt.title('\u00c9volution de la pr\u00e9cision')\\n\",         \"plt.xlabel('\u00c9poque')\\n\",         \"plt.ylabel('Pr\u00e9cision')\\n\",         \"plt.legend()\\n\",         \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",         \"\\n\",         \"# Graphique de perte\\n\",         \"plt.subplot(1, 2, 2)\\n\",         \"plt.plot(history.history['loss'], label='Entra\u00eenement')\\n\",         \"plt.plot(history.history['val_loss'], label='Validation')\\n\",         \"plt.title('\u00c9volution de la perte')\\n\",         \"plt.xlabel('\u00c9poque')\\n\",         \"plt.ylabel('Perte')\\n\",         \"plt.legend()\\n\",         \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",         \"\\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 5. \u00c9valuation du mod\u00e8le\\n\",         \"\\n\",         \"\u00c9valuons notre mod\u00e8le sur l'ensemble de test.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# \u00c9valuation sur l'ensemble de test\\n\",         \"test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)\\n\",         \"print(f\\\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",         \"\\n\",         \"# Pr\u00e9dictions\\n\",         \"y_pred = model.predict(X_test)\\n\",         \"y_pred_classes = np.argmax(y_pred, axis=1)\\n\",         \"\\n\",         \"# Matrice de confusion\\n\",         \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",         \"plt.figure(figsize=(10, 8))\\n\",         \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\\n\",         \"plt.xlabel('Pr\u00e9dit')\\n\",         \"plt.ylabel('R\u00e9el')\\n\",         \"plt.title('Matrice de confusion')\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation des exemples mal classifi\u00e9s\\n\",         \"\\n\",         \"Explorons quelques exemples que notre mod\u00e8le a mal classifi\u00e9s.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Identifier les erreurs\\n\",         \"misclassified_indices = np.where(y_pred_classes != y_test)[0]\\n\",         \"misclassified_count = len(misclassified_indices)\\n\",         \"print(f\\\"Nombre total d'erreurs: {misclassified_count} sur {len(y_test)} images de test\\\")\\n\",         \"\\n\",         \"# Afficher quelques exemples mal classifi\u00e9s\\n\",         \"num_examples = min(10, misclassified_count)\\n\",         \"plt.figure(figsize=(15, 6))\\n\",         \"\\n\",         \"for i, idx in enumerate(misclassified_indices[:num_examples]):\\n\",         \"    plt.subplot(2, 5, i+1)\\n\",         \"    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\\n\",         \"    plt.title(f\\\"R\u00e9el: {y_test[idx]}\\\\nPr\u00e9dit: {y_pred_classes[idx]}\\\")\\n\",         \"    plt.axis('off')\\n\",         \"    \\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### \ud83e\udde0 R\u00e9flexion sur les erreurs\\n\",         \"\\n\",         \"**Question**: En observant les exemples mal classifi\u00e9s, quelles pourraient \u00eatre les raisons de ces erreurs? Notez vos observations et hypoth\u00e8ses ci-dessous.\\n\",         \"\\n\",         \"**Points \u00e0 consid\u00e9rer:**\\n\",         \"- Certains chiffres sont-ils plus souvent confondus que d'autres?\\n\",         \"- Quelles caract\u00e9ristiques visuelles communes peuvent expliquer les erreurs?\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"*\u00c9crivez vos observations ici...*\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 6. Visualisation des filtres et feature maps\\n\",         \"\\n\",         \"Une des grandes forces des CNNs est leur interpr\u00e9tabilit\u00e9 visuelle. Explorons ce que le r\u00e9seau \\\"voit\\\" r\u00e9ellement.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Fonction pour visualiser les filtres de convolution\\n\",         \"def visualize_filters(model, layer_name, num_filters=8):\\n\",         \"    \\\"\\\"\\\"Visualise les filtres d'une couche de convolution\\\"\\\"\\\"\\n\",         \"    \\n\",         \"    # R\u00e9cup\u00e9rer les poids du filtre de la couche sp\u00e9cifi\u00e9e\\n\",         \"    filters, biases = model.get_layer(layer_name).get_weights()\\n\",         \"    \\n\",         \"    # Normaliser les filtres pour une meilleure visualisation\\n\",         \"    f_min, f_max = filters.min(), filters.max()\\n\",         \"    filters = (filters - f_min) / (f_max - f_min)\\n\",         \"    \\n\",         \"    # Afficher les premiers filtres\\n\",         \"    plt.figure(figsize=(12, 4))\\n\",         \"    for i in range(num_filters):\\n\",         \"        plt.subplot(2, 4, i+1)\\n\",         \"        # Pour la premi\u00e8re couche de convolution, les filtres sont 3D (hauteur, largeur, canaux)\\n\",         \"        # Nous affichons le filtre pour le premier canal (0)\\n\",         \"        plt.imshow(filters[:, :, 0, i], cmap='viridis')\\n\",         \"        plt.title(f'Filtre {i+1}')\\n\",         \"        plt.axis('off')\\n\",         \"    plt.suptitle(f'Filtres de la couche {layer_name}')\\n\",         \"    plt.tight_layout()\\n\",         \"    plt.show()\\n\",         \"\\n\",         \"# Visualiser les filtres de la premi\u00e8re couche de convolution\\n\",         \"visualize_filters(model, 'conv1')\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation des feature maps (cartes d'activation)\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def visualize_feature_maps(model, image, layer_name, num_features=8):\\n\",         \"    \\\"\\\"\\\"Visualise les feature maps (activations) d'une couche pour une image donn\u00e9e\\\"\\\"\\\"\\n\",         \"    \\n\",         \"    # Cr\u00e9er un mod\u00e8le qui renvoie les activations de la couche sp\u00e9cifi\u00e9e\\n\",         \"    layer_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\\n\",         \"    \\n\",         \"    # Obtenir les activations pour une image\\n\",         \"    feature_maps = layer_model.predict(image.reshape(1, 28, 28, 1))\\n\",         \"    \\n\",         \"    # Afficher les premi\u00e8res cartes d'activation\\n\",         \"    plt.figure(figsize=(12, 4))\\n\",         \"    for i in range(min(num_features, feature_maps.shape[3])):\\n\",         \"        plt.subplot(2, 4, i+1)\\n\",         \"        plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\\n\",         \"        plt.title(f'Feature {i+1}')\\n\",         \"        plt.axis('off')\\n\",         \"    plt.suptitle(f'Feature Maps de la couche {layer_name}')\\n\",         \"    plt.tight_layout()\\n\",         \"    plt.show()\\n\",         \"\\n\",         \"# Choisir une image de test\\n\",         \"sample_idx = 12  # Vous pouvez essayer avec diff\u00e9rents indices\\n\",         \"sample_image = X_test[sample_idx]\\n\",         \"\\n\",         \"# Afficher l'image originale\\n\",         \"plt.figure(figsize=(3, 3))\\n\",         \"plt.imshow(sample_image.reshape(28, 28), cmap='gray')\\n\",         \"plt.title(f\\\"Chiffre: {y_test[sample_idx]}\\\")\\n\",         \"plt.axis('off')\\n\",         \"plt.show()\\n\",         \"\\n\",         \"# Visualiser les feature maps pour chaque couche de convolution\\n\",         \"print(\\\"Feature maps de la premi\u00e8re couche de convolution:\\\")\\n\",         \"visualize_feature_maps(model, sample_image, 'conv1')\\n\",         \"\\n\",         \"print(\\\"Feature maps de la deuxi\u00e8me couche de convolution:\\\")\\n\",         \"visualize_feature_maps(model, sample_image, 'conv2')\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### \ud83d\udca1 Interpr\u00e9tation des feature maps\\n\",         \"\\n\",         \"Les feature maps nous montrent ce que \\\"voit\\\" chaque filtre de convolution :\\n\",         \"\\n\",         \"- **Premi\u00e8re couche** : D\u00e9tecte principalement des caract\u00e9ristiques de base comme les bords et les contours\\n\",         \"- **Deuxi\u00e8me couche** : Combine ces caract\u00e9ristiques de base pour d\u00e9tecter des formes plus complexes\\n\",         \"\\n\",         \"Cette hi\u00e9rarchie de repr\u00e9sentations est ce qui rend les CNNs si puissants pour la vision par ordinateur.\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 7. Test avec des images bruit\u00e9es\\n\",         \"\\n\",         \"Testons la robustesse de notre mod\u00e8le face \u00e0 des perturbations.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Fonction pour ajouter du bruit aux images\\n\",         \"def add_noise(images, noise_level=0.2):\\n\",         \"    \\\"\\\"\\\"Ajoute du bruit gaussien aux images\\\"\\\"\\\"\\n\",         \"    noisy_images = images.copy()\\n\",         \"    noise = np.random.normal(0, noise_level, images.shape)\\n\",         \"    noisy_images = noisy_images + noise\\n\",         \"    # Assurer que les valeurs restent entre 0 et 1\\n\",         \"    return np.clip(noisy_images, 0, 1)\\n\",         \"\\n\",         \"# Cr\u00e9er des versions bruit\u00e9es de quelques images de test\\n\",         \"num_test_images = 5\\n\",         \"test_samples = X_test[:num_test_images]\\n\",         \"noisy_samples = add_noise(test_samples, noise_level=0.3)\\n\",         \"\\n\",         \"# Visualiser les images originales et bruit\u00e9es\\n\",         \"plt.figure(figsize=(12, 4))\\n\",         \"for i in range(num_test_images):\\n\",         \"    # Image originale\\n\",         \"    plt.subplot(2, num_test_images, i+1)\\n\",         \"    plt.imshow(test_samples[i].reshape(28, 28), cmap='gray')\\n\",         \"    plt.title(f\\\"Original: {y_test[i]}\\\")\\n\",         \"    plt.axis('off')\\n\",         \"    \\n\",         \"    # Image bruit\u00e9e\\n\",         \"    plt.subplot(2, num_test_images, i+num_test_images+1)\\n\",         \"    plt.imshow(noisy_samples[i].reshape(28, 28), cmap='gray')\\n\",         \"    plt.axis('off')\\n\",         \"    \\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\\n\",         \"\\n\",         \"# Pr\u00e9dire sur les images bruit\u00e9es\\n\",         \"noisy_predictions = model.predict(noisy_samples)\\n\",         \"noisy_pred_classes = np.argmax(noisy_predictions, axis=1)\\n\",         \"\\n\",         \"# Afficher les r\u00e9sultats\\n\",         \"print(\\\"R\u00e9sultats des pr\u00e9dictions sur les images bruit\u00e9es:\\\")\\n\",         \"for i in range(num_test_images):\\n\",         \"    status = \\\"\u2713\\\" if noisy_pred_classes[i] == y_test[i] else \\\"\u2717\\\"\\n\",         \"    print(f\\\"Image {i+1} - R\u00e9el: {y_test[i]}, Pr\u00e9dit: {noisy_pred_classes[i]} {status}\\\")\\n\",         \"\\n\",         \"# Calculer la pr\u00e9cision sur les images bruit\u00e9es\\n\",         \"accuracy_on_noisy = np.mean(noisy_pred_classes == y_test[:num_test_images]) * 100\\n\",         \"print(f\\\"\\\\nPr\u00e9cision sur les images bruit\u00e9es: {accuracy_on_noisy:.1f}%\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 8. Exercice : Am\u00e9lioration du mod\u00e8le\\n\",         \"\\n\",         \"\u00c0 vous de jouer ! Essayez de modifier l'architecture du mod\u00e8le pour am\u00e9liorer ses performances. Voici quelques suggestions :\\n\",         \"\\n\",         \"1. Ajouter plus de couches de convolution\\n\",         \"2. Modifier le nombre de filtres\\n\",         \"3. Changer la taille des filtres\\n\",         \"4. Ajuster les param\u00e8tres d'entra\u00eenement (epochs, batch_size)\\n\",         \"\\n\",         \"Copiez le code de cr\u00e9ation du mod\u00e8le ci-dessous et modifiez-le :\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# VOTRE CODE ICI\\n\",         \"# Cr\u00e9ez votre propre mod\u00e8le am\u00e9lior\u00e9\\n\",         \"\\n\",         \"improved_model = Sequential([\\n\",         \"    # Modifiez l'architecture ici\\n\",         \"    \\n\",         \"])\\n\",         \"\\n\",         \"# Compiler le mod\u00e8le\\n\",         \"improved_model.compile(\\n\",         \"    optimizer='adam',\\n\",         \"    loss='categorical_crossentropy',\\n\",         \"    metrics=['accuracy']\\n\",         \")\\n\",         \"\\n\",         \"# Afficher le r\u00e9sum\u00e9\\n\",         \"improved_model.summary()\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Entra\u00eenez votre mod\u00e8le am\u00e9lior\u00e9\\n\",         \"# history = improved_model.fit(...)\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 9. Conclusion\\n\",         \"\\n\",         \"Dans ce notebook, vous avez :\\n\",         \"- Cr\u00e9\u00e9 et entra\u00een\u00e9 un r\u00e9seau de neurones convolutif (CNN) pour la classification d'images\\n\",         \"- Visualis\u00e9 les filtres et les feature maps pour comprendre ce que \\\"voit\\\" le r\u00e9seau\\n\",         \"- \u00c9valu\u00e9 les performances du mod\u00e8le et sa robustesse face au bruit\\n\",         \"\\n\",         \"Les CNN sont la base de nombreuses applications modernes de vision par ordinateur comme la reconnaissance faciale, la d\u00e9tection d'objets, et bien d'autres.\"       ]     }   ],   \"metadata\": {     \"kernelspec\": {       \"display_name\": \"Python 3\",       \"language\": \"python\",       \"name\": \"python3\"     },     \"language_info\": {       \"codemirror_mode\": {         \"name\": \"ipython\",         \"version\": 3       },       \"file_extension\": \".py\",       \"mimetype\": \"text/x-python\",       \"name\": \"python\",       \"nbconvert_exporter\": \"python\",       \"pygments_lexer\": \"ipython3\",       \"version\": \"3.8.5\"     }   },   \"nbformat\": 4,   \"nbformat_minor\": 4 }"},{"location":"ressources/notebooks/hello-world-dl/","title":"Hello world dl","text":"In\u00a0[2]: Copied! <pre>{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# \ud83d\ude80 Hello World du Deep Learning\\n\",\n    \"\\n\",\n    \"## Reconnaissance de chiffres manuscrits avec TensorFlow et Keras\\n\",\n    \"\\n\",\n    \"### Objectifs de ce notebook\\n\",\n    \"\\n\",\n    \"- Charger et pr\u00e9parer un jeu de donn\u00e9es de chiffres manuscrits\\n\",\n    \"- Cr\u00e9er un r\u00e9seau de neurones simple\\n\",\n    \"- Entra\u00eener le mod\u00e8le\\n\",\n    \"- Visualiser les r\u00e9sultats\\n\",\n    \"- Tester le mod\u00e8le avec vos propres dessins\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# Importation des biblioth\u00e8ques n\u00e9cessaires\\n\",\n    \"import numpy as np\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import tensorflow as tf\\n\",\n    \"from tensorflow import keras\\n\",\n    \"from tensorflow.keras import layers\\n\",\n    \"\\n\",\n    \"# V\u00e9rification de la version de TensorFlow\\n\",\n    \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\\n\",\n    \"print(f\\\"Keras version: {keras.__version__}\\\")\\n\",\n    \"\\n\",\n    \"# V\u00e9rification du GPU (si disponible)\\n\",\n    \"print(\\\"GPU disponible :\\\", tf.test.is_gpu_available())\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# Chargement du dataset MNIST\\n\",\n    \"(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\\n\",\n    \"\\n\",\n    \"# Pr\u00e9traitement des donn\u00e9es\\n\",\n    \"X_train = X_train.reshape((60000, 28, 28, 1)) / 255.0\\n\",\n    \"X_test = X_test.reshape((10000, 28, 28, 1)) / 255.0\\n\",\n    \"\\n\",\n    \"# Conversion des labels en cat\u00e9gories\\n\",\n    \"y_train = keras.utils.to_categorical(y_train)\\n\",\n    \"y_test = keras.utils.to_categorical(y_test)\\n\",\n    \"\\n\",\n    \"# Affichage de quelques exemples\\n\",\n    \"plt.figure(figsize=(10, 2))\\n\",\n    \"for i in range(10):\\n\",\n    \"    plt.subplot(1, 10, i+1)\\n\",\n    \"    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\\n\",\n    \"    plt.axis('off')\\n\",\n    \"plt.suptitle(\\\"Exemples de chiffres manuscrits\\\")\\n\",\n    \"plt.show()\\n\",\n    \"\\n\",\n    \"print(f\\\"Nombre d'exemples d'entra\u00eenement : {X_train.shape[0]}\\\")\\n\",\n    \"print(f\\\"Nombre d'exemples de test : {X_test.shape[0]}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# Cr\u00e9ation du mod\u00e8le de r\u00e9seau de neurones\\n\",\n    \"model = keras.Sequential([\\n\",\n    \"    # Couche de convolution\\n\",\n    \"    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\\n\",\n    \"    layers.MaxPooling2D((2, 2)),\\n\",\n    \"    \\n\",\n    \"    # Couche de convolution suppl\u00e9mentaire\\n\",\n    \"    layers.Conv2D(64, (3, 3), activation='relu'),\\n\",\n    \"    layers.MaxPooling2D((2, 2)),\\n\",\n    \"    \\n\",\n    \"    # Aplatissement\\n\",\n    \"    layers.Flatten(),\\n\",\n    \"    \\n\",\n    \"    # Couche dense\\n\",\n    \"    layers.Dense(64, activation='relu'),\\n\",\n    \"    \\n\",\n    \"    # Couche de sortie\\n\",\n    \"    layers.Dense(10, activation='softmax')\\n\",\n    \"])\\n\",\n    \"\\n\",\n    \"# Compilation du mod\u00e8le\\n\",\n    \"model.compile(\\n\",\n    \"    optimizer='adam',\\n\",\n    \"    loss='categorical_crossentropy',\\n\",\n    \"    metrics=['accuracy']\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Affichage du r\u00e9sum\u00e9 du mod\u00e8le\\n\",\n    \"model.summary()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\\n\",\n   \"execution_count\": null,\\n\",\n   \"metadata\": {},\\n\",\n   \"source\": [\n    \"# Entra\u00eenement du mod\u00e8le\\n\",\n    \"# Note : Nombre d'\u00e9poques r\u00e9duit pour la d\u00e9monstration\\n\",\n    \"history = model.fit(\\n\",\n    \"    X_train, y_train,\\n\",\n    \"    epochs=5,\\n\",\n    \"    batch_size=64,\\n\",\n    \"    validation_split=0.2,\\n\",\n    \"    verbose=1\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# \u00c9valuation du mod\u00e8le\\n\",\n    \"test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\\n\",\n    \"print(f\\\"\\\\nPr\u00e9cision sur l'ensemble de test : {test_accuracy*100:.2f}%\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\\n\",\n   \"execution_count\": null,\\n\",\n   \"metadata\": {},\\n\",\n   \"source\": [\n    \"# Visualisation de la pr\u00e9cision et de la perte\\n\",\n    \"plt.figure(figsize=(12, 4))\\n\",\n    \"\\n\",\n    \"# Pr\u00e9cision\\n\",\n    \"plt.subplot(1, 2, 1)\\n\",\n    \"plt.plot(history.history['accuracy'], label='Pr\u00e9cision entra\u00eenement')\\n\",\n    \"plt.plot(history.history['val_accuracy'], label='Pr\u00e9cision validation')\\n\",\n    \"plt.title('Pr\u00e9cision du mod\u00e8le')\\n\",\n    \"plt.xlabel('\u00c9poque')\\n\",\n    \"plt.ylabel('Pr\u00e9cision')\\n\",\n    \"plt.legend()\\n\",\n    \"\\n\",\n    \"# Perte\\n\",\n    \"plt.subplot(1, 2, 2)\\n\",\n    \"plt.plot(history.history['loss'], label='Perte entra\u00eenement')\\n\",\n    \"plt.plot(history.history['val_loss'], label='Perte validation')\\n\",\n    \"plt.title('Perte du mod\u00e8le')\\n\",\n    \"plt.xlabel('\u00c9poque')\\n\",\n    \"plt.ylabel('Perte')\\n\",\n    \"plt.legend()\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\\n\",\n   \"execution_count\": null,\\n\",\n   \"metadata\": {},\\n\",\n   \"source\": [\n    \"# Pr\u00e9dictions et visualisation\\n\",\n    \"# Pr\u00e9dire sur quelques images de test\\n\",\n    \"predictions = model.predict(X_test[:10])\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(15, 6))\\n\",\n    \"for i in range(10):\\n\",\n    \"    plt.subplot(2, 10, i+1)\\n\",\n    \"    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\\n\",\n    \"    plt.axis('off')\\n\",\n    \"    \\n\",\n    \"    plt.subplot(2, 10, i+11)\\n\",\n    \"    plt.bar(range(10), predictions[i])\\n\",\n    \"    plt.title(f\\\"Pr\u00e9diction: {np.argmax(predictions[i])}\\\")\\n\",\n    \"    plt.xticks(range(10))\\n\",\n    \"    plt.ylim(0, 1)\\n\",\n    \"\\n\",\n    \"plt.suptitle(\\\"Pr\u00e9dictions du mod\u00e8le\\\")\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\\n\",\n   \"metadata\": {},\\n\",\n   \"source\": [\n    \"## \ud83e\udd14 Questions de r\u00e9flexion\\n\",\n    \"\\n\",\n    \"1. Que se passe-t-il si vous augmentez le nombre d'\u00e9poques ?\\n\",\n    \"2. Comment changeriez-vous l'architecture du r\u00e9seau pour am\u00e9liorer les performances ?\\n\",\n    \"3. Quelles diff\u00e9rences observez-vous entre la pr\u00e9cision d'entra\u00eenement et de validation ?\\n\",\n    \"\\n\",\n    \"## \ud83d\ude80 D\u00e9fis\\n\",\n    \"\\n\",\n    \"- Essayez de modifier le nombre de neurones dans les couches denses\\n\",\n    \"- Changez la fonction d'activation dans certaines couches\\n\",\n    \"- Ajoutez une couche de dropout pour r\u00e9duire le surapprentissage\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n</pre> {  \"cells\": [   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"# \ud83d\ude80 Hello World du Deep Learning\\n\",     \"\\n\",     \"## Reconnaissance de chiffres manuscrits avec TensorFlow et Keras\\n\",     \"\\n\",     \"### Objectifs de ce notebook\\n\",     \"\\n\",     \"- Charger et pr\u00e9parer un jeu de donn\u00e9es de chiffres manuscrits\\n\",     \"- Cr\u00e9er un r\u00e9seau de neurones simple\\n\",     \"- Entra\u00eener le mod\u00e8le\\n\",     \"- Visualiser les r\u00e9sultats\\n\",     \"- Tester le mod\u00e8le avec vos propres dessins\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"source\": [     \"# Importation des biblioth\u00e8ques n\u00e9cessaires\\n\",     \"import numpy as np\\n\",     \"import matplotlib.pyplot as plt\\n\",     \"import tensorflow as tf\\n\",     \"from tensorflow import keras\\n\",     \"from tensorflow.keras import layers\\n\",     \"\\n\",     \"# V\u00e9rification de la version de TensorFlow\\n\",     \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\\n\",     \"print(f\\\"Keras version: {keras.__version__}\\\")\\n\",     \"\\n\",     \"# V\u00e9rification du GPU (si disponible)\\n\",     \"print(\\\"GPU disponible :\\\", tf.test.is_gpu_available())\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"source\": [     \"# Chargement du dataset MNIST\\n\",     \"(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\\n\",     \"\\n\",     \"# Pr\u00e9traitement des donn\u00e9es\\n\",     \"X_train = X_train.reshape((60000, 28, 28, 1)) / 255.0\\n\",     \"X_test = X_test.reshape((10000, 28, 28, 1)) / 255.0\\n\",     \"\\n\",     \"# Conversion des labels en cat\u00e9gories\\n\",     \"y_train = keras.utils.to_categorical(y_train)\\n\",     \"y_test = keras.utils.to_categorical(y_test)\\n\",     \"\\n\",     \"# Affichage de quelques exemples\\n\",     \"plt.figure(figsize=(10, 2))\\n\",     \"for i in range(10):\\n\",     \"    plt.subplot(1, 10, i+1)\\n\",     \"    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\\n\",     \"    plt.axis('off')\\n\",     \"plt.suptitle(\\\"Exemples de chiffres manuscrits\\\")\\n\",     \"plt.show()\\n\",     \"\\n\",     \"print(f\\\"Nombre d'exemples d'entra\u00eenement : {X_train.shape[0]}\\\")\\n\",     \"print(f\\\"Nombre d'exemples de test : {X_test.shape[0]}\\\")\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"source\": [     \"# Cr\u00e9ation du mod\u00e8le de r\u00e9seau de neurones\\n\",     \"model = keras.Sequential([\\n\",     \"    # Couche de convolution\\n\",     \"    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\\n\",     \"    layers.MaxPooling2D((2, 2)),\\n\",     \"    \\n\",     \"    # Couche de convolution suppl\u00e9mentaire\\n\",     \"    layers.Conv2D(64, (3, 3), activation='relu'),\\n\",     \"    layers.MaxPooling2D((2, 2)),\\n\",     \"    \\n\",     \"    # Aplatissement\\n\",     \"    layers.Flatten(),\\n\",     \"    \\n\",     \"    # Couche dense\\n\",     \"    layers.Dense(64, activation='relu'),\\n\",     \"    \\n\",     \"    # Couche de sortie\\n\",     \"    layers.Dense(10, activation='softmax')\\n\",     \"])\\n\",     \"\\n\",     \"# Compilation du mod\u00e8le\\n\",     \"model.compile(\\n\",     \"    optimizer='adam',\\n\",     \"    loss='categorical_crossentropy',\\n\",     \"    metrics=['accuracy']\\n\",     \")\\n\",     \"\\n\",     \"# Affichage du r\u00e9sum\u00e9 du mod\u00e8le\\n\",     \"model.summary()\"    ]   },   {    \"cell_type\": \"code\",\\n\",    \"execution_count\": null,\\n\",    \"metadata\": {},\\n\",    \"source\": [     \"# Entra\u00eenement du mod\u00e8le\\n\",     \"# Note : Nombre d'\u00e9poques r\u00e9duit pour la d\u00e9monstration\\n\",     \"history = model.fit(\\n\",     \"    X_train, y_train,\\n\",     \"    epochs=5,\\n\",     \"    batch_size=64,\\n\",     \"    validation_split=0.2,\\n\",     \"    verbose=1\\n\",     \")\\n\",     \"\\n\",     \"# \u00c9valuation du mod\u00e8le\\n\",     \"test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\\n\",     \"print(f\\\"\\\\nPr\u00e9cision sur l'ensemble de test : {test_accuracy*100:.2f}%\\\")\"    ]   },   {    \"cell_type\": \"code\",\\n\",    \"execution_count\": null,\\n\",    \"metadata\": {},\\n\",    \"source\": [     \"# Visualisation de la pr\u00e9cision et de la perte\\n\",     \"plt.figure(figsize=(12, 4))\\n\",     \"\\n\",     \"# Pr\u00e9cision\\n\",     \"plt.subplot(1, 2, 1)\\n\",     \"plt.plot(history.history['accuracy'], label='Pr\u00e9cision entra\u00eenement')\\n\",     \"plt.plot(history.history['val_accuracy'], label='Pr\u00e9cision validation')\\n\",     \"plt.title('Pr\u00e9cision du mod\u00e8le')\\n\",     \"plt.xlabel('\u00c9poque')\\n\",     \"plt.ylabel('Pr\u00e9cision')\\n\",     \"plt.legend()\\n\",     \"\\n\",     \"# Perte\\n\",     \"plt.subplot(1, 2, 2)\\n\",     \"plt.plot(history.history['loss'], label='Perte entra\u00eenement')\\n\",     \"plt.plot(history.history['val_loss'], label='Perte validation')\\n\",     \"plt.title('Perte du mod\u00e8le')\\n\",     \"plt.xlabel('\u00c9poque')\\n\",     \"plt.ylabel('Perte')\\n\",     \"plt.legend()\\n\",     \"\\n\",     \"plt.tight_layout()\\n\",     \"plt.show()\"    ]   },   {    \"cell_type\": \"code\",\\n\",    \"execution_count\": null,\\n\",    \"metadata\": {},\\n\",    \"source\": [     \"# Pr\u00e9dictions et visualisation\\n\",     \"# Pr\u00e9dire sur quelques images de test\\n\",     \"predictions = model.predict(X_test[:10])\\n\",     \"\\n\",     \"plt.figure(figsize=(15, 6))\\n\",     \"for i in range(10):\\n\",     \"    plt.subplot(2, 10, i+1)\\n\",     \"    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\\n\",     \"    plt.axis('off')\\n\",     \"    \\n\",     \"    plt.subplot(2, 10, i+11)\\n\",     \"    plt.bar(range(10), predictions[i])\\n\",     \"    plt.title(f\\\"Pr\u00e9diction: {np.argmax(predictions[i])}\\\")\\n\",     \"    plt.xticks(range(10))\\n\",     \"    plt.ylim(0, 1)\\n\",     \"\\n\",     \"plt.suptitle(\\\"Pr\u00e9dictions du mod\u00e8le\\\")\\n\",     \"plt.tight_layout()\\n\",     \"plt.show()\"    ]   },   {    \"cell_type\": \"markdown\",\\n\",    \"metadata\": {},\\n\",    \"source\": [     \"## \ud83e\udd14 Questions de r\u00e9flexion\\n\",     \"\\n\",     \"1. Que se passe-t-il si vous augmentez le nombre d'\u00e9poques ?\\n\",     \"2. Comment changeriez-vous l'architecture du r\u00e9seau pour am\u00e9liorer les performances ?\\n\",     \"3. Quelles diff\u00e9rences observez-vous entre la pr\u00e9cision d'entra\u00eenement et de validation ?\\n\",     \"\\n\",     \"## \ud83d\ude80 D\u00e9fis\\n\",     \"\\n\",     \"- Essayez de modifier le nombre de neurones dans les couches denses\\n\",     \"- Changez la fonction d'activation dans certaines couches\\n\",     \"- Ajoutez une couche de dropout pour r\u00e9duire le surapprentissage\"    ]   }  ],  \"metadata\": {   \"kernelspec\": {    \"display_name\": \"Python 3\",    \"language\": \"python\",    \"name\": \"python3\"   },   \"language_info\": {    \"name\": \"python\",    \"version\": \"3.8.0\"   }  },  \"nbformat\": 4,  \"nbformat_minor\": 2 } <pre>\n  Cell In[2], line 106\n    \"cell_type\": \"code\",\\n\",\n                         ^\nSyntaxError: unexpected character after line continuation character\n</pre>"},{"location":"ressources/notebooks/hello-world-dl/","title":"\ud83d\ude80 Hello World du Deep Learning","text":""},{"location":"ressources/notebooks/hello-world-dl/#reconnaissance-de-chiffres-manuscrits-avec-tensorflow-et-keras","title":"Reconnaissance de chiffres manuscrits avec TensorFlow et Keras","text":""},{"location":"ressources/notebooks/hello-world-dl/#objectifs-de-ce-notebook","title":"Objectifs de ce notebook","text":"<ul> <li>Charger et pr\u00e9parer un jeu de donn\u00e9es de chiffres manuscrits</li> <li>Cr\u00e9er un r\u00e9seau de neurones simple</li> <li>Entra\u00eener le mod\u00e8le</li> <li>Visualiser les r\u00e9sultats</li> <li>Tester le mod\u00e8le avec vos propres dessins</li> </ul> <p>```python</p>"},{"location":"ressources/notebooks/hello-world-dl/#importation-des-bibliotheques-necessaires","title":"Importation des biblioth\u00e8ques n\u00e9cessaires","text":"<p>import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers</p>"},{"location":"ressources/notebooks/hello-world-dl/#verification-de-la-version-de-tensorflow","title":"V\u00e9rification de la version de TensorFlow","text":"<p>print(f\"TensorFlow version: {tf.version}\") print(f\"Keras version: {keras.version}\")</p>"},{"location":"ressources/notebooks/hello-world-dl/#verification-du-gpu-si-disponible","title":"V\u00e9rification du GPU (si disponible)","text":"<p>print(\"GPU disponible :\", tf.test.is_gpu_available())</p>"},{"location":"ressources/notebooks/model-improvement/","title":"Model improvement","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>{\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"# Challenge d'am\u00e9lioration de mod\u00e8le CNN\\n\",\n        \"\\n\",\n        \"## BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux et applications\\n\",\n        \"\\n\",\n        \"Ce notebook vous guidera \u00e0 travers un challenge d'am\u00e9lioration d'un mod\u00e8le CNN pour la classification d'images de v\u00eatements (Fashion MNIST). Vous partirez d'un mod\u00e8le de base volontairement sous-optimal et explorerez diff\u00e9rentes strat\u00e9gies pour am\u00e9liorer ses performances.\\n\",\n        \"\\n\",\n        \"### Objectifs d'apprentissage:\\n\",\n        \"- Diagnostiquer les faiblesses d'un mod\u00e8le de Deep Learning\\n\",\n        \"- Exp\u00e9rimenter avec diff\u00e9rentes architectures et hyperparam\u00e8tres\\n\",\n        \"- Appliquer des techniques d'optimisation (dropout, batch normalization, etc.)\\n\",\n        \"- Mesurer et comparer quantitativement les am\u00e9liorations\\n\",\n        \"- Documenter m\u00e9thodiquement les modifications et leurs impacts\\n\",\n        \"\\n\",\n        \"### Pr\u00e9requis:\\n\",\n        \"- Connaissances de base en TensorFlow/Keras\\n\",\n        \"- Compr\u00e9hension des principes des r\u00e9seaux CNN\\n\",\n        \"- Avoir suivi la premi\u00e8re partie du TP sur les CNN\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 1. Configuration de l'environnement\\n\",\n        \"\\n\",\n        \"Commen\u00e7ons par importer les biblioth\u00e8ques n\u00e9cessaires.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"import tensorflow as tf\\n\",\n        \"from tensorflow.keras.models import Sequential, load_model\\n\",\n        \"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\\n\",\n        \"from tensorflow.keras.optimizers import Adam, RMSprop, SGD\\n\",\n        \"from tensorflow.keras.preprocessing.image import ImageDataGenerator\\n\",\n        \"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\\n\",\n        \"from tensorflow.keras.datasets import fashion_mnist\\n\",\n        \"import numpy as np\\n\",\n        \"import matplotlib.pyplot as plt\\n\",\n        \"import pandas as pd\\n\",\n        \"import time\\n\",\n        \"import os\\n\",\n        \"import seaborn as sns\\n\",\n        \"from sklearn.metrics import confusion_matrix\\n\",\n        \"\\n\",\n        \"# Configuration pour reproductibilit\u00e9\\n\",\n        \"np.random.seed(42)\\n\",\n        \"tf.random.set_seed(42)\\n\",\n        \"\\n\",\n        \"# V\u00e9rifier la version de TensorFlow\\n\",\n        \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 2. Chargement du dataset Fashion MNIST\\n\",\n        \"\\n\",\n        \"Fashion MNIST est un dataset similaire au MNIST original, mais avec des images de v\u00eatements au lieu de chiffres. C'est un excellent dataset pour tester des mod\u00e8les de vision par ordinateur.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"print(\\\"Chargement du dataset Fashion MNIST...\\\")\\n\",\n        \"(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\\n\",\n        \"\\n\",\n        \"# Normalisation et reshape pour correspondre au format attendu par le CNN\\n\",\n        \"x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\\n\",\n        \"x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\\n\",\n        \"\\n\",\n        \"# Noms des classes pour l'affichage\\n\",\n        \"class_names = ['T-shirt/top', 'Pantalon', 'Pull', 'Robe', 'Manteau',\\n\",\n        \"               'Sandale', 'Chemise', 'Basket', 'Sac', 'Bottine']\\n\",\n        \"\\n\",\n        \"print(f\\\"Forme des donn\u00e9es d'entra\u00eenement: {x_train.shape}\\\")\\n\",\n        \"print(f\\\"Forme des donn\u00e9es de test: {x_test.shape}\\\")\\n\",\n        \"print(f\\\"Nombre de classes: {len(class_names)}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation de quelques exemples\\n\",\n        \"\\n\",\n        \"Examinons \u00e0 quoi ressemblent les images de notre dataset.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"plt.figure(figsize=(10, 10))\\n\",\n        \"for i in range(25):\\n\",\n        \"    plt.subplot(5, 5, i+1)\\n\",\n        \"    plt.xticks([])\\n\",\n        \"    plt.yticks([])\\n\",\n        \"    plt.grid(False)\\n\",\n        \"    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')\\n\",\n        \"    plt.xlabel(class_names[y_train[i]])\\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 3. Tableau de bord des r\u00e9sultats\\n\",\n        \"\\n\",\n        \"Cr\u00e9ons une classe pour suivre et comparer les performances des diff\u00e9rents mod\u00e8les que nous allons tester.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"class ModelImprovementDashboard:\\n\",\n        \"    \\\"\\\"\\\"Classe pour suivre et afficher les r\u00e9sultats des diff\u00e9rentes am\u00e9liorations\\\"\\\"\\\"\\n\",\n        \"    \\n\",\n        \"    def __init__(self):\\n\",\n        \"        self.results = []\\n\",\n        \"    \\n\",\n        \"    def add_result(self, model_name, metrics, notes=\\\"\\\"):\\n\",\n        \"        \\\"\\\"\\\"Ajoute un r\u00e9sultat au tableau de bord\\\"\\\"\\\"\\n\",\n        \"        result = {\\n\",\n        \"            'model_name': model_name,\\n\",\n        \"            'accuracy': metrics['test_accuracy'],\\n\",\n        \"            'loss': metrics['test_loss'],\\n\",\n        \"            'training_time': metrics['training_time'],\\n\",\n        \"            'epochs': metrics['epochs_completed'],\\n\",\n        \"            'notes': notes\\n\",\n        \"        }\\n\",\n        \"        self.results.append(result)\\n\",\n        \"    \\n\",\n        \"    def show_results(self):\\n\",\n        \"        \\\"\\\"\\\"Affiche un tableau comparatif des r\u00e9sultats\\\"\\\"\\\"\\n\",\n        \"        if not self.results:\\n\",\n        \"            print(\\\"Aucun r\u00e9sultat \u00e0 afficher.\\\")\\n\",\n        \"            return\\n\",\n        \"        \\n\",\n        \"        # Cr\u00e9er un DataFrame\\n\",\n        \"        df = pd.DataFrame(self.results)\\n\",\n        \"        \\n\",\n        \"        # Trier par pr\u00e9cision (descendant)\\n\",\n        \"        df = df.sort_values(by='accuracy', ascending=False)\\n\",\n        \"        \\n\",\n        \"        # Formater les colonnes\\n\",\n        \"        df['accuracy'] = df['accuracy'].apply(lambda x: f\\\"{x:.2f}%\\\")\\n\",\n        \"        df['loss'] = df['loss'].apply(lambda x: f\\\"{x:.4f}\\\")\\n\",\n        \"        df['training_time'] = df['training_time'].apply(lambda x: f\\\"{x:.2f}s\\\")\\n\",\n        \"        \\n\",\n        \"        print(\\\"\\\\n=== TABLEAU COMPARATIF DES MOD\u00c8LES ===\\\")\\n\",\n        \"        print(df)\\n\",\n        \"        \\n\",\n        \"        return df\\n\",\n        \"    \\n\",\n        \"    def plot_comparison(self):\\n\",\n        \"        \\\"\\\"\\\"Visualise la comparaison des mod\u00e8les\\\"\\\"\\\"\\n\",\n        \"        if not self.results:\\n\",\n        \"            print(\\\"Aucun r\u00e9sultat \u00e0 afficher.\\\")\\n\",\n        \"            return\\n\",\n        \"        \\n\",\n        \"        # Pr\u00e9parer les donn\u00e9es\\n\",\n        \"        models = [r['model_name'] for r in self.results]\\n\",\n        \"        accuracies = [float(r['accuracy'].strip('%')) for r in self.results]\\n\",\n        \"        times = [float(r['training_time'].strip('s')) for r in self.results]\\n\",\n        \"        \\n\",\n        \"        # Cr\u00e9er le graphique\\n\",\n        \"        plt.figure(figsize=(12, 6))\\n\",\n        \"        \\n\",\n        \"        # Graphique de pr\u00e9cision\\n\",\n        \"        plt.subplot(1, 2, 1)\\n\",\n        \"        bars = plt.bar(models, accuracies, color='skyblue')\\n\",\n        \"        plt.title('Comparaison des pr\u00e9cisions')\\n\",\n        \"        plt.xlabel('Mod\u00e8le')\\n\",\n        \"        plt.ylabel('Pr\u00e9cision (%)')\\n\",\n        \"        plt.xticks(rotation=45, ha='right')\\n\",\n        \"        \\n\",\n        \"        # Ajouter les valeurs sur les barres\\n\",\n        \"        for bar in bars:\\n\",\n        \"            height = bar.get_height()\\n\",\n        \"            plt.text(bar.get_x() + bar.get_width()/2., height,\\n\",\n        \"                     f'{height:.2f}%',\\n\",\n        \"                     ha='center', va='bottom')\\n\",\n        \"        \\n\",\n        \"        # Graphique de temps d'entra\u00eenement\\n\",\n        \"        plt.subplot(1, 2, 2)\\n\",\n        \"        bars = plt.bar(models, times, color='salmon')\\n\",\n        \"        plt.title('Comparaison des temps d\\\\'entra\u00eenement')\\n\",\n        \"        plt.xlabel('Mod\u00e8le')\\n\",\n        \"        plt.ylabel('Temps (secondes)')\\n\",\n        \"        plt.xticks(rotation=45, ha='right')\\n\",\n        \"        \\n\",\n        \"        # Ajouter les valeurs sur les barres\\n\",\n        \"        for bar in bars:\\n\",\n        \"            height = bar.get_height()\\n\",\n        \"            plt.text(bar.get_x() + bar.get_width()/2., height,\\n\",\n        \"                     f'{height:.2f}s',\\n\",\n        \"                     ha='center', va='bottom')\\n\",\n        \"        \\n\",\n        \"        plt.tight_layout()\\n\",\n        \"        plt.show()\\n\",\n        \"\\n\",\n        \"# Initialiser le tableau de bord\\n\",\n        \"dashboard = ModelImprovementDashboard()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 4. Fonctions d'\u00e9valuation de mod\u00e8le\\n\",\n        \"\\n\",\n        \"D\u00e9finissons des fonctions pour entra\u00eener, \u00e9valuer et visualiser les mod\u00e8les de mani\u00e8re coh\u00e9rente.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def evaluate_model(model, x_train, y_train, x_test, y_test, epochs=5, batch_size=128, data_augmentation=False):\\n\",\n        \"    \\\"\\\"\\\"Entra\u00eene et \u00e9value un mod\u00e8le, retourne les m\u00e9triques de performance\\\"\\\"\\\"\\n\",\n        \"    \\n\",\n        \"    # Configuration pour l'augmentation de donn\u00e9es (si activ\u00e9e)\\n\",\n        \"    if data_augmentation:\\n\",\n        \"        train_datagen = ImageDataGenerator(\\n\",\n        \"            rotation_range=10,\\n\",\n        \"            width_shift_range=0.1,\\n\",\n        \"            height_shift_range=0.1,\\n\",\n        \"            zoom_range=0.1,\\n\",\n        \"        )\\n\",\n        \"        train_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)\\n\",\n        \"    \\n\",\n        \"    # Callbacks pour am\u00e9liorer l'entra\u00eenement\\n\",\n        \"    callbacks = []\\n\",\n        \"    if epochs &gt; 5:\\n\",\n        \"        callbacks = [\\n\",\n        \"            EarlyStopping(patience=5, restore_best_weights=True),\\n\",\n        \"            ReduceLROnPlateau(factor=0.2, patience=3, min_lr=0.0001)\\n\",\n        \"        ]\\n\",\n        \"    \\n\",\n        \"    # Mesure du temps d'entra\u00eenement\\n\",\n        \"    start_time = time.time()\\n\",\n        \"    \\n\",\n        \"    # Entra\u00eenement du mod\u00e8le\\n\",\n        \"    if data_augmentation:\\n\",\n        \"        history = model.fit(\\n\",\n        \"            train_generator,\\n\",\n        \"            epochs=epochs,\\n\",\n        \"            steps_per_epoch=len(x_train) // batch_size,\\n\",\n        \"            validation_data=(x_test, y_test),\\n\",\n        \"            callbacks=callbacks,\\n\",\n        \"            verbose=1\\n\",\n        \"        )\\n\",\n        \"    else:\\n\",\n        \"        history = model.fit(\\n\",\n        \"            x_train, y_train,\\n\",\n        \"            batch_size=batch_size,\\n\",\n        \"            epochs=epochs,\\n\",\n        \"            validation_data=(x_test, y_test),\\n\",\n        \"            callbacks=callbacks,\\n\",\n        \"            verbose=1\\n\",\n        \"        )\\n\",\n        \"    \\n\",\n        \"    training_time = time.time() - start_time\\n\",\n        \"    \\n\",\n        \"    # \u00c9valuation du mod\u00e8le\\n\",\n        \"    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\\n\",\n        \"    \\n\",\n        \"    # Pr\u00e9parer les m\u00e9triques\\n\",\n        \"    metrics = {\\n\",\n        \"        'test_accuracy': test_acc * 100,\\n\",\n        \"        'test_loss': test_loss,\\n\",\n        \"        'training_time': training_time,\\n\",\n        \"        'epochs_completed': len(history.history['loss']),\\n\",\n        \"        'history': history\\n\",\n        \"    }\\n\",\n        \"    \\n\",\n        \"    return metrics\\n\",\n        \"\\n\",\n        \"def plot_training_history(history):\\n\",\n        \"    \\\"\\\"\\\"Visualise l'historique d'entra\u00eenement\\\"\\\"\\\"\\n\",\n        \"    plt.figure(figsize=(12, 5))\\n\",\n        \"    \\n\",\n        \"    # Graphique de pr\u00e9cision\\n\",\n        \"    plt.subplot(1, 2, 1)\\n\",\n        \"    plt.plot(history.history['accuracy'], label='Entra\u00eenement')\\n\",\n        \"    plt.plot(history.history['val_accuracy'], label='Validation')\\n\",\n        \"    plt.title('\u00c9volution de la pr\u00e9cision')\\n\",\n        \"    plt.xlabel('\u00c9poque')\\n\",\n        \"    plt.ylabel('Pr\u00e9cision')\\n\",\n        \"    plt.legend()\\n\",\n        \"    plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n        \"    \\n\",\n        \"    # Graphique de perte\\n\",\n        \"    plt.subplot(1, 2, 2)\\n\",\n        \"    plt.plot(history.history['loss'], label='Entra\u00eenement')\\n\",\n        \"    plt.plot(history.history['val_loss'], label='Validation')\\n\",\n        \"    plt.title('\u00c9volution de la perte')\\n\",\n        \"    plt.xlabel('\u00c9poque')\\n\",\n        \"    plt.ylabel('Perte')\\n\",\n        \"    plt.legend()\\n\",\n        \"    plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n        \"    \\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\\n\",\n        \"\\n\",\n        \"def plot_confusion_matrix(model, x_test, y_test):\\n\",\n        \"    \\\"\\\"\\\"Visualise la matrice de confusion du mod\u00e8le\\\"\\\"\\\"\\n\",\n        \"    # Pr\u00e9dictions\\n\",\n        \"    y_pred = model.predict(x_test)\\n\",\n        \"    y_pred_classes = np.argmax(y_pred, axis=1)\\n\",\n        \"    \\n\",\n        \"    # Calculer la matrice de confusion\\n\",\n        \"    conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",\n        \"    \\n\",\n        \"    # Visualisation\\n\",\n        \"    plt.figure(figsize=(10, 8))\\n\",\n        \"    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\\n\",\n        \"                xticklabels=class_names,\\n\",\n        \"                yticklabels=class_names)\\n\",\n        \"    plt.xlabel('Pr\u00e9dit')\\n\",\n        \"    plt.ylabel('R\u00e9el')\\n\",\n        \"    plt.title('Matrice de confusion')\\n\",\n        \"    plt.xticks(rotation=45, ha='right')\\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\\n\",\n        \"\\n\",\n        \"def show_misclassified_examples(model, x_test, y_test, n=10):\\n\",\n        \"    \\\"\\\"\\\"Affiche des exemples d'images mal classifi\u00e9es\\\"\\\"\\\"\\n\",\n        \"    predictions = model.predict(x_test)\\n\",\n        \"    predicted_classes = np.argmax(predictions, axis=1)\\n\",\n        \"    \\n\",\n        \"    # Trouver les erreurs\\n\",\n        \"    errors = (predicted_classes != y_test)\\n\",\n        \"    error_indices = np.where(errors)[0]\\n\",\n        \"    \\n\",\n        \"    if len(error_indices) == 0:\\n\",\n        \"        print(\\\"Aucune erreur trouv\u00e9e!\\\")\\n\",\n        \"        return\\n\",\n        \"    \\n\",\n        \"    # S\u00e9lectionner un \u00e9chantillon d'erreurs\\n\",\n        \"    sample_size = min(n, len(error_indices))\\n\",\n        \"    sample_indices = np.random.choice(error_indices, size=sample_size, replace=False)\\n\",\n        \"    \\n\",\n        \"    # Afficher les exemples\\n\",\n        \"    plt.figure(figsize=(15, 3*sample_size//5 + 3))\\n\",\n        \"    for i, idx in enumerate(sample_indices):\\n\",\n        \"        plt.subplot(sample_size//5 + 1, 5, i+1)\\n\",\n        \"        plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\\n\",\n        \"        plt.title(f\\\"R\u00e9el: {class_names[y_test[idx]]}\\\\nPr\u00e9dit: {class_names[predicted_classes[idx]]}\\\")\\n\",\n        \"        plt.axis('off')\\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 5. Mod\u00e8le de base (sous-optimal)\\n\",\n        \"\\n\",\n        \"Commen\u00e7ons par cr\u00e9er et \u00e9valuer un mod\u00e8le CNN de base, volontairement sous-optimal, qui servira de point de r\u00e9f\u00e9rence pour nos am\u00e9liorations.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def create_baseline_model():\\n\",\n        \"    \\\"\\\"\\\"Cr\u00e9e un mod\u00e8le CNN de base volontairement sous-performant\\\"\\\"\\\"\\n\",\n        \"    model = Sequential([\\n\",\n        \"        Conv2D(8, (3, 3), activation='relu', input_shape=(28, 28, 1)),\\n\",\n        \"        MaxPooling2D((2, 2)),\\n\",\n        \"        Flatten(),\\n\",\n        \"        Dense(16, activation='relu'),\\n\",\n        \"        Dense(10, activation='softmax')\\n\",\n        \"    ])\\n\",\n        \"    \\n\",\n        \"    model.compile(\\n\",\n        \"        optimizer=Adam(learning_rate=0.01),  # Learning rate trop \u00e9lev\u00e9\\n\",\n        \"        loss='sparse_categorical_crossentropy',\\n\",\n        \"        metrics=['accuracy']\\n\",\n        \"    )\\n\",\n        \"    \\n\",\n        \"    return model\\n\",\n        \"\\n\",\n        \"# Cr\u00e9er et afficher le mod\u00e8le de base\\n\",\n        \"baseline_model = create_baseline_model()\\n\",\n        \"baseline_model.summary()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Entra\u00eenement et \u00e9valuation du mod\u00e8le de base\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"print(\\\"\\\\n--- Mod\u00e8le de base ---\\\")\\n\",\n        \"baseline_metrics = evaluate_model(baseline_model, x_train, y_train, x_test, y_test, epochs=5)\\n\",\n        \"print(f\\\"Pr\u00e9cision du mod\u00e8le de base: {baseline_metrics['test_accuracy']:.2f}%\\\")\\n\",\n        \"print(f\\\"Temps d'entra\u00eenement: {baseline_metrics['training_time']:.2f} secondes\\\")\\n\",\n        \"\\n\",\n        \"# Visualiser l'historique d'entra\u00eenement\\n\",\n        \"plot_training_history(baseline_metrics['history'])\\n\",\n        \"\\n\",\n        \"# Ajouter au tableau de bord\\n\",\n        \"dashboard.add_result(\\\"Mod\u00e8le de base\\\", baseline_metrics, \\n\",\n        \"                     \\\"CNN simple, peu de filtres, learning rate \u00e9lev\u00e9\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Analyse des erreurs du mod\u00e8le de base\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Afficher la matrice de confusion\\n\",\n        \"plot_confusion_matrix(baseline_model, x_test, y_test)\\n\",\n        \"\\n\",\n        \"# Afficher des exemples d'erreurs\\n\",\n        \"print(\\\"\\\\nExemples d'erreurs de classification du mod\u00e8le de base:\\\")\\n\",\n        \"show_misclassified_examples(baseline_model, x_test, y_test)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### \ud83d\udd0d Diagnostic du mod\u00e8le de base\\n\",\n        \"\\n\",\n        \"Avant de passer aux am\u00e9liorations, analysons les probl\u00e8mes du mod\u00e8le de base :\\n\",\n        \"\\n\",\n        \"1. **Architecture trop simple** : \\n\",\n        \"   - Seulement 8 filtres dans la couche de convolution\\n\",\n        \"   - Une seule couche de convolution\\n\",\n        \"   - Seulement 16 neurones dans la couche dense\\n\",\n        \"   \\n\",\n        \"2. **Optimisation probl\u00e9matique** :\\n\",\n        \"   - Taux d'apprentissage trop \u00e9lev\u00e9 (0.01)\\n\",\n        \"   - Pas de r\u00e9gularisation (dropout, etc.)\\n\",\n        \"   - Nombre d'\u00e9poques potentiellement insuffisant\\n\",\n        \"   \\n\",\n        \"3. **Pr\u00e9traitement minimal** :\\n\",\n        \"   - Pas d'augmentation de donn\u00e9es\\n\",\n        \"   - Pas de normalisation batch\\n\",\n        \"\\n\",\n        \"Ces observations nous guideront dans nos tentatives d'am\u00e9lioration.\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 6. Premi\u00e8re am\u00e9lioration : Architecture plus profonde\\n\",\n        \"\\n\",\n        \"Pour notre premi\u00e8re am\u00e9lioration, nous allons :\\n\",\n        \"- Augmenter le nombre de filtres\\n\",\n        \"- Ajouter une couche de convolution suppl\u00e9mentaire\\n\",\n        \"- Augmenter le nombre de neurones dans la couche dense\\n\",\n        \"- R\u00e9duire le taux d'apprentissage\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def create_improved_model_1():\\n\",\n        \"    \\\"\\\"\\\"Premier exemple d'am\u00e9lioration: architecture plus profonde\\\"\\\"\\\"\\n\",\n        \"    model = Sequential([\\n\",\n        \"        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\\n\",\n        \"        MaxPooling2D((2, 2)),\\n\",\n        \"        Conv2D(64, (3, 3), activation='relu'),\\n\",\n        \"        MaxPooling2D((2, 2)),\\n\",\n        \"        Flatten(),\\n\",\n        \"        Dense(128, activation='relu'),\\n\",\n        \"        Dense(10, activation='softmax')\\n\",\n        \"    ])\\n\",\n        \"    \\n\",\n        \"    model.compile(\\n\",\n        \"        optimizer=Adam(learning_rate=0.001),  # Taux d'apprentissage r\u00e9duit\\n\",\n        \"        loss='sparse_categorical_crossentropy',\\n\",\n        \"        metrics=['accuracy']\\n\",\n        \"    )\\n\",\n        \"    \\n\",\n        \"    return model\\n\",\n        \"\\n\",\n        \"# Cr\u00e9er et afficher le mod\u00e8le am\u00e9lior\u00e9 1\\n\",\n        \"improved_model_1 = create_improved_model_1()\\n\",\n        \"improved_model_1.summary()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Entra\u00eenement et \u00e9valuation du mod\u00e8le am\u00e9lior\u00e9 1\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"print(\\\"\\\\n--- Mod\u00e8le am\u00e9lior\u00e9 1 ---\\\")\\n\",\n        \"improved_metrics_1 = evaluate_model(improved_model_1, x_train, y_train, x_test, y_test, epochs=10)\\n\",\n        \"print(f\\\"Pr\u00e9cision du mod\u00e8le am\u00e9lior\u00e9 1: {improved_metrics_1['test_accuracy']:.2f}%\\\")\\n\",\n        \"print(f\\\"Temps d'entra\u00eenement: {improved_metrics_1['training_time']:.2f} secondes\\\")\\n\",\n        \"\\n\",\n        \"# Visualiser l'historique d'entra\u00eenement\\n\",\n        \"plot_training_history(improved_metrics_1['history'])\\n\",\n        \"\\n\",\n        \"# Ajouter au tableau de bord\\n\",\n        \"dashboard.add_result(\\\"Mod\u00e8le am\u00e9lior\u00e9 1\\\", improved_metrics_1, \\n\",\n        \"                    \\\"Plus de filtres, couche suppl\u00e9mentaire, learning rate plus bas\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Analyse des r\u00e9sultats du mod\u00e8le am\u00e9lior\u00e9 1\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Visualiser les r\u00e9sultats de l'am\u00e9lioration\\n\",\n        \"print(\\\"Comparaison des mod\u00e8les jusqu'\u00e0 pr\u00e9sent:\\\")\\n\",\n        \"dashboard.show_results()\\n\",\n        \"\\n\",\n        \"# Voir les nouvelles erreurs\\n\",\n        \"print(\\\"\\\\nExemples d'erreurs apr\u00e8s la premi\u00e8re am\u00e9lioration:\\\")\\n\",\n        \"show_misclassified_examples(improved_model_1, x_test, y_test)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 7. Deuxi\u00e8me am\u00e9lioration : R\u00e9gularisation et augmentation de donn\u00e9es\\n\",\n        \"\\n\",\n        \"Pour notre deuxi\u00e8me am\u00e9lioration, nous allons :\\n\",\n        \"- Ajouter du dropout pour \u00e9viter le surapprentissage\\n\",\n        \"- Int\u00e9grer la normalisation par batch (batch normalization)\\n\",\n        \"- Utiliser l'augmentation de donn\u00e9es pour am\u00e9liorer la g\u00e9n\u00e9ralisation\\n\",\n        \"\\n\",\n        \"### Architecture du mod\u00e8le am\u00e9lior\u00e9 2\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def create_improved_model_2():\\n\",\n        \"    \\\"\\\"\\\"Deuxi\u00e8me exemple d'am\u00e9lioration: ajout de dropout et batch normalization\\\"\\\"\\\"\\n\",\n        \"    model = Sequential([\\n\",\n        \"        # Premi\u00e8re couche de convolution avec batch normalization\\n\",\n        \"        Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1)),\\n\",\n        \"        BatchNormalization(),\\n\",\n        \"        Activation('relu'),\\n\",\n        \"        MaxPooling2D((2, 2)),\\n\",\n        \"        \\n\",\n        \"        # Deuxi\u00e8me couche de convolution avec batch normalization\\n\",\n        \"        Conv2D(64, (3, 3), padding='same'),\\n\",\n        \"        BatchNormalization(),\\n\",\n        \"        Activation('relu'),\\n\",\n        \"        MaxPooling2D((2, 2)),\\n\",\n        \"        \\n\",\n        \"        # Aplatissement\\n\",\n        \"        Flatten(),\\n\",\n        \"        \\n\",\n        \"        # Couche dense avec batch normalization et dropout\\n\",\n        \"        Dense(128),\\n\",\n        \"        BatchNormalization(),\\n\",\n        \"        Activation('relu'),\\n\",\n        \"        Dropout(0.5),  # 50% de dropout pour la r\u00e9gularisation\\n\",\n        \"        \\n\",\n        \"        # Couche de sortie\\n\",\n        \"        Dense(10, activation='softmax')\\n\",\n        \"    ])\\n\",\n        \"    \\n\",\n        \"    model.compile(\\n\",\n        \"        optimizer=Adam(learning_rate=0.001),\\n\",\n        \"        loss='sparse_categorical_crossentropy',\\n\",\n        \"        metrics=['accuracy']\\n\",\n        \"    )\\n\",\n        \"    \\n\",\n        \"    return model\\n\",\n        \"\\n\",\n        \"# Cr\u00e9er et afficher le mod\u00e8le am\u00e9lior\u00e9 2\\n\",\n        \"improved_model_2 = create_improved_model_2()\\n\",\n        \"improved_model_2.summary()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Entra\u00eenement avec augmentation de donn\u00e9es\\n\",\n        \"\\n\",\n        \"Pour cette am\u00e9lioration, nous allons \u00e9galement utiliser l'augmentation de donn\u00e9es qui permet de g\u00e9n\u00e9rer artificiellement plus d'exemples d'entra\u00eenement en appliquant des transformations aux images existantes. Cela am\u00e9liore la robustesse du mod\u00e8le face aux variations qu'il pourrait rencontrer en conditions r\u00e9elles.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"print(\\\"\\\\n--- Mod\u00e8le am\u00e9lior\u00e9 2 (avec augmentation de donn\u00e9es) ---\\\")\\n\",\n        \"improved_metrics_2 = evaluate_model(improved_model_2, x_train, y_train, x_test, y_test, \\n\",\n        \"                                   epochs=15, data_augmentation=True)\\n\",\n        \"print(f\\\"Pr\u00e9cision du mod\u00e8le am\u00e9lior\u00e9 2: {improved_metrics_2['test_accuracy']:.2f}%\\\")\\n\",\n        \"print(f\\\"Temps d'entra\u00eenement: {improved_metrics_2['training_time']:.2f} secondes\\\")\\n\",\n        \"\\n\",\n        \"# Visualiser l'historique d'entra\u00eenement\\n\",\n        \"plot_training_history(improved_metrics_2['history'])\\n\",\n        \"\\n\",\n        \"# Ajouter au tableau de bord\\n\",\n        \"dashboard.add_result(\\\"Mod\u00e8le am\u00e9lior\u00e9 2\\\", improved_metrics_2, \\n\",\n        \"                    \\\"Dropout, BatchNorm, augmentation de donn\u00e9es\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Analyse des r\u00e9sultats du mod\u00e8le am\u00e9lior\u00e9 2\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Visualiser la matrice de confusion\\n\",\n        \"plot_confusion_matrix(improved_model_2, x_test, y_test)\\n\",\n        \"\\n\",\n        \"# Afficher des exemples d'erreurs\\n\",\n        \"print(\\\"\\\\nExemples d'erreurs apr\u00e8s la deuxi\u00e8me am\u00e9lioration:\\\")\\n\",\n        \"show_misclassified_examples(improved_model_2, x_test, y_test)\\n\",\n        \"\\n\",\n        \"# Comparer tous les mod\u00e8les\\n\",\n        \"dashboard.show_results()\\n\",\n        \"dashboard.plot_comparison()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 8. Cr\u00e9ation de votre propre mod\u00e8le am\u00e9lior\u00e9\\n\",\n        \"\\n\",\n        \"C'est maintenant \u00e0 vous de concevoir votre propre am\u00e9lioration! Vous pouvez explorer diff\u00e9rentes architectures, techniques d'optimisation, ou combinaisons d'approches.\\n\",\n        \"\\n\",\n        \"Voici quelques pistes d'am\u00e9lioration possibles:\\n\",\n        \"- Essayer diff\u00e9rentes architectures (plus/moins de couches, filtres, etc.)\\n\",\n        \"- Exp\u00e9rimenter avec d'autres optimiseurs (RMSprop, SGD avec momentum, etc.)\\n\",\n        \"- Tester diff\u00e9rentes techniques de r\u00e9gularisation\\n\",\n        \"- Modifier les param\u00e8tres d'augmentation de donn\u00e9es\\n\",\n        \"- Utiliser des connexions r\u00e9siduelles (comme dans les architectures ResNet)\\n\",\n        \"- Combiner les meilleures pratiques des mod\u00e8les pr\u00e9c\u00e9dents\"\n      ]\n    },\n.\\n\",\n        \"        \\n\",\n        \"        # Couche de sortie\\n\",\n        \"        Dense(10, activation='softmax')\\n\",\n        \"    ])\\n\",\n        \"    \\n\",\n        \"    # Compilation\\n\",\n        \"    model.compile(\\n\",\n        \"        optimizer='adam',  # Modifiez selon vos pr\u00e9f\u00e9rences\\n\",\n        \"        loss='sparse_categorical_crossentropy',\\n\",\n        \"        metrics=['accuracy']\\n\",\n        \"    )\\n\",\n        \"    \\n\",\n        \"    return model\\n\",\n        \"\\n\",\n        \"# Si vous \u00eates pr\u00eat \u00e0 tester votre mod\u00e8le, d\u00e9commentez les lignes suivantes\\n\",\n        \"#your_model = create_your_improved_model()\\n\",\n        \"#your_model.summary()\"\n      ]\n    },\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n   \n</pre> {   \"cells\": [     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"# Challenge d'am\u00e9lioration de mod\u00e8le CNN\\n\",         \"\\n\",         \"## BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux et applications\\n\",         \"\\n\",         \"Ce notebook vous guidera \u00e0 travers un challenge d'am\u00e9lioration d'un mod\u00e8le CNN pour la classification d'images de v\u00eatements (Fashion MNIST). Vous partirez d'un mod\u00e8le de base volontairement sous-optimal et explorerez diff\u00e9rentes strat\u00e9gies pour am\u00e9liorer ses performances.\\n\",         \"\\n\",         \"### Objectifs d'apprentissage:\\n\",         \"- Diagnostiquer les faiblesses d'un mod\u00e8le de Deep Learning\\n\",         \"- Exp\u00e9rimenter avec diff\u00e9rentes architectures et hyperparam\u00e8tres\\n\",         \"- Appliquer des techniques d'optimisation (dropout, batch normalization, etc.)\\n\",         \"- Mesurer et comparer quantitativement les am\u00e9liorations\\n\",         \"- Documenter m\u00e9thodiquement les modifications et leurs impacts\\n\",         \"\\n\",         \"### Pr\u00e9requis:\\n\",         \"- Connaissances de base en TensorFlow/Keras\\n\",         \"- Compr\u00e9hension des principes des r\u00e9seaux CNN\\n\",         \"- Avoir suivi la premi\u00e8re partie du TP sur les CNN\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 1. Configuration de l'environnement\\n\",         \"\\n\",         \"Commen\u00e7ons par importer les biblioth\u00e8ques n\u00e9cessaires.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"import tensorflow as tf\\n\",         \"from tensorflow.keras.models import Sequential, load_model\\n\",         \"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\\n\",         \"from tensorflow.keras.optimizers import Adam, RMSprop, SGD\\n\",         \"from tensorflow.keras.preprocessing.image import ImageDataGenerator\\n\",         \"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\\n\",         \"from tensorflow.keras.datasets import fashion_mnist\\n\",         \"import numpy as np\\n\",         \"import matplotlib.pyplot as plt\\n\",         \"import pandas as pd\\n\",         \"import time\\n\",         \"import os\\n\",         \"import seaborn as sns\\n\",         \"from sklearn.metrics import confusion_matrix\\n\",         \"\\n\",         \"# Configuration pour reproductibilit\u00e9\\n\",         \"np.random.seed(42)\\n\",         \"tf.random.set_seed(42)\\n\",         \"\\n\",         \"# V\u00e9rifier la version de TensorFlow\\n\",         \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 2. Chargement du dataset Fashion MNIST\\n\",         \"\\n\",         \"Fashion MNIST est un dataset similaire au MNIST original, mais avec des images de v\u00eatements au lieu de chiffres. C'est un excellent dataset pour tester des mod\u00e8les de vision par ordinateur.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"print(\\\"Chargement du dataset Fashion MNIST...\\\")\\n\",         \"(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\\n\",         \"\\n\",         \"# Normalisation et reshape pour correspondre au format attendu par le CNN\\n\",         \"x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\\n\",         \"x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\\n\",         \"\\n\",         \"# Noms des classes pour l'affichage\\n\",         \"class_names = ['T-shirt/top', 'Pantalon', 'Pull', 'Robe', 'Manteau',\\n\",         \"               'Sandale', 'Chemise', 'Basket', 'Sac', 'Bottine']\\n\",         \"\\n\",         \"print(f\\\"Forme des donn\u00e9es d'entra\u00eenement: {x_train.shape}\\\")\\n\",         \"print(f\\\"Forme des donn\u00e9es de test: {x_test.shape}\\\")\\n\",         \"print(f\\\"Nombre de classes: {len(class_names)}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation de quelques exemples\\n\",         \"\\n\",         \"Examinons \u00e0 quoi ressemblent les images de notre dataset.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"plt.figure(figsize=(10, 10))\\n\",         \"for i in range(25):\\n\",         \"    plt.subplot(5, 5, i+1)\\n\",         \"    plt.xticks([])\\n\",         \"    plt.yticks([])\\n\",         \"    plt.grid(False)\\n\",         \"    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')\\n\",         \"    plt.xlabel(class_names[y_train[i]])\\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 3. Tableau de bord des r\u00e9sultats\\n\",         \"\\n\",         \"Cr\u00e9ons une classe pour suivre et comparer les performances des diff\u00e9rents mod\u00e8les que nous allons tester.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"class ModelImprovementDashboard:\\n\",         \"    \\\"\\\"\\\"Classe pour suivre et afficher les r\u00e9sultats des diff\u00e9rentes am\u00e9liorations\\\"\\\"\\\"\\n\",         \"    \\n\",         \"    def __init__(self):\\n\",         \"        self.results = []\\n\",         \"    \\n\",         \"    def add_result(self, model_name, metrics, notes=\\\"\\\"):\\n\",         \"        \\\"\\\"\\\"Ajoute un r\u00e9sultat au tableau de bord\\\"\\\"\\\"\\n\",         \"        result = {\\n\",         \"            'model_name': model_name,\\n\",         \"            'accuracy': metrics['test_accuracy'],\\n\",         \"            'loss': metrics['test_loss'],\\n\",         \"            'training_time': metrics['training_time'],\\n\",         \"            'epochs': metrics['epochs_completed'],\\n\",         \"            'notes': notes\\n\",         \"        }\\n\",         \"        self.results.append(result)\\n\",         \"    \\n\",         \"    def show_results(self):\\n\",         \"        \\\"\\\"\\\"Affiche un tableau comparatif des r\u00e9sultats\\\"\\\"\\\"\\n\",         \"        if not self.results:\\n\",         \"            print(\\\"Aucun r\u00e9sultat \u00e0 afficher.\\\")\\n\",         \"            return\\n\",         \"        \\n\",         \"        # Cr\u00e9er un DataFrame\\n\",         \"        df = pd.DataFrame(self.results)\\n\",         \"        \\n\",         \"        # Trier par pr\u00e9cision (descendant)\\n\",         \"        df = df.sort_values(by='accuracy', ascending=False)\\n\",         \"        \\n\",         \"        # Formater les colonnes\\n\",         \"        df['accuracy'] = df['accuracy'].apply(lambda x: f\\\"{x:.2f}%\\\")\\n\",         \"        df['loss'] = df['loss'].apply(lambda x: f\\\"{x:.4f}\\\")\\n\",         \"        df['training_time'] = df['training_time'].apply(lambda x: f\\\"{x:.2f}s\\\")\\n\",         \"        \\n\",         \"        print(\\\"\\\\n=== TABLEAU COMPARATIF DES MOD\u00c8LES ===\\\")\\n\",         \"        print(df)\\n\",         \"        \\n\",         \"        return df\\n\",         \"    \\n\",         \"    def plot_comparison(self):\\n\",         \"        \\\"\\\"\\\"Visualise la comparaison des mod\u00e8les\\\"\\\"\\\"\\n\",         \"        if not self.results:\\n\",         \"            print(\\\"Aucun r\u00e9sultat \u00e0 afficher.\\\")\\n\",         \"            return\\n\",         \"        \\n\",         \"        # Pr\u00e9parer les donn\u00e9es\\n\",         \"        models = [r['model_name'] for r in self.results]\\n\",         \"        accuracies = [float(r['accuracy'].strip('%')) for r in self.results]\\n\",         \"        times = [float(r['training_time'].strip('s')) for r in self.results]\\n\",         \"        \\n\",         \"        # Cr\u00e9er le graphique\\n\",         \"        plt.figure(figsize=(12, 6))\\n\",         \"        \\n\",         \"        # Graphique de pr\u00e9cision\\n\",         \"        plt.subplot(1, 2, 1)\\n\",         \"        bars = plt.bar(models, accuracies, color='skyblue')\\n\",         \"        plt.title('Comparaison des pr\u00e9cisions')\\n\",         \"        plt.xlabel('Mod\u00e8le')\\n\",         \"        plt.ylabel('Pr\u00e9cision (%)')\\n\",         \"        plt.xticks(rotation=45, ha='right')\\n\",         \"        \\n\",         \"        # Ajouter les valeurs sur les barres\\n\",         \"        for bar in bars:\\n\",         \"            height = bar.get_height()\\n\",         \"            plt.text(bar.get_x() + bar.get_width()/2., height,\\n\",         \"                     f'{height:.2f}%',\\n\",         \"                     ha='center', va='bottom')\\n\",         \"        \\n\",         \"        # Graphique de temps d'entra\u00eenement\\n\",         \"        plt.subplot(1, 2, 2)\\n\",         \"        bars = plt.bar(models, times, color='salmon')\\n\",         \"        plt.title('Comparaison des temps d\\\\'entra\u00eenement')\\n\",         \"        plt.xlabel('Mod\u00e8le')\\n\",         \"        plt.ylabel('Temps (secondes)')\\n\",         \"        plt.xticks(rotation=45, ha='right')\\n\",         \"        \\n\",         \"        # Ajouter les valeurs sur les barres\\n\",         \"        for bar in bars:\\n\",         \"            height = bar.get_height()\\n\",         \"            plt.text(bar.get_x() + bar.get_width()/2., height,\\n\",         \"                     f'{height:.2f}s',\\n\",         \"                     ha='center', va='bottom')\\n\",         \"        \\n\",         \"        plt.tight_layout()\\n\",         \"        plt.show()\\n\",         \"\\n\",         \"# Initialiser le tableau de bord\\n\",         \"dashboard = ModelImprovementDashboard()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 4. Fonctions d'\u00e9valuation de mod\u00e8le\\n\",         \"\\n\",         \"D\u00e9finissons des fonctions pour entra\u00eener, \u00e9valuer et visualiser les mod\u00e8les de mani\u00e8re coh\u00e9rente.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def evaluate_model(model, x_train, y_train, x_test, y_test, epochs=5, batch_size=128, data_augmentation=False):\\n\",         \"    \\\"\\\"\\\"Entra\u00eene et \u00e9value un mod\u00e8le, retourne les m\u00e9triques de performance\\\"\\\"\\\"\\n\",         \"    \\n\",         \"    # Configuration pour l'augmentation de donn\u00e9es (si activ\u00e9e)\\n\",         \"    if data_augmentation:\\n\",         \"        train_datagen = ImageDataGenerator(\\n\",         \"            rotation_range=10,\\n\",         \"            width_shift_range=0.1,\\n\",         \"            height_shift_range=0.1,\\n\",         \"            zoom_range=0.1,\\n\",         \"        )\\n\",         \"        train_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)\\n\",         \"    \\n\",         \"    # Callbacks pour am\u00e9liorer l'entra\u00eenement\\n\",         \"    callbacks = []\\n\",         \"    if epochs &gt; 5:\\n\",         \"        callbacks = [\\n\",         \"            EarlyStopping(patience=5, restore_best_weights=True),\\n\",         \"            ReduceLROnPlateau(factor=0.2, patience=3, min_lr=0.0001)\\n\",         \"        ]\\n\",         \"    \\n\",         \"    # Mesure du temps d'entra\u00eenement\\n\",         \"    start_time = time.time()\\n\",         \"    \\n\",         \"    # Entra\u00eenement du mod\u00e8le\\n\",         \"    if data_augmentation:\\n\",         \"        history = model.fit(\\n\",         \"            train_generator,\\n\",         \"            epochs=epochs,\\n\",         \"            steps_per_epoch=len(x_train) // batch_size,\\n\",         \"            validation_data=(x_test, y_test),\\n\",         \"            callbacks=callbacks,\\n\",         \"            verbose=1\\n\",         \"        )\\n\",         \"    else:\\n\",         \"        history = model.fit(\\n\",         \"            x_train, y_train,\\n\",         \"            batch_size=batch_size,\\n\",         \"            epochs=epochs,\\n\",         \"            validation_data=(x_test, y_test),\\n\",         \"            callbacks=callbacks,\\n\",         \"            verbose=1\\n\",         \"        )\\n\",         \"    \\n\",         \"    training_time = time.time() - start_time\\n\",         \"    \\n\",         \"    # \u00c9valuation du mod\u00e8le\\n\",         \"    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\\n\",         \"    \\n\",         \"    # Pr\u00e9parer les m\u00e9triques\\n\",         \"    metrics = {\\n\",         \"        'test_accuracy': test_acc * 100,\\n\",         \"        'test_loss': test_loss,\\n\",         \"        'training_time': training_time,\\n\",         \"        'epochs_completed': len(history.history['loss']),\\n\",         \"        'history': history\\n\",         \"    }\\n\",         \"    \\n\",         \"    return metrics\\n\",         \"\\n\",         \"def plot_training_history(history):\\n\",         \"    \\\"\\\"\\\"Visualise l'historique d'entra\u00eenement\\\"\\\"\\\"\\n\",         \"    plt.figure(figsize=(12, 5))\\n\",         \"    \\n\",         \"    # Graphique de pr\u00e9cision\\n\",         \"    plt.subplot(1, 2, 1)\\n\",         \"    plt.plot(history.history['accuracy'], label='Entra\u00eenement')\\n\",         \"    plt.plot(history.history['val_accuracy'], label='Validation')\\n\",         \"    plt.title('\u00c9volution de la pr\u00e9cision')\\n\",         \"    plt.xlabel('\u00c9poque')\\n\",         \"    plt.ylabel('Pr\u00e9cision')\\n\",         \"    plt.legend()\\n\",         \"    plt.grid(True, linestyle='--', alpha=0.6)\\n\",         \"    \\n\",         \"    # Graphique de perte\\n\",         \"    plt.subplot(1, 2, 2)\\n\",         \"    plt.plot(history.history['loss'], label='Entra\u00eenement')\\n\",         \"    plt.plot(history.history['val_loss'], label='Validation')\\n\",         \"    plt.title('\u00c9volution de la perte')\\n\",         \"    plt.xlabel('\u00c9poque')\\n\",         \"    plt.ylabel('Perte')\\n\",         \"    plt.legend()\\n\",         \"    plt.grid(True, linestyle='--', alpha=0.6)\\n\",         \"    \\n\",         \"    plt.tight_layout()\\n\",         \"    plt.show()\\n\",         \"\\n\",         \"def plot_confusion_matrix(model, x_test, y_test):\\n\",         \"    \\\"\\\"\\\"Visualise la matrice de confusion du mod\u00e8le\\\"\\\"\\\"\\n\",         \"    # Pr\u00e9dictions\\n\",         \"    y_pred = model.predict(x_test)\\n\",         \"    y_pred_classes = np.argmax(y_pred, axis=1)\\n\",         \"    \\n\",         \"    # Calculer la matrice de confusion\\n\",         \"    conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",         \"    \\n\",         \"    # Visualisation\\n\",         \"    plt.figure(figsize=(10, 8))\\n\",         \"    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\\n\",         \"                xticklabels=class_names,\\n\",         \"                yticklabels=class_names)\\n\",         \"    plt.xlabel('Pr\u00e9dit')\\n\",         \"    plt.ylabel('R\u00e9el')\\n\",         \"    plt.title('Matrice de confusion')\\n\",         \"    plt.xticks(rotation=45, ha='right')\\n\",         \"    plt.tight_layout()\\n\",         \"    plt.show()\\n\",         \"\\n\",         \"def show_misclassified_examples(model, x_test, y_test, n=10):\\n\",         \"    \\\"\\\"\\\"Affiche des exemples d'images mal classifi\u00e9es\\\"\\\"\\\"\\n\",         \"    predictions = model.predict(x_test)\\n\",         \"    predicted_classes = np.argmax(predictions, axis=1)\\n\",         \"    \\n\",         \"    # Trouver les erreurs\\n\",         \"    errors = (predicted_classes != y_test)\\n\",         \"    error_indices = np.where(errors)[0]\\n\",         \"    \\n\",         \"    if len(error_indices) == 0:\\n\",         \"        print(\\\"Aucune erreur trouv\u00e9e!\\\")\\n\",         \"        return\\n\",         \"    \\n\",         \"    # S\u00e9lectionner un \u00e9chantillon d'erreurs\\n\",         \"    sample_size = min(n, len(error_indices))\\n\",         \"    sample_indices = np.random.choice(error_indices, size=sample_size, replace=False)\\n\",         \"    \\n\",         \"    # Afficher les exemples\\n\",         \"    plt.figure(figsize=(15, 3*sample_size//5 + 3))\\n\",         \"    for i, idx in enumerate(sample_indices):\\n\",         \"        plt.subplot(sample_size//5 + 1, 5, i+1)\\n\",         \"        plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\\n\",         \"        plt.title(f\\\"R\u00e9el: {class_names[y_test[idx]]}\\\\nPr\u00e9dit: {class_names[predicted_classes[idx]]}\\\")\\n\",         \"        plt.axis('off')\\n\",         \"    plt.tight_layout()\\n\",         \"    plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 5. Mod\u00e8le de base (sous-optimal)\\n\",         \"\\n\",         \"Commen\u00e7ons par cr\u00e9er et \u00e9valuer un mod\u00e8le CNN de base, volontairement sous-optimal, qui servira de point de r\u00e9f\u00e9rence pour nos am\u00e9liorations.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def create_baseline_model():\\n\",         \"    \\\"\\\"\\\"Cr\u00e9e un mod\u00e8le CNN de base volontairement sous-performant\\\"\\\"\\\"\\n\",         \"    model = Sequential([\\n\",         \"        Conv2D(8, (3, 3), activation='relu', input_shape=(28, 28, 1)),\\n\",         \"        MaxPooling2D((2, 2)),\\n\",         \"        Flatten(),\\n\",         \"        Dense(16, activation='relu'),\\n\",         \"        Dense(10, activation='softmax')\\n\",         \"    ])\\n\",         \"    \\n\",         \"    model.compile(\\n\",         \"        optimizer=Adam(learning_rate=0.01),  # Learning rate trop \u00e9lev\u00e9\\n\",         \"        loss='sparse_categorical_crossentropy',\\n\",         \"        metrics=['accuracy']\\n\",         \"    )\\n\",         \"    \\n\",         \"    return model\\n\",         \"\\n\",         \"# Cr\u00e9er et afficher le mod\u00e8le de base\\n\",         \"baseline_model = create_baseline_model()\\n\",         \"baseline_model.summary()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Entra\u00eenement et \u00e9valuation du mod\u00e8le de base\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"print(\\\"\\\\n--- Mod\u00e8le de base ---\\\")\\n\",         \"baseline_metrics = evaluate_model(baseline_model, x_train, y_train, x_test, y_test, epochs=5)\\n\",         \"print(f\\\"Pr\u00e9cision du mod\u00e8le de base: {baseline_metrics['test_accuracy']:.2f}%\\\")\\n\",         \"print(f\\\"Temps d'entra\u00eenement: {baseline_metrics['training_time']:.2f} secondes\\\")\\n\",         \"\\n\",         \"# Visualiser l'historique d'entra\u00eenement\\n\",         \"plot_training_history(baseline_metrics['history'])\\n\",         \"\\n\",         \"# Ajouter au tableau de bord\\n\",         \"dashboard.add_result(\\\"Mod\u00e8le de base\\\", baseline_metrics, \\n\",         \"                     \\\"CNN simple, peu de filtres, learning rate \u00e9lev\u00e9\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Analyse des erreurs du mod\u00e8le de base\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Afficher la matrice de confusion\\n\",         \"plot_confusion_matrix(baseline_model, x_test, y_test)\\n\",         \"\\n\",         \"# Afficher des exemples d'erreurs\\n\",         \"print(\\\"\\\\nExemples d'erreurs de classification du mod\u00e8le de base:\\\")\\n\",         \"show_misclassified_examples(baseline_model, x_test, y_test)\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### \ud83d\udd0d Diagnostic du mod\u00e8le de base\\n\",         \"\\n\",         \"Avant de passer aux am\u00e9liorations, analysons les probl\u00e8mes du mod\u00e8le de base :\\n\",         \"\\n\",         \"1. **Architecture trop simple** : \\n\",         \"   - Seulement 8 filtres dans la couche de convolution\\n\",         \"   - Une seule couche de convolution\\n\",         \"   - Seulement 16 neurones dans la couche dense\\n\",         \"   \\n\",         \"2. **Optimisation probl\u00e9matique** :\\n\",         \"   - Taux d'apprentissage trop \u00e9lev\u00e9 (0.01)\\n\",         \"   - Pas de r\u00e9gularisation (dropout, etc.)\\n\",         \"   - Nombre d'\u00e9poques potentiellement insuffisant\\n\",         \"   \\n\",         \"3. **Pr\u00e9traitement minimal** :\\n\",         \"   - Pas d'augmentation de donn\u00e9es\\n\",         \"   - Pas de normalisation batch\\n\",         \"\\n\",         \"Ces observations nous guideront dans nos tentatives d'am\u00e9lioration.\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 6. Premi\u00e8re am\u00e9lioration : Architecture plus profonde\\n\",         \"\\n\",         \"Pour notre premi\u00e8re am\u00e9lioration, nous allons :\\n\",         \"- Augmenter le nombre de filtres\\n\",         \"- Ajouter une couche de convolution suppl\u00e9mentaire\\n\",         \"- Augmenter le nombre de neurones dans la couche dense\\n\",         \"- R\u00e9duire le taux d'apprentissage\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def create_improved_model_1():\\n\",         \"    \\\"\\\"\\\"Premier exemple d'am\u00e9lioration: architecture plus profonde\\\"\\\"\\\"\\n\",         \"    model = Sequential([\\n\",         \"        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\\n\",         \"        MaxPooling2D((2, 2)),\\n\",         \"        Conv2D(64, (3, 3), activation='relu'),\\n\",         \"        MaxPooling2D((2, 2)),\\n\",         \"        Flatten(),\\n\",         \"        Dense(128, activation='relu'),\\n\",         \"        Dense(10, activation='softmax')\\n\",         \"    ])\\n\",         \"    \\n\",         \"    model.compile(\\n\",         \"        optimizer=Adam(learning_rate=0.001),  # Taux d'apprentissage r\u00e9duit\\n\",         \"        loss='sparse_categorical_crossentropy',\\n\",         \"        metrics=['accuracy']\\n\",         \"    )\\n\",         \"    \\n\",         \"    return model\\n\",         \"\\n\",         \"# Cr\u00e9er et afficher le mod\u00e8le am\u00e9lior\u00e9 1\\n\",         \"improved_model_1 = create_improved_model_1()\\n\",         \"improved_model_1.summary()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Entra\u00eenement et \u00e9valuation du mod\u00e8le am\u00e9lior\u00e9 1\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"print(\\\"\\\\n--- Mod\u00e8le am\u00e9lior\u00e9 1 ---\\\")\\n\",         \"improved_metrics_1 = evaluate_model(improved_model_1, x_train, y_train, x_test, y_test, epochs=10)\\n\",         \"print(f\\\"Pr\u00e9cision du mod\u00e8le am\u00e9lior\u00e9 1: {improved_metrics_1['test_accuracy']:.2f}%\\\")\\n\",         \"print(f\\\"Temps d'entra\u00eenement: {improved_metrics_1['training_time']:.2f} secondes\\\")\\n\",         \"\\n\",         \"# Visualiser l'historique d'entra\u00eenement\\n\",         \"plot_training_history(improved_metrics_1['history'])\\n\",         \"\\n\",         \"# Ajouter au tableau de bord\\n\",         \"dashboard.add_result(\\\"Mod\u00e8le am\u00e9lior\u00e9 1\\\", improved_metrics_1, \\n\",         \"                    \\\"Plus de filtres, couche suppl\u00e9mentaire, learning rate plus bas\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Analyse des r\u00e9sultats du mod\u00e8le am\u00e9lior\u00e9 1\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Visualiser les r\u00e9sultats de l'am\u00e9lioration\\n\",         \"print(\\\"Comparaison des mod\u00e8les jusqu'\u00e0 pr\u00e9sent:\\\")\\n\",         \"dashboard.show_results()\\n\",         \"\\n\",         \"# Voir les nouvelles erreurs\\n\",         \"print(\\\"\\\\nExemples d'erreurs apr\u00e8s la premi\u00e8re am\u00e9lioration:\\\")\\n\",         \"show_misclassified_examples(improved_model_1, x_test, y_test)\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 7. Deuxi\u00e8me am\u00e9lioration : R\u00e9gularisation et augmentation de donn\u00e9es\\n\",         \"\\n\",         \"Pour notre deuxi\u00e8me am\u00e9lioration, nous allons :\\n\",         \"- Ajouter du dropout pour \u00e9viter le surapprentissage\\n\",         \"- Int\u00e9grer la normalisation par batch (batch normalization)\\n\",         \"- Utiliser l'augmentation de donn\u00e9es pour am\u00e9liorer la g\u00e9n\u00e9ralisation\\n\",         \"\\n\",         \"### Architecture du mod\u00e8le am\u00e9lior\u00e9 2\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def create_improved_model_2():\\n\",         \"    \\\"\\\"\\\"Deuxi\u00e8me exemple d'am\u00e9lioration: ajout de dropout et batch normalization\\\"\\\"\\\"\\n\",         \"    model = Sequential([\\n\",         \"        # Premi\u00e8re couche de convolution avec batch normalization\\n\",         \"        Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1)),\\n\",         \"        BatchNormalization(),\\n\",         \"        Activation('relu'),\\n\",         \"        MaxPooling2D((2, 2)),\\n\",         \"        \\n\",         \"        # Deuxi\u00e8me couche de convolution avec batch normalization\\n\",         \"        Conv2D(64, (3, 3), padding='same'),\\n\",         \"        BatchNormalization(),\\n\",         \"        Activation('relu'),\\n\",         \"        MaxPooling2D((2, 2)),\\n\",         \"        \\n\",         \"        # Aplatissement\\n\",         \"        Flatten(),\\n\",         \"        \\n\",         \"        # Couche dense avec batch normalization et dropout\\n\",         \"        Dense(128),\\n\",         \"        BatchNormalization(),\\n\",         \"        Activation('relu'),\\n\",         \"        Dropout(0.5),  # 50% de dropout pour la r\u00e9gularisation\\n\",         \"        \\n\",         \"        # Couche de sortie\\n\",         \"        Dense(10, activation='softmax')\\n\",         \"    ])\\n\",         \"    \\n\",         \"    model.compile(\\n\",         \"        optimizer=Adam(learning_rate=0.001),\\n\",         \"        loss='sparse_categorical_crossentropy',\\n\",         \"        metrics=['accuracy']\\n\",         \"    )\\n\",         \"    \\n\",         \"    return model\\n\",         \"\\n\",         \"# Cr\u00e9er et afficher le mod\u00e8le am\u00e9lior\u00e9 2\\n\",         \"improved_model_2 = create_improved_model_2()\\n\",         \"improved_model_2.summary()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Entra\u00eenement avec augmentation de donn\u00e9es\\n\",         \"\\n\",         \"Pour cette am\u00e9lioration, nous allons \u00e9galement utiliser l'augmentation de donn\u00e9es qui permet de g\u00e9n\u00e9rer artificiellement plus d'exemples d'entra\u00eenement en appliquant des transformations aux images existantes. Cela am\u00e9liore la robustesse du mod\u00e8le face aux variations qu'il pourrait rencontrer en conditions r\u00e9elles.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"print(\\\"\\\\n--- Mod\u00e8le am\u00e9lior\u00e9 2 (avec augmentation de donn\u00e9es) ---\\\")\\n\",         \"improved_metrics_2 = evaluate_model(improved_model_2, x_train, y_train, x_test, y_test, \\n\",         \"                                   epochs=15, data_augmentation=True)\\n\",         \"print(f\\\"Pr\u00e9cision du mod\u00e8le am\u00e9lior\u00e9 2: {improved_metrics_2['test_accuracy']:.2f}%\\\")\\n\",         \"print(f\\\"Temps d'entra\u00eenement: {improved_metrics_2['training_time']:.2f} secondes\\\")\\n\",         \"\\n\",         \"# Visualiser l'historique d'entra\u00eenement\\n\",         \"plot_training_history(improved_metrics_2['history'])\\n\",         \"\\n\",         \"# Ajouter au tableau de bord\\n\",         \"dashboard.add_result(\\\"Mod\u00e8le am\u00e9lior\u00e9 2\\\", improved_metrics_2, \\n\",         \"                    \\\"Dropout, BatchNorm, augmentation de donn\u00e9es\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Analyse des r\u00e9sultats du mod\u00e8le am\u00e9lior\u00e9 2\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Visualiser la matrice de confusion\\n\",         \"plot_confusion_matrix(improved_model_2, x_test, y_test)\\n\",         \"\\n\",         \"# Afficher des exemples d'erreurs\\n\",         \"print(\\\"\\\\nExemples d'erreurs apr\u00e8s la deuxi\u00e8me am\u00e9lioration:\\\")\\n\",         \"show_misclassified_examples(improved_model_2, x_test, y_test)\\n\",         \"\\n\",         \"# Comparer tous les mod\u00e8les\\n\",         \"dashboard.show_results()\\n\",         \"dashboard.plot_comparison()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 8. Cr\u00e9ation de votre propre mod\u00e8le am\u00e9lior\u00e9\\n\",         \"\\n\",         \"C'est maintenant \u00e0 vous de concevoir votre propre am\u00e9lioration! Vous pouvez explorer diff\u00e9rentes architectures, techniques d'optimisation, ou combinaisons d'approches.\\n\",         \"\\n\",         \"Voici quelques pistes d'am\u00e9lioration possibles:\\n\",         \"- Essayer diff\u00e9rentes architectures (plus/moins de couches, filtres, etc.)\\n\",         \"- Exp\u00e9rimenter avec d'autres optimiseurs (RMSprop, SGD avec momentum, etc.)\\n\",         \"- Tester diff\u00e9rentes techniques de r\u00e9gularisation\\n\",         \"- Modifier les param\u00e8tres d'augmentation de donn\u00e9es\\n\",         \"- Utiliser des connexions r\u00e9siduelles (comme dans les architectures ResNet)\\n\",         \"- Combiner les meilleures pratiques des mod\u00e8les pr\u00e9c\u00e9dents\"       ]     }, .\\n\",         \"        \\n\",         \"        # Couche de sortie\\n\",         \"        Dense(10, activation='softmax')\\n\",         \"    ])\\n\",         \"    \\n\",         \"    # Compilation\\n\",         \"    model.compile(\\n\",         \"        optimizer='adam',  # Modifiez selon vos pr\u00e9f\u00e9rences\\n\",         \"        loss='sparse_categorical_crossentropy',\\n\",         \"        metrics=['accuracy']\\n\",         \"    )\\n\",         \"    \\n\",         \"    return model\\n\",         \"\\n\",         \"# Si vous \u00eates pr\u00eat \u00e0 tester votre mod\u00e8le, d\u00e9commentez les lignes suivantes\\n\",         \"#your_model = create_your_improved_model()\\n\",         \"#your_model.summary()\"       ]     },\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": ["},{"location":"ressources/notebooks/rnn-sequence/","title":"Rnn sequence","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>{\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"# RNN/LSTM pour l'analyse de sentiment\\n\",\n        \"\\n\",\n        \"##  S\u00e9ance 2: Types de r\u00e9seaux de neurones\\n\",\n        \"\\n\",\n        \"Ce notebook vous guidera \u00e0 travers l'impl\u00e9mentation d'un mod\u00e8le LSTM (Long Short-Term Memory) pour l'analyse de sentiment. Vous d\u00e9couvrirez comment les r\u00e9seaux r\u00e9currents peuvent \u00eatre utilis\u00e9s pour comprendre et classifier du texte.\\n\",\n        \"\\n\",\n        \"### Objectifs d'apprentissage:\\n\",\n        \"- Comprendre le pr\u00e9traitement du texte pour les mod\u00e8les de Deep Learning\\n\",\n        \"- D\u00e9couvrir l'architecture et le fonctionnement des r\u00e9seaux LSTM\\n\",\n        \"- Apprendre \u00e0 \u00e9valuer un mod\u00e8le d'analyse de sentiment\\n\",\n        \"- Visualiser et interpr\u00e9ter les embeddings de mots\\n\",\n        \"\\n\",\n        \"### Pr\u00e9requis:\\n\",\n        \"- Connaissances de base en Python\\n\",\n        \"- Notions fondamentales de r\u00e9seaux de neurones\\n\",\n        \"- Avoir suivi la s\u00e9ance 1 d'introduction au Deep Learning\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 1. Configuration de l'environnement\\n\",\n        \"\\n\",\n        \"Commen\u00e7ons par importer les biblioth\u00e8ques n\u00e9cessaires et configurer notre environnement.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"import numpy as np\\n\",\n        \"import matplotlib.pyplot as plt\\n\",\n        \"import tensorflow as tf\\n\",\n        \"from tensorflow.keras.models import Sequential\\n\",\n        \"from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\\n\",\n        \"from tensorflow.keras.preprocessing.text import Tokenizer\\n\",\n        \"from tensorflow.keras.preprocessing.sequence import pad_sequences\\n\",\n        \"from tensorflow.keras.callbacks import EarlyStopping\\n\",\n        \"import pandas as pd\\n\",\n        \"import re\\n\",\n        \"import time\\n\",\n        \"import seaborn as sns\\n\",\n        \"from sklearn.metrics import confusion_matrix, classification_report\\n\",\n        \"\\n\",\n        \"# Configuration pour reproductibilit\u00e9\\n\",\n        \"np.random.seed(42)\\n\",\n        \"tf.random.set_seed(42)\\n\",\n        \"\\n\",\n        \"# V\u00e9rifier la version de TensorFlow\\n\",\n        \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 2. Pr\u00e9paration des donn\u00e9es\\n\",\n        \"\\n\",\n        \"Pour ce TP, nous allons utiliser un petit dataset simul\u00e9 d'avis sur des films. Chaque avis sera class\u00e9 comme positif, n\u00e9gatif ou neutre.\\n\",\n        \"\\n\",\n        \"Dans un projet r\u00e9el, vous pourriez utiliser des datasets plus importants comme IMDB, Amazon Reviews, etc.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Cr\u00e9er un petit jeu de donn\u00e9es d'avis sur les films (simul\u00e9)\\n\",\n        \"reviews = [\\n\",\n        \"    \\\"Ce film \u00e9tait excellent, j'ai vraiment ador\u00e9 les performances des acteurs.\\\",\\n\",\n        \"    \\\"Une exp\u00e9rience cin\u00e9matographique incroyable, absolument \u00e0 voir !\\\",\\n\",\n        \"    \\\"Un chef-d'\u0153uvre du cin\u00e9ma, magnifiquement r\u00e9alis\u00e9.\\\",\\n\",\n        \"    \\\"J'ai beaucoup appr\u00e9ci\u00e9 l'histoire et les personnages \u00e9taient bien d\u00e9velopp\u00e9s.\\\",\\n\",\n        \"    \\\"Visuellement \u00e9poustouflant avec une histoire captivante.\\\",\\n\",\n        \"    \\\"Un film d\u00e9cevant avec un sc\u00e9nario plein de trous.\\\",\\n\",\n        \"    \\\"Vraiment terrible, je n'ai pas aim\u00e9 du tout.\\\",\\n\",\n        \"    \\\"Un g\u00e2chis complet de temps et d'argent, \u00e9vitez \u00e0 tout prix.\\\",\\n\",\n        \"    \\\"Ennuyeux et pr\u00e9visible, les acteurs semblaient d\u00e9sint\u00e9ress\u00e9s.\\\",\\n\",\n        \"    \\\"Une d\u00e9ception totale, l'intrigue ne fait aucun sens.\\\",\\n\",\n        \"    \\\"C'\u00e9tait correct, ni bon ni mauvais.\\\",\\n\",\n        \"    \\\"Un film moyen avec quelques bons moments.\\\",\\n\",\n        \"    \\\"Certaines sc\u00e8nes \u00e9taient bonnes, mais dans l'ensemble assez moyen.\\\",\\n\",\n        \"    \\\"Pas aussi bon que je l'esp\u00e9rais, mais pas horrible non plus.\\\",\\n\",\n        \"    \\\"Une histoire int\u00e9ressante mais mal ex\u00e9cut\u00e9e.\\\",\\n\",\n        \"    \\\"Un film brillant qui m'a fait r\u00e9fl\u00e9chir pendant des jours.\\\",\\n\",\n        \"    \\\"Absolument sublime, l'un des meilleurs films que j'ai jamais vus.\\\",\\n\",\n        \"    \\\"Un d\u00e9sastre total, je me suis endormi au milieu.\\\",\\n\",\n        \"    \\\"Pas du tout ce \u00e0 quoi je m'attendais, tr\u00e8s d\u00e9\u00e7u.\\\",\\n\",\n        \"    \\\"Le jeu d'acteur \u00e9tait fantastique, mais l'histoire \u00e9tait faible.\\\"\\n\",\n        \"]\\n\",\n        \"\\n\",\n        \"# Attribuer des sentiments (0 = n\u00e9gatif, 1 = neutre, 2 = positif)\\n\",\n        \"sentiments = [2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 0, 0, 1]\\n\",\n        \"\\n\",\n        \"# Convertir en DataFrame pour faciliter la manipulation\\n\",\n        \"df = pd.DataFrame({\\n\",\n        \"    'review': reviews,\\n\",\n        \"    'sentiment': sentiments\\n\",\n        \"})\\n\",\n        \"\\n\",\n        \"# Afficher quelques informations sur le dataset\\n\",\n        \"print(f\\\"Nombre total d'avis: {len(df)}\\\")\\n\",\n        \"print(f\\\"R\u00e9partition des sentiments: {df['sentiment'].value_counts().sort_index()}\\\")\\n\",\n        \"\\n\",\n        \"# Afficher quelques exemples\\n\",\n        \"print(\\\"\\\\nExemples d'avis:\\\")\\n\",\n        \"for sentiment in [0, 1, 2]:\\n\",\n        \"    sample = df[df['sentiment'] == sentiment].iloc[0]\\n\",\n        \"    print(f\\\"Sentiment {sentiment}: '{sample['review']}'\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation de la distribution des sentiments\\n\",\n        \"\\n\",\n        \"V\u00e9rifions que notre jeu de donn\u00e9es est \u00e9quilibr\u00e9 entre les diff\u00e9rentes classes.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Visualiser la distribution des sentiments\\n\",\n        \"plt.figure(figsize=(8, 5))\\n\",\n        \"ax = sns.countplot(x='sentiment', data=df)\\n\",\n        \"plt.title('Distribution des sentiments')\\n\",\n        \"plt.xlabel('Sentiment (0=n\u00e9gatif, 1=neutre, 2=positif)')\\n\",\n        \"plt.ylabel('Nombre d\\\\'avis')\\n\",\n        \"\\n\",\n        \"# Ajouter les valeurs sur les barres\\n\",\n        \"for p in ax.patches:\\n\",\n        \"    ax.annotate(f\\\"{p.get_height()}\\\", (p.get_x() + p.get_width()/2., p.get_height()),\\n\",\n        \"                ha='center', va='bottom')\\n\",\n        \"\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 3. Pr\u00e9traitement du texte\\n\",\n        \"\\n\",\n        \"Avant de pouvoir utiliser le texte avec notre mod\u00e8le LSTM, nous devons le pr\u00e9traiter. Cela implique plusieurs \u00e9tapes:\\n\",\n        \"1. Nettoyage (minuscules, suppression de ponctuation, etc.)\\n\",\n        \"2. Tokenisation (conversion du texte en s\u00e9quences de nombres)\\n\",\n        \"3. Padding (uniformisation de la longueur des s\u00e9quences)\\n\",\n        \"\\n\",\n        \"Commen\u00e7ons par le nettoyage de texte:\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def preprocess_text(text):\\n\",\n        \"    \\\"\\\"\\\"Fonction pour nettoyer et normaliser le texte\\\"\\\"\\\"\\n\",\n        \"    # Convertir en minuscules\\n\",\n        \"    text = text.lower()\\n\",\n        \"    # Supprimer la ponctuation et les caract\u00e8res sp\u00e9ciaux\\n\",\n        \"    text = re.sub(r'[^\\\\w\\\\s]', '', text)\\n\",\n        \"    # Supprimer les chiffres\\n\",\n        \"    text = re.sub(r'\\\\d+', '', text)\\n\",\n        \"    # Supprimer les espaces multiples\\n\",\n        \"    text = re.sub(r'\\\\s+', ' ', text).strip()\\n\",\n        \"    return text\\n\",\n        \"\\n\",\n        \"# Appliquer le pr\u00e9traitement \u00e0 nos avis\\n\",\n        \"df['processed_review'] = df['review'].apply(preprocess_text)\\n\",\n        \"\\n\",\n        \"# Afficher un exemple avant et apr\u00e8s pr\u00e9traitement\\n\",\n        \"example_idx = 0\\n\",\n        \"print(f\\\"Avant: {df['review'][example_idx]}\\\")\\n\",\n        \"print(f\\\"Apr\u00e8s: {df['processed_review'][example_idx]}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Tokenisation du texte\\n\",\n        \"\\n\",\n        \"La tokenisation convertit le texte en s\u00e9quences num\u00e9riques que notre r\u00e9seau de neurones peut traiter.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Configuration pour la tokenisation\\n\",\n        \"max_words = 1000  # Taille du vocabulaire\\n\",\n        \"max_len = 100     # Longueur maximale des s\u00e9quences\\n\",\n        \"\\n\",\n        \"# Cr\u00e9er et configurer le tokenizer\\n\",\n        \"tokenizer = Tokenizer(num_words=max_words, oov_token='&lt;OOV&gt;')\\n\",\n        \"tokenizer.fit_on_texts(df['processed_review'])\\n\",\n        \"\\n\",\n        \"# Convertir les textes en s\u00e9quences de tokens\\n\",\n        \"sequences = tokenizer.texts_to_sequences(df['processed_review'])\\n\",\n        \"\\n\",\n        \"# Appliquer le padding pour uniformiser la longueur des s\u00e9quences\\n\",\n        \"padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\\n\",\n        \"\\n\",\n        \"print(f\\\"Taille du vocabulaire: {len(tokenizer.word_index)}\\\")\\n\",\n        \"print(f\\\"Forme des s\u00e9quences apr\u00e8s padding: {padded_sequences.shape}\\\")\\n\",\n        \"\\n\",\n        \"# Afficher le mapping de quelques mots vers leurs tokens\\n\",\n        \"print(\\\"\\\\nExemples de mapping mot -&gt; token:\\\")\\n\",\n        \"sample_words = ['film', 'bon', 'mauvais', 'excellent', 'terrible']\\n\",\n        \"for word in sample_words:\\n\",\n        \"    if word in tokenizer.word_index:\\n\",\n        \"        print(f\\\"{word} -&gt; {tokenizer.word_index[word]}\\\")\\n\",\n        \"    else:\\n\",\n        \"        print(f\\\"{word} -&gt; Non trouv\u00e9 dans le vocabulaire\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation d'une s\u00e9quence tokenis\u00e9e\\n\",\n        \"\\n\",\n        \"Pour mieux comprendre la tokenisation, visualisons comment un avis est converti en s\u00e9quence de tokens.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def visualize_tokenized_sequence(text, tokens):\\n\",\n        \"    \\\"\\\"\\\"Visualise la correspondance entre mots et tokens\\\"\\\"\\\"\\n\",\n        \"    words = text.split()\\n\",\n        \"    plt.figure(figsize=(15, 3))\\n\",\n        \"    plt.bar(range(len(tokens)), tokens)\\n\",\n        \"    plt.xticks(range(len(tokens)), words, rotation=45, ha='right')\\n\",\n        \"    plt.ylabel('Token ID')\\n\",\n        \"    plt.title('Repr\u00e9sentation tokenis\u00e9e d\\\\'un avis')\\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\\n\",\n        \"\\n\",\n        \"sample_idx = 0\\n\",\n        \"sample_text = df['processed_review'][sample_idx].split()[:15]  # Limiter \u00e0 15 mots pour lisibilit\u00e9\\n\",\n        \"sample_tokens = sequences[sample_idx][:15]\\n\",\n        \"\\n\",\n        \"print(f\\\"Exemple d'avis: {' '.join(sample_text)}\\\")\\n\",\n        \"print(f\\\"Tokens correspondants: {sample_tokens}\\\")\\n\",\n        \"visualize_tokenized_sequence(' '.join(sample_text), sample_tokens)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 4. Division en ensembles d'entra\u00eenement et de test\\n\",\n        \"\\n\",\n        \"Avant de cr\u00e9er notre mod\u00e8le, divisons nos donn\u00e9es en ensembles d'entra\u00eenement et de test pour \u00e9valuer ses performances.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"from sklearn.model_selection import train_test_split\\n\",\n        \"\\n\",\n        \"# Division 70-30 avec stratification pour conserver la distribution des classes\\n\",\n        \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n        \"    padded_sequences, \\n\",\n        \"    df['sentiment'],\\n\",\n        \"    test_size=0.3,\\n\",\n        \"    random_state=42,\\n\",\n        \"    stratify=df['sentiment']  # Assurer une r\u00e9partition \u00e9quilibr\u00e9e des classes\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"print(f\\\"Forme des donn\u00e9es d'entra\u00eenement: {X_train.shape}\\\")\\n\",\n        \"print(f\\\"Forme des donn\u00e9es de test: {X_test.shape}\\\")\\n\",\n        \"print(f\\\"Distribution des classes (entra\u00eenement): {pd.Series(y_train).value_counts().sort_index()}\\\")\\n\",\n        \"print(f\\\"Distribution des classes (test): {pd.Series(y_test).value_counts().sort_index()}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 5. Cr\u00e9ation du mod\u00e8le LSTM\\n\",\n        \"\\n\",\n        \"Nous allons maintenant cr\u00e9er notre mod\u00e8le d'analyse de sentiment en utilisant une architecture LSTM bidirectionnelle.\\n\",\n        \"\\n\",\n        \"### Architecture du mod\u00e8le\\n\",\n        \"- **Couche d'embedding**: Convertit les tokens en vecteurs denses\\n\",\n        \"- **Couches LSTM bidirectionnelles**: Capture les d\u00e9pendances \u00e0 long terme dans les deux directions\\n\",\n        \"- **Dropout**: \u00c9vite le surapprentissage\\n\",\n        \"- **Couche dense finale**: Classification en 3 cat\u00e9gories (n\u00e9gatif, neutre, positif)\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Param\u00e8tres du mod\u00e8le\\n\",\n        \"embedding_dim = 32  # Dimension de l'espace d'embedding\\n\",\n        \"\\n\",\n        \"# Cr\u00e9ation du mod\u00e8le\\n\",\n        \"model = Sequential([\\n\",\n        \"    # Couche d'embedding pour convertir les tokens en vecteurs denses\\n\",\n        \"    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\\n\",\n        \"    \\n\",\n        \"    # Couche LSTM bidirectionnelle\\n\",\n        \"    Bidirectional(LSTM(64, return_sequences=True)),\\n\",\n        \"    \\n\",\n        \"    # Deuxi\u00e8me couche LSTM suivie de dropout pour r\u00e9gularisation\\n\",\n        \"    Bidirectional(LSTM(32)),\\n\",\n        \"    Dropout(0.5),\\n\",\n        \"    \\n\",\n        \"    # Couche de classification (3 classes: n\u00e9gatif, neutre, positif)\\n\",\n        \"    Dense(3, activation='softmax')\\n\",\n        \"])\\n\",\n        \"\\n\",\n        \"# Afficher un r\u00e9sum\u00e9 du mod\u00e8le\\n\",\n        \"model.summary()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### \ud83d\udca1 Points cl\u00e9s \u00e0 observer dans l'architecture\\n\",\n        \"\\n\",\n        \"- **LSTM bidirectionnel** : Lit le texte de gauche \u00e0 droite ET de droite \u00e0 gauche, capturant mieux le contexte\\n\",\n        \"- **return_sequences=True** : Permet d'empiler plusieurs couches LSTM\\n\",\n        \"- **Dropout** : D\u00e9sactive al\u00e9atoirement 50% des neurones pendant l'entra\u00eenement pour \u00e9viter le surapprentissage\\n\",\n        \"- **Activation softmax** : G\u00e9n\u00e8re une distribution de probabilit\u00e9 sur les 3 classes\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 6. Compilation et entra\u00eenement du mod\u00e8le\\n\",\n        \"\\n\",\n        \"Maintenant, compilons et entra\u00eenons notre mod\u00e8le LSTM.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Compiler le mod\u00e8le\\n\",\n        \"model.compile(\\n\",\n        \"    optimizer='adam',\\n\",\n        \"    loss='sparse_categorical_crossentropy',  # Pour les \u00e9tiquettes sous forme d'entiers (non one-hot)\\n\",\n        \"    metrics=['accuracy']\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"# Early stopping pour \u00e9viter le surapprentissage\\n\",\n        \"early_stopping = EarlyStopping(\\n\",\n        \"    monitor='val_loss',\\n\",\n        \"    patience=3,\\n\",\n        \"    restore_best_weights=True\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"# Mesure du temps d'entra\u00eenement\\n\",\n        \"start_time = time.time()\\n\",\n        \"\\n\",\n        \"# Entra\u00eenement du mod\u00e8le\\n\",\n        \"history = model.fit(\\n\",\n        \"    X_train, \\n\",\n        \"    y_train, \\n\",\n        \"    epochs=20,\\n\",\n        \"    batch_size=4,  # Petit batch size en raison de la petite taille du dataset\\n\",\n        \"    validation_split=0.2,  # 20% des donn\u00e9es d'entra\u00eenement serviront \u00e0 la validation\\n\",\n        \"    callbacks=[early_stopping],\\n\",\n        \"    verbose=1\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"training_time = time.time() - start_time\\n\",\n        \"print(f\\\"\\\\nTemps d'entra\u00eenement: {training_time:.2f} secondes\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation de l'\u00e9volution de l'entra\u00eenement\\n\",\n        \"\\n\",\n        \"Observons comment la pr\u00e9cision et la perte ont \u00e9volu\u00e9 au cours de l'entra\u00eenement.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Visualisation de l'entra\u00eenement\\n\",\n        \"plt.figure(figsize=(12, 5))\\n\",\n        \"\\n\",\n        \"# Graphique de pr\u00e9cision\\n\",\n        \"plt.subplot(1, 2, 1)\\n\",\n        \"plt.plot(history.history['accuracy'], label='Entra\u00eenement')\\n\",\n        \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",\n        \"plt.title('\u00c9volution de la pr\u00e9cision')\\n\",\n        \"plt.xlabel('\u00c9poque')\\n\",\n        \"plt.ylabel('Pr\u00e9cision')\\n\",\n        \"plt.legend()\\n\",\n        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n        \"\\n\",\n        \"# Graphique de perte\\n\",\n        \"plt.subplot(1, 2, 2)\\n\",\n        \"plt.plot(history.history['loss'], label='Entra\u00eenement')\\n\",\n        \"plt.plot(history.history['val_loss'], label='Validation')\\n\",\n        \"plt.title('\u00c9volution de la perte')\\n\",\n        \"plt.xlabel('\u00c9poque')\\n\",\n        \"plt.ylabel('Perte')\\n\",\n        \"plt.legend()\\n\",\n        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n        \"\\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 7. \u00c9valuation du mod\u00e8le\\n\",\n        \"\\n\",\n        \"Maintenant, \u00e9valuons les performances de notre mod\u00e8le sur l'ensemble de test.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# \u00c9valuation sur l'ensemble de test\\n\",\n        \"test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\\n\",\n        \"print(f\\\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",\n        \"\\n\",\n        \"# G\u00e9n\u00e9rer les pr\u00e9dictions\\n\",\n        \"y_pred_proba = model.predict(X_test)\\n\",\n        \"y_pred_classes = np.argmax(y_pred_proba, axis=1)\\n\",\n        \"\\n\",\n        \"# Matrice de confusion\\n\",\n        \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",\n        \"plt.figure(figsize=(8, 6))\\n\",\n        \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', \\n\",\n        \"            xticklabels=['N\u00e9gatif', 'Neutre', 'Positif'],\\n\",\n        \"            yticklabels=['N\u00e9gatif', 'Neutre', 'Positif'])\\n\",\n        \"plt.xlabel('Pr\u00e9dit')\\n\",\n        \"plt.ylabel('R\u00e9el')\\n\",\n        \"plt.title('Matrice de confusion')\\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\\n\",\n        \"\\n\",\n        \"# Rapport de classification\\n\",\n        \"print(\\\"\\\\nRapport de classification d\u00e9taill\u00e9:\\\")\\n\",\n        \"target_names = ['N\u00e9gatif', 'Neutre', 'Positif']\\n\",\n        \"print(classification_report(y_test, y_pred_classes, target_names=target_names))\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### \ud83e\udde0 R\u00e9flexions sur les r\u00e9sultats\\n\",\n        \"\\n\",\n        \"- **Analysez la matrice de confusion**: Quelles classes sont le mieux reconnues? Y a-t-il des confusions particuli\u00e8res?\\n\",\n        \"- **Pr\u00e9cision vs Rappel**: Y a-t-il un d\u00e9s\u00e9quilibre? Quelle m\u00e9trique privil\u00e9gier selon le contexte?\\n\",\n        \"- **Taille du dataset**: Comment les r\u00e9sultats pourraient-ils \u00eatre affect\u00e9s par la petite taille de notre jeu de donn\u00e9es?\\n\",\n        \"\\n\",\n        \"\ud83d\udc49 **Discussion**: Notez vos observations ci-dessous:\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"*\u00c9crivez vos observations ici...*\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 8. Test avec de nouveaux avis\\n\",\n        \"\\n\",\n        \"Testons maintenant notre mod\u00e8le avec quelques nouveaux avis qui n'ont pas \u00e9t\u00e9 utilis\u00e9s pour l'entra\u00eenement.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Nouveaux avis \u00e0 tester\\n\",\n        \"new_reviews = [\\n\",\n        \"    \\\"Ce film \u00e9tait vraiment fantastique, j'ai ador\u00e9 chaque minute.\\\",\\n\",\n        \"    \\\"Je n'ai pas du tout aim\u00e9 ce film, c'\u00e9tait une perte de temps compl\u00e8te.\\\",\\n\",\n        \"    \\\"C'\u00e9tait un film correct, ni bon ni mauvais.\\\"\\n\",\n        \"]\\n\",\n        \"\\n\",\n        \"# Pr\u00e9traitement des nouveaux avis\\n\",\n        \"processed_new_reviews = [preprocess_text(review) for review in new_reviews]\\n\",\n        \"sequences_new = tokenizer.texts_to_sequences(processed_new_reviews)\\n\",\n        \"padded_new = pad_sequences(sequences_new, maxlen=max_len, padding='post', truncating='post')\\n\",\n        \"\\n\",\n        \"# Pr\u00e9dictions\\n\",\n        \"predictions = model.predict(padded_new)\\n\",\n        \"predicted_classes = np.argmax(predictions, axis=1)\\n\",\n        \"\\n\",\n        \"# Afficher les r\u00e9sultats\\n\",\n        \"sentiment_labels = {0: \\\"N\u00e9gatif\\\", 1: \\\"Neutre\\\", 2: \\\"Positif\\\"}\\n\",\n        \"\\n\",\n        \"print(\\\"Pr\u00e9dictions pour les nouveaux avis:\\\\n\\\")\\n\",\n        \"for i, review in enumerate(new_reviews):\\n\",\n        \"    pred_class = predicted_classes[i]\\n\",\n        \"    confidence = predictions[i][pred_class] * 100\\n\",\n        \"    \\n\",\n        \"    print(f\\\"Avis: {review}\\\")\\n\",\n        \"    print(f\\\"Sentiment pr\u00e9dit: {sentiment_labels[pred_class]} (confiance: {confidence:.2f}%)\\\")\\n\",\n        \"    print(\\\"Probabilit\u00e9s pour chaque classe:\\\")\\n\",\n        \"    for j, label in sentiment_labels.items():\\n\",\n        \"        print(f\\\"  {label}: {predictions[i][j]*100:.2f}%\\\")\\n\",\n        \"    print()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation graphique des pr\u00e9dictions\\n\",\n        \"\\n\",\n        \"Visualisons les probabilit\u00e9s pour chaque classe pour les nouveaux avis.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Visualisation des probabilit\u00e9s pour chaque avis\\n\",\n        \"plt.figure(figsize=(15, 5))\\n\",\n        \"labels = ['N\u00e9gatif', 'Neutre', 'Positif']\\n\",\n        \"\\n\",\n        \"for i, review in enumerate(new_reviews):\\n\",\n        \"    plt.subplot(1, 3, i+1)\\n\",\n        \"    plt.bar(labels, predictions[i], color=['red', 'gray', 'green'])\\n\",\n        \"    plt.title(f\\\"Avis {i+1}\\\")\\n\",\n        \"    plt.ylim(0, 1)\\n\",\n        \"    plt.ylabel('Probabilit\u00e9')\\n\",\n        \"    plt.xticks(rotation=45)\\n\",\n        \"    \\n\",\n        \"    # Ajouter les valeurs sur les barres\\n\",\n        \"    for j, p in enumerate(predictions[i]):\\n\",\n        \"        plt.text(j, p + 0.02, f\\\"{p*100:.1f}%\\\", ha='center')\\n\",\n        \"        \\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\"\n      ]\n    },\n</pre> {   \"cells\": [     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"# RNN/LSTM pour l'analyse de sentiment\\n\",         \"\\n\",         \"##  S\u00e9ance 2: Types de r\u00e9seaux de neurones\\n\",         \"\\n\",         \"Ce notebook vous guidera \u00e0 travers l'impl\u00e9mentation d'un mod\u00e8le LSTM (Long Short-Term Memory) pour l'analyse de sentiment. Vous d\u00e9couvrirez comment les r\u00e9seaux r\u00e9currents peuvent \u00eatre utilis\u00e9s pour comprendre et classifier du texte.\\n\",         \"\\n\",         \"### Objectifs d'apprentissage:\\n\",         \"- Comprendre le pr\u00e9traitement du texte pour les mod\u00e8les de Deep Learning\\n\",         \"- D\u00e9couvrir l'architecture et le fonctionnement des r\u00e9seaux LSTM\\n\",         \"- Apprendre \u00e0 \u00e9valuer un mod\u00e8le d'analyse de sentiment\\n\",         \"- Visualiser et interpr\u00e9ter les embeddings de mots\\n\",         \"\\n\",         \"### Pr\u00e9requis:\\n\",         \"- Connaissances de base en Python\\n\",         \"- Notions fondamentales de r\u00e9seaux de neurones\\n\",         \"- Avoir suivi la s\u00e9ance 1 d'introduction au Deep Learning\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 1. Configuration de l'environnement\\n\",         \"\\n\",         \"Commen\u00e7ons par importer les biblioth\u00e8ques n\u00e9cessaires et configurer notre environnement.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"import numpy as np\\n\",         \"import matplotlib.pyplot as plt\\n\",         \"import tensorflow as tf\\n\",         \"from tensorflow.keras.models import Sequential\\n\",         \"from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\\n\",         \"from tensorflow.keras.preprocessing.text import Tokenizer\\n\",         \"from tensorflow.keras.preprocessing.sequence import pad_sequences\\n\",         \"from tensorflow.keras.callbacks import EarlyStopping\\n\",         \"import pandas as pd\\n\",         \"import re\\n\",         \"import time\\n\",         \"import seaborn as sns\\n\",         \"from sklearn.metrics import confusion_matrix, classification_report\\n\",         \"\\n\",         \"# Configuration pour reproductibilit\u00e9\\n\",         \"np.random.seed(42)\\n\",         \"tf.random.set_seed(42)\\n\",         \"\\n\",         \"# V\u00e9rifier la version de TensorFlow\\n\",         \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 2. Pr\u00e9paration des donn\u00e9es\\n\",         \"\\n\",         \"Pour ce TP, nous allons utiliser un petit dataset simul\u00e9 d'avis sur des films. Chaque avis sera class\u00e9 comme positif, n\u00e9gatif ou neutre.\\n\",         \"\\n\",         \"Dans un projet r\u00e9el, vous pourriez utiliser des datasets plus importants comme IMDB, Amazon Reviews, etc.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Cr\u00e9er un petit jeu de donn\u00e9es d'avis sur les films (simul\u00e9)\\n\",         \"reviews = [\\n\",         \"    \\\"Ce film \u00e9tait excellent, j'ai vraiment ador\u00e9 les performances des acteurs.\\\",\\n\",         \"    \\\"Une exp\u00e9rience cin\u00e9matographique incroyable, absolument \u00e0 voir !\\\",\\n\",         \"    \\\"Un chef-d'\u0153uvre du cin\u00e9ma, magnifiquement r\u00e9alis\u00e9.\\\",\\n\",         \"    \\\"J'ai beaucoup appr\u00e9ci\u00e9 l'histoire et les personnages \u00e9taient bien d\u00e9velopp\u00e9s.\\\",\\n\",         \"    \\\"Visuellement \u00e9poustouflant avec une histoire captivante.\\\",\\n\",         \"    \\\"Un film d\u00e9cevant avec un sc\u00e9nario plein de trous.\\\",\\n\",         \"    \\\"Vraiment terrible, je n'ai pas aim\u00e9 du tout.\\\",\\n\",         \"    \\\"Un g\u00e2chis complet de temps et d'argent, \u00e9vitez \u00e0 tout prix.\\\",\\n\",         \"    \\\"Ennuyeux et pr\u00e9visible, les acteurs semblaient d\u00e9sint\u00e9ress\u00e9s.\\\",\\n\",         \"    \\\"Une d\u00e9ception totale, l'intrigue ne fait aucun sens.\\\",\\n\",         \"    \\\"C'\u00e9tait correct, ni bon ni mauvais.\\\",\\n\",         \"    \\\"Un film moyen avec quelques bons moments.\\\",\\n\",         \"    \\\"Certaines sc\u00e8nes \u00e9taient bonnes, mais dans l'ensemble assez moyen.\\\",\\n\",         \"    \\\"Pas aussi bon que je l'esp\u00e9rais, mais pas horrible non plus.\\\",\\n\",         \"    \\\"Une histoire int\u00e9ressante mais mal ex\u00e9cut\u00e9e.\\\",\\n\",         \"    \\\"Un film brillant qui m'a fait r\u00e9fl\u00e9chir pendant des jours.\\\",\\n\",         \"    \\\"Absolument sublime, l'un des meilleurs films que j'ai jamais vus.\\\",\\n\",         \"    \\\"Un d\u00e9sastre total, je me suis endormi au milieu.\\\",\\n\",         \"    \\\"Pas du tout ce \u00e0 quoi je m'attendais, tr\u00e8s d\u00e9\u00e7u.\\\",\\n\",         \"    \\\"Le jeu d'acteur \u00e9tait fantastique, mais l'histoire \u00e9tait faible.\\\"\\n\",         \"]\\n\",         \"\\n\",         \"# Attribuer des sentiments (0 = n\u00e9gatif, 1 = neutre, 2 = positif)\\n\",         \"sentiments = [2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 0, 0, 1]\\n\",         \"\\n\",         \"# Convertir en DataFrame pour faciliter la manipulation\\n\",         \"df = pd.DataFrame({\\n\",         \"    'review': reviews,\\n\",         \"    'sentiment': sentiments\\n\",         \"})\\n\",         \"\\n\",         \"# Afficher quelques informations sur le dataset\\n\",         \"print(f\\\"Nombre total d'avis: {len(df)}\\\")\\n\",         \"print(f\\\"R\u00e9partition des sentiments: {df['sentiment'].value_counts().sort_index()}\\\")\\n\",         \"\\n\",         \"# Afficher quelques exemples\\n\",         \"print(\\\"\\\\nExemples d'avis:\\\")\\n\",         \"for sentiment in [0, 1, 2]:\\n\",         \"    sample = df[df['sentiment'] == sentiment].iloc[0]\\n\",         \"    print(f\\\"Sentiment {sentiment}: '{sample['review']}'\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation de la distribution des sentiments\\n\",         \"\\n\",         \"V\u00e9rifions que notre jeu de donn\u00e9es est \u00e9quilibr\u00e9 entre les diff\u00e9rentes classes.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Visualiser la distribution des sentiments\\n\",         \"plt.figure(figsize=(8, 5))\\n\",         \"ax = sns.countplot(x='sentiment', data=df)\\n\",         \"plt.title('Distribution des sentiments')\\n\",         \"plt.xlabel('Sentiment (0=n\u00e9gatif, 1=neutre, 2=positif)')\\n\",         \"plt.ylabel('Nombre d\\\\'avis')\\n\",         \"\\n\",         \"# Ajouter les valeurs sur les barres\\n\",         \"for p in ax.patches:\\n\",         \"    ax.annotate(f\\\"{p.get_height()}\\\", (p.get_x() + p.get_width()/2., p.get_height()),\\n\",         \"                ha='center', va='bottom')\\n\",         \"\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 3. Pr\u00e9traitement du texte\\n\",         \"\\n\",         \"Avant de pouvoir utiliser le texte avec notre mod\u00e8le LSTM, nous devons le pr\u00e9traiter. Cela implique plusieurs \u00e9tapes:\\n\",         \"1. Nettoyage (minuscules, suppression de ponctuation, etc.)\\n\",         \"2. Tokenisation (conversion du texte en s\u00e9quences de nombres)\\n\",         \"3. Padding (uniformisation de la longueur des s\u00e9quences)\\n\",         \"\\n\",         \"Commen\u00e7ons par le nettoyage de texte:\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def preprocess_text(text):\\n\",         \"    \\\"\\\"\\\"Fonction pour nettoyer et normaliser le texte\\\"\\\"\\\"\\n\",         \"    # Convertir en minuscules\\n\",         \"    text = text.lower()\\n\",         \"    # Supprimer la ponctuation et les caract\u00e8res sp\u00e9ciaux\\n\",         \"    text = re.sub(r'[^\\\\w\\\\s]', '', text)\\n\",         \"    # Supprimer les chiffres\\n\",         \"    text = re.sub(r'\\\\d+', '', text)\\n\",         \"    # Supprimer les espaces multiples\\n\",         \"    text = re.sub(r'\\\\s+', ' ', text).strip()\\n\",         \"    return text\\n\",         \"\\n\",         \"# Appliquer le pr\u00e9traitement \u00e0 nos avis\\n\",         \"df['processed_review'] = df['review'].apply(preprocess_text)\\n\",         \"\\n\",         \"# Afficher un exemple avant et apr\u00e8s pr\u00e9traitement\\n\",         \"example_idx = 0\\n\",         \"print(f\\\"Avant: {df['review'][example_idx]}\\\")\\n\",         \"print(f\\\"Apr\u00e8s: {df['processed_review'][example_idx]}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Tokenisation du texte\\n\",         \"\\n\",         \"La tokenisation convertit le texte en s\u00e9quences num\u00e9riques que notre r\u00e9seau de neurones peut traiter.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Configuration pour la tokenisation\\n\",         \"max_words = 1000  # Taille du vocabulaire\\n\",         \"max_len = 100     # Longueur maximale des s\u00e9quences\\n\",         \"\\n\",         \"# Cr\u00e9er et configurer le tokenizer\\n\",         \"tokenizer = Tokenizer(num_words=max_words, oov_token='')\\n\",         \"tokenizer.fit_on_texts(df['processed_review'])\\n\",         \"\\n\",         \"# Convertir les textes en s\u00e9quences de tokens\\n\",         \"sequences = tokenizer.texts_to_sequences(df['processed_review'])\\n\",         \"\\n\",         \"# Appliquer le padding pour uniformiser la longueur des s\u00e9quences\\n\",         \"padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\\n\",         \"\\n\",         \"print(f\\\"Taille du vocabulaire: {len(tokenizer.word_index)}\\\")\\n\",         \"print(f\\\"Forme des s\u00e9quences apr\u00e8s padding: {padded_sequences.shape}\\\")\\n\",         \"\\n\",         \"# Afficher le mapping de quelques mots vers leurs tokens\\n\",         \"print(\\\"\\\\nExemples de mapping mot -&gt; token:\\\")\\n\",         \"sample_words = ['film', 'bon', 'mauvais', 'excellent', 'terrible']\\n\",         \"for word in sample_words:\\n\",         \"    if word in tokenizer.word_index:\\n\",         \"        print(f\\\"{word} -&gt; {tokenizer.word_index[word]}\\\")\\n\",         \"    else:\\n\",         \"        print(f\\\"{word} -&gt; Non trouv\u00e9 dans le vocabulaire\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation d'une s\u00e9quence tokenis\u00e9e\\n\",         \"\\n\",         \"Pour mieux comprendre la tokenisation, visualisons comment un avis est converti en s\u00e9quence de tokens.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def visualize_tokenized_sequence(text, tokens):\\n\",         \"    \\\"\\\"\\\"Visualise la correspondance entre mots et tokens\\\"\\\"\\\"\\n\",         \"    words = text.split()\\n\",         \"    plt.figure(figsize=(15, 3))\\n\",         \"    plt.bar(range(len(tokens)), tokens)\\n\",         \"    plt.xticks(range(len(tokens)), words, rotation=45, ha='right')\\n\",         \"    plt.ylabel('Token ID')\\n\",         \"    plt.title('Repr\u00e9sentation tokenis\u00e9e d\\\\'un avis')\\n\",         \"    plt.tight_layout()\\n\",         \"    plt.show()\\n\",         \"\\n\",         \"sample_idx = 0\\n\",         \"sample_text = df['processed_review'][sample_idx].split()[:15]  # Limiter \u00e0 15 mots pour lisibilit\u00e9\\n\",         \"sample_tokens = sequences[sample_idx][:15]\\n\",         \"\\n\",         \"print(f\\\"Exemple d'avis: {' '.join(sample_text)}\\\")\\n\",         \"print(f\\\"Tokens correspondants: {sample_tokens}\\\")\\n\",         \"visualize_tokenized_sequence(' '.join(sample_text), sample_tokens)\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 4. Division en ensembles d'entra\u00eenement et de test\\n\",         \"\\n\",         \"Avant de cr\u00e9er notre mod\u00e8le, divisons nos donn\u00e9es en ensembles d'entra\u00eenement et de test pour \u00e9valuer ses performances.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"from sklearn.model_selection import train_test_split\\n\",         \"\\n\",         \"# Division 70-30 avec stratification pour conserver la distribution des classes\\n\",         \"X_train, X_test, y_train, y_test = train_test_split(\\n\",         \"    padded_sequences, \\n\",         \"    df['sentiment'],\\n\",         \"    test_size=0.3,\\n\",         \"    random_state=42,\\n\",         \"    stratify=df['sentiment']  # Assurer une r\u00e9partition \u00e9quilibr\u00e9e des classes\\n\",         \")\\n\",         \"\\n\",         \"print(f\\\"Forme des donn\u00e9es d'entra\u00eenement: {X_train.shape}\\\")\\n\",         \"print(f\\\"Forme des donn\u00e9es de test: {X_test.shape}\\\")\\n\",         \"print(f\\\"Distribution des classes (entra\u00eenement): {pd.Series(y_train).value_counts().sort_index()}\\\")\\n\",         \"print(f\\\"Distribution des classes (test): {pd.Series(y_test).value_counts().sort_index()}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 5. Cr\u00e9ation du mod\u00e8le LSTM\\n\",         \"\\n\",         \"Nous allons maintenant cr\u00e9er notre mod\u00e8le d'analyse de sentiment en utilisant une architecture LSTM bidirectionnelle.\\n\",         \"\\n\",         \"### Architecture du mod\u00e8le\\n\",         \"- **Couche d'embedding**: Convertit les tokens en vecteurs denses\\n\",         \"- **Couches LSTM bidirectionnelles**: Capture les d\u00e9pendances \u00e0 long terme dans les deux directions\\n\",         \"- **Dropout**: \u00c9vite le surapprentissage\\n\",         \"- **Couche dense finale**: Classification en 3 cat\u00e9gories (n\u00e9gatif, neutre, positif)\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Param\u00e8tres du mod\u00e8le\\n\",         \"embedding_dim = 32  # Dimension de l'espace d'embedding\\n\",         \"\\n\",         \"# Cr\u00e9ation du mod\u00e8le\\n\",         \"model = Sequential([\\n\",         \"    # Couche d'embedding pour convertir les tokens en vecteurs denses\\n\",         \"    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\\n\",         \"    \\n\",         \"    # Couche LSTM bidirectionnelle\\n\",         \"    Bidirectional(LSTM(64, return_sequences=True)),\\n\",         \"    \\n\",         \"    # Deuxi\u00e8me couche LSTM suivie de dropout pour r\u00e9gularisation\\n\",         \"    Bidirectional(LSTM(32)),\\n\",         \"    Dropout(0.5),\\n\",         \"    \\n\",         \"    # Couche de classification (3 classes: n\u00e9gatif, neutre, positif)\\n\",         \"    Dense(3, activation='softmax')\\n\",         \"])\\n\",         \"\\n\",         \"# Afficher un r\u00e9sum\u00e9 du mod\u00e8le\\n\",         \"model.summary()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### \ud83d\udca1 Points cl\u00e9s \u00e0 observer dans l'architecture\\n\",         \"\\n\",         \"- **LSTM bidirectionnel** : Lit le texte de gauche \u00e0 droite ET de droite \u00e0 gauche, capturant mieux le contexte\\n\",         \"- **return_sequences=True** : Permet d'empiler plusieurs couches LSTM\\n\",         \"- **Dropout** : D\u00e9sactive al\u00e9atoirement 50% des neurones pendant l'entra\u00eenement pour \u00e9viter le surapprentissage\\n\",         \"- **Activation softmax** : G\u00e9n\u00e8re une distribution de probabilit\u00e9 sur les 3 classes\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 6. Compilation et entra\u00eenement du mod\u00e8le\\n\",         \"\\n\",         \"Maintenant, compilons et entra\u00eenons notre mod\u00e8le LSTM.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Compiler le mod\u00e8le\\n\",         \"model.compile(\\n\",         \"    optimizer='adam',\\n\",         \"    loss='sparse_categorical_crossentropy',  # Pour les \u00e9tiquettes sous forme d'entiers (non one-hot)\\n\",         \"    metrics=['accuracy']\\n\",         \")\\n\",         \"\\n\",         \"# Early stopping pour \u00e9viter le surapprentissage\\n\",         \"early_stopping = EarlyStopping(\\n\",         \"    monitor='val_loss',\\n\",         \"    patience=3,\\n\",         \"    restore_best_weights=True\\n\",         \")\\n\",         \"\\n\",         \"# Mesure du temps d'entra\u00eenement\\n\",         \"start_time = time.time()\\n\",         \"\\n\",         \"# Entra\u00eenement du mod\u00e8le\\n\",         \"history = model.fit(\\n\",         \"    X_train, \\n\",         \"    y_train, \\n\",         \"    epochs=20,\\n\",         \"    batch_size=4,  # Petit batch size en raison de la petite taille du dataset\\n\",         \"    validation_split=0.2,  # 20% des donn\u00e9es d'entra\u00eenement serviront \u00e0 la validation\\n\",         \"    callbacks=[early_stopping],\\n\",         \"    verbose=1\\n\",         \")\\n\",         \"\\n\",         \"training_time = time.time() - start_time\\n\",         \"print(f\\\"\\\\nTemps d'entra\u00eenement: {training_time:.2f} secondes\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation de l'\u00e9volution de l'entra\u00eenement\\n\",         \"\\n\",         \"Observons comment la pr\u00e9cision et la perte ont \u00e9volu\u00e9 au cours de l'entra\u00eenement.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Visualisation de l'entra\u00eenement\\n\",         \"plt.figure(figsize=(12, 5))\\n\",         \"\\n\",         \"# Graphique de pr\u00e9cision\\n\",         \"plt.subplot(1, 2, 1)\\n\",         \"plt.plot(history.history['accuracy'], label='Entra\u00eenement')\\n\",         \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",         \"plt.title('\u00c9volution de la pr\u00e9cision')\\n\",         \"plt.xlabel('\u00c9poque')\\n\",         \"plt.ylabel('Pr\u00e9cision')\\n\",         \"plt.legend()\\n\",         \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",         \"\\n\",         \"# Graphique de perte\\n\",         \"plt.subplot(1, 2, 2)\\n\",         \"plt.plot(history.history['loss'], label='Entra\u00eenement')\\n\",         \"plt.plot(history.history['val_loss'], label='Validation')\\n\",         \"plt.title('\u00c9volution de la perte')\\n\",         \"plt.xlabel('\u00c9poque')\\n\",         \"plt.ylabel('Perte')\\n\",         \"plt.legend()\\n\",         \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",         \"\\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 7. \u00c9valuation du mod\u00e8le\\n\",         \"\\n\",         \"Maintenant, \u00e9valuons les performances de notre mod\u00e8le sur l'ensemble de test.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# \u00c9valuation sur l'ensemble de test\\n\",         \"test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\\n\",         \"print(f\\\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",         \"\\n\",         \"# G\u00e9n\u00e9rer les pr\u00e9dictions\\n\",         \"y_pred_proba = model.predict(X_test)\\n\",         \"y_pred_classes = np.argmax(y_pred_proba, axis=1)\\n\",         \"\\n\",         \"# Matrice de confusion\\n\",         \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",         \"plt.figure(figsize=(8, 6))\\n\",         \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', \\n\",         \"            xticklabels=['N\u00e9gatif', 'Neutre', 'Positif'],\\n\",         \"            yticklabels=['N\u00e9gatif', 'Neutre', 'Positif'])\\n\",         \"plt.xlabel('Pr\u00e9dit')\\n\",         \"plt.ylabel('R\u00e9el')\\n\",         \"plt.title('Matrice de confusion')\\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\\n\",         \"\\n\",         \"# Rapport de classification\\n\",         \"print(\\\"\\\\nRapport de classification d\u00e9taill\u00e9:\\\")\\n\",         \"target_names = ['N\u00e9gatif', 'Neutre', 'Positif']\\n\",         \"print(classification_report(y_test, y_pred_classes, target_names=target_names))\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### \ud83e\udde0 R\u00e9flexions sur les r\u00e9sultats\\n\",         \"\\n\",         \"- **Analysez la matrice de confusion**: Quelles classes sont le mieux reconnues? Y a-t-il des confusions particuli\u00e8res?\\n\",         \"- **Pr\u00e9cision vs Rappel**: Y a-t-il un d\u00e9s\u00e9quilibre? Quelle m\u00e9trique privil\u00e9gier selon le contexte?\\n\",         \"- **Taille du dataset**: Comment les r\u00e9sultats pourraient-ils \u00eatre affect\u00e9s par la petite taille de notre jeu de donn\u00e9es?\\n\",         \"\\n\",         \"\ud83d\udc49 **Discussion**: Notez vos observations ci-dessous:\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"*\u00c9crivez vos observations ici...*\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 8. Test avec de nouveaux avis\\n\",         \"\\n\",         \"Testons maintenant notre mod\u00e8le avec quelques nouveaux avis qui n'ont pas \u00e9t\u00e9 utilis\u00e9s pour l'entra\u00eenement.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Nouveaux avis \u00e0 tester\\n\",         \"new_reviews = [\\n\",         \"    \\\"Ce film \u00e9tait vraiment fantastique, j'ai ador\u00e9 chaque minute.\\\",\\n\",         \"    \\\"Je n'ai pas du tout aim\u00e9 ce film, c'\u00e9tait une perte de temps compl\u00e8te.\\\",\\n\",         \"    \\\"C'\u00e9tait un film correct, ni bon ni mauvais.\\\"\\n\",         \"]\\n\",         \"\\n\",         \"# Pr\u00e9traitement des nouveaux avis\\n\",         \"processed_new_reviews = [preprocess_text(review) for review in new_reviews]\\n\",         \"sequences_new = tokenizer.texts_to_sequences(processed_new_reviews)\\n\",         \"padded_new = pad_sequences(sequences_new, maxlen=max_len, padding='post', truncating='post')\\n\",         \"\\n\",         \"# Pr\u00e9dictions\\n\",         \"predictions = model.predict(padded_new)\\n\",         \"predicted_classes = np.argmax(predictions, axis=1)\\n\",         \"\\n\",         \"# Afficher les r\u00e9sultats\\n\",         \"sentiment_labels = {0: \\\"N\u00e9gatif\\\", 1: \\\"Neutre\\\", 2: \\\"Positif\\\"}\\n\",         \"\\n\",         \"print(\\\"Pr\u00e9dictions pour les nouveaux avis:\\\\n\\\")\\n\",         \"for i, review in enumerate(new_reviews):\\n\",         \"    pred_class = predicted_classes[i]\\n\",         \"    confidence = predictions[i][pred_class] * 100\\n\",         \"    \\n\",         \"    print(f\\\"Avis: {review}\\\")\\n\",         \"    print(f\\\"Sentiment pr\u00e9dit: {sentiment_labels[pred_class]} (confiance: {confidence:.2f}%)\\\")\\n\",         \"    print(\\\"Probabilit\u00e9s pour chaque classe:\\\")\\n\",         \"    for j, label in sentiment_labels.items():\\n\",         \"        print(f\\\"  {label}: {predictions[i][j]*100:.2f}%\\\")\\n\",         \"    print()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation graphique des pr\u00e9dictions\\n\",         \"\\n\",         \"Visualisons les probabilit\u00e9s pour chaque classe pour les nouveaux avis.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Visualisation des probabilit\u00e9s pour chaque avis\\n\",         \"plt.figure(figsize=(15, 5))\\n\",         \"labels = ['N\u00e9gatif', 'Neutre', 'Positif']\\n\",         \"\\n\",         \"for i, review in enumerate(new_reviews):\\n\",         \"    plt.subplot(1, 3, i+1)\\n\",         \"    plt.bar(labels, predictions[i], color=['red', 'gray', 'green'])\\n\",         \"    plt.title(f\\\"Avis {i+1}\\\")\\n\",         \"    plt.ylim(0, 1)\\n\",         \"    plt.ylabel('Probabilit\u00e9')\\n\",         \"    plt.xticks(rotation=45)\\n\",         \"    \\n\",         \"    # Ajouter les valeurs sur les barres\\n\",         \"    for j, p in enumerate(predictions[i]):\\n\",         \"        plt.text(j, p + 0.02, f\\\"{p*100:.1f}%\\\", ha='center')\\n\",         \"        \\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\"       ]     },"},{"location":"seance1/","title":"S\u00e9ance 1 : Introduction au Deep Learning par l'exp\u00e9rimentation","text":""},{"location":"seance1/#objectifs-de-la-seance","title":"Objectifs de la s\u00e9ance","text":"<p>\u00c0 l'issue de cette s\u00e9ance, vous serez capable de :</p> <ul> <li>Manipuler concr\u00e8tement un r\u00e9seau de neurones simple</li> <li>Comprendre les diff\u00e9rences entre Machine Learning classique et Deep Learning</li> <li>Expliquer le fonctionnement de base d'un r\u00e9seau de neurones</li> <li>Appliquer des techniques d'am\u00e9lioration d'un mod\u00e8le de Deep Learning</li> </ul>"},{"location":"seance1/#programme-4h","title":"Programme (4h)","text":"<p>Cette s\u00e9ance se d\u00e9roule en quatre phases distinctes, chacune con\u00e7ue pour vous faire d\u00e9couvrir le Deep Learning par la pratique plut\u00f4t que par la th\u00e9orie.</p>"},{"location":"seance1/#phase-1-mise-en-situation-pratique-1h","title":"Phase 1 : Mise en situation pratique (1h)","text":"<p>D\u00e9couvrez le Deep Learning \u00e0 travers des exemples concrets, sans vous pr\u00e9occuper de la th\u00e9orie pour le moment.</p> <ul> <li>D\u00e9monstrations d'applications concr\u00e8tes (GitHub Copilot, reconnaissance d'objets...)</li> <li>Premier contact avec un r\u00e9seau de neurones simple</li> <li>Challenge d'exp\u00e9rimentation guid\u00e9e sur un mod\u00e8le MNIST</li> </ul>"},{"location":"seance1/#phase-2-decouverte-des-concepts-1h30","title":"Phase 2 : D\u00e9couverte des concepts (1h30)","text":"<p>Comparez les approches du Machine Learning classique et du Deep Learning pour comprendre leurs diff\u00e9rences fondamentales.</p> <ul> <li>Atelier \"Bo\u00eete noire\" : exploration parall\u00e8le des deux approches</li> <li>D\u00e9fi de g\u00e9n\u00e9ralisation sur des donn\u00e9es modifi\u00e9es</li> <li>Exploration interactive d'un neurone et d'un r\u00e9seau simple</li> </ul> <p>Notebooks associ\u00e9s : - Machine Learning classique - Deep Learning - Anatomie d'un r\u00e9seau</p> <p>Ressource \u00e0 compl\u00e9ter : Tableau comparatif ML vs DL</p>"},{"location":"seance1/#phase-3-auto-evaluation-et-synthese-30min","title":"Phase 3 : Auto-\u00e9valuation et synth\u00e8se (30min)","text":"<p>Consolidez vos connaissances et \u00e9valuez votre compr\u00e9hension de mani\u00e8re autonome.</p> <ul> <li>QCM sur les concepts fondamentaux</li> <li>Synth\u00e8se personnelle \u00e0 r\u00e9diger</li> <li>Sch\u00e9ma conceptuel \u00e0 compl\u00e9ter</li> </ul> <p>Ressources : - QCM d'\u00e9valuation - Sch\u00e9ma \u00e0 compl\u00e9ter</p> <p>Livrables attendus</p> <p>\u00c0 l'issue de cette s\u00e9ance, vous devrez avoir produit :</p> <ul> <li>La fiche d'observations compl\u00e9t\u00e9e sur le \"Hello World du Deep Learning\"</li> <li>Le tableau comparatif Machine Learning vs Deep Learning rempli</li> <li>Le sch\u00e9ma annot\u00e9 d'un r\u00e9seau de neurones</li> </ul>"},{"location":"seance1/#competences-bts-sio-developpees","title":"Comp\u00e9tences BTS SIO d\u00e9velopp\u00e9es","text":"<p>Cette s\u00e9ance vous permet d'acqu\u00e9rir plusieurs comp\u00e9tences du r\u00e9f\u00e9rentiel BTS SIO  :</p> Comp\u00e9tence Description Activit\u00e9s associ\u00e9es B1.3 Gestion des donn\u00e9es Manipulation des datasets d'images B2.2 Conception et d\u00e9veloppement Am\u00e9lioration des mod\u00e8les de Deep Learning B2.3 Conception et d\u00e9veloppement d'IHM Analyse des interfaces de notebooks interactifs B3.2 V\u00e9rification et validation \u00c9valuation de la performance des mod\u00e8les"},{"location":"seance1/#support-technique","title":"Support technique","text":"<p>Si vous rencontrez des difficult\u00e9s :</p> <ol> <li>Consultez la FAQ disponible ici</li> <li>Demandez de l'aide \u00e0 vos camarades</li> <li>Sollicitez l'enseignant pour les probl\u00e8mes persistants</li> </ol>"},{"location":"seance1/#preparation-a-la-seance-suivante","title":"Pr\u00e9paration \u00e0 la s\u00e9ance suivante","text":"<p>La prochaine s\u00e9ance portera sur les applications professionnelles des diff\u00e9rents types de r\u00e9seaux neuronaux. Pour vous y pr\u00e9parer, nous vous recommandons de :</p> <ul> <li>R\u00e9viser les concepts vus aujourd'hui</li> <li>Consulter le glossaire du Deep Learning</li> </ul> <p>Pr\u00eat \u00e0 plonger dans le monde du Deep Learning ? Commen\u00e7ons par la mise en situation pratique !</p>"},{"location":"seance1/partie2-decouverte-concepts/","title":"Phase 2 : D\u00e9couverte des concepts par l'exp\u00e9rimentation (1h30)","text":""},{"location":"seance1/partie2-decouverte-concepts/#objectifs-de-la-phase","title":"Objectifs de la phase","text":"<p>Dans cette phase, vous allez : - Comparer exp\u00e9rimentalement le Machine Learning classique et le Deep Learning - Observer les diff\u00e9rences fondamentales en termes de pr\u00e9paration des donn\u00e9es et de performances - D\u00e9couvrir l'anatomie d'un r\u00e9seau de neurones en manipulant directement ses composants - Comprendre par la pratique comment l'information circule dans un r\u00e9seau de neurones</p>"},{"location":"seance1/partie2-decouverte-concepts/#comparaison-pratique-machine-learning-vs-deep-learning-30-min","title":"Comparaison pratique : Machine Learning vs Deep Learning (30 min)","text":""},{"location":"seance1/partie2-decouverte-concepts/#objectif","title":"Objectif","text":"<p>Comprendre par l'observation directe les diff\u00e9rences fondamentales entre le Machine Learning classique et le Deep Learning, en les appliquant au m\u00eame jeu de donn\u00e9es.</p>"},{"location":"seance1/partie2-decouverte-concepts/#instructions","title":"Instructions","text":"<ol> <li>Suivre les instructions pour ouvrez le notebook dans Google Colab </li> <li>Ex\u00e9cutez les cellules dans l'ordre indiqu\u00e9</li> <li>Observez comment chaque approche traite les donn\u00e9es MNIST (chiffres manuscrits)</li> <li>Notez les diff\u00e9rences en termes de :</li> <li>Pr\u00e9paration des donn\u00e9es</li> <li>Complexit\u00e9 d'impl\u00e9mentation</li> <li>Temps d'entra\u00eenement</li> <li>Performance sur donn\u00e9es normales et bruit\u00e9es</li> </ol>"},{"location":"seance1/partie2-decouverte-concepts/#points-cles-a-identifier","title":"Points cl\u00e9s \u00e0 identifier","text":"<p>\u00c0 travers cette exp\u00e9rimentation, identifiez par vous-m\u00eames ces concepts fondamentaux :</p> <ul> <li>Comment les caract\u00e9ristiques (features) sont trait\u00e9es dans chaque approche</li> <li>Le r\u00f4le de la repr\u00e9sentation des donn\u00e9es</li> <li>La capacit\u00e9 d'abstraction des diff\u00e9rents mod\u00e8les</li> <li>Les compromis entre temps d'entra\u00eenement et performance</li> </ul>"},{"location":"seance1/partie2-decouverte-concepts/#tableau-comparatif","title":"Tableau comparatif","text":"<p>Utilisez ce tableau pour noter vos observations :</p> Aspect observ\u00e9 Machine Learning (Random Forest) Deep Learning (CNN) Pr\u00e9paration des donn\u00e9es Extraction de caract\u00e9ristiques Temps d'entra\u00eenement Pr\u00e9cision globale Pr\u00e9cision sur donn\u00e9es bruit\u00e9es Facilit\u00e9 d'impl\u00e9mentation"},{"location":"seance1/partie2-decouverte-concepts/#atelier-pratique-boite-noire-machine-learning-vs-deep-learning-30-min","title":"Atelier pratique \"Bo\u00eete noire\" : Machine Learning vs Deep Learning (30 min)","text":""},{"location":"seance1/partie2-decouverte-concepts/#contexte-et-preparation-5-min","title":"Contexte et pr\u00e9paration (5 min)","text":"<p>Nous allons explorer les diff\u00e9rences fondamentales entre le Machine Learning classique et le Deep Learning non pas par la th\u00e9orie, mais en observant leurs comportements sur un m\u00eame jeu de donn\u00e9es.</p> <p>Organisation : - Travaillez en bin\u00f4me - Chaque bin\u00f4me doit explorer en parall\u00e8le les deux notebooks fournis</p> <p>Mat\u00e9riel n\u00e9cessaire :</p> <ul> <li>Compte Google (pour acc\u00e9der \u00e0 Google Colab)</li> <li>Liens vers deux notebooks compl\u00e9mentaires :</li> <li>Notebook A : Classification avec un algorithme classique (Random Forest)</li> <li>Notebook B : Classification avec un r\u00e9seau de neurones simple</li> </ul> <p>Jeu de donn\u00e9es : MNIST (chiffres manuscrits)</p>"},{"location":"seance1/partie2-decouverte-concepts/#etape-1-exploration-parallele-25-min","title":"\u00c9tape 1 : Exploration parall\u00e8le (25 min)","text":"<ol> <li>Ouvrez les deux notebooks (A et B) dans deux onglets s\u00e9par\u00e9s.</li> <li>Ex\u00e9cutez les cellules dans l'ordre indiqu\u00e9 (Ctrl+Enter ou bouton \u25b6\ufe0f).</li> <li>Pour chaque notebook, observez et notez :</li> <li>Comment les donn\u00e9es sont pr\u00e9par\u00e9es</li> <li>Quels param\u00e8tres peuvent \u00eatre ajust\u00e9s</li> <li>Le temps n\u00e9cessaire \u00e0 l'entra\u00eenement</li> <li>Les performances obtenues (pr\u00e9cision, rappel, F1-score)</li> <li>Les types d'erreurs commises par chaque mod\u00e8le</li> <li>Utilisez ce tableau comparatif pour vos notes :</li> </ol> Aspect observ\u00e9 Machine Learning (Random Forest) Deep Learning (R\u00e9seau de neurones) Pr\u00e9paration des donn\u00e9es Param\u00e8tres principaux Temps d'entra\u00eenement Pr\u00e9cision globale Types d'erreurs fr\u00e9quentes"},{"location":"seance1/partie2-decouverte-concepts/#etape-2-defi-de-generalisation-10-min","title":"\u00c9tape 2 : D\u00e9fi de g\u00e9n\u00e9ralisation (10 min)","text":"<ol> <li>Dans chaque notebook, localisez la section \"D\u00e9fi de g\u00e9n\u00e9ralisation\".</li> <li>Ex\u00e9cutez les cellules qui chargent le nouveau jeu de test.</li> <li>Observez comment chaque mod\u00e8le performe sur ces nouvelles donn\u00e9es.</li> <li>Notez les diff\u00e9rences de performance entre les deux approches.</li> </ol> <p>Questions \u00e0 discuter :</p> <ul> <li>Lequel des mod\u00e8les g\u00e9n\u00e9ralise le mieux aux nouvelles donn\u00e9es ?</li> <li>Pourquoi pensez-vous qu'il y a cette diff\u00e9rence ?</li> <li>Quels avantages et inconv\u00e9nients voyez-vous pour chaque approche ?</li> </ul>"},{"location":"seance1/partie2-decouverte-concepts/#etape-3-mise-en-commun-5-min","title":"\u00c9tape 3 : Mise en commun (5 min)","text":"<p>Pr\u00e9parez-vous \u00e0 partager vos observations avec le reste de la classe :</p> <ul> <li>Principales diff\u00e9rences constat\u00e9es</li> <li>Surprises ou d\u00e9couvertes int\u00e9ressantes</li> <li>Questions que cette exp\u00e9rimentation a soulev\u00e9es</li> </ul> <p>Concepts cl\u00e9s \u00e0 identifier</p> <p>\u00c0 travers cette exp\u00e9rimentation, essayez d'identifier par vous-m\u00eames ces concepts fondamentaux :</p> <ul> <li>Comment les caract\u00e9ristiques (features) sont trait\u00e9es dans chaque approche</li> <li>Le r\u00f4le de la repr\u00e9sentation des donn\u00e9es</li> <li>La capacit\u00e9 d'abstraction des diff\u00e9rents mod\u00e8les</li> <li>Les compromis entre temps d'entra\u00eenement et performance</li> </ul> <p>Ressources suppl\u00e9mentaires</p> <ul> <li>Documentation scikit-learn (Random Forest)</li> <li>Documentation TensorFlow/Keras</li> <li>Visualisations interactives TensorFlow Playground</li> </ul>"},{"location":"seance1/partie2-decouverte-concepts/#tp-guide-anatomie-dun-reseau-de-neurones-45-min","title":"TP guid\u00e9 : Anatomie d'un r\u00e9seau de neurones (45 min)","text":"<p>Dans cette seconde partie, vous allez plonger au c\u0153ur des r\u00e9seaux de neurones pour comprendre leur fonctionnement interne.</p>"},{"location":"seance1/partie2-decouverte-concepts/#materiel-et-ressources","title":"Mat\u00e9riel et ressources","text":"<ul> <li>Notebook interactif \"Anatomie d'un r\u00e9seau de neurones\"</li> <li>Sch\u00e9ma \u00e0 compl\u00e9ter pour la synth\u00e8se</li> <li>Fiche r\u00e9capitulative des termes techniques</li> </ul>"},{"location":"seance1/partie2-decouverte-concepts/#instructions_1","title":"Instructions","text":""},{"location":"seance1/partie2-decouverte-concepts/#partie-1-exploration-dun-neurone-unique-15-min","title":"Partie 1 : Exploration d'un neurone unique (15 min)","text":"<p>Dans cette partie, vous allez manipuler un neurone artificiel unique pour comprendre son fonctionnement de base.</p> <ol> <li>Ex\u00e9cutez les cellules d'importation des biblioth\u00e8ques et de configuration.</li> <li>Localisez la section \"Neurone unique\" et ex\u00e9cutez la cellule d'initialisation.</li> <li>Exp\u00e9rimentez avec les contr\u00f4les interactifs pour :</li> <li>Modifier les valeurs d'entr\u00e9e (x\u2081, x\u2082)</li> <li>Ajuster les poids (w\u2081, w\u2082)</li> <li>Changer la valeur du biais (b)</li> <li>Observer l'effet sur la sortie du neurone</li> </ol> <p>Questions \u00e0 explorer :</p> <ul> <li>Que se passe-t-il si tous les poids sont \u00e0 z\u00e9ro ?</li> <li>Comment pouvez-vous configurer le neurone pour qu'il s'active uniquement si les deux entr\u00e9es sont \u00e9lev\u00e9es ?</li> <li>Quel est l'effet du biais sur le \"seuil\" d'activation ?</li> <li>Comment la fonction d'activation ReLU transforme-t-elle la sortie ?</li> </ul>"},{"location":"seance1/partie2-decouverte-concepts/#partie-2-de-lunique-au-reseau-15-min","title":"Partie 2 : De l'unique au r\u00e9seau (15 min)","text":"<p>Nous allons maintenant passer \u00e0 un petit r\u00e9seau de neurones pour comprendre comment l'information circule \u00e0 travers les couches.</p> <ol> <li>Localisez la section \"R\u00e9seau simple\" et ex\u00e9cutez les cellules d'initialisation.</li> <li>Explorez le r\u00e9seau compos\u00e9 de :</li> <li>Une couche d'entr\u00e9e (2 neurones)</li> <li>Une couche cach\u00e9e (3 neurones)</li> <li>Une couche de sortie (1 neurone)</li> <li>R\u00e9alisez les exp\u00e9riences suivantes :</li> <li>Observez comment le signal se propage \u00e0 travers les couches</li> <li>Suivez le parcours d'une information sp\u00e9cifique (valeur d'entr\u00e9e)</li> <li>Identifiez les \"motifs d'activation\" qui se forment pour diff\u00e9rentes entr\u00e9es</li> <li>Testez diff\u00e9rentes fonctions d'activation (ReLU, Sigmoid, Tanh)</li> </ol> <p>**Exercice pratique : ** Essayez de configurer manuellement les poids pour que le r\u00e9seau r\u00e9alise la fonction logique XOR (entr\u00e9es : [0,0]\u21920, [0,1]\u21921, [1,0]\u21921, [1,1]\u21920).</p>"},{"location":"seance1/partie2-decouverte-concepts/#partie-3-visualisation-de-lentrainement-10-min","title":"Partie 3 : Visualisation de l'entra\u00eenement (10 min)","text":"<p>Dans cette partie, vous allez observer comment un r\u00e9seau apprend au fil du temps.</p> <ol> <li>Localisez la section \"Entra\u00eenement\" et ex\u00e9cutez la cellule d'initialisation.</li> <li>Lancez la visualisation de l'entra\u00eenement en temps r\u00e9el.</li> <li>Observez :</li> <li>L'\u00e9volution des poids \u00e0 chaque it\u00e9ration</li> <li>Comment la \"fronti\u00e8re de d\u00e9cision\" se modifie</li> <li>La diminution de l'erreur au fil des \u00e9poques</li> <li>Essayez de modifier :</li> <li>Le taux d'apprentissage (learning rate)</li> <li>La complexit\u00e9 du probl\u00e8me (type de donn\u00e9es)</li> <li>L'architecture du r\u00e9seau (nombre de neurones)</li> </ol>"},{"location":"seance1/partie2-decouverte-concepts/#partie-4-synthese-et-verbalisation-5-min","title":"Partie 4 : Synth\u00e8se et verbalisation (5 min)","text":"<ol> <li>Compl\u00e9tez le sch\u00e9ma du r\u00e9seau de neurones fourni en fin de notebook.</li> <li>Identifiez et nommez correctement :</li> <li>Les entr\u00e9es et sorties</li> <li>Les poids et biais</li> <li>Les fonctions d'activation</li> <li>Les couches cach\u00e9es</li> <li>R\u00e9digez un court paragraphe (5-7 lignes) expliquant avec vos propres mots :</li> <li>Comment un r\u00e9seau de neurones traite l'information</li> <li>Comment il peut apprendre \u00e0 partir d'exemples</li> </ol>"},{"location":"seance1/partie2-decouverte-concepts/#points-cles-a-retenir","title":"Points cl\u00e9s \u00e0 retenir","text":"<p>\u00c0 travers cette exploration, vous devriez avoir d\u00e9couvert :</p> <ul> <li>Le r\u00f4le fondamental des poids et biais</li> <li>L'importance des fonctions d'activation pour introduire la non-lin\u00e9arit\u00e9</li> <li>Comment l'information se propage \u00e0 travers un r\u00e9seau (forward propagation)</li> <li>Les bases du processus d'apprentissage (ajustement des poids)</li> </ul>"},{"location":"seance1/partie2-decouverte-concepts/#ressources-supplementaires","title":"Ressources suppl\u00e9mentaires","text":"<ul> <li>Visualisations interactives : Playground TensorFlow</li> <li>Tutoriel : Comprendre les r\u00e9seaux de neurones</li> <li>Document : Math\u00e9matiques des r\u00e9seaux de neurones simplifi\u00e9es</li> </ul>"},{"location":"seance1/partie2-decouverte-concepts/#conclusion","title":"Conclusion","text":"<p>Cette phase vous a permis de passer de l'observation pure \u00e0 une compr\u00e9hension plus approfondie des m\u00e9canismes internes du Deep Learning, tout en conservant une approche tr\u00e8s pratique et exp\u00e9rimentale. Les concepts d\u00e9couverts serviront de fondation pour la suite du parcours.</p> <p>Retour \u00e0 la S\u00e9ance 1 Continuer vers la Phase 3</p>"},{"location":"seance1/partie3-debrief/","title":"Auto-\u00e9valuation : S\u00e9ance 1","text":""},{"location":"seance1/partie3-debrief/#consolidation-des-acquis-sur-le-deep-learning","title":"Consolidation des acquis sur le Deep Learning","text":"<p>Cette auto-\u00e9valuation vous permettra de faire le point sur les connaissances et comp\u00e9tences acquises lors de cette premi\u00e8re s\u00e9ance sur le Deep Learning. Elle vous aidera \u00e9galement \u00e0 identifier les points \u00e0 approfondir pour la suite du parcours.</p>"},{"location":"seance1/partie3-debrief/#1-evaluation-des-connaissances-fondamentales","title":"1. \u00c9valuation des connaissances fondamentales","text":""},{"location":"seance1/partie3-debrief/#terminologie-du-deep-learning","title":"Terminologie du Deep Learning","text":"<p>Associez chaque terme \u00e0 sa d\u00e9finition correcte en tra\u00e7ant des liens :</p> Terme D\u00e9finition Neurone artificiel A. Algorithme d'optimisation qui ajuste progressivement les poids pour minimiser l'erreur Couche cach\u00e9e B. Param\u00e8tre ajustable qui d\u00e9termine l'importance de chaque entr\u00e9e d'un neurone Poids (weight) C. Fonction non-lin\u00e9aire appliqu\u00e9e \u00e0 la somme pond\u00e9r\u00e9e pour introduire de la complexit\u00e9 Biais (bias) D. Unit\u00e9 de calcul qui applique une fonction d'activation \u00e0 une somme pond\u00e9r\u00e9e d'entr\u00e9es Fonction d'activation E. Couche situ\u00e9e entre la couche d'entr\u00e9e et la couche de sortie d'un r\u00e9seau Descente de gradient F. Param\u00e8tre suppl\u00e9mentaire qui permet au neurone de s'activer m\u00eame si toutes les entr\u00e9es sont nulles <p>R\u00e9ponses : D, E, B, F, C, A</p>"},{"location":"seance1/partie3-debrief/#comprehension-des-concepts","title":"Compr\u00e9hension des concepts","text":"<p>\u00c9valuez votre compr\u00e9hension en r\u00e9pondant \u00e0 ces questions :</p> <ol> <li>Quelle est la principale diff\u00e9rence entre le Machine Learning classique et le Deep Learning ?</li> <li> Le Deep Learning est plus rapide \u00e0 entra\u00eener</li> <li> Le Deep Learning extrait automatiquement les caract\u00e9ristiques pertinentes</li> <li> Le Deep Learning n\u00e9cessite moins de donn\u00e9es</li> <li> <p> Le Deep Learning est toujours plus pr\u00e9cis</p> </li> <li> <p>Pourquoi utilise-t-on des fonctions d'activation non-lin\u00e9aires dans les r\u00e9seaux de neurones ?</p> </li> <li> Pour acc\u00e9l\u00e9rer l'entra\u00eenement</li> <li> Pour permettre au r\u00e9seau d'apprendre des relations complexes non-lin\u00e9aires</li> <li> Pour r\u00e9duire le nombre de param\u00e8tres</li> <li> <p> Pour \u00e9viter le surapprentissage</p> </li> <li> <p>Qu'est-ce qui permet \u00e0 un r\u00e9seau de neurones d'apprendre ?</p> </li> <li> L'augmentation progressive du nombre de neurones</li> <li> L'ajustement des poids en fonction des erreurs commises</li> <li> La modification automatique de l'architecture</li> <li> L'addition de couches au fur et \u00e0 mesure de l'entra\u00eenement</li> </ol>"},{"location":"seance1/partie3-debrief/#2-competences-pratiques","title":"2. Comp\u00e9tences pratiques","text":"<p>\u00c9valuez vos comp\u00e9tences pratiques acquises durant cette s\u00e9ance :</p> Comp\u00e9tence D\u00e9butant Interm\u00e9diaire Avanc\u00e9 Manipulation d'un notebook Google Colab Compr\u00e9hension du fonctionnement d'un neurone Visualisation du flux d'information dans un r\u00e9seau Modification des hyperparam\u00e8tres et observation des effets Comparaison des approches ML classique vs DL <p>Cochez la case correspondant \u00e0 votre niveau pour chaque comp\u00e9tence</p>"},{"location":"seance1/partie3-debrief/#3-reflexion-personnelle","title":"3. R\u00e9flexion personnelle","text":"<p>Prenez quelques minutes pour r\u00e9fl\u00e9chir \u00e0 votre apprentissage :</p> <ol> <li>Les concepts qui me semblent les plus clairs :</li> </ol> <p>[Votre r\u00e9ponse ici]</p> <ol> <li>Les points que je dois encore approfondir :</li> </ol> <p>[Votre r\u00e9ponse ici]</p> <ol> <li>Ce qui m'a le plus surpris ou int\u00e9ress\u00e9 :</li> </ol> <p>[Votre r\u00e9ponse ici]</p> <ol> <li>Comment je pourrais appliquer ces connaissances :</li> </ol> <p>[Votre r\u00e9ponse ici]</p>"},{"location":"seance1/partie3-debrief/#4-preparation-a-la-prochaine-seance","title":"4. Pr\u00e9paration \u00e0 la prochaine s\u00e9ance","text":"<p>Pour pr\u00e9parer efficacement la s\u00e9ance 2 sur les types de r\u00e9seaux de neurones et leurs applications, je devrais :</p> <ul> <li> Revoir les concepts fondamentaux des r\u00e9seaux de neurones</li> <li> Explorer les ressources suppl\u00e9mentaires sur les CNN et RNN</li> </ul>"},{"location":"seance1/partie3-debrief/#5-carte-mentale-des-concepts-cles","title":"5. Carte mentale des concepts cl\u00e9s","text":"<p>Compl\u00e9tez cette carte mentale en reliant les concepts et en ajoutant les termes manquants :</p> <pre><code>mindmap\n  root((Deep Learning))\n    Fondamentaux\n      Neurone artificiel\n        Poids\n        (?)\n        (?)\n      Couches\n        Entr\u00e9e\n        (?)\n        Sortie\n    Apprentissage\n      (?)\n      Fonction de perte\n      (?)\n    Applications\n      (?)\n      (?)\n      (?)</code></pre>"},{"location":"seance1/partie3-debrief/#6-evaluation-par-les-pairs-optionnel","title":"6. \u00c9valuation par les pairs (optionnel)","text":"<p>Si vous travaillez en bin\u00f4me, prenez un moment pour \u00e9changer vos auto-\u00e9valuations et discuter de vos perceptions respectives :</p> Question R\u00e9ponse du partenaire Quels concepts ai-je bien expliqu\u00e9s ? Quels concepts devrais-je approfondir ? Suggestions pour am\u00e9liorer ma compr\u00e9hension"},{"location":"seance1/partie3-debrief/#ressources-pour-approfondir","title":"Ressources pour approfondir","text":"<p>Si vous souhaitez consolider certains concepts avant la prochaine s\u00e9ance :</p> <ul> <li>Neural Networks and Deep Learning - Un livre en ligne gratuit (en anglais)</li> <li>Playground TensorFlow - Une visualisation interactive des r\u00e9seaux de neurones</li> <li>3Blue1Brown: Neural Networks - Une excellente s\u00e9rie de vid\u00e9os explicatives</li> </ul> <p>Cette auto-\u00e9valuation est \u00e0 compl\u00e9ter individuellement. Vous pouvez la soumettre \u00e0 votre formateur pour obtenir des retours personnalis\u00e9s ou la conserver comme r\u00e9f\u00e9rence personnelle pour suivre votre progression.</p> <p>Passer \u00e0 la S\u00e9ance 2</p>"},{"location":"seance1/structure/","title":"Structure","text":"<p>seance1/ \u2502 \u251c\u2500\u2500 index.md                                       # Page principale de la s\u00e9ance 1 \u2502 \u251c\u2500\u2500 partie1-mise-en-situation/                     # Phase 1: Mise en situation pratique \u2502   \u251c\u2500\u2500 partie1-mise-en-situation.md               # Guide de la mise en situation \u2502   \u251c\u2500\u2500 hello-world-dl.ipynb                       # Notebook \"Hello World du Deep Learning\" \u2502   \u251c\u2500\u2500 presentation-slides.pdf                    # Slides de pr\u00e9sentation des exemples \u2502   \u2514\u2500\u2500 images/                                    # Images pour les d\u00e9monstrations \u2502 \u251c\u2500\u2500 partie2-decouverte-concepts/                   # Phase 2: D\u00e9couverte des concepts \u2502   \u251c\u2500\u2500 partie2-decouverte-concepts.md             # Guide pour la d\u00e9couverte des concepts \u2502   \u251c\u2500\u2500 notebooks/ \u2502   \u2502   \u251c\u2500\u2500 machine-learning-classique.ipynb       # Notebook ML classique \u2502   \u2502   \u2514\u2500\u2500 deep-learning.ipynb                    # Notebook Deep Learning \u2502   \u251c\u2500\u2500 fiche-comparaison-ml-dl.pdf                # Fiche \u00e0 compl\u00e9ter pour la comparaison \u2502   \u2514\u2500\u2500 datasets/                                  # Datasets pour les exp\u00e9rimentations \u2502 \u251c\u2500\u2500 partie3-mini-projet/                           # Phase 3: Mini-projet collaboratif \u2502   \u251c\u2500\u2500 partie3-mini-projet.md                     # Guide du mini-projet \u2502   \u251c\u2500\u2500 model-template.ipynb                       # Notebook template du mod\u00e8le \u2502   \u2514\u2500\u2500 challenge-instructions.pdf                 # Instructions du challenge \u2502 \u251c\u2500\u2500 partie4-debrief/                               # Phase 4: D\u00e9brief et conceptualisation \u2502   \u251c\u2500\u2500 partie4-debrief.md                         # Guide du d\u00e9brief \u2502   \u2514\u2500\u2500 glossaire-dl-interactif.pdf                # Glossaire interactif \u00e0 compl\u00e9ter \u2502 \u251c\u2500\u2500 tp-guid\u00e9/                                      # TP guid\u00e9 sur les r\u00e9seaux de neurones \u2502   \u251c\u2500\u2500 anatomie-reseau.ipynb                      # Notebook sur l'anatomie d'un r\u00e9seau \u2502   \u2514\u2500\u2500 schema-reseau-annoter.pdf                  # Sch\u00e9ma \u00e0 annoter \u2502 \u2514\u2500\u2500 ressources/                                    # Ressources communes     \u251c\u2500\u2500 fiches/     \u2502   \u251c\u2500\u2500 qcm-evaluation.md                   # QCM d'\u00e9valuation     \u2502   \u251c\u2500\u2500 schema-a-completer.md               # Sch\u00e9ma conceptuel \u00e0 compl\u00e9ter     \u2502   \u251c\u2500\u2500 fiche-observations.pdf                 # Fiche d'observations     \u2502   \u251c\u2500\u2500 comparaison-ml-dl.pdf                  # Fiche de comparaison ML vs DL     \u2502   \u2514\u2500\u2500 glossaire-dl.pdf                       # Glossaire du Deep Learning     \u2514\u2500\u2500 images/         \u251c\u2500\u2500 convolution-principle.png              # Image explicative des convolutions         \u251c\u2500\u2500 rnn-principle.png                      # Image explicative des RNN         \u2514\u2500\u2500 autres images explicatives...</p>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/","title":"dataset MNIST","text":"<p>Challenge d'exp\u00e9rimentation guid\u00e9e sur un mod\u00e8le MNIST</p> <p>Suivez les instructions dans le notebook pour :</p> <ul> <li>Charger le dataset MNIST (chiffres manuscrits)</li> <li>Construire un r\u00e9seau de neurones simple</li> <li>Entra\u00eener le mod\u00e8le et observer son apprentissage</li> <li>Tester le mod\u00e8le sur de nouvelles images</li> </ul> <p>Compl\u00e9tez la fiche d'observations au fur et \u00e0 mesure.</p>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#instructions-etape-par-etape","title":"Instructions \u00e9tape par \u00e9tape","text":""},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#exploration-des-donnees-5-min","title":"Exploration des donn\u00e9es (5 min)","text":"<ul> <li>Ex\u00e9cutez les cellules d'importation et de chargement des donn\u00e9es</li> <li>Observez quelques exemples d'images de chiffres manuscrits</li> <li>Notez la taille du dataset et le format des images</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#construction-du-modele-10-min","title":"Construction du mod\u00e8le (10 min)","text":"<ul> <li>Identifiez la cellule qui d\u00e9finit l'architecture du r\u00e9seau</li> <li>Observez la structure : couche d'entr\u00e9e, couche cach\u00e9e, couche de sortie</li> <li>Notez les dimensions et les fonctions d'activation</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#entrainement-du-modele-10-min","title":"Entra\u00eenement du mod\u00e8le (10 min)","text":"<ul> <li>Lancez l'entra\u00eenement en ex\u00e9cutant la cellule correspondante</li> <li>Observez l'\u00e9volution de la pr\u00e9cision et de la perte (loss)</li> <li>Notez le temps d'entra\u00eenement</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#test-du-modele-5-min","title":"Test du mod\u00e8le (5 min)","text":"<ul> <li>Testez le mod\u00e8le sur les donn\u00e9es de validation</li> <li>Observez la matrice de confusion (quelles classes sont confondues)</li> <li>Si disponible, utilisez l'interface de dessin pour tester vos propres chiffres</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#challenge-dexperimentation-15-min","title":"Challenge d'exp\u00e9rimentation (15 min)","text":"<p>\u00c0 vous de jouer ! Modifiez le mod\u00e8le et observez les effets sur ses performances.</p>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#experience-1-variation-du-nombre-depoques","title":"Exp\u00e9rience 1 : Variation du nombre d'\u00e9poques","text":"<ul> <li>Modifiez la variable <code>epochs</code> \u00e0 5, 10 puis 20</li> <li>Observez l'impact sur la pr\u00e9cision et le temps d'entra\u00eenement</li> <li>Question : Y a-t-il un nombre optimal d'\u00e9poques ?</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#experience-2-modification-de-larchitecture","title":"Exp\u00e9rience 2 : Modification de l'architecture","text":"<ul> <li>Changez le nombre de neurones dans la couche cach\u00e9e (32, 64, 128, 256)</li> <li>Ajoutez une seconde couche cach\u00e9e (un exemple est fourni en commentaire)</li> <li>Question : Une architecture plus complexe donne-t-elle toujours de meilleurs r\u00e9sultats ?</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#experience-3-changement-de-la-fonction-dactivation","title":"Exp\u00e9rience 3 : Changement de la fonction d'activation","text":"<ul> <li>Testez diff\u00e9rentes fonctions d'activation ('relu', 'sigmoid', 'tanh')</li> <li>Observez l'impact sur la vitesse d'apprentissage et la pr\u00e9cision finale</li> <li>Question : Quelle fonction d'activation semble la plus adapt\u00e9e pour ce probl\u00e8me ?</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#conclusion-et-transition-5-min","title":"Conclusion et transition (5 min)","text":"<ul> <li>Sauvegardez votre notebook avec vos exp\u00e9rimentations</li> <li>Compl\u00e9tez enti\u00e8rement votre fiche d'observations</li> <li>Pr\u00e9parez-vous \u00e0 partager vos d\u00e9couvertes</li> </ul> <p>Ce que nous venons d'observer :</p> <ul> <li>Les r\u00e9seaux de neurones apprennent \u00e0 partir d'exemples</li> <li>Leurs performances d\u00e9pendent de nombreux param\u00e8tres ajustables</li> <li>Le processus d'entra\u00eenement am\u00e9liore progressivement la pr\u00e9cision</li> </ul> <p>Dans la prochaine phase, nous allons comparer cette approche avec le Machine Learning classique et comprendre plus en d\u00e9tail le fonctionnement des r\u00e9seaux de neurones.</p> <p>Continuer vers la Phase 2 : D\u00e9couverte des concepts</p>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#annexe-contenu-du-notebook-hello-world-du-deep-learning","title":"Annexe : Contenu du notebook \"Hello World du Deep Learning\"","text":"<p>Le notebook que vous allez explorer contient les sections suivantes :</p>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#1-importation-des-bibliotheques","title":"1. Importation des biblioth\u00e8ques","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import to_categorical\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#2-chargement-et-preparation-des-donnees","title":"2. Chargement et pr\u00e9paration des donn\u00e9es","text":"<p>Nous utilisons le dataset MNIST, qui contient 70 000 images de chiffres manuscrits en niveaux de gris (28x28 pixels).</p> <pre><code># Chargement des donn\u00e9es\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Affichage des dimensions\nprint(f\"Donn\u00e9es d'entra\u00eenement: {x_train.shape} images, labels: {y_train.shape}\")\nprint(f\"Donn\u00e9es de test: {x_test.shape} images, labels: {y_test.shape}\")\n</code></pre> <p>Visualisation des exemples d'images :</p> <pre><code># Afficher 5 exemples d'images\nplt.figure(figsize=(10, 2))\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(x_train[i], cmap='gray')\n    plt.title(f\"Label: {y_train[i]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Pr\u00e9paration des donn\u00e9es pour l'entra\u00eenement :</p> <pre><code># Normalisation des pixels (de 0-255 \u00e0 0-1)\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n\n# Reshape des images en vecteurs (28x28 -&gt; 784)\nx_train = x_train.reshape(60000, 784)\nx_test = x_test.reshape(10000, 784)\n\n# One-hot encoding des labels (transformer les chiffres en vecteurs)\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\nprint(\"Forme des donn\u00e9es apr\u00e8s pr\u00e9paration:\")\nprint(f\"x_train: {x_train.shape}\")\nprint(f\"y_train: {y_train.shape}\")\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#3-construction-du-modele","title":"3. Construction du mod\u00e8le","text":"<p>Nous allons cr\u00e9er un r\u00e9seau de neurones simple avec : - Une couche d'entr\u00e9e (784 neurones pour les 784 pixels) - Une couche cach\u00e9e (64 neurones) - Une couche de sortie (10 neurones pour les 10 chiffres)</p> <pre><code># Construction du mod\u00e8le\nmodel = keras.Sequential([\n    # Couche d'entr\u00e9e -&gt; couche cach\u00e9e\n    layers.Dense(64, activation=\"relu\", input_shape=(784,)),\n\n    # Couche cach\u00e9e -&gt; couche de sortie\n    layers.Dense(10, activation=\"softmax\")\n])\n\n# R\u00e9sum\u00e9 du mod\u00e8le\nmodel.summary()\n</code></pre> <p>Compilation du mod\u00e8le avec les param\u00e8tres d'entra\u00eenement :</p> <pre><code># Compilation du mod\u00e8le\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#4-entrainement-du-modele","title":"4. Entra\u00eenement du mod\u00e8le","text":"<pre><code># D\u00e9finition des param\u00e8tres d'entra\u00eenement\nbatch_size = 128\nepochs = 5\n\n# Entra\u00eenement du mod\u00e8le\nhistory = model.fit(\n    x_train, y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=0.1,\n    verbose=1\n)\n</code></pre> <p>Visualisation de l'\u00e9volution de l'entra\u00eenement :</p> <pre><code># Visualisation de l'\u00e9volution de l'entra\u00eenement\nplt.figure(figsize=(12, 4))\n\n# \u00c9volution de la pr\u00e9cision\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Entra\u00eenement')\nplt.plot(history.history['val_accuracy'], label='Validation')\nplt.title('Pr\u00e9cision')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Pr\u00e9cision')\nplt.legend()\n\n# \u00c9volution de la perte\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Entra\u00eenement')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.title('Perte (Loss)')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Perte')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#5-evaluation-du-modele","title":"5. \u00c9valuation du mod\u00e8le","text":"<pre><code># \u00c9valuation sur les donn\u00e9es de test\ntest_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\nprint(f\"Pr\u00e9cision sur les donn\u00e9es de test: {test_acc:.4f}\")\n</code></pre> <p>Examen d\u00e9taill\u00e9 des pr\u00e9dictions :</p> <pre><code># Faire des pr\u00e9dictions\ny_pred = model.predict(x_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true_classes = np.argmax(y_test, axis=1)\n\n# Matrice de confusion\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(y_true_classes, y_pred_classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Pr\u00e9diction')\nplt.ylabel('R\u00e9alit\u00e9')\nplt.title('Matrice de confusion')\nplt.show()\n\n# Rapport de classification\nprint(\"Rapport de classification :\")\nprint(classification_report(y_true_classes, y_pred_classes))\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#6-visualisation-des-predictions","title":"6. Visualisation des pr\u00e9dictions","text":"<pre><code># S\u00e9lectionner quelques exemples al\u00e9atoires\nnum_examples = 5\nexample_indices = np.random.choice(len(x_test), num_examples, replace=False)\n\nplt.figure(figsize=(15, 3))\nfor i, idx in enumerate(example_indices):\n    # Afficher l'image\n    plt.subplot(1, num_examples, i+1)\n    img = x_test[idx].reshape(28, 28)\n    plt.imshow(img, cmap='gray')\n\n    # Pr\u00e9diction\n    pred = y_pred[idx]\n    pred_class = np.argmax(pred)\n    true_class = y_true_classes[idx]\n\n    # Titre avec pr\u00e9diction et confiance\n    title = f\"Pr\u00e9dit: {pred_class}\\n\"\n    title += f\"R\u00e9el: {true_class}\\n\"\n    title += f\"Confiance: {pred[pred_class]:.2f}\"\n    plt.title(title, color=(\"green\" if pred_class == true_class else \"red\"))\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#7-experimentations","title":"7. Exp\u00e9rimentations","text":"<pre><code># Modifier le nombre d'\u00e9poques\n# epochs = 10  # Essayez avec 5, 10, 20 \u00e9poques\n\n# Modifier l'architecture du r\u00e9seau\n\"\"\"\nmodel = keras.Sequential([\n    # Couche d'entr\u00e9e -&gt; premi\u00e8re couche cach\u00e9e\n    layers.Dense(128, activation=\"relu\", input_shape=(784,)),\n\n    # Deuxi\u00e8me couche cach\u00e9e (ajout d'une couche)\n    layers.Dense(64, activation=\"relu\"),\n\n    # Couche cach\u00e9e -&gt; couche de sortie\n    layers.Dense(10, activation=\"softmax\")\n])\n\"\"\"\n\n# Changer la fonction d'activation\n\"\"\"\nmodel = keras.Sequential([\n    # Essayez avec 'relu', 'sigmoid', 'tanh'\n    layers.Dense(64, activation=\"tanh\", input_shape=(784,)),\n    layers.Dense(10, activation=\"softmax\")\n])\n\"\"\"\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/dataset-MNIST/#8-interface-de-dessin-interactive","title":"8. Interface de dessin interactive","text":"<p>Le notebook inclut une interface de dessin permettant de tester le mod\u00e8le avec vos propres dessins de chiffres (disponible dans Google Colab).</p>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/","title":"Applications du Deep Learning","text":""},{"location":"seance1/partie1-mise-en-situation/demonstrations/#demonstrations-pratiques","title":"D\u00e9monstrations pratiques","text":""},{"location":"seance1/partie1-mise-en-situation/demonstrations/#demonstration-1-github-copilot","title":"D\u00e9monstration 1 : GitHub Copilot","text":"<p>GitHub Copilot utilise le Deep Learning pour g\u00e9n\u00e9rer des suggestions de code. Observez comment il peut : - Entra\u00een\u00e9 sur des millions de d\u00e9p\u00f4ts GitHub publics - Utilise un mod\u00e8le de langage bas\u00e9 sur des r\u00e9seaux neuronaux - Analyse le contexte (code existant, commentaires, nom des fonctions) - G\u00e9n\u00e8re des suggestions pertinentes</p>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#github-copilot-exemples","title":"GitHub Copilot - Exemples","text":"<p>Exemple 1 : G\u00e9n\u00e9ration \u00e0 partir d'un commentaire <pre><code># Fonction qui calcule la factorielle d'un nombre n de fa\u00e7on r\u00e9cursive\ndef factorielle(n):\n    if n &lt;= 1:\n        return 1\n    else:\n        return n * factorielle(n-1)\n</code></pre></p> <p>Exemple 2 : Compl\u00e9tion de code existant <pre><code>def trier_par_age(personnes):\n    # Trie la liste de personnes par \u00e2ge croissant\n    return sorted(personnes, key=lambda personne: personne['age'])\n</code></pre></p>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#github-copilot-exemples-suite","title":"GitHub Copilot - Exemples (suite)","text":"<p>Exemple 3 : G\u00e9n\u00e9ration de tests <pre><code># Tests unitaires pour la fonction factorielle\ndef test_factorielle():\n    assert factorielle(0) == 1\n    assert factorielle(1) == 1\n    assert factorielle(5) == 120\n    assert factorielle(10) == 3628800\n</code></pre></p>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#reflexion-impact-sur-le-developpement-logiciel","title":"R\u00e9flexion : Impact sur le d\u00e9veloppement logiciel","text":"<ul> <li>Comment ces outils changent-ils la nature du travail de d\u00e9veloppeur ?</li> <li>Quelles comp\u00e9tences deviennent plus importantes ?</li> <li>Limites et risques potentiels ?</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#demonstration-2-reconnaissance-dobjets","title":"D\u00e9monstration 2 : Reconnaissance d'objets","text":"<p>Une application de Deep Learning peut identifier des objets dans des photos ou vid\u00e9os en temps r\u00e9el :</p> <ul> <li>Nous allons utiliser l'application Teachable Machine </li> <li>Observez la d\u00e9tection en temps r\u00e9el des objets pr\u00e9sents dans la salle</li> <li>Notez le niveau de confiance (pourcentage) pour chaque pr\u00e9diction</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#comment-ca-fonctionne","title":"Comment \u00e7a fonctionne ?","text":"<ul> <li>Utilise des r\u00e9seaux neuronaux convolutifs (CNN)</li> <li>Entra\u00een\u00e9 sur des millions d'images labellis\u00e9es</li> <li>D\u00e9tecte les caract\u00e9ristiques visuelles \u00e0 diff\u00e9rents niveaux d'abstraction</li> <li>Identifie et localise les objets dans l'image</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#reconnaissance-dobjets-observation","title":"Reconnaissance d'objets - Observation","text":""},{"location":"seance1/partie1-mise-en-situation/demonstrations/#ce-que-nous-allons-observer","title":"Ce que nous allons observer","text":"<ul> <li>Reconnaissance en temps r\u00e9el</li> <li>Niveau de confiance des pr\u00e9dictions</li> <li>Robustesse face aux variations (angle, \u00e9clairage)</li> </ul> <ul> <li>Le mod\u00e8le identifie plusieurs objets simultan\u00e9ment</li> <li>Chaque objet est encadr\u00e9 et \u00e9tiquet\u00e9</li> <li>Un score de confiance est associ\u00e9 \u00e0 chaque pr\u00e9diction</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#quest-ce-que-la-convolution","title":"Qu'est-ce que la convolution ?","text":"<ul> <li>Le filtre se d\u00e9place sur l'image et effectue des calculs \u00e0 chaque position</li> <li>Chaque case du filtre est multipli\u00e9e par la case correspondante de l'image</li> <li>Les r\u00e9sultats sont additionn\u00e9s pour obtenir une seule valeur</li> <li>Cette op\u00e9ration se r\u00e9p\u00e8te pour cr\u00e9er une nouvelle \"carte de caract\u00e9ristiques\"</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#fonctionnement-interne-dun-cnn","title":"Fonctionnement interne d'un CNN","text":"<ol> <li>Convolution : application de filtres pour d\u00e9tecter des caract\u00e9ristiques</li> <li>Pooling : r\u00e9duction de la dimension spatiale</li> <li>Activation : introduction de non-lin\u00e9arit\u00e9 (ReLU)</li> <li>Classification : couches enti\u00e8rement connect\u00e9es pour la pr\u00e9diction</li> </ol> <p>Ces op\u00e9rations permettent d'extraire automatiquement des caract\u00e9ristiques de plus en plus abstraites.</p>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#comment-un-cnn-reconnait-les-images","title":"Comment un CNN reconna\u00eet les images","text":"<ul> <li>L'ordinateur analyse l'image \u00e9tape par \u00e9tape, comme notre cerveau</li> <li>Il d\u00e9tecte d'abord les contours simples, puis les formes</li> <li>Il combine ces informations pour comprendre ce qu'il \"voit\"</li> <li>De simples pixels \u00e0 une identification compl\u00e8te</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#demonstration-3-generation-de-texte","title":"D\u00e9monstration 3 : G\u00e9n\u00e9ration de texte","text":"<p>Les mod\u00e8les de langage utilisent des r\u00e9seaux neuronaux pour g\u00e9n\u00e9rer du texte coh\u00e9rent : - outil populaire pour l'exp\u00e9rimentation et l'analyse de donn\u00e9es - Nous allons utiliser un mod\u00e8le simplifi\u00e9 de g\u00e9n\u00e9ration de texte - Donnez une amorce (d\u00e9but de phrase ou paragraphe) - Observez comment le mod\u00e8le compl\u00e8te le texte de mani\u00e8re coh\u00e9rente - Essayez avec diff\u00e9rents styles : formel, informel, technique, cr\u00e9atif</p>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#comment-ca-fonctionne_1","title":"Comment \u00e7a fonctionne ?","text":"<ul> <li>Utilise des mod\u00e8les de langage comme GPT ou Mistral</li> <li>Entra\u00een\u00e9 sur d'\u00e9normes corpus de texte</li> <li>Apprend les patterns statistiques du langage</li> </ul> <p>Cette technologie repose sur des mod\u00e8les de type Transformer qui utilisent des m\u00e9canismes d'attention pour : 1. Analyser les relations entre les mots et leur contexte 2. Pr\u00e9dire les tokens (mots/parties de mots) les plus probables 3. Construire progressivement un texte coh\u00e9rent</p>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#generation-de-texte-observation","title":"G\u00e9n\u00e9ration de texte - Observation","text":""},{"location":"seance1/partie1-mise-en-situation/demonstrations/#ce-que-nous-allons-observer_1","title":"Ce que nous allons observer","text":"<ul> <li>Compl\u00e9tion de texte \u00e0 partir d'une amorce</li> <li>Adaptation au style et au contexte</li> <li>Coh\u00e9rence \u00e0 court et moyen terme</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#interfaces-de-demonstration","title":"Interfaces de d\u00e9monstration","text":"<ul> <li>Demo Mistral AI : https://mistral.ai/</li> <li>GPT-3.5/Claude via Poe : https://poe.com/</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#exemples-damorces-a-tester","title":"Exemples d'amorces \u00e0 tester","text":"<p>Exemple 1 : Style technique (informatique) <pre><code>Les r\u00e9seaux de neurones convolutifs sont particuli\u00e8rement efficaces pour \n</code></pre></p> <p>Exemple 2 : Style cr\u00e9atif (narration) <pre><code>Dans les profondeurs de l'oc\u00e9an, un \u00e9trange ph\u00e9nom\u00e8ne lumineux attira l'attention des chercheurs qui\n</code></pre></p>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#exemples-damorces-a-tester-suite","title":"Exemples d'amorces \u00e0 tester (suite)","text":"<p>Exemple 3 : Style formel (business) <pre><code>L'int\u00e9gration de l'intelligence artificielle dans le processus client permet aux entreprises de\n</code></pre></p> <p>Exemple 4 : Style instructif (tutoriel) <pre><code>Pour d\u00e9velopper une application web en React, suivez ces \u00e9tapes :\n</code></pre></p>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#points-a-observer-pendant-la-demonstration","title":"Points \u00e0 observer pendant la d\u00e9monstration","text":"<ol> <li>Coh\u00e9rence contextuelle : Comment le mod\u00e8le maintient le sujet et le contexte</li> <li>Adaptation au style : Comment le ton et le vocabulaire s'adaptent \u00e0 l'amorce</li> <li>Connaissances int\u00e9gr\u00e9es : Informations factuelles que le mod\u00e8le peut restituer</li> <li>Limitations : Moments o\u00f9 le mod\u00e8le peut g\u00e9n\u00e9rer des informations incorrectes</li> </ol>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#application-en-developpement","title":"Application en d\u00e9veloppement","text":"<p>En tant que d\u00e9veloppeurs, vous pourriez int\u00e9grer cette technologie pour : - G\u00e9n\u00e9rer automatiquement des descriptions de produits - Cr\u00e9er des assistants virtuels pour guider les utilisateurs - Produire des r\u00e9sum\u00e9s de documents techniques - Proposer des suggestions de r\u00e9ponses dans une application de service client</p>"},{"location":"seance1/partie1-mise-en-situation/demonstrations/#ce-que-ces-applications-ont-en-commun","title":"Ce que ces applications ont en commun","text":"<ul> <li>Bas\u00e9es sur des architectures de r\u00e9seaux neuronaux avanc\u00e9es</li> <li>Entra\u00een\u00e9es sur d'\u00e9normes volumes de donn\u00e9es</li> <li>Capables d'extraire automatiquement des patterns complexes</li> <li>Produisent des r\u00e9sultats qui semblent \"intelligents\"</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/","title":"\ud83d\udccb Fiche d'observations - Hello World du Deep Learning (CORRIG\u00c9)","text":"<p>Cette fiche d'observations vous accompagne \u00e9tape par \u00e9tape dans l'exploration du notebook. Pour chaque section, notez les r\u00e9f\u00e9rences aux cellules correspondantes du notebook.</p>"},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#informations-generales","title":"Informations g\u00e9n\u00e9rales","text":"<p>Nom et pr\u00e9nom : ____</p> <p>Date : ____</p> <p>Groupe : ____</p>"},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#partie-1-configuration-et-verification-de-lenvironnement-cellule-2","title":"Partie 1 : Configuration et v\u00e9rification de l'environnement (Cellule 2)","text":"Question Observation Version de TensorFlow d\u00e9tect\u00e9e 2.x.x (la version exacte d\u00e9pendra de l'environnement Colab) GPU disponible ? (Oui/Non) Oui (g\u00e9n\u00e9ralement disponible dans Colab) Quelle est l'importance d'avoir un GPU pour le Deep Learning ? Les GPUs sont essentiels pour acc\u00e9l\u00e9rer l'entra\u00eenement des mod\u00e8les de Deep Learning car ils peuvent effectuer des calculs matriciels en parall\u00e8le, r\u00e9duisant consid\u00e9rablement le temps d'entra\u00eenement par rapport aux CPUs. Sans GPU, l'entra\u00eenement de mod\u00e8les complexes peut prendre des heures, voire des jours au lieu de minutes."},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#partie-2-chargement-et-preparation-des-donnees-cellule-3","title":"Partie 2 : Chargement et pr\u00e9paration des donn\u00e9es (Cellule 3)","text":"Question Observation Combien d'exemples d'entra\u00eenement sont disponibles ? 60 000 exemples Combien d'exemples de test sont disponibles ? 10 000 exemples Quelle est la dimension des images ? 28 x 28 pixels Pourquoi normalise-t-on les valeurs des pixels entre 0 et 1 ? La normalisation permet d'avoir des valeurs dans une plage uniforme, ce qui am\u00e9liore la stabilit\u00e9 et la vitesse de convergence lors de l'entra\u00eenement. Sans normalisation, les gradients peuvent exploser ou dispara\u00eetre pendant la r\u00e9tropropagation. D'apr\u00e8s les exemples affich\u00e9s, quelles difficult\u00e9s pourrait rencontrer le mod\u00e8le ? Variations dans l'\u00e9criture (forme, \u00e9paisseur, inclinaison), certains chiffres qui se ressemblent (comme 1 et 7, ou 3 et 8), qualit\u00e9 variable des images, positions l\u00e9g\u00e8rement diff\u00e9rentes des chiffres dans l'image."},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#partie-3-architecture-du-modele-cellule-4","title":"Partie 3 : Architecture du mod\u00e8le (Cellule 4)","text":"<p>Dessinez le sch\u00e9ma simplifi\u00e9 de l'architecture du r\u00e9seau de neurones utilis\u00e9 :</p> <pre><code>Input (28x28x1) \u2192 Conv2D (32 filtres) \u2192 MaxPooling \u2192 Conv2D (64 filtres) \u2192 MaxPooling \u2192 Flatten \u2192 Dense (64) \u2192 Dense (10, softmax)\n</code></pre> Question Observation Combien de couches comporte le mod\u00e8le ? 7 couches (Input, 2 couches Conv2D, 2 couches MaxPooling, Flatten, 2 couches Dense) Combien de param\u00e8tres entra\u00eenables au total ? Environ 93,322 param\u00e8tres (le nombre exact d\u00e9pend de l'impl\u00e9mentation) Quel est le r\u00f4le des couches de convolution ? Les couches de convolution appliquent des filtres sur l'image pour d\u00e9tecter des caract\u00e9ristiques (bords, textures, formes). Elles permettent d'extraire des informations spatiales en analysant des r\u00e9gions locales de l'image d'entr\u00e9e. Quel est le r\u00f4le des couches de pooling ? Les couches de pooling r\u00e9duisent la dimension spatiale (hauteur et largeur) pour diminuer le nombre de param\u00e8tres et la complexit\u00e9 du calcul, tout en conservant les caract\u00e9ristiques les plus importantes. Elles rendent aussi le mod\u00e8le plus robuste aux petites variations de position. Pourquoi utilise-t-on 'softmax' comme activation de la derni\u00e8re couche ? Softmax transforme les sorties en une distribution de probabilit\u00e9s (somme = 1) sur les 10 classes (chiffres 0-9), ce qui permet d'interpr\u00e9ter les sorties comme des probabilit\u00e9s d'appartenance \u00e0 chaque classe."},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#partie-4-entrainement-du-modele-cellule-5","title":"Partie 4 : Entra\u00eenement du mod\u00e8le (Cellule 5)","text":"Question Observation Combien d'\u00e9poques ont \u00e9t\u00e9 effectu\u00e9es ? 5 \u00e9poques Quelle est la pr\u00e9cision finale sur les donn\u00e9es d'entra\u00eenement ? ~99% (la valeur exacte varie l\u00e9g\u00e8rement \u00e0 chaque ex\u00e9cution) Quelle est la pr\u00e9cision finale sur les donn\u00e9es de validation ? ~98-99% (varie l\u00e9g\u00e8rement) Quelle est la pr\u00e9cision sur l'ensemble de test ? ~98-99% (varie l\u00e9g\u00e8rement) Y a-t-il un signe de surapprentissage (overfitting) ? Pourquoi ? Tr\u00e8s l\u00e9ger ou inexistant car l'\u00e9cart entre la pr\u00e9cision d'entra\u00eenement et de validation est faible. Le mod\u00e8le g\u00e9n\u00e9ralise bien aux donn\u00e9es non vues pendant l'entra\u00eenement."},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#partie-5-visualisation-des-resultats-cellule-6","title":"Partie 5 : Visualisation des r\u00e9sultats (Cellule 6)","text":"<p>Analysez les graphiques d'apprentissage :</p> Question Observation La courbe de pr\u00e9cision d'entra\u00eenement est-elle croissante ? Oui, elle augmente rapidement au d\u00e9but puis se stabilise. La courbe de perte d'entra\u00eenement est-elle d\u00e9croissante ? Oui, elle diminue rapidement au d\u00e9but puis se stabilise. Y a-t-il un \u00e9cart important entre les courbes d'entra\u00eenement et de validation ? Non, l'\u00e9cart est faible, ce qui indique une bonne g\u00e9n\u00e9ralisation. D'apr\u00e8s vous, l'entra\u00eenement a-t-il \u00e9t\u00e9 suffisant (nombre d'\u00e9poques) ? Oui, les courbes semblent se stabiliser apr\u00e8s 3-4 \u00e9poques, donc 5 \u00e9poques semblent suffisantes pour ce mod\u00e8le et ce jeu de donn\u00e9es. Augmenter davantage pourrait amener \u00e0 du surapprentissage."},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#partie-6-predictions-sur-des-exemples-de-test-cellule-7","title":"Partie 6 : Pr\u00e9dictions sur des exemples de test (Cellule 7)","text":"<p>Observez les 10 exemples de pr\u00e9diction :</p> Question Observation Combien de pr\u00e9dictions sont correctes sur les 10 exemples ? G\u00e9n\u00e9ralement 9-10 sur 10 (peut varier selon l'initialisation al\u00e9atoire) Pour les pr\u00e9dictions incorrectes, quelles pourraient \u00eatre les raisons d'erreur ? Similitudes visuelles entre certains chiffres (ex: 4/9, 3/8), \u00e9critures atypiques, bruit dans l'image, ou manque d'exemples similaires dans les donn\u00e9es d'entra\u00eenement. Certains chiffres semblent-ils plus difficiles \u00e0 reconna\u00eetre que d'autres ? G\u00e9n\u00e9ralement, les chiffres 4, 7, 9 peuvent se confondre, ainsi que 3 et 8. Le mod\u00e8le peut h\u00e9siter davantage sur ces paires de chiffres comme le montrent les barres de probabilit\u00e9."},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#partie-7-test-avec-votre-propre-dessin-cellule-8","title":"Partie 7 : Test avec votre propre dessin (Cellule 8)","text":"Question Observation Quels chiffres avez-vous dessin\u00e9s ? [R\u00e9ponses variables selon les essais des \u00e9tudiants] Combien ont \u00e9t\u00e9 correctement pr\u00e9dits ? [R\u00e9ponses variables] Pour ceux mal pr\u00e9dits, quelle \u00e9tait la pr\u00e9diction et pourquoi selon vous ? Raisons possibles : style d'\u00e9criture diff\u00e9rent de MNIST, \u00e9paisseur du trait, centrage de l'image, pr\u00e9traitement qui modifie trop l'image, etc. Comment le pr\u00e9traitement de l'image a-t-il transform\u00e9 votre dessin ? L'image est redimensionn\u00e9e \u00e0 28x28 pixels, convertie en niveaux de gris, et les couleurs sont invers\u00e9es si n\u00e9cessaire pour avoir un fond noir et un chiffre blanc comme dans le jeu de donn\u00e9es MNIST original."},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#partie-8-experimentations-cellule-9","title":"Partie 8 : Exp\u00e9rimentations (Cellule 9)","text":"<p>Documentez vos exp\u00e9rimentations en modifiant le mod\u00e8le ou les param\u00e8tres :</p>"},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#experimentation-1","title":"Exp\u00e9rimentation 1","text":"<p>Modification effectu\u00e9e : Augmentation du nombre d'\u00e9poques \u00e0 10</p> Param\u00e8tre Valeur originale Nouvelle valeur epochs 5 10 <p>R\u00e9sultats : - Pr\u00e9cision test : ~99% - Observations : L\u00e9g\u00e8re am\u00e9lioration de la pr\u00e9cision, mais risque d'overfitting accru. Les courbes montrent une stabilisation apr\u00e8s 6-7 \u00e9poques.</p>"},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#experimentation-2","title":"Exp\u00e9rimentation 2","text":"<p>Modification effectu\u00e9e : Ajout d'une couche Dropout apr\u00e8s la premi\u00e8re couche Dense</p> Param\u00e8tre Valeur originale Nouvelle valeur Architecture Sans Dropout Avec Dropout (0.2) <p>R\u00e9sultats : - Pr\u00e9cision test : ~98.8% - Observations : L\u00e9g\u00e8re diminution de la pr\u00e9cision sur les donn\u00e9es d'entra\u00eenement mais meilleure g\u00e9n\u00e9ralisation, \u00e9cart r\u00e9duit entre courbes d'entra\u00eenement et validation.</p>"},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#conclusion","title":"Conclusion","text":"Question R\u00e9ponse Quels sont les 3 principaux apprentissages de ce TP ? 1. Les r\u00e9seaux de neurones convolutifs sont particuli\u00e8rement adapt\u00e9s \u00e0 la reconnaissance d'images.2. La pr\u00e9paration des donn\u00e9es (normalisation) est cruciale pour la performance du mod\u00e8le.3. L'architecture du r\u00e9seau (nombre et types de couches) d\u00e9termine sa capacit\u00e9 \u00e0 extraire des caract\u00e9ristiques pertinentes. Quelles am\u00e9liorations pourriez-vous sugg\u00e9rer pour ce mod\u00e8le ? Augmentation de donn\u00e9es (rotations, translations), architecture plus profonde, utilisation de techniques de r\u00e9gularisation comme le Dropout, ajout de couches Batch Normalization, ou essayer des architectures plus avanc\u00e9es comme ResNet. Comment ce mod\u00e8le se compare-t-il aux capacit\u00e9s humaines de reconnaissance de chiffres ? Le mod\u00e8le atteint ~99% de pr\u00e9cision, comparable aux performances humaines sur MNIST (~98%). Cependant, il est moins robuste face aux variations extr\u00eames ou aux styles d'\u00e9criture tr\u00e8s diff\u00e9rents de ceux des donn\u00e9es d'entra\u00eenement. Quelles autres applications de la vision par ordinateur vous int\u00e9ressent ? Reconnaissance faciale, d\u00e9tection d'objets, segmentation d'images m\u00e9dicales, syst\u00e8mes de conduite autonome, reconnaissance de gestes, analyse d'images satellites, etc."},{"location":"seance1/partie1-mise-en-situation/fiche-observation-bts-sio-correction/#glossaire-des-termes-cles-rencontres","title":"Glossaire des termes cl\u00e9s rencontr\u00e9s","text":"Terme Votre d\u00e9finition Convolution Op\u00e9ration math\u00e9matique qui applique un filtre \u00e0 une image pour extraire des caract\u00e9ristiques sp\u00e9cifiques comme les bords, les textures ou les formes. Pooling Technique de r\u00e9duction de dimension qui conserve les informations importantes tout en diminuant la taille des repr\u00e9sentations. Max pooling prend la valeur maximale dans une r\u00e9gion d\u00e9finie. Epoch (\u00e9poque) Un passage complet \u00e0 travers l'ensemble des donn\u00e9es d'entra\u00eenement pendant l'apprentissage d'un mod\u00e8le. Batch Sous-ensemble des donn\u00e9es d'entra\u00eenement trait\u00e9 en une seule fois avant la mise \u00e0 jour des poids. Dropout Technique de r\u00e9gularisation qui d\u00e9sactive al\u00e9atoirement certains neurones pendant l'entra\u00eenement pour \u00e9viter le surapprentissage. Softmax Fonction d'activation utilis\u00e9e en sortie qui transforme un vecteur de nombres r\u00e9els en distribution de probabilit\u00e9s. Overfitting (surapprentissage) Ph\u00e9nom\u00e8ne o\u00f9 un mod\u00e8le apprend trop bien les donn\u00e9es d'entra\u00eenement, incluant le bruit, ce qui diminue sa capacit\u00e9 de g\u00e9n\u00e9ralisation \u00e0 de nouvelles donn\u00e9es."},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/","title":"\ud83d\udccb Fiche d'observations - Hello World du Deep Learning","text":"<p>Cette fiche d'observations vous accompagne \u00e9tape par \u00e9tape dans l'exploration du notebook. Pour chaque section, notez les r\u00e9f\u00e9rences aux cellules correspondantes du notebook.</p>"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#informations-generales","title":"Informations g\u00e9n\u00e9rales","text":"<p>Nom et pr\u00e9nom : ____</p> <p>Date : ____</p> <p>Groupe : ____</p>"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#partie-1-configuration-et-verification-de-lenvironnement-cellule-2","title":"Partie 1 : Configuration et v\u00e9rification de l'environnement (Cellule 2)","text":"Question Observation Version de TensorFlow d\u00e9tect\u00e9e GPU disponible ? (Oui/Non) Quelle est l'importance d'avoir un GPU pour le Deep Learning ?"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#partie-2-chargement-et-preparation-des-donnees-cellule-3","title":"Partie 2 : Chargement et pr\u00e9paration des donn\u00e9es (Cellule 3)","text":"Question Observation Combien d'exemples d'entra\u00eenement sont disponibles ? Combien d'exemples de test sont disponibles ? Quelle est la dimension des images ? Pourquoi normalise-t-on les valeurs des pixels entre 0 et 1 ? D'apr\u00e8s les exemples affich\u00e9s, quelles difficult\u00e9s pourrait rencontrer le mod\u00e8le ?"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#partie-3-architecture-du-modele-cellule-4","title":"Partie 3 : Architecture du mod\u00e8le (Cellule 4)","text":"<p>Dessinez le sch\u00e9ma simplifi\u00e9 de l'architecture du r\u00e9seau de neurones utilis\u00e9 :</p> <pre><code>[Sch\u00e9ma \u00e0 compl\u00e9ter]\n</code></pre> Question Observation Combien de couches comporte le mod\u00e8le ? Combien de param\u00e8tres entra\u00eenables au total ? Quel est le r\u00f4le des couches de convolution ? Quel est le r\u00f4le des couches de pooling ? Pourquoi utilise-t-on 'softmax' comme activation de la derni\u00e8re couche ?"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#partie-4-entrainement-du-modele-cellule-5","title":"Partie 4 : Entra\u00eenement du mod\u00e8le (Cellule 5)","text":"Question Observation Combien d'\u00e9poques ont \u00e9t\u00e9 effectu\u00e9es ? Quelle est la pr\u00e9cision finale sur les donn\u00e9es d'entra\u00eenement ? Quelle est la pr\u00e9cision finale sur les donn\u00e9es de validation ? Quelle est la pr\u00e9cision sur l'ensemble de test ? Y a-t-il un signe de surapprentissage (overfitting) ? Pourquoi ?"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#partie-5-visualisation-des-resultats-cellule-6","title":"Partie 5 : Visualisation des r\u00e9sultats (Cellule 6)","text":"<p>Analysez les graphiques d'apprentissage :</p> Question Observation La courbe de pr\u00e9cision d'entra\u00eenement est-elle croissante ? La courbe de perte d'entra\u00eenement est-elle d\u00e9croissante ? Y a-t-il un \u00e9cart important entre les courbes d'entra\u00eenement et de validation ? D'apr\u00e8s vous, l'entra\u00eenement a-t-il \u00e9t\u00e9 suffisant (nombre d'\u00e9poques) ?"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#partie-6-predictions-sur-des-exemples-de-test-cellule-7","title":"Partie 6 : Pr\u00e9dictions sur des exemples de test (Cellule 7)","text":"<p>Observez les 10 exemples de pr\u00e9diction :</p> Question Observation Combien de pr\u00e9dictions sont correctes sur les 10 exemples ? Pour les pr\u00e9dictions incorrectes, quelles pourraient \u00eatre les raisons d'erreur ? Certains chiffres semblent-ils plus difficiles \u00e0 reconna\u00eetre que d'autres ?"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#partie-7-test-avec-votre-propre-dessin-cellule-8","title":"Partie 7 : Test avec votre propre dessin (Cellule 8)","text":"Question Observation Quels chiffres avez-vous dessin\u00e9s ? Combien ont \u00e9t\u00e9 correctement pr\u00e9dits ? Pour ceux mal pr\u00e9dits, quelle \u00e9tait la pr\u00e9diction et pourquoi selon vous ? Comment le pr\u00e9traitement de l'image a-t-il transform\u00e9 votre dessin ?"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#partie-8-experimentations-cellule-9","title":"Partie 8 : Exp\u00e9rimentations (Cellule 9)","text":"<p>Documentez vos exp\u00e9rimentations en modifiant le mod\u00e8le ou les param\u00e8tres :</p>"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#experimentation-1","title":"Exp\u00e9rimentation 1","text":"<p>Modification effectu\u00e9e : _______</p> Param\u00e8tre Valeur originale Nouvelle valeur <p>R\u00e9sultats : - Pr\u00e9cision test : _% - Observations : _____</p>"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#experimentation-2","title":"Exp\u00e9rimentation 2","text":"<p>Modification effectu\u00e9e : _______</p> Param\u00e8tre Valeur originale Nouvelle valeur <p>R\u00e9sultats : - Pr\u00e9cision test : _% - Observations : _____</p>"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#conclusion","title":"Conclusion","text":"Question R\u00e9ponse Quels sont les 3 principaux apprentissages de ce TP ? 1.2.3. Quelles am\u00e9liorations pourriez-vous sugg\u00e9rer pour ce mod\u00e8le ? Comment ce mod\u00e8le se compare-t-il aux capacit\u00e9s humaines de reconnaissance de chiffres ? Quelles autres applications de la vision par ordinateur vous int\u00e9ressent ?"},{"location":"seance1/partie1-mise-en-situation/fiche-observations-bts-sio/#glossaire-des-termes-cles-rencontres","title":"Glossaire des termes cl\u00e9s rencontr\u00e9s","text":"Terme Votre d\u00e9finition Convolution Pooling Epoch (\u00e9poque) Batch Dropout Softmax Overfitting (surapprentissage)"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/","title":"Guide d'utilisation de Google Colab","text":""},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#introduction-a-google-colab","title":"Introduction \u00e0 Google Colab","text":"<p>Google Colab (ou Colaboratory) est un environnement de notebook Jupyter h\u00e9berg\u00e9 par Google. Il permet d'ex\u00e9cuter du code Python dans votre navigateur et est particuli\u00e8rement adapt\u00e9 au machine learning, \u00e0 l'analyse de donn\u00e9es et \u00e0 l'\u00e9ducation.</p>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#avantages-de-google-colab","title":"Avantages de Google Colab","text":"<ul> <li>Gratuit : pas besoin d'installer Python ou des biblioth\u00e8ques sur votre ordinateur</li> <li>Puissant : acc\u00e8s \u00e0 des GPU et TPU gratuits</li> <li>Collaboratif : facilit\u00e9 de partage et de travail en \u00e9quipe</li> <li>Pr\u00eat \u00e0 l'emploi : biblioth\u00e8ques populaires d\u00e9j\u00e0 install\u00e9es (TensorFlow, PyTorch, etc.)</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#acceder-a-google-colab","title":"Acc\u00e9der \u00e0 Google Colab","text":"<ol> <li>Allez sur colab.research.google.com</li> <li>Connectez-vous avec votre compte Google</li> <li>Sur la page d'accueil, vous pouvez:</li> <li>Cr\u00e9er un nouveau notebook</li> <li>Ouvrir un notebook existant</li> <li>Acc\u00e9der \u00e0 des tutoriels</li> </ol>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#interface-de-colab","title":"Interface de Colab","text":"<p>L'interface de Colab est compos\u00e9e de:</p> <ol> <li>Barre de menu : Fichier, \u00c9dition, Affichage, etc.</li> <li>Barre d'outils : actions rapides</li> <li>Panneau de cellules : o\u00f9 vous \u00e9crivez et ex\u00e9cutez votre code</li> <li>Panneau lat\u00e9ral : pour acc\u00e9der aux fichiers, tableaux, etc.</li> </ol>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#types-de-cellules","title":"Types de cellules","text":"<p>Dans Colab, il existe deux types principaux de cellules:</p> <ul> <li>Cellules de code : pour ex\u00e9cuter du code Python</li> <li>Cellules de texte : pour \u00e9crire des commentaires en Markdown</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#cellules-de-code","title":"Cellules de code","text":"<pre><code># Exemple de cellule de code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.plot(x, y)\nplt.title(\"Fonction sinus\")\nplt.show()\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#cellules-de-texte-markdown","title":"Cellules de texte (Markdown)","text":"<p>Les cellules de texte utilisent la syntaxe Markdown:</p> <pre><code># Titre principal\n## Sous-titre\n\nTexte normal avec **texte en gras** et *texte en italique*.\n\nListe \u00e0 puces:\n- Item 1\n- Item 2\n\n\u00c9quation math\u00e9matique: $y = mx + b$\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#executer-du-code","title":"Ex\u00e9cuter du code","text":"<p>Pour ex\u00e9cuter une cellule: - Cliquez sur le bouton \u25b6\ufe0f \u00e0 gauche de la cellule - Ou utilisez le raccourci clavier <code>Shift+Enter</code></p> <p>Le r\u00e9sultat s'affiche directement sous la cellule.</p>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#raccourcis-clavier-utiles","title":"Raccourcis clavier utiles","text":"<ul> <li><code>Ctrl+Enter</code> : Ex\u00e9cuter la cellule</li> <li><code>Shift+Enter</code> : Ex\u00e9cuter la cellule et passer \u00e0 la suivante</li> <li><code>Alt+Enter</code> : Ex\u00e9cuter la cellule et ins\u00e9rer une nouvelle cellule en dessous</li> <li><code>Ctrl+M D</code> : Supprimer la cellule</li> <li><code>Ctrl+M A</code> : Ins\u00e9rer une cellule au-dessus</li> <li><code>Ctrl+M B</code> : Ins\u00e9rer une cellule en-dessous</li> <li><code>Ctrl+M M</code> : Transformer en cellule Markdown</li> <li><code>Ctrl+M Y</code> : Transformer en cellule de code</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#utiliser-le-gputpu","title":"Utiliser le GPU/TPU","text":"<p>Pour acc\u00e9l\u00e9rer l'ex\u00e9cution de votre code:</p> <ol> <li>Cliquez sur <code>Modifier</code> &gt; <code>Param\u00e8tres du notebook</code></li> <li>Sous <code>Acc\u00e9l\u00e9rateur mat\u00e9riel</code>, s\u00e9lectionnez <code>GPU</code> ou <code>TPU</code></li> <li>Cliquez sur <code>Enregistrer</code></li> </ol>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#installer-des-bibliotheques","title":"Installer des biblioth\u00e8ques","text":"<p>Colab poss\u00e8de d\u00e9j\u00e0 de nombreuses biblioth\u00e8ques install\u00e9es, mais vous pouvez en ajouter d'autres:</p> <pre><code>!pip install nom_de_la_biblioth\u00e8que\n</code></pre> <p>Exemple: <pre><code>!pip install transformers\n</code></pre></p> <p>Apr\u00e8s l'installation, red\u00e9marrez l'environnement d'ex\u00e9cution: 1. <code>Ex\u00e9cution</code> &gt; <code>Red\u00e9marrer l'environnement d'ex\u00e9cution...</code></p>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#gerer-les-fichiers","title":"G\u00e9rer les fichiers","text":""},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#importer-des-fichiers","title":"Importer des fichiers","text":"<ol> <li>Cliquez sur l'ic\u00f4ne \ud83d\udcc2 dans le panneau lat\u00e9ral gauche</li> <li>Cliquez sur <code>Importer</code> pour t\u00e9l\u00e9charger un fichier</li> </ol> <p>Ou via le code: <pre><code>from google.colab import files\nuploaded = files.upload()\n</code></pre></p>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#acceder-aux-fichiers-de-google-drive","title":"Acc\u00e9der aux fichiers de Google Drive","text":"<pre><code>from google.colab import drive\ndrive.mount('/content/drive')\n\n# Acc\u00e9der aux fichiers dans Drive\n!ls \"/content/drive/My Drive\"\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#telecharger-des-fichiers","title":"T\u00e9l\u00e9charger des fichiers","text":"<pre><code>from google.colab import files\nfiles.download('nom_du_fichier.ext')\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#enregistrer-votre-travail","title":"Enregistrer votre travail","text":"<p>Colab enregistre automatiquement votre travail dans Google Drive, mais vous pouvez aussi:</p> <ol> <li><code>Fichier</code> &gt; <code>Enregistrer une copie dans Drive</code></li> <li><code>Fichier</code> &gt; <code>T\u00e9l\u00e9charger</code> &gt; <code>T\u00e9l\u00e9charger .ipynb</code></li> </ol>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#partager-un-notebook","title":"Partager un notebook","text":"<ol> <li>Cliquez sur <code>Partager</code> en haut \u00e0 droite</li> <li>Entrez les adresses e-mail ou obtenez un lien de partage</li> <li>D\u00e9finissez les autorisations d'acc\u00e8s (Lecteur ou \u00c9diteur)</li> </ol>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#depannage-courant","title":"D\u00e9pannage courant","text":""},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#erreur-cuda-out-of-memory","title":"Erreur \"CUDA out of memory\"","text":"<ul> <li>Red\u00e9marrez l'environnement d'ex\u00e9cution (Ex\u00e9cution &gt; Red\u00e9marrer...)</li> <li>R\u00e9duisez la taille de votre mod\u00e8le ou de vos donn\u00e9es</li> <li>Utilisez un lot (batch) plus petit</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#deconnexion-apres-inactivite","title":"D\u00e9connexion apr\u00e8s inactivit\u00e9","text":"<ul> <li>Colab se d\u00e9connecte apr\u00e8s environ 90 minutes d'inactivit\u00e9</li> <li>Utilisez <code>Outils</code> &gt; <code>Param\u00e8tres</code> &gt; <code>Param\u00e8tres avanc\u00e9s</code> &gt; <code>D\u00e9sactiver l'interruption apr\u00e8s inactivit\u00e9</code></li> </ul>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#limites-de-temps-dexecution","title":"Limites de temps d'ex\u00e9cution","text":"<ul> <li>Les sessions sont limit\u00e9es \u00e0 environ 12 heures</li> <li>Pour des calculs plus longs, enregistrez p\u00e9riodiquement votre travail</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#perte-de-variables","title":"Perte de variables","text":"<ul> <li>Si vous ex\u00e9cutez les cellules dans un ordre diff\u00e9rent, certaines variables peuvent \u00eatre perdues</li> <li>Mieux vaut ex\u00e9cuter les cellules dans l'ordre s\u00e9quentiel</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#astuces-pour-les-tps-de-deep-learning","title":"Astuces pour les TPs de Deep Learning","text":"<ol> <li> <p>V\u00e9rifiez l'acc\u00e9l\u00e9rateur mat\u00e9riel avant de commencer un entra\u00eenement lourd    <pre><code>import tensorflow as tf\nprint(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))\n</code></pre></p> </li> <li> <p>Sauvegardez vos mod\u00e8les r\u00e9guli\u00e8rement    <pre><code>model.save('mon_modele.h5')\n</code></pre></p> </li> <li> <p>Visualisez vos donn\u00e9es avant l'entra\u00eenement    <pre><code>import matplotlib.pyplot as plt\nplt.imshow(X_train[0])\nplt.show()\n</code></pre></p> </li> <li> <p>Utilisez tqdm pour les barres de progression    <pre><code>!pip install tqdm\nfrom tqdm.notebook import tqdm\n\nfor epoch in tqdm(range(100)):\n    # votre boucle d'entra\u00eenement\n</code></pre></p> </li> <li> <p>Profitez de TensorBoard <pre><code>%load_ext tensorboard\n%tensorboard --logdir logs\n</code></pre></p> </li> </ol>"},{"location":"seance1/partie1-mise-en-situation/guide_utilisation_colab/#ressources-supplementaires","title":"Ressources suppl\u00e9mentaires","text":"<ul> <li>Documentation officielle de Google Colab</li> <li>Tutoriels TensorFlow dans Colab</li> <li>Tutoriels PyTorch dans Colab</li> </ul> <p>Bonne exploration et bon apprentissage du Deep Learning avec Google Colab !</p>"},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/","title":"Phase 1 : Mise en situation pratique (1h)","text":""},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#objectif","title":"Objectif","text":"<p>D\u00e9couvrir le Deep Learning par la pratique et l'observation, sans vous pr\u00e9occuper des aspects th\u00e9oriques qui seront abord\u00e9s plus tard.</p>"},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#introduction-par-lexemple-15-min","title":"Introduction par l'exemple (15 min)","text":"<ul> <li> <p>D\u00e9monstration 1 : GitHub Copilot (5 min)</p> </li> <li> <p>D\u00e9monstration 2 : Reconnaissance d'objets en temps r\u00e9el (5 min)</p> </li> <li> <p>D\u00e9monstration 3 : G\u00e9n\u00e9ration de texte (5 min)</p> </li> </ul>"},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#prise-en-main-immediate-30-min","title":"Prise en main imm\u00e9diate (30 min)","text":""},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#premier-contact-avec-un-reseau-de-neurones","title":"Premier contact avec un r\u00e9seau de neurones","text":"<p>Colab est un service Jupyter Notebook h\u00e9berg\u00e9 qui ne n\u00e9cessite aucune configuration et offre un acc\u00e8s gratuit aux ressources de calcul, notamment aux GPU et aux TPU. Colab est particuli\u00e8rement adapt\u00e9 \u00e0 l'apprentissage automatique, \u00e0 la science des donn\u00e9es et \u00e0 l'\u00e9ducation.</p>"},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#instructions-detaillees","title":"Instructions d\u00e9taill\u00e9es","text":""},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#1-creer-un-nouveau-notebook-colab","title":"1. Cr\u00e9er un nouveau notebook Colab","text":"<p>Allez sur Google Colab Cliquez sur \"Fichier\" &gt; \"Nouveau notebook\"</p>"},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#2-copier-coller-les-cellules-de-code","title":"2. Copier-coller les cellules de code","text":"<p>Dans votre nouveau notebook, vous allez cr\u00e9er 9 cellules en copiant-collant les blocs ci-dessous. Pour chaque cellule :</p> <p>Cliquez sur \"+\" Collez le contenu dans la nouvelle cellule Ex\u00e9cutez la cellule en cliquant sur le bouton \u25b6\ufe0f ou avec Ctrl+Entr\u00e9e Observez les r\u00e9sultats et notez vos observations dans la fiche</p>"},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#3-contenu-des-cellules-a-creer","title":"3. Contenu des cellules \u00e0 cr\u00e9er","text":"<p>Le fichier premier_contact_reseau_neurones.md contient toutes les cellules de code \u00e0 copier. Suivez ces cellules dans l'ordre:</p> <ul> <li>Introduction (cellule Markdown)</li> <li>Configuration (cellule Code)</li> <li>Chargement des donn\u00e9es (cellule Code)</li> <li>Cr\u00e9ation du mod\u00e8le (cellule Code)</li> <li>Entra\u00eenement (cellule Code)</li> <li>Visualisation (cellule Code)</li> <li>Pr\u00e9dictions (cellule Code)</li> <li>Dessin interactif (cellule Code)</li> <li>Exp\u00e9rimentation (cellule Markdown)</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#4-remplir-la-fiche-dobservations","title":"4. Remplir la fiche d'observations","text":"<ul> <li>Notez les r\u00e9sultats obtenus \u00e0 chaque \u00e9tape</li> <li>R\u00e9alisez les exp\u00e9rimentations sugg\u00e9r\u00e9es</li> <li>Compl\u00e9tez l'auto-\u00e9valuation</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#5-exploration-personnelle","title":"5. Exploration personnelle","text":"<ul> <li>Modifiez l'architecture du r\u00e9seau (nombre de filtres, nombre de couches)</li> <li>Changez les param\u00e8tres d'entra\u00eenement (\u00e9poques, taille de batch)</li> <li>Testez le mod\u00e8le avec vos propres dessins</li> <li>Notez l'impact de ces changements</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#6-rendu-final","title":"6. Rendu final","text":"<p>\u00c0 la fin de la s\u00e9ance, remettez :</p> <ul> <li>Votre notebook Colab compl\u00e9t\u00e9 (partagez le lien ou exportez en .ipynb)</li> <li>Votre fiche d'observations remplie</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#en-cas-de-difficultes","title":"En cas de difficult\u00e9s","text":"<ul> <li>R\u00e9f\u00e9rez-vous au guide d'utilisation de Google Colab fourni</li> <li>Assurez-vous que les cellules sont ex\u00e9cut\u00e9es dans l'ordre</li> <li>Si une erreur appara\u00eet, lisez attentivement le message et v\u00e9rifiez votre code</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/partie1-mise-en-situation/#fichiers-fournis","title":"Fichiers fournis","text":"<ul> <li>premier_contact_reseau_neurones.md : Contient toutes les cellules de code \u00e0 copier-coller</li> <li>fiche-observations-bts-sio.md : Document \u00e0 remplir pendant l'exercice</li> <li>guide_utilisation_colab.md : Aide pour les d\u00e9butants sur Google Colab</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/premier_contact_reseau_neurones/","title":"Premier contact reseau neurones","text":""},{"location":"seance1/partie1-mise-en-situation/premier_contact_reseau_neurones/#cellules-corrigees-pour-le-notebook","title":"Cellules corrig\u00e9es pour le notebook","text":""},{"location":"seance1/partie1-mise-en-situation/premier_contact_reseau_neurones/#cellule-1-introduction-markdown","title":"Cellule 1 (Introduction - Markdown)","text":"<pre><code># \ud83d\ude80 Hello World du Deep Learning\n\n## Reconnaissance de chiffres manuscrits avec TensorFlow et Keras\n\n### Objectifs de ce notebook\n\n- Charger et pr\u00e9parer un jeu de donn\u00e9es de chiffres manuscrits\n- Cr\u00e9er un r\u00e9seau de neurones simple\n- Entra\u00eener le mod\u00e8le\n- Visualiser les r\u00e9sultats\n- Tester le mod\u00e8le avec vos propres dessins\n\n### BTS SIO  - D\u00e9couverte du Deep Learning\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/premier_contact_reseau_neurones/#cellule-2-configuration-code","title":"Cellule 2 (Configuration - Code)","text":"<pre><code># Importation des biblioth\u00e8ques n\u00e9cessaires\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# V\u00e9rification de la version de TensorFlow\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\n\n# V\u00e9rification du GPU (m\u00e9thode recommand\u00e9e)\nprint(\"GPU disponible :\", len(tf.config.list_physical_devices('GPU')) &gt; 0)\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/premier_contact_reseau_neurones/#cellule-3-chargement-des-donnees-code","title":"Cellule 3 (Chargement des donn\u00e9es - Code)","text":"<pre><code># Chargement du dataset MNIST\n(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n\n# Pr\u00e9traitement des donn\u00e9es\nX_train = X_train.reshape((60000, 28, 28, 1)) / 255.0\nX_test = X_test.reshape((10000, 28, 28, 1)) / 255.0\n\n# Conversion des labels en cat\u00e9gories\ny_train = keras.utils.to_categorical(y_train)\ny_test = keras.utils.to_categorical(y_test)\n\n# Affichage de quelques exemples\nplt.figure(figsize=(10, 2))\nfor i in range(10):\n    plt.subplot(1, 10, i+1)\n    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n    plt.axis('off')\nplt.suptitle(\"Exemples de chiffres manuscrits\")\nplt.show()\n\nprint(f\"Nombre d'exemples d'entra\u00eenement : {X_train.shape[0]}\")\nprint(f\"Nombre d'exemples de test : {X_test.shape[0]}\")\nprint(f\"Dimensions d'une image : {X_train.shape[1:3]}\")\nprint(f\"Valeurs des pixels apr\u00e8s normalisation : de 0 \u00e0 1\")\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/premier_contact_reseau_neurones/#cellule-4-creation-du-modele-code","title":"Cellule 4 (Cr\u00e9ation du mod\u00e8le - Code)","text":"<pre><code># Cr\u00e9ation du mod\u00e8le de r\u00e9seau de neurones\n# On utilise Input comme premi\u00e8re couche (recommand\u00e9)\ninputs = keras.Input(shape=(28, 28, 1))\n\n# Couche de convolution\nx = layers.Conv2D(32, (3, 3), activation='relu')(inputs)\nx = layers.MaxPooling2D((2, 2))(x)\n\n# Couche de convolution suppl\u00e9mentaire\nx = layers.Conv2D(64, (3, 3), activation='relu')(x)\nx = layers.MaxPooling2D((2, 2))(x)\n\n# Aplatissement\nx = layers.Flatten()(x)\n\n# Couche dense\nx = layers.Dense(64, activation='relu')(x)\n\n# Couche de sortie\noutputs = layers.Dense(10, activation='softmax')(x)\n\n# Cr\u00e9ation du mod\u00e8le\nmodel = keras.Model(inputs, outputs, name=\"mnist_model\")\n\n# Compilation du mod\u00e8le\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Affichage du r\u00e9sum\u00e9 du mod\u00e8le\nmodel.summary()\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/premier_contact_reseau_neurones/#cellule-5-entrainement-code","title":"Cellule 5 (Entra\u00eenement - Code)","text":"<pre><code># Entra\u00eenement du mod\u00e8le\n# Note : Nombre d'\u00e9poques r\u00e9duit pour la d\u00e9monstration\nhistory = model.fit(\n    X_train, y_train,\n    epochs=5,\n    batch_size=64,\n    validation_split=0.2,\n    verbose=1\n)\n\n# \u00c9valuation du mod\u00e8le\ntest_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"\\nPr\u00e9cision sur l'ensemble de test : {test_accuracy*100:.2f}%\")\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/premier_contact_reseau_neurones/#cellule-6-visualisation-code","title":"Cellule 6 (Visualisation - Code)","text":"<pre><code># Visualisation de la pr\u00e9cision et de la perte\nplt.figure(figsize=(12, 4))\n\n# Pr\u00e9cision\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Pr\u00e9cision entra\u00eenement')\nplt.plot(history.history['val_accuracy'], label='Pr\u00e9cision validation')\nplt.title('Pr\u00e9cision du mod\u00e8le')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Pr\u00e9cision')\nplt.legend()\n\n# Perte\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Perte entra\u00eenement')\nplt.plot(history.history['val_loss'], label='Perte validation')\nplt.title('Perte du mod\u00e8le')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Perte')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/premier_contact_reseau_neurones/#cellule-7-predictions-code","title":"Cellule 7 (Pr\u00e9dictions - Code)","text":"<pre><code># Pr\u00e9dictions et visualisation\n# Pr\u00e9dire sur quelques images de test\npredictions = model.predict(X_test[:10])\n\nplt.figure(figsize=(15, 6))\nfor i in range(10):\n    plt.subplot(2, 10, i+1)\n    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"R\u00e9el: {np.argmax(y_test[i])}\")\n    plt.axis('off')\n\n    plt.subplot(2, 10, i+11)\n    plt.bar(range(10), predictions[i])\n    plt.title(f\"Pr\u00e9dit: {np.argmax(predictions[i])}\")\n    plt.xticks(range(10))\n    plt.ylim(0, 1)\n\nplt.suptitle(\"Pr\u00e9dictions du mod\u00e8le\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/premier_contact_reseau_neurones/#cellule-8-dessin-interactif-code","title":"Cellule 8 (Dessin interactif - Code)","text":"<pre><code># Interface interactive pour dessiner et pr\u00e9dire\n# Cette cellule permet de dessiner un chiffre directement dans Colab\n\nfrom google.colab import output\nfrom IPython.display import display, HTML\nimport io\nimport base64\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Fonction pour cr\u00e9er un canvas HTML\n\ndef create_canvas():\n    canvas_html = \"\"\"\n    &lt;canvas id=\"canvas\" width=\"280\" height=\"280\" style=\"border: 2px solid black; background-color: white;\"&gt;&lt;/canvas&gt;\n    &lt;div style=\"margin-top: 10px;\"&gt;\n      &lt;button id=\"predict_button\" style=\"padding: 5px 10px; background-color: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer;\"&gt;Pr\u00e9dire&lt;/button&gt;\n      &lt;button id=\"clear_button\" style=\"margin-left: 10px; padding: 5px 10px; background-color: #f44336; color: white; border: none; border-radius: 4px; cursor: pointer;\"&gt;Effacer&lt;/button&gt;\n    &lt;/div&gt;\n    &lt;div id=\"result\" style=\"margin-top: 10px; font-weight: bold;\"&gt;&lt;/div&gt;\n\n    &lt;script&gt;\n      var canvas = document.getElementById('canvas');\n      var ctx = canvas.getContext('2d');\n      var isDrawing = false;\n\n      // Remplir le fond en blanc d\u00e8s le d\u00e9part\n      ctx.fillStyle = \"white\";\n      ctx.fillRect(0, 0, canvas.width, canvas.height);\n\n      ctx.lineWidth = 15;\n      ctx.lineCap = 'round';\n      ctx.lineJoin = 'round';\n      ctx.strokeStyle = 'black';\n\n      canvas.addEventListener('mousedown', function(e) {\n        isDrawing = true;\n        ctx.beginPath();\n        ctx.moveTo(e.clientX - canvas.getBoundingClientRect().left, e.clientY - canvas.getBoundingClientRect().top);\n      });\n\n      canvas.addEventListener('mousemove', function(e) {\n        if (isDrawing) {\n          ctx.lineTo(e.clientX - canvas.getBoundingClientRect().left, e.clientY - canvas.getBoundingClientRect().top);\n          ctx.stroke();\n        }\n      });\n\n      canvas.addEventListener('mouseup', function() {\n        isDrawing = false;\n      });\n\n      canvas.addEventListener('mouseleave', function() {\n        isDrawing = false;\n      });\n\n      document.getElementById('clear_button').addEventListener('click', function() {\n        ctx.fillStyle = \"white\";\n        ctx.fillRect(0, 0, canvas.width, canvas.height);\n        document.getElementById('result').innerHTML = '';\n      });\n\n      document.getElementById('predict_button').addEventListener('click', function() {\n        var imageData = canvas.toDataURL('image/png');\n        document.getElementById('result').innerHTML = 'Analyse en cours...';\n        google.colab.kernel.invokeFunction('notebook.predict', [imageData], {});\n      });\n    &lt;/script&gt;\n    \"\"\"\n    return canvas_html\n\n# Fonction pour pr\u00e9traiter l'image dessin\u00e9e\ndef preprocess_image(image_data):\n    image_data = image_data.split(',')[1]\n    image = Image.open(io.BytesIO(base64.b64decode(image_data)))\n\n    # Convertir en niveaux de gris et redimensionner\n    image = image.convert('L').resize((28, 28))\n\n    # Convertir en tableau numpy\n    image_array = np.array(image)\n\n    # V\u00e9rification si inversion des couleurs est n\u00e9cessaire\n    if np.mean(image_array) &gt; 127:  # Fond clair, chiffre sombre\n        image_array = 255 - image_array\n\n    # Normaliser les pixels\n    image_array = image_array / 255.0\n\n    return image_array\n\n# Fonction de pr\u00e9diction\ndef predict_digit(image_data):\n    image_array = preprocess_image(image_data)\n    image_array = image_array.reshape(1, 28, 28, 1)\n\n    # Affichage de l'image pr\u00e9trait\u00e9e pour v\u00e9rifier\n    plt.figure(figsize=(6, 3))\n    plt.subplot(1, 2, 1)\n    plt.imshow(image_array.reshape(28, 28), cmap='gray')\n    plt.title(\"Image pr\u00e9trait\u00e9e\")\n    plt.axis('off')\n\n    # Pr\u00e9diction\n    prediction = model.predict(image_array)[0]\n    digit = np.argmax(prediction)\n    confidence = prediction[digit] * 100\n\n    # Affichage du graphique des probabilit\u00e9s\n    plt.subplot(1, 2, 2)\n    plt.bar(range(10), prediction)\n    plt.title(f\"Pr\u00e9diction: {digit}\")\n    plt.xlabel(\"Chiffre\")\n    plt.ylabel(\"Confiance\")\n    plt.xticks(range(10))\n    plt.show()\n\n    # Afficher le r\u00e9sultat dans le notebook\n    output.eval_js(f\"\"\"\n    document.getElementById('result').innerHTML = 'Pr\u00e9diction: {digit} (Confiance: {confidence:.2f}%)';\n    \"\"\")\n\n# Enregistrer la fonction pour \u00eatre appel\u00e9e depuis JavaScript\noutput.register_callback('notebook.predict', predict_digit)\n\n# Afficher le canvas\ndisplay(HTML(create_canvas()))\nprint(\"Dessinez un chiffre dans le canvas ci-dessus et cliquez sur 'Pr\u00e9dire'\")\n\n\n\n### Cellule 9 (Exp\u00e9rimentation - Markdown)\n```markdown\n## \ud83e\uddea Exp\u00e9rimentations\n\nVoici quelques modifications que vous pouvez essayer pour am\u00e9liorer ou observer les effets sur le mod\u00e8le :\n\n1. **Modifier l'architecture du r\u00e9seau :**\n   - Augmenter/diminuer le nombre de filtres dans les couches Conv2D\n   - Ajouter/retirer des couches convolutives ou denses\n   - Ajouter une couche Dropout pour r\u00e9duire le surapprentissage\n\n2. **Changer les hyperparam\u00e8tres d'entra\u00eenement :**\n   - Augmenter le nombre d'\u00e9poques\n   - Modifier la taille du batch\n   - Essayer diff\u00e9rents optimiseurs (SGD, RMSprop, etc.)\n\n3. **Augmenter les donn\u00e9es :**\n   - Appliquer des rotations ou d\u00e9calages aux images d'entra\u00eenement\n\nPour chaque modification, observez l'impact sur :\n- La pr\u00e9cision finale\n- La vitesse d'entra\u00eenement\n- Les courbes d'apprentissage\n- Le comportement face \u00e0 vos propres dessins\n\nN'h\u00e9sitez pas \u00e0 documenter vos observations dans la fiche fournie.\n</code></pre>"},{"location":"seance1/partie1-mise-en-situation/presentation-slides/","title":"Deep Learning en action","text":""},{"location":"seance1/partie1-mise-en-situation/presentation-slides/#applications-concretes-et-demonstrations","title":"Applications concr\u00e8tes et d\u00e9monstrations","text":""},{"location":"seance1/partie1-mise-en-situation/presentation-slides/#quest-ce-que-le-deep-learning","title":"Qu'est-ce que le Deep Learning ?","text":"<ul> <li>Sous-domaine du Machine Learning</li> <li>Utilise des r\u00e9seaux de neurones \u00e0 plusieurs couches</li> <li>Apprend automatiquement les caract\u00e9ristiques importantes des donn\u00e9es</li> <li>Particuli\u00e8rement performant sur les donn\u00e9es complexes (images, texte, son)</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/presentation-slides/#pourquoi-le-deep-learning-change-la-donne","title":"Pourquoi le Deep Learning change la donne","text":"<ul> <li>Capacit\u00e9 \u00e0 traiter des donn\u00e9es non structur\u00e9es (texte, image, son)</li> <li>Performances sup\u00e9rieures sur des t\u00e2ches complexes</li> <li>Extraction automatique des caract\u00e9ristiques pertinentes</li> <li>Apprentissage de bout en bout (end-to-end learning)</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/presentation-slides/#applications-que-nous-allons-explorer-aujourdhui","title":"Applications que nous allons explorer aujourd'hui","text":"<ol> <li>GitHub Copilot : Assistant de programmation IA</li> <li>Reconnaissance d'objets : D\u00e9tection et classification d'objets dans des images</li> <li>G\u00e9n\u00e9ration de texte : Cr\u00e9ation automatique de contenu textuel coh\u00e9rent</li> </ol> <p>![Applications du Deep Learning](../../images/deep-learning-action.svg</p>"},{"location":"seance1/partie1-mise-en-situation/presentation-slides/#mais-comment-ca-marche-vraiment","title":"Mais comment \u00e7a marche vraiment ?","text":"<ul> <li>Le Deep Learning n'est pas \"magique\"</li> <li>Repose sur des principes math\u00e9matiques solides</li> <li>Apprend par optimisation statistique</li> <li>Ne \"comprend\" pas le monde comme les humains</li> </ul>"},{"location":"seance1/partie1-mise-en-situation/presentation-slides/#a-votre-tour","title":"\u00c0 votre tour !","text":"<p>Passons maintenant \u00e0 une exp\u00e9rience pratique :</p> <ul> <li>Construire vous-m\u00eame un r\u00e9seau de neurones simple</li> <li>L'entra\u00eener sur un jeu de donn\u00e9es de chiffres manuscrits (MNIST)</li> <li>Tester ses performances et l'optimiser</li> </ul> <p></p>"},{"location":"seance1/partie1-mise-en-situation/presentation-slides/#objectifs-de-lexercice-pratique","title":"Objectifs de l'exercice pratique","text":"<ol> <li>Se familiariser avec les outils (Google Colab, TensorFlow/Keras)</li> <li>Comprendre les \u00e9tapes de cr\u00e9ation et d'entra\u00eenement d'un mod\u00e8le</li> <li>Observer l'impact des param\u00e8tres sur les performances</li> <li>Visualiser le processus d'apprentissage en action</li> </ol>"},{"location":"seance1/partie1-mise-en-situation/presentation-slides/#commencer-lexercice-pratique","title":"Commencer l'exercice pratique \u2192","text":"<p>Mise en situation du Deep Learning</p> <p>Bon apprentissage !</p>"},{"location":"seance1/ressources/comparaison-ml-dl/","title":"Comparaison Machine Learning vs Deep Learning","text":""},{"location":"seance1/ressources/comparaison-ml-dl/#tableau-comparatif-detaille","title":"Tableau Comparatif D\u00e9taill\u00e9","text":"Crit\u00e8re Machine Learning Classique Deep Learning D\u00e9finition Algorithmes qui apprennent \u00e0 partir de donn\u00e9es en identifiant des patterns pr\u00e9d\u00e9finis Sous-ensemble du Machine Learning bas\u00e9 sur des r\u00e9seaux de neurones artificiels multicouches Extraction des caract\u00e9ristiques N\u00e9cessite un travail manuel d'ing\u00e9nierie des caract\u00e9ristiques (feature engineering) Capacit\u00e9 \u00e0 extraire automatiquement les caract\u00e9ristiques pertinentes Volume de donn\u00e9es requis Fonctionne bien avec des petits \u00e0 moyens ensembles de donn\u00e9es Performant avec de tr\u00e8s grands volumes de donn\u00e9es Puissance de calcul Peut fonctionner sur des ordinateurs standard N\u00e9cessite g\u00e9n\u00e9ralement des GPU ou des ressources de calcul puissantes Types de probl\u00e8mes Classification, r\u00e9gression, clustering simples Probl\u00e8mes complexes : reconnaissance d'image, traitement du langage naturel, g\u00e9n\u00e9ration de contenu Interpr\u00e9tabilit\u00e9 Souvent plus facile \u00e0 interpr\u00e9ter Plus difficile \u00e0 comprendre (bo\u00eete noire) Complexit\u00e9 d'impl\u00e9mentation Plus simple \u00e0 mettre en place Plus complexe, n\u00e9cessite une expertise plus pointe"},{"location":"seance1/ressources/comparaison-ml-dl/#exemple-pratique-de-difference","title":"Exemple Pratique de Diff\u00e9rence","text":""},{"location":"seance1/ressources/comparaison-ml-dl/#machine-learning-classique-random-forest","title":"Machine Learning Classique (Random Forest)","text":"<ul> <li>Processus :</li> <li>Extraction manuelle des caract\u00e9ristiques</li> <li>S\u00e9lection des attributs pertinents</li> <li>Entra\u00eenement sur des donn\u00e9es pr\u00e9par\u00e9es</li> <li>Forces :</li> <li>Interpr\u00e9tabilit\u00e9</li> <li>Efficacit\u00e9 sur des donn\u00e9es structur\u00e9es</li> <li>Limites :</li> <li>Performance limit\u00e9e sur des donn\u00e9es complexes</li> <li>N\u00e9cessite une expertise en feature engineering</li> </ul>"},{"location":"seance1/ressources/comparaison-ml-dl/#deep-learning-reseau-de-neurones","title":"Deep Learning (R\u00e9seau de Neurones)","text":"<ul> <li>Processus :</li> <li>Alimentation directe des donn\u00e9es brutes</li> <li>Extraction automatique des caract\u00e9ristiques</li> <li>Apprentissage multicouche</li> <li>Forces :</li> <li>Apprentissage automatique des caract\u00e9ristiques</li> <li>Haute performance sur des donn\u00e9es non structur\u00e9es</li> <li>Capacit\u00e9 \u00e0 g\u00e9rer des probl\u00e8mes complexes</li> <li>Limites :</li> <li>Besoin de grandes quantit\u00e9s de donn\u00e9es</li> <li>Ressources de calcul importantes</li> <li>Moins interpr\u00e9table</li> </ul>"},{"location":"seance1/ressources/comparaison-ml-dl/#analogie-explicative","title":"Analogie Explicative","text":"<p>\ud83e\udde9 Machine Learning Classique : Comme un puzzle o\u00f9 vous devez trier et placer manuellement chaque pi\u00e8ce.</p> <p>\ud83e\udde0 Deep Learning : Comme un cerveau qui apprend \u00e0 reconna\u00eetre le puzzle automatiquement, en comprenant les relations entre les pi\u00e8ces.</p>"},{"location":"seance1/ressources/comparaison-ml-dl/#quand-utiliser-quoi","title":"Quand Utiliser Quoi ?","text":""},{"location":"seance1/ressources/comparaison-ml-dl/#machine-learning-classique","title":"Machine Learning Classique","text":"<ul> <li>Donn\u00e9es structur\u00e9es limit\u00e9es</li> <li>Probl\u00e8mes simples de pr\u00e9diction</li> <li>Besoin d'interpr\u00e9tabilit\u00e9</li> <li>Ressources de calcul limit\u00e9es</li> </ul>"},{"location":"seance1/ressources/comparaison-ml-dl/#deep-learning","title":"Deep Learning","text":"<ul> <li>Grandes quantit\u00e9s de donn\u00e9es</li> <li>Probl\u00e8mes complexes (image, son, texte)</li> <li>N\u00e9cessit\u00e9 de features avanc\u00e9es</li> <li>Ressources de calcul disponibles</li> </ul>"},{"location":"seance1/ressources/comparaison-ml-dl/#exercice-pratique","title":"Exercice Pratique","text":"<p>Durant la s\u00e9ance, vous allez : 1. Impl\u00e9menter un mod\u00e8le de Machine Learning classique 2. Cr\u00e9er un r\u00e9seau de neurones simple 3. Comparer leurs performances 4. Comprendre les diff\u00e9rences concr\u00e8tes</p>"},{"location":"seance1/ressources/comparaison-ml-dl/#ressources-complementaires","title":"Ressources Compl\u00e9mentaires","text":"<ul> <li>\ud83d\udcd8 Livres recommand\u00e9s</li> <li>\ud83c\udf10 Liens vers des tutoriels</li> <li>\ud83d\udcf9 Vid\u00e9os explicatives</li> </ul>"},{"location":"seance1/ressources/comparaison-ml-dl/#reflexion-personnelle","title":"R\u00e9flexion Personnelle","text":"<p>Prenez des notes sur : - Les diff\u00e9rences que vous observez - Les surprises lors de votre exp\u00e9rimentation - Les questions qui \u00e9mergent</p> <p>Objectif : D\u00e9velopper une compr\u00e9hension intuitive, pas seulement technique !</p>"},{"location":"seance1/ressources/fiche-observations/","title":"Fiche d'Observations : Hello World du Deep Learning","text":""},{"location":"seance1/ressources/fiche-observations/#informations-personnelles","title":"Informations Personnelles","text":"<ul> <li>Nom : ________</li> <li>Pr\u00e9nom : ________</li> <li>Groupe : ________</li> <li>Date : ________</li> </ul>"},{"location":"seance1/ressources/fiche-observations/#partie-1-exploration-du-modele","title":"Partie 1 : Exploration du Mod\u00e8le","text":""},{"location":"seance1/ressources/fiche-observations/#configuration-initiale","title":"Configuration Initiale","text":"<ul> <li>Biblioth\u00e8ques utilis\u00e9es :   \u25a1 NumPy   \u25a1 Matplotlib   \u25a1 TensorFlow   \u25a1 Keras</li> </ul>"},{"location":"seance1/ressources/fiche-observations/#jeu-de-donnees-mnist","title":"Jeu de Donn\u00e9es MNIST","text":"<ul> <li>Nombre total d'images d'entra\u00eenement : ______</li> <li>Nombre total d'images de test : ______</li> <li>Taille des images : __ x ____</li> </ul>"},{"location":"seance1/ressources/fiche-observations/#partie-2-architecture-du-reseau-de-neurones","title":"Partie 2 : Architecture du R\u00e9seau de Neurones","text":""},{"location":"seance1/ressources/fiche-observations/#structure-du-modele","title":"Structure du Mod\u00e8le","text":"<p>Notez les couches et leurs caract\u00e9ristiques :</p> <ol> <li>Couche 1 (Convolution) :</li> <li>Nombre de filtres : _</li> <li>Taille des filtres : _ </li> <li> <p>Fonction d'activation : _</p> </li> <li> <p>Couche de Pooling :</p> </li> <li>Type : _</li> <li> <p>Taille : _</p> </li> <li> <p>Couches Denses :</p> </li> <li>Nombre de neurones (1\u00e8re couche dense) : _</li> <li>Fonction d'activation (1\u00e8re couche dense) : _</li> <li>Nombre de neurones (couche de sortie) : _</li> <li>Fonction d'activation (couche de sortie) : _</li> </ol>"},{"location":"seance1/ressources/fiche-observations/#partie-3-entrainement-du-modele","title":"Partie 3 : Entra\u00eenement du Mod\u00e8le","text":""},{"location":"seance1/ressources/fiche-observations/#parametres-dentrainement","title":"Param\u00e8tres d'Entra\u00eenement","text":"<ul> <li>Nombre d'\u00e9poques : _</li> <li>Taille du batch : _</li> <li>Optimiseur : _</li> <li>Fonction de perte : _</li> </ul>"},{"location":"seance1/ressources/fiche-observations/#metriques-de-performance","title":"M\u00e9triques de Performance","text":"M\u00e9trique Entra\u00eenement Validation Pr\u00e9cision ______ ______ Perte ______ ______"},{"location":"seance1/ressources/fiche-observations/#partie-4-observations-et-reflexions","title":"Partie 4 : Observations et R\u00e9flexions","text":""},{"location":"seance1/ressources/fiche-observations/#evolution-de-lapprentissage","title":"\u00c9volution de l'Apprentissage","text":"<p>D\u00e9crivez ce que vous observez dans les graphiques de pr\u00e9cision et de perte :</p>"},{"location":"seance1/ressources/fiche-observations/#predictions","title":"Pr\u00e9dictions","text":"<p>Notez quelques exemples de pr\u00e9dictions correctes et incorrectes :</p> <p>Pr\u00e9dictions correctes : 1. ____ 2. ______</p> <p>Pr\u00e9dictions incorrectes : 1. ____ 2. ______</p>"},{"location":"seance1/ressources/fiche-observations/#partie-5-experimentation-personnelle","title":"Partie 5 : Exp\u00e9rimentation Personnelle","text":""},{"location":"seance1/ressources/fiche-observations/#modifications-testees","title":"Modifications Test\u00e9es","text":"<p>Indiquez les modifications que vous avez apport\u00e9es et leurs effets :</p> <ol> <li> <p>Modification : ____    Effet observ\u00e9 : ______</p> </li> <li> <p>Modification : ____    Effet observ\u00e9 : ______</p> </li> </ol>"},{"location":"seance1/ressources/fiche-observations/#reflexion-finale","title":"R\u00e9flexion Finale","text":"<p>Quels sont les trois concepts cl\u00e9s que vous avez appris aujourd'hui ? 1. ____ 2. ___ 3. _____</p>"},{"location":"seance1/ressources/fiche-observations/#questions-ouvertes","title":"Questions Ouvertes","text":"<p>Quelles questions restent sans r\u00e9ponse pour vous apr\u00e8s cette s\u00e9ance ?</p>"},{"location":"seance1/ressources/fiche-observations/#evaluation-personnelle","title":"\u00c9valuation Personnelle","text":"<p>Sur une \u00e9chelle de 1 \u00e0 5, \u00e9valuez votre compr\u00e9hension : - Fonctionnement de base d'un r\u00e9seau de neurones : [  ] - Impact des param\u00e8tres sur l'apprentissage : [  ] - Processus de reconnaissance d'images : [  ]</p> <p>1 = Tr\u00e8s faible, 5 = Excellent</p>"},{"location":"seance1/ressources/fiche-observations/#commentaires-supplementaires","title":"Commentaires Suppl\u00e9mentaires","text":"<p>Merci d'avoir particip\u00e9 \u00e0 cette premi\u00e8re exploration du Deep Learning !</p>"},{"location":"seance1/ressources/glossaire-dl/","title":"Glossaire du Deep Learning","text":""},{"location":"seance1/ressources/glossaire-dl/#termes-fondamentaux","title":"Termes fondamentaux","text":"Terme D\u00e9finition Exemple concret Deep Learning Sous-domaine du Machine Learning utilisant des r\u00e9seaux de neurones \u00e0 plusieurs couches Reconnaissance d'objets dans des photos R\u00e9seau de neurones Syst\u00e8me inspir\u00e9 du cerveau humain compos\u00e9 de n\u0153uds (neurones) interconnect\u00e9s R\u00e9seau capable de reconna\u00eetre des chiffres manuscrits Neurone artificiel Unit\u00e9 de calcul de base qui re\u00e7oit des entr\u00e9es, applique une transformation et produit une sortie Un neurone qui s'active quand il d\u00e9tecte un contour vertical Couche Ensemble de neurones situ\u00e9s au m\u00eame niveau dans le r\u00e9seau Couche d'entr\u00e9e, couche cach\u00e9e, couche de sortie Poids Valeurs num\u00e9riques qui d\u00e9finissent l'importance relative de chaque connexion Un poids \u00e9lev\u00e9 (ex: 0.8) indique une forte influence Biais Valeur ajout\u00e9e \u00e0 la somme pond\u00e9r\u00e9e pour ajuster le seuil d'activation Permet \u00e0 un neurone de s'activer m\u00eame si toutes les entr\u00e9es sont nulles Fonction d'activation Fonction math\u00e9matique qui d\u00e9termine la sortie d'un neurone ReLU, Sigmoid, Tanh"},{"location":"seance1/ressources/glossaire-dl/#architectures-de-reseaux","title":"Architectures de r\u00e9seaux","text":"Terme D\u00e9finition Cas d'utilisation R\u00e9seau dense R\u00e9seau o\u00f9 chaque neurone est connect\u00e9 \u00e0 tous les neurones de la couche pr\u00e9c\u00e9dente Classification d'images simples, pr\u00e9diction de valeurs R\u00e9seau convolutif (CNN) R\u00e9seau sp\u00e9cialis\u00e9 dans le traitement des donn\u00e9es en grille comme les images Reconnaissance d'objets, classification d'images R\u00e9seau r\u00e9current (RNN) R\u00e9seau avec des connexions formant des cycles, adapt\u00e9 aux donn\u00e9es s\u00e9quentielles Traduction automatique, g\u00e9n\u00e9ration de texte LSTM/GRU Types de RNN capables de m\u00e9moriser l'information sur de longues s\u00e9quences Analyse de texte long, pr\u00e9diction de s\u00e9ries temporelles"},{"location":"seance1/ressources/glossaire-dl/#apprentissage","title":"Apprentissage","text":"Terme D\u00e9finition Exemple Forward propagation Passage des donn\u00e9es d'entr\u00e9e \u00e0 travers le r\u00e9seau pour produire une pr\u00e9diction Calcul de la sortie d'un mod\u00e8le pour une image d'entr\u00e9e Loss (perte) Mesure de l'\u00e9cart entre les pr\u00e9dictions et les valeurs r\u00e9elles Erreur quadratique moyenne, entropie crois\u00e9e Backpropagation Algorithme qui calcule le gradient de l'erreur par rapport aux poids Calcul de la contribution de chaque poids \u00e0 l'erreur totale Descente de gradient Algorithme d'optimisation qui ajuste les poids pour minimiser l'erreur Modification it\u00e9rative des poids dans la direction du gradient n\u00e9gatif \u00c9poque Un passage complet \u00e0 travers l'ensemble des donn\u00e9es d'entra\u00eenement Entra\u00eener un mod\u00e8le pendant 10 \u00e9poques Batch Sous-ensemble des donn\u00e9es trait\u00e9 avant une mise \u00e0 jour des poids Traiter les donn\u00e9es par lots de 32 exemples"},{"location":"seance1/ressources/glossaire-dl/#hyperparametres-et-optimisation","title":"Hyperparam\u00e8tres et optimisation","text":"Terme D\u00e9finition Impact Learning rate Taux qui contr\u00f4le l'ampleur des ajustements des poids Trop \u00e9lev\u00e9: divergence, trop faible: apprentissage lent Dropout Technique o\u00f9 des neurones sont al\u00e9atoirement d\u00e9sactiv\u00e9s pendant l'entra\u00eenement R\u00e9duit l'overfitting en for\u00e7ant le r\u00e9seau \u00e0 \u00eatre redondant Batch normalization Normalisation des activations d'une couche pour stabiliser l'apprentissage Acc\u00e9l\u00e8re l'entra\u00eenement et am\u00e9liore la convergence Early stopping Arr\u00eat de l'entra\u00eenement quand les performances sur la validation cessent de s'am\u00e9liorer \u00c9vite l'overfitting en emp\u00eachant le surajustement aux donn\u00e9es d'entra\u00eenement"},{"location":"seance1/ressources/glossaire-dl/#problemes-courants","title":"Probl\u00e8mes courants","text":"Terme D\u00e9finition Solution possible Overfitting Le mod\u00e8le apprend trop bien les donn\u00e9es d'entra\u00eenement au d\u00e9triment de la g\u00e9n\u00e9ralisation R\u00e9gularisation, dropout, plus de donn\u00e9es Underfitting Le mod\u00e8le est trop simple pour capturer la complexit\u00e9 des donn\u00e9es Augmenter la complexit\u00e9 du mod\u00e8le, entra\u00eener plus longtemps Vanishing gradient Probl\u00e8me o\u00f9 le gradient devient tr\u00e8s petit, ralentissant l'apprentissage Utiliser ReLU, LSTM, initialisation des poids adapt\u00e9e Exploding gradient Probl\u00e8me o\u00f9 le gradient devient tr\u00e8s grand, d\u00e9stabilisant l'apprentissage Gradient clipping, normalisation des poids"},{"location":"seance1/ressources/math-simplified-nn/","title":"Les bases du Deep Learning : une approche simplifi\u00e9e","text":""},{"location":"seance1/ressources/math-simplified-nn/#introduction","title":"Introduction","text":"<p>Ce document pr\u00e9sente les concepts fondamentaux du Deep Learning de mani\u00e8re accessible, sans les math\u00e9matiques complexes. Notre objectif est de vous donner une compr\u00e9hension intuitive du fonctionnement des r\u00e9seaux de neurones.</p>"},{"location":"seance1/ressources/math-simplified-nn/#1-le-neurone-artificiel-comprendre-la-brique-de-base","title":"1. Le neurone artificiel : comprendre la brique de base","text":""},{"location":"seance1/ressources/math-simplified-nn/#comment-fonctionne-un-neurone-artificiel","title":"Comment fonctionne un neurone artificiel ?","text":"<p>Un neurone artificiel s'inspire du fonctionnement des neurones biologiques. Il combine plusieurs informations d'entr\u00e9e pour produire une d\u00e9cision.</p> <p></p> <p>Fonctionnement en 5 \u00e9tapes :</p> <ol> <li>Recevoir des entr\u00e9es : Le neurone re\u00e7oit plusieurs valeurs (comme la taille et le poids d'un fruit)</li> <li>Pond\u00e9rer ces entr\u00e9es : Certaines entr\u00e9es sont plus importantes que d'autres (le poids peut \u00eatre plus d\u00e9terminant que la taille)</li> <li>Faire la somme : Additionner toutes ces valeurs pond\u00e9r\u00e9es</li> <li>Ajouter un d\u00e9calage (biais) : Ajuster le seuil de d\u00e9cision</li> <li>Appliquer une fonction d'activation : Transformer cette somme en une sortie utile</li> </ol> <p>Analogie : Si vous d\u00e9cidez d'acheter un t\u00e9l\u00e9phone, vous consid\u00e9rez plusieurs facteurs (prix, performances, appareil photo) avec diff\u00e9rentes importances. Vous additionnez mentalement ces consid\u00e9rations, puis prenez une d\u00e9cision (acheter ou non).</p>"},{"location":"seance1/ressources/math-simplified-nn/#la-fonction-dactivation-le-pouvoir-de-decision","title":"La fonction d'activation : le pouvoir de d\u00e9cision","text":"<p>La fonction d'activation est comme un interrupteur qui d\u00e9termine si le neurone \"s'active\" ou non.</p> <p>Types principaux :</p> <ol> <li>ReLU (Rectified Linear Unit) : </li> <li>Simple : si le nombre est n\u00e9gatif, il devient 0; sinon, il reste inchang\u00e9</li> <li> <p>Comme un interrupteur qui ne laisse passer que les valeurs positives</p> </li> <li> <p>Sigmoid : </p> </li> <li>Transforme n'importe quel nombre en une valeur entre 0 et 1</li> <li>Pratique pour exprimer des probabilit\u00e9s (ex: probabilit\u00e9 que l'image contienne un chat)</li> </ol> <p>Visualisation :</p> <p></p>"},{"location":"seance1/ressources/math-simplified-nn/#2-le-reseau-de-neurones-une-equipe-organisee","title":"2. Le r\u00e9seau de neurones : une \u00e9quipe organis\u00e9e","text":"<p>Un r\u00e9seau de neurones est simplement une organisation de neurones en couches qui travaillent ensemble.</p>"},{"location":"seance1/ressources/math-simplified-nn/#organisation-en-couches","title":"Organisation en couches","text":"<ul> <li>Couche d'entr\u00e9e : Re\u00e7oit les donn\u00e9es brutes (pixels d'une image, mots d'un texte...)</li> <li>Couches cach\u00e9es : Traitent l'information de fa\u00e7on de plus en plus abstraite</li> <li>Couche de sortie : Donne le r\u00e9sultat final (classification, pr\u00e9diction...)</li> </ul> <p>Analogie : Dans une entreprise, les informations passent par plusieurs services avant d'aboutir \u00e0 une d\u00e9cision finale. Chaque service (couche) traite l'information \u00e0 son niveau.</p>"},{"location":"seance1/ressources/math-simplified-nn/#comment-linformation-circule","title":"Comment l'information circule","text":"<ol> <li>Les donn\u00e9es entrent par la premi\u00e8re couche</li> <li>Chaque neurone calcule sa sortie et la transmet aux neurones de la couche suivante</li> <li>L'information se propage ainsi jusqu'\u00e0 la couche de sortie</li> </ol> <p>C'est ce qu'on appelle la propagation avant ou \"forward propagation\".</p>"},{"location":"seance1/ressources/math-simplified-nn/#3-lapprentissage-comment-le-reseau-devient-intelligent","title":"3. L'apprentissage : comment le r\u00e9seau devient intelligent","text":""},{"location":"seance1/ressources/math-simplified-nn/#le-principe-de-base","title":"Le principe de base","text":"<p>L'apprentissage d'un r\u00e9seau de neurones se r\u00e9sume \u00e0 : 1. Faire des pr\u00e9dictions 2. Mesurer les erreurs 3. Ajuster les poids pour r\u00e9duire ces erreurs 4. Recommencer</p> <p></p>"},{"location":"seance1/ressources/math-simplified-nn/#mesurer-lerreur","title":"Mesurer l'erreur","text":"<p>Pour savoir si le r\u00e9seau fait bien son travail, on calcule la diff\u00e9rence entre : - Ce que le r\u00e9seau pr\u00e9dit - Ce qu'il aurait d\u00fb pr\u00e9dire (la v\u00e9rit\u00e9)</p> <p>Plus cette diff\u00e9rence est petite, meilleur est le r\u00e9seau.</p> <p>Exemple : Si le r\u00e9seau pr\u00e9dit qu'une image a 80% de chances de contenir un chat alors qu'il y a effectivement un chat, l'erreur est de 20%.</p>"},{"location":"seance1/ressources/math-simplified-nn/#lajustement-des-poids-la-descente-de-gradient","title":"L'ajustement des poids : la descente de gradient","text":"<p>Pour am\u00e9liorer le r\u00e9seau, on ajuste les poids dans la bonne direction :</p> <ol> <li>On d\u00e9termine si chaque poids doit \u00eatre augment\u00e9 ou diminu\u00e9</li> <li>On modifie chaque poids d'un petit pas dans la bonne direction</li> <li>On v\u00e9rifie si la pr\u00e9diction s'am\u00e9liore</li> </ol> <p>Analogie : Imaginez que vous \u00eates dans le brouillard en montagne et que vous voulez descendre. Vous t\u00e2tez le terrain autour de vous pour sentir o\u00f9 \u00e7a descend, puis vous faites un pas dans cette direction. Vous r\u00e9p\u00e9tez jusqu'\u00e0 atteindre le fond de la vall\u00e9e.</p> <p></p>"},{"location":"seance1/ressources/math-simplified-nn/#retropropagation-distribuer-la-responsabilite","title":"R\u00e9tropropagation : distribuer la responsabilit\u00e9","text":"<p>Quand le r\u00e9seau fait une erreur, comment savoir quels poids ajuster ? La r\u00e9tropropagation consiste \u00e0 :</p> <ol> <li>Calculer l'erreur \u00e0 la sortie</li> <li>\"Remonter\" cette erreur dans le r\u00e9seau</li> <li>D\u00e9terminer la contribution de chaque connexion \u00e0 l'erreur totale</li> <li>Ajuster chaque poids en cons\u00e9quence</li> </ol> <p>Analogie : Dans une \u00e9quipe qui a commis une erreur, on analyse la responsabilit\u00e9 de chaque membre pour savoir qui doit ajuster son comportement et comment.</p>"},{"location":"seance1/ressources/math-simplified-nn/#4-les-architectures-populaires-simplifiees","title":"4. Les architectures populaires simplifi\u00e9es","text":""},{"location":"seance1/ressources/math-simplified-nn/#les-reseaux-convolutifs-cnn-pour-les-images","title":"Les r\u00e9seaux convolutifs (CNN) pour les images","text":"<p>Les CNN sont sp\u00e9cialement con\u00e7us pour traiter des images :</p> <p></p> <ul> <li>Ils utilisent des \"filtres\" qui balayent l'image pour d\u00e9tecter des motifs</li> <li>Les premi\u00e8res couches d\u00e9tectent des \u00e9l\u00e9ments simples (lignes, coins)</li> <li>Les couches suivantes combinent ces \u00e9l\u00e9ments pour reconna\u00eetre des formes plus complexes (yeux, visages, objets)</li> </ul> <p>Analogie : C'est comme si vous regardiez une image avec diff\u00e9rentes loupes, chacune sp\u00e9cialis\u00e9e pour rep\u00e9rer un certain type de d\u00e9tail.</p>"},{"location":"seance1/ressources/math-simplified-nn/#les-reseaux-recurrents-rnn-pour-les-sequences","title":"Les r\u00e9seaux r\u00e9currents (RNN) pour les s\u00e9quences","text":"<p>Les RNN sont adapt\u00e9s aux donn\u00e9es s\u00e9quentielles (texte, parole, vid\u00e9o) :</p> <p></p> <ul> <li>Ils ont une \"m\u00e9moire\" qui conserve les informations importantes des \u00e9tapes pr\u00e9c\u00e9dentes</li> <li>Cette m\u00e9moire permet de comprendre le contexte (comme le sens d'un mot en fonction des mots pr\u00e9c\u00e9dents)</li> </ul> <p>Analogie : C'est comme lire un livre en se souvenant des chapitres pr\u00e9c\u00e9dents pour comprendre le chapitre actuel.</p>"},{"location":"seance1/ressources/math-simplified-nn/#5-astuces-pour-ameliorer-les-reseaux","title":"5. Astuces pour am\u00e9liorer les r\u00e9seaux","text":""},{"location":"seance1/ressources/math-simplified-nn/#la-regularisation-eviter-le-par-cur","title":"La r\u00e9gularisation : \u00e9viter le \"par c\u0153ur\"","text":"<p>Parfois, un r\u00e9seau apprend trop bien les exemples d'entra\u00eenement mais ne g\u00e9n\u00e9ralise pas aux nouveaux cas. C'est le surapprentissage.</p> <p>La r\u00e9gularisation est comme imposer des contraintes au r\u00e9seau pour l'emp\u00eacher de trop se sp\u00e9cialiser :</p> <ul> <li>Dropout : D\u00e9sactiver al\u00e9atoirement certains neurones pendant l'entra\u00eenement</li> <li>L1/L2 : P\u00e9naliser les poids trop grands</li> </ul> <p>Analogie : C'est comme \u00e9tudier pour un examen en variant les conditions d'\u00e9tude pour s'assurer de comprendre le concept et pas juste m\u00e9moriser.</p>"},{"location":"seance1/ressources/math-simplified-nn/#le-taux-dapprentissage-la-taille-des-pas","title":"Le taux d'apprentissage : la taille des pas","text":"<p>Le taux d'apprentissage d\u00e9termine l'ampleur des ajustements \u00e0 chaque \u00e9tape : - Trop grand : on risque de \"sauter\" par-dessus la solution - Trop petit : l'apprentissage est tr\u00e8s lent</p> <p></p> <p>Analogie : C'est comme r\u00e9gler la sensibilit\u00e9 du volant d'une voiture - trop sensible et vous zigzaguez, pas assez et vous tournez trop lentement.</p>"},{"location":"seance1/ressources/math-simplified-nn/#conclusion","title":"Conclusion","text":"<p>Vous avez maintenant une compr\u00e9hension intuitive des concepts fondamentaux du Deep Learning :</p> <ol> <li>Les neurones artificiels combinent et transforment les informations</li> <li>Les r\u00e9seaux organisent ces neurones en couches pour traiter l'information progressivement</li> <li>L'apprentissage consiste \u00e0 ajuster les poids pour minimiser les erreurs</li> <li>Diff\u00e9rentes architectures sont sp\u00e9cialis\u00e9es pour diff\u00e9rents types de donn\u00e9es</li> <li>Des techniques sp\u00e9cifiques permettent d'am\u00e9liorer les performances</li> </ol> <p>Cette approche simplifi\u00e9e vous donne les bases n\u00e9cessaires pour comprendre les discussions sur le Deep Learning et pour explorer plus en profondeur si vous le souhaitez.</p>"},{"location":"seance1/ressources/math-simplified-nn/#pour-aller-plus-loin","title":"Pour aller plus loin","text":"<p>Si ces concepts vous int\u00e9ressent, vous pouvez explorer :</p> <ul> <li>TensorFlow Playground - Visualiser et exp\u00e9rimenter avec des r\u00e9seaux de neurones simples</li> <li>3Blue1Brown - Neural Networks - S\u00e9rie de vid\u00e9os avec d'excellentes visualisations</li> <li>Machine Learning for Beginners - Cours de Microsoft avec approche pratique</li> </ul> <p>Retour aux ressources</p>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/","title":"Comparaison entre Machine Learning classique et Deep Learning","text":""},{"location":"seance1/ressources/ml-vs-dl-comparaison/#introduction","title":"Introduction","text":"<p>Ce notebook interactif vous propose d'explorer par la pratique les diff\u00e9rences fondamentales entre le Machine Learning classique et le Deep Learning. Vous manipulerez en parall\u00e8le deux approches diff\u00e9rentes pour r\u00e9soudre un m\u00eame probl\u00e8me de classification d'images.</p>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#objectifs-pedagogiques","title":"Objectifs p\u00e9dagogiques","text":"<p>\u00c0 la fin de cette activit\u00e9, vous serez capable de : - Identifier les diff\u00e9rences cl\u00e9s entre les deux approches - Comprendre les forces et faiblesses de chaque m\u00e9thode - Analyser les performances dans diff\u00e9rents contextes - Expliquer pourquoi le Deep Learning excelle dans certaines t\u00e2ches</p>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#partie-1-preparation-et-configuration","title":"Partie 1 : Pr\u00e9paration et configuration","text":""},{"location":"seance1/ressources/ml-vs-dl-comparaison/#11-importation-des-bibliotheques","title":"1.1 Importation des biblioth\u00e8ques","text":"<p>Pour le notebook A (Machine Learning classique) :</p> <pre><code># Importation des biblioth\u00e8ques n\u00e9cessaires\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\n</code></pre> <p>Pour le notebook B (Deep Learning) :</p> <pre><code># Importation des biblioth\u00e8ques n\u00e9cessaires\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.utils import to_categorical\nimport time\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#12-chargement-des-donnees-mnist","title":"1.2 Chargement des donn\u00e9es MNIST","text":"<p>Le jeu de donn\u00e9es MNIST contient des images 28x28 pixels en niveaux de gris de chiffres manuscrits (0 \u00e0 9).</p> <pre><code># Chargement du jeu de donn\u00e9es MNIST\nfrom tensorflow.keras.datasets import mnist\n\n# Chargement et s\u00e9paration des donn\u00e9es\n(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n\n# Utiliser un sous-ensemble pour acc\u00e9l\u00e9rer les tests\nsample_size = 10000\nX_train = X_train_full[:sample_size]\ny_train = y_train_full[:sample_size]\n\n# Afficher les dimensions\nprint(f\"Dimensions de X_train : {X_train.shape}\")\nprint(f\"Dimensions de y_train : {y_train.shape}\")\nprint(f\"Dimensions de X_test : {X_test.shape}\")\nprint(f\"Dimensions de y_test : {y_test.shape}\")\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#13-visualisation-de-quelques-exemples","title":"1.3 Visualisation de quelques exemples","text":"<pre><code># Afficher quelques exemples d'images\nplt.figure(figsize=(10, 5))\nfor i in range(10):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(X_train[i], cmap='gray')\n    plt.title(f\"Label: {y_train[i]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.suptitle(\"Exemples de chiffres manuscrits\", y=1.05)\nplt.show()\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#partie-2-approche-machine-learning-classique","title":"Partie 2 : Approche Machine Learning classique","text":""},{"location":"seance1/ressources/ml-vs-dl-comparaison/#21-pretraitement-des-donnees-pour-le-ml-classique","title":"2.1 Pr\u00e9traitement des donn\u00e9es pour le ML classique","text":"<p>Pour le ML classique, nous devons transformer les images 2D en vecteurs 1D et r\u00e9duire la dimensionnalit\u00e9 pour acc\u00e9l\u00e9rer l'entra\u00eenement.</p> <pre><code># Aplatir les images 28x28 en vecteurs 784\nX_train_flat = X_train.reshape(X_train.shape[0], -1)\nX_test_flat = X_test.reshape(X_test.shape[0], -1)\n\n# Normaliser les valeurs de pixels entre 0 et 1\nX_train_flat = X_train_flat / 255.0\nX_test_flat = X_test_flat / 255.0\n\n# R\u00e9duction de dimensionnalit\u00e9 avec PCA\nn_components = 50  # R\u00e9duire de 784 \u00e0 50 dimensions\npca = PCA(n_components=n_components)\nX_train_pca = pca.fit_transform(X_train_flat)\nX_test_pca = pca.transform(X_test_flat)\n\nprint(f\"Forme originale : {X_train_flat.shape}\")\nprint(f\"Forme apr\u00e8s PCA : {X_train_pca.shape}\")\nprint(f\"Variance expliqu\u00e9e : {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#22-creation-et-entrainement-du-modele-random-forest","title":"2.2 Cr\u00e9ation et entra\u00eenement du mod\u00e8le Random Forest","text":"<pre><code># Param\u00e8tres du mod\u00e8le\nn_estimators = 100  # Nombre d'arbres\nmax_depth = 15      # Profondeur maximale des arbres\n\n# Cr\u00e9ation du mod\u00e8le\nrf_model = RandomForestClassifier(\n    n_estimators=n_estimators,\n    max_depth=max_depth,\n    random_state=42,\n    n_jobs=-1  # Utiliser tous les c\u0153urs disponibles\n)\n\n# Mesurer le temps d'entra\u00eenement\nstart_time = time.time()\nprint(\"Entra\u00eenement du mod\u00e8le Random Forest...\")\nrf_model.fit(X_train_pca, y_train)\nend_time = time.time()\ntraining_time_rf = end_time - start_time\n\nprint(f\"Temps d'entra\u00eenement : {training_time_rf:.2f} secondes\")\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#23-evaluation-du-modele-random-forest","title":"2.3 \u00c9valuation du mod\u00e8le Random Forest","text":"<pre><code># Pr\u00e9dictions sur l'ensemble de test\ny_pred_rf = rf_model.predict(X_test_pca)\n\n# Calcul des m\u00e9triques\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nconf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\nclass_report_rf = classification_report(y_test, y_pred_rf)\n\nprint(f\"Pr\u00e9cision globale : {accuracy_rf*100:.2f}%\")\nprint(\"\\nMatrice de confusion :\")\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Pr\u00e9dictions')\nplt.ylabel('Valeurs r\u00e9elles')\nplt.title('Matrice de confusion - Random Forest')\nplt.show()\n\nprint(\"\\nRapport de classification :\")\nprint(class_report_rf)\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#24-visualisation-des-erreurs-du-random-forest","title":"2.4 Visualisation des erreurs du Random Forest","text":"<pre><code># Identifier les erreurs\nerror_indices_rf = np.where(y_pred_rf != y_test)[0]\nn_errors = min(10, len(error_indices_rf))\n\nif n_errors &gt; 0:\n    plt.figure(figsize=(12, 4))\n    for i in range(n_errors):\n        plt.subplot(2, 5, i + 1)\n        idx = error_indices_rf[i]\n        img = X_test[idx]\n        plt.imshow(img, cmap='gray')\n        plt.title(f\"R\u00e9el: {y_test[idx]}\\nPr\u00e9dit: {y_pred_rf[idx]}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.suptitle(\"Erreurs de classification - Random Forest\", y=1.05)\n    plt.show()\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#partie-3-approche-deep-learning","title":"Partie 3 : Approche Deep Learning","text":""},{"location":"seance1/ressources/ml-vs-dl-comparaison/#31-pretraitement-des-donnees-pour-le-deep-learning","title":"3.1 Pr\u00e9traitement des donn\u00e9es pour le Deep Learning","text":"<pre><code># Redimensionner et normaliser les donn\u00e9es\nX_train_dl = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\nX_test_dl = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n\n# Conversion des labels en cat\u00e9gories one-hot\ny_train_cat = to_categorical(y_train, 10)\ny_test_cat = to_categorical(y_test, 10)\n\nprint(f\"Forme de X_train_dl : {X_train_dl.shape}\")\nprint(f\"Forme de y_train_cat : {y_train_cat.shape}\")\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#32-creation-du-modele-cnn","title":"3.2 Cr\u00e9ation du mod\u00e8le CNN","text":"<pre><code># Cr\u00e9ation d'un mod\u00e8le CNN simple\ncnn_model = Sequential([\n    # Premi\u00e8re couche de convolution\n    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    # Deuxi\u00e8me couche de convolution\n    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    # Aplatissement et couches denses\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),  # Pour r\u00e9duire le surapprentissage\n    Dense(10, activation='softmax')  # 10 classes (chiffres 0-9)\n])\n\n# Compilation du mod\u00e8le\ncnn_model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Afficher un r\u00e9sum\u00e9 du mod\u00e8le\ncnn_model.summary()\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#33-entrainement-du-modele-cnn","title":"3.3 Entra\u00eenement du mod\u00e8le CNN","text":"<pre><code># Mesurer le temps d'entra\u00eenement\nstart_time = time.time()\nprint(\"Entra\u00eenement du mod\u00e8le CNN...\")\n\n# Entra\u00eenement\nhistory = cnn_model.fit(\n    X_train_dl, y_train_cat,\n    batch_size=128,\n    epochs=5,  # Nombre d'\u00e9poques r\u00e9duit pour la d\u00e9monstration\n    validation_split=0.1,\n    verbose=1\n)\n\nend_time = time.time()\ntraining_time_cnn = end_time - start_time\n\nprint(f\"Temps d'entra\u00eenement : {training_time_cnn:.2f} secondes\")\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#34-evaluation-du-modele-cnn","title":"3.4 \u00c9valuation du mod\u00e8le CNN","text":"<pre><code># \u00c9valuation sur l'ensemble de test\ntest_loss, test_acc = cnn_model.evaluate(X_test_dl, y_test_cat, verbose=0)\nprint(f\"Pr\u00e9cision sur l'ensemble de test : {test_acc*100:.2f}%\")\n\n# Pr\u00e9dictions\ny_pred_cnn = cnn_model.predict(X_test_dl)\ny_pred_classes = np.argmax(y_pred_cnn, axis=1)\n\n# Matrice de confusion\nconf_matrix_cnn = confusion_matrix(y_test, y_pred_classes)\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix_cnn, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Pr\u00e9dictions')\nplt.ylabel('Valeurs r\u00e9elles')\nplt.title('Matrice de confusion - CNN')\nplt.show()\n\n# Rapport de classification\nprint(\"\\nRapport de classification :\")\nprint(classification_report(y_test, y_pred_classes))\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#35-visualisation-des-erreurs-du-cnn","title":"3.5 Visualisation des erreurs du CNN","text":"<pre><code># Identifier les erreurs\nerror_indices_cnn = np.where(y_pred_classes != y_test)[0]\nn_errors = min(10, len(error_indices_cnn))\n\nif n_errors &gt; 0:\n    plt.figure(figsize=(12, 4))\n    for i in range(n_errors):\n        plt.subplot(2, 5, i + 1)\n        idx = error_indices_cnn[i]\n        img = X_test[idx]\n        plt.imshow(img, cmap='gray')\n        plt.title(f\"R\u00e9el: {y_test[idx]}\\nPr\u00e9dit: {y_pred_classes[idx]}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.suptitle(\"Erreurs de classification - CNN\", y=1.05)\n    plt.show()\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#36-visualisation-de-lapprentissage","title":"3.6 Visualisation de l'apprentissage","text":"<pre><code># Visualiser l'\u00e9volution de l'apprentissage\nplt.figure(figsize=(12, 4))\n\n# \u00c9volution de la pr\u00e9cision\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Entra\u00eenement')\nplt.plot(history.history['val_accuracy'], label='Validation')\nplt.title('\u00c9volution de la pr\u00e9cision')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Pr\u00e9cision')\nplt.legend()\n\n# \u00c9volution de la perte\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Entra\u00eenement')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.title('\u00c9volution de la perte')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Perte')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#partie-4-defi-de-generalisation","title":"Partie 4 : D\u00e9fi de g\u00e9n\u00e9ralisation","text":"<p>Dans cette partie, nous allons tester les deux mod\u00e8les sur des donn\u00e9es transform\u00e9es pour \u00e9valuer leur capacit\u00e9 de g\u00e9n\u00e9ralisation.</p>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#41-creation-de-donnees-modifiees","title":"4.1 Cr\u00e9ation de donn\u00e9es modifi\u00e9es","text":"<pre><code># Fonction pour ajouter du bruit aux images\ndef add_noise(images, noise_level=0.2):\n    noisy_images = images.copy()\n    noise = np.random.normal(0, noise_level, images.shape)\n    noisy_images = noisy_images + noise\n    return np.clip(noisy_images, 0, 1)  # Limiter les valeurs entre 0 et 1\n\n# Fonction pour appliquer une rotation aux images\ndef rotate_images(images, max_angle=15):\n    from scipy.ndimage import rotate\n    rotated_images = np.zeros_like(images)\n    for i in range(len(images)):\n        angle = np.random.uniform(-max_angle, max_angle)\n        rotated = rotate(images[i], angle, reshape=False)\n        rotated_images[i] = rotated\n    return rotated_images\n\n# Cr\u00e9er des versions modifi\u00e9es du jeu de test\nX_test_noisy = add_noise(X_test / 255.0)\nX_test_rotated = rotate_images(X_test / 255.0)\n\n# Visualiser quelques exemples\nplt.figure(figsize=(12, 8))\nfor i in range(5):\n    # Original\n    plt.subplot(3, 5, i + 1)\n    plt.imshow(X_test[i], cmap='gray')\n    plt.title(f\"Original: {y_test[i]}\")\n    plt.axis('off')\n\n    # Avec bruit\n    plt.subplot(3, 5, i + 6)\n    plt.imshow(X_test_noisy[i], cmap='gray')\n    plt.title(\"Avec bruit\")\n    plt.axis('off')\n\n    # Avec rotation\n    plt.subplot(3, 5, i + 11)\n    plt.imshow(X_test_rotated[i], cmap='gray')\n    plt.title(\"Avec rotation\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.suptitle(\"Exemples de chiffres modifi\u00e9s\", y=1.02)\nplt.show()\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#42-evaluation-sur-les-donnees-modifiees","title":"4.2 \u00c9valuation sur les donn\u00e9es modifi\u00e9es","text":"<pre><code># Pr\u00e9paration pour Random Forest\nX_test_noisy_flat = X_test_noisy.reshape(X_test.shape[0], -1)\nX_test_rotated_flat = X_test_rotated.reshape(X_test.shape[0], -1)\nX_test_noisy_pca = pca.transform(X_test_noisy_flat)\nX_test_rotated_pca = pca.transform(X_test_rotated_flat)\n\n# Pr\u00e9paration pour CNN\nX_test_noisy_dl = X_test_noisy.reshape(X_test.shape[0], 28, 28, 1)\nX_test_rotated_dl = X_test_rotated.reshape(X_test.shape[0], 28, 28, 1)\n\n# \u00c9valuation de Random Forest\ny_pred_rf_noisy = rf_model.predict(X_test_noisy_pca)\ny_pred_rf_rotated = rf_model.predict(X_test_rotated_pca)\naccuracy_rf_noisy = accuracy_score(y_test, y_pred_rf_noisy)\naccuracy_rf_rotated = accuracy_score(y_test, y_pred_rf_rotated)\n\n# \u00c9valuation de CNN\ny_pred_cnn_noisy = cnn_model.predict(X_test_noisy_dl)\ny_pred_cnn_rotated = cnn_model.predict(X_test_rotated_dl)\ny_pred_cnn_noisy_classes = np.argmax(y_pred_cnn_noisy, axis=1)\ny_pred_cnn_rotated_classes = np.argmax(y_pred_cnn_rotated, axis=1)\naccuracy_cnn_noisy = accuracy_score(y_test, y_pred_cnn_noisy_classes)\naccuracy_cnn_rotated = accuracy_score(y_test, y_pred_cnn_rotated_classes)\n\n# Affichage des r\u00e9sultats\nprint(\"Performances sur les donn\u00e9es originales :\")\nprint(f\"Random Forest : {accuracy_rf*100:.2f}%\")\nprint(f\"CNN : {test_acc*100:.2f}%\")\nprint(\"\\nPerformances sur les donn\u00e9es avec bruit :\")\nprint(f\"Random Forest : {accuracy_rf_noisy*100:.2f}%\")\nprint(f\"CNN : {accuracy_cnn_noisy*100:.2f}%\")\nprint(\"\\nPerformances sur les donn\u00e9es avec rotation :\")\nprint(f\"Random Forest : {accuracy_rf_rotated*100:.2f}%\")\nprint(f\"CNN : {accuracy_cnn_rotated*100:.2f}%\")\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#43-visualisation-comparative-des-performances","title":"4.3 Visualisation comparative des performances","text":"<pre><code># Cr\u00e9er un dataframe pour les comparaisons\nimport pandas as pd\n\ndata = {\n    'Type de donn\u00e9es': ['Originales', 'Avec bruit', 'Avec rotation'],\n    'Random Forest': [accuracy_rf*100, accuracy_rf_noisy*100, accuracy_rf_rotated*100],\n    'CNN': [test_acc*100, accuracy_cnn_noisy*100, accuracy_cnn_rotated*100]\n}\n\ndf = pd.DataFrame(data)\n\n# Visualisation en barres group\u00e9es\nplt.figure(figsize=(10, 6))\nx = np.arange(len(df['Type de donn\u00e9es']))\nwidth = 0.35\n\nplt.bar(x - width/2, df['Random Forest'], width, label='Random Forest')\nplt.bar(x + width/2, df['CNN'], width, label='CNN')\n\nplt.xlabel('Type de donn\u00e9es')\nplt.ylabel('Pr\u00e9cision (%)')\nplt.title('Comparaison des performances ML vs DL')\nplt.xticks(x, df['Type de donn\u00e9es'])\nplt.legend()\n\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#partie-5-analyse-comparative","title":"Partie 5 : Analyse comparative","text":""},{"location":"seance1/ressources/ml-vs-dl-comparaison/#51-tableau-comparatif-des-approches","title":"5.1 Tableau comparatif des approches","text":"<p>Compl\u00e9tez le tableau ci-dessous en vous basant sur vos observations :</p> Aspect observ\u00e9 Machine Learning (Random Forest) Deep Learning (CNN) Pr\u00e9paration des donn\u00e9es Complexit\u00e9 du mod\u00e8le Temps d'entra\u00eenement Pr\u00e9cision sur donn\u00e9es originales Pr\u00e9cision sur donn\u00e9es bruit\u00e9es Pr\u00e9cision sur donn\u00e9es avec rotation Interpr\u00e9tabilit\u00e9 Ressources n\u00e9cessaires"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#52-questions-de-reflexion","title":"5.2 Questions de r\u00e9flexion","text":"<ol> <li>Quelles sont les principales diff\u00e9rences que vous avez observ\u00e9es entre les deux approches ?</li> <li>Pourquoi le Deep Learning performe-t-il diff\u00e9remment du Machine Learning classique sur des donn\u00e9es transform\u00e9es ?</li> <li>Dans quels contextes recommanderiez-vous d'utiliser :</li> <li>Le Machine Learning classique ?</li> <li>Le Deep Learning ?</li> <li>Quels compromis devez-vous consid\u00e9rer lors du choix entre ces deux approches ?</li> <li>Comment pourriez-vous am\u00e9liorer chacun des mod\u00e8les pour obtenir de meilleures performances ?</li> </ol>"},{"location":"seance1/ressources/ml-vs-dl-comparaison/#conclusion","title":"Conclusion","text":"<p>Cette exploration comparative vous a permis de d\u00e9couvrir et d'exp\u00e9rimenter les diff\u00e9rences fondamentales entre le Machine Learning classique et le Deep Learning. Vous avez pu observer comment chaque approche traite les donn\u00e9es, comment elles apprennent et g\u00e9n\u00e9ralisent, ainsi que leurs forces et faiblesses respectives.</p> <p>Points cl\u00e9s \u00e0 retenir : - Le Deep Learning excelle dans l'apprentissage automatique des caract\u00e9ristiques pertinentes - Le Machine Learning classique n\u00e9cessite souvent une extraction manuelle des caract\u00e9ristiques - La capacit\u00e9 de g\u00e9n\u00e9ralisation diff\u00e8re significativement entre les deux approches - Chaque m\u00e9thode a ses propres avantages en termes de temps d'apprentissage, pr\u00e9cision et facilit\u00e9 d'utilisation</p> <p>Dans les prochaines s\u00e9ances, nous approfondirons ces concepts et explorerons plus en d\u00e9tail les architectures sp\u00e9cifiques de r\u00e9seaux de neurones.</p>"},{"location":"seance2/","title":"S\u00e9ance 2 : Types de r\u00e9seaux et leurs applications","text":""},{"location":"seance2/#objectifs-de-la-seance","title":"Objectifs de la s\u00e9ance","text":"<p>Cette s\u00e9ance vous permettra de :</p> <ul> <li>Comprendre et manipuler les architectures CNN pour la vision par ordinateur</li> <li>Explorer les r\u00e9seaux RNN pour le traitement des s\u00e9quences et du langage naturel</li> <li>Int\u00e9grer des mod\u00e8les de Deep Learning dans des applications web</li> <li>Exp\u00e9rimenter avec l'API Mistral AI pour des t\u00e2ches de g\u00e9n\u00e9ration de texte</li> <li>D\u00e9velopper une approche d'am\u00e9lioration it\u00e9rative des mod\u00e8les</li> </ul>"},{"location":"seance2/#structure-de-la-seance-4h","title":"Structure de la s\u00e9ance (4h)","text":"gantt     dateFormat  HH:mm     axisFormat %H:%M     section Planning     Mini-projet CNN                  :milestone, 00:00, 1h30m     Mini-projet RNN/LSTM             :milestone, 01:30, 1h30m     Pause                           :milestone, 03:00, 15m     Challenge d'am\u00e9lioration        :milestone, 03:15, 45m"},{"location":"seance2/#trois-phases-dapprentissage","title":"Trois phases d'apprentissage","text":""},{"location":"seance2/#phase-1-mini-projet-cnn-pour-la-vision-par-ordinateur-1h30","title":"Phase 1 : Mini-projet CNN pour la vision par ordinateur (1h30)","text":"<p>Plongez dans l'univers des r\u00e9seaux de neurones convolutifs (CNN) et apprenez \u00e0 les utiliser pour la classification d'images.</p> <ul> <li>Principes des convolutions et du pooling</li> <li>Impl\u00e9mentation d'un CNN avec TensorFlow/Keras</li> <li>Visualisation des filtres et feature maps</li> <li>Int\u00e9gration d'un mod\u00e8le CNN dans une application web</li> </ul>"},{"location":"seance2/#phase-2-mini-projet-rnn-pour-le-traitement-du-langage-1h30","title":"Phase 2 : Mini-projet RNN pour le traitement du langage (1h30)","text":"<p>D\u00e9couvrez les r\u00e9seaux r\u00e9currents (RNN/LSTM) et leur application au traitement du langage naturel.</p> <ul> <li>Principes des r\u00e9seaux r\u00e9currents et mod\u00e8les LSTM</li> <li>Impl\u00e9mentation d'un mod\u00e8le d'analyse de sentiment</li> <li>Exp\u00e9rimentation avec l'API Mistral AI</li> <li>Premier pas vers le projet de chatbot p\u00e9dagogique</li> </ul>"},{"location":"seance2/#phase-3-challenge-damelioration-de-modele-45min","title":"Phase 3 : Challenge d'am\u00e9lioration de mod\u00e8le (45min)","text":"<p>Travaillez en \u00e9quipe pour am\u00e9liorer les performances d'un mod\u00e8le sous-optimal.</p> <ul> <li>Diagnostic des faiblesses d'un mod\u00e8le existant</li> <li>Exp\u00e9rimentation avec diff\u00e9rentes architectures et hyperparam\u00e8tres</li> <li>Documentation et analyse des am\u00e9liorations</li> <li>Comparaison des r\u00e9sultats entre \u00e9quipes</li> </ul> <p>Pour des instructions d\u00e9taill\u00e9es sur l'installation et l'utilisation des scripts d'int\u00e9gration, consultez les instructions d'int\u00e9gration.</p>"},{"location":"seance2/#competences-developpees","title":"Comp\u00e9tences d\u00e9velopp\u00e9es","text":"<p>Cette s\u00e9ance vous permettra de d\u00e9velopper plusieurs comp\u00e9tences du r\u00e9f\u00e9rentiel BTS SIO  :</p> <ul> <li>B2.2 : Conception de solutions applicatives (architecture CNN/RNN)</li> <li>B2.3 : D\u00e9veloppement (impl\u00e9mentation des mod\u00e8les et int\u00e9gration)</li> <li>B1.4 : Exploitation des standards (APIs REST)</li> <li>B1.3 : Gestion des donn\u00e9es (datasets d'images et de texte)</li> </ul>"},{"location":"seance2/#livrables-attendus","title":"Livrables attendus","text":"<p>\u00c0 l'issue de cette s\u00e9ance, vous devrez avoir produit :</p> <ol> <li>Un mod\u00e8le CNN fonctionnel pour la classification d'images avec visualisations</li> <li>Un mod\u00e8le RNN/LSTM pour l'analyse de sentiment avec premiers tests d'int\u00e9gration API</li> <li>Un rapport d'am\u00e9lioration documentant vos exp\u00e9rimentations sur le mod\u00e8le sous-optimal</li> </ol> <p>Ces livrables seront utilis\u00e9s comme base pour le d\u00e9veloppement du chatbot p\u00e9dagogique lors des s\u00e9ances suivantes.</p>"},{"location":"seance2/#pret-a-commencer","title":"Pr\u00eat \u00e0 commencer ?","text":"<p>Commencez par la premi\u00e8re phase pour explorer les r\u00e9seaux convolutifs (CNN) et leur application \u00e0 la vision par ordinateur.</p> <p>Commencer la Phase 1: CNN</p>"},{"location":"seance2/arborescence/","title":"Arborescence","text":"<p>seance2/ \u2502 \u251c\u2500\u2500 index.md                                       # Page principale de la s\u00e9ance 2 \u2502 \u251c\u2500\u2500 atelier1-reconnaissance-images/                # Atelier 1 \u2502   \u251c\u2500\u2500 guide-atelier1.md                          # Guide de travail pratique \u2502   \u251c\u2500\u2500 notebooks/ \u2502   \u2502   \u251c\u2500\u2500 00_workshop_intro.ipynb                # Introduction \u00e0 l'atelier \u2502   \u2502   \u251c\u2500\u2500 01_model_exploration.ipynb             # Exploration du mod\u00e8le pr\u00e9-entra\u00een\u00e9 \u2502   \u2502   \u251c\u2500\u2500 02_model_adaptation.ipynb              # Adaptation du mod\u00e8le au catalogue \u2502   \u2502   \u2514\u2500\u2500 03_business_adaptation.ipynb           # Configuration des seuils et cas sp\u00e9ciaux \u2502   \u251c\u2500\u2500 api_project/ \u2502   \u2502   \u251c\u2500\u2500 app.py                                 # Squelette de l'API Flask \u2502   \u2502   \u251c\u2500\u2500 requirements.txt                       # D\u00e9pendances Python \u2502   \u2502   \u2514\u2500\u2500 Dockerfile                             # Template pour la conteneurisation \u2502   \u251c\u2500\u2500 api_docs/ \u2502   \u2502   \u2514\u2500\u2500 openapi.yaml                           # Documentation OpenAPI/Swagger \u2502   \u2514\u2500\u2500 data/ \u2502       \u251c\u2500\u2500 examples/                              # Images d'exemple pour les tests \u2502       \u2514\u2500\u2500 techretail/                            # Dataset client (images + m\u00e9tadonn\u00e9es) \u2502 \u251c\u2500\u2500 atelier2-assistant-virtuel/                    # Atelier 2 \u2502   \u251c\u2500\u2500 guide-atelier2.md                          # Guide de travail pratique \u2502   \u251c\u2500\u2500 notebooks/ \u2502   \u2502   \u251c\u2500\u2500 01_data_exploration.ipynb              # Analyse du dataset de tickets \u2502   \u2502   \u251c\u2500\u2500 02_data_preparation.ipynb              # Pr\u00e9traitement du texte \u2502   \u2502   \u251c\u2500\u2500 03_model_setup.ipynb                   # Configuration du mod\u00e8le NLP \u2502   \u2502   \u2514\u2500\u2500 04_model_training.ipynb                # Entra\u00eenement et \u00e9valuation \u2502   \u251c\u2500\u2500 knowledge_base/                            # Base de connaissances pour l'assistant \u2502   \u2502   \u251c\u2500\u2500 common_issues.json                     # Probl\u00e8mes fr\u00e9quents \u2502   \u2502   \u251c\u2500\u2500 hardware_issues.json                   # Probl\u00e8mes mat\u00e9riels \u2502   \u2502   \u251c\u2500\u2500 software_issues.json                   # Probl\u00e8mes logiciels \u2502   \u2502   \u2514\u2500\u2500 network_issues.json                    # Probl\u00e8mes r\u00e9seau \u2502   \u251c\u2500\u2500 assistant_api/ \u2502   \u2502   \u251c\u2500\u2500 app.py                                 # Serveur Flask pour l'assistant \u2502   \u2502   \u2514\u2500\u2500 utils/                                 # Fonctions utilitaires \u2502   \u251c\u2500\u2500 chat_interface/ \u2502   \u2502   \u251c\u2500\u2500 index.html                             # Interface web de chat \u2502   \u2502   \u251c\u2500\u2500 styles.css                             # Styles CSS \u2502   \u2502   \u2514\u2500\u2500 script.js                              # Script JavaScript \u00e0 compl\u00e9ter \u2502   \u251c\u2500\u2500 routing_system.py                          # Syst\u00e8me de routage des demandes \u2502   \u2514\u2500\u2500 test_assistant.py                          # Outil de test \u2502 \u251c\u2500\u2500 atelier3-optimisation-deploiement/              # Atelier 3 \u2502   \u251c\u2500\u2500 guide-atelier3.md                          # Guide de travail pratique \u2502   \u251c\u2500\u2500 notebooks/ \u2502   \u2502   \u251c\u2500\u2500 01_performance_analysis.ipynb          # Analyse des performances \u2502   \u2502   \u251c\u2500\u2500 02_quantization.ipynb                  # Techniques de quantification \u2502   \u2502   \u2514\u2500\u2500 03_inference_optimization.ipynb        # Optimisation des op\u00e9rations d'inf\u00e9rence \u2502   \u251c\u2500\u2500 image_recognition_service/                 # Service de reconnaissance d'images \u2502   \u2502   \u251c\u2500\u2500 Dockerfile                             # Template \u00e0 compl\u00e9ter \u2502   \u2502   \u2514\u2500\u2500 app/                                   # Code source du service \u2502   \u251c\u2500\u2500 assistant_service/                         # Service d'assistant virtuel \u2502   \u2502   \u251c\u2500\u2500 Dockerfile                             # Template \u00e0 compl\u00e9ter \u2502   \u2502   \u2514\u2500\u2500 app/                                   # Code source du service \u2502   \u251c\u2500\u2500 monitoring/                                # Configuration du monitoring \u2502   \u2502   \u251c\u2500\u2500 prometheus/                            # Configuration Prometheus \u2502   \u2502   \u2514\u2500\u2500 grafana/                               # Dashboards Grafana \u2502   \u251c\u2500\u2500 docker-compose.yml                         # Configuration multi-services \u2502   \u251c\u2500\u2500 deploy.sh                                  # Script de d\u00e9ploiement \u2502   \u2514\u2500\u2500 health_check.sh                            # Script de v\u00e9rification \u2502 \u2514\u2500\u2500 ressources/                                    # Ressources communes     \u251c\u2500\u2500 guides/     \u2502   \u251c\u2500\u2500 bonnes_pratiques_api.md                # Guide des bonnes pratiques API REST     \u2502   \u251c\u2500\u2500 conteneurisation_ml.md                 # Guide de conteneurisation pour ML     \u2502   \u2514\u2500\u2500 optimisation_modeles.md                # Guide d'optimisation des mod\u00e8les     \u251c\u2500\u2500 templates/     \u2502   \u251c\u2500\u2500 compte_rendu.md                        # Template de compte-rendu     \u2502   \u2514\u2500\u2500 documentation_technique.md             # Template de documentation technique     \u2514\u2500\u2500 evaluation/         \u2514\u2500\u2500 grille_evaluation.md                   # Grille d'\u00e9valuation d\u00e9taill\u00e9e</p>"},{"location":"seance2/partie1-cnn/","title":"Phase 1 : Mini-projet CNN pour la vision par ordinateur","text":""},{"location":"seance2/partie1-cnn/#objectifs-de-la-phase","title":"Objectifs de la phase","text":"<p>Dans cette phase, vous allez :</p> <ul> <li>Comprendre les principes fondamentaux des r\u00e9seaux de neurones convolutifs (CNN)</li> <li>Impl\u00e9menter un CNN pour la classification d'images avec TensorFlow/Keras</li> <li>Visualiser et interpr\u00e9ter les filtres et feature maps d'un CNN</li> <li>Int\u00e9grer un mod\u00e8le CNN dans une application web simple</li> </ul>"},{"location":"seance2/partie1-cnn/#partie-1-principes-des-cnn-20-min","title":"Partie 1: Principes des CNN (20 min)","text":""},{"location":"seance2/partie1-cnn/#defi-de-reflexion-initiale","title":"D\u00e9fi de r\u00e9flexion initiale","text":"<p>Avant de plonger dans les CNN, prenez 2 minutes pour r\u00e9fl\u00e9chir \u00e0 cette question :</p> <p>Question \u00e0 m\u00e9diter : Comment reconnaissez-vous un visage dans une photo, quelle que soit sa position ou l'\u00e9clairage ? Qu'est-ce qui rend cette t\u00e2che si facile pour vous et si difficile pour un ordinateur ?</p>"},{"location":"seance2/partie1-cnn/#activite-guidee-decouverte-de-larchitecture-cnn","title":"Activit\u00e9 guid\u00e9e : D\u00e9couverte de l'architecture CNN","text":"<p>\u00c9tape 1 : Observation (3 min) Examinez ces deux visualisations en parall\u00e8le :</p> <ul> <li>L'image originale d'un chiffre '7' manuscrit et son traitement par les diff\u00e9rentes couches d'un CNN</li> </ul> <p></p> <ul> <li>Les diff\u00e9rentes caract\u00e9ristiques extraites \u00e0 chaque niveau d'un CNN d\u00e9j\u00e0 entra\u00een\u00e9</li> </ul> <p></p> <p>\u00c9tape 2 : Mini-investigation (5 min) Formez des bin\u00f4mes et discutez :</p> <ul> <li>Quels types de d\u00e9tails la premi\u00e8re couche semble-t-elle rep\u00e9rer dans l'image?</li> <li>Comment ce que \"voit\" le r\u00e9seau change-t-il entre la premi\u00e8re et la derni\u00e8re couche?</li> <li>Pourquoi est-il utile pour le r\u00e9seau de transformer l'image \u00e0 chaque \u00e9tape?</li> </ul> <p>Les r\u00e9seaux de neurones convolutifs (CNN) offrent plusieurs avantages, notamment :</p> <ul> <li>Extraction automatique des caract\u00e9ristiques</li> </ul> <p>Contrairement aux m\u00e9thodes traditionnelles de vision par ordinateur qui n\u00e9cessitent une extraction manuelle des caract\u00e9ristiques, les CNN apprennent automatiquement les motifs pertinents (bords, textures, formes) \u00e0 partir des donn\u00e9es.</p> <ul> <li>Partage des poids et r\u00e9duction du nombre de param\u00e8tres </li> </ul> <p>Gr\u00e2ce aux filtres de convolution partag\u00e9s sur toute l\u2019image, les CNN r\u00e9duisent consid\u00e9rablement le nombre de param\u00e8tres \u00e0 entra\u00eener, ce qui diminue les besoins en m\u00e9moire et en calcul par rapport aux r\u00e9seaux de neurones enti\u00e8rement connect\u00e9s.</p> <ul> <li>Invariance aux translations et robustesse aux variations</li> </ul> <p>Les couches de convolution et de pooling permettent aux CNN d\u2019\u00eatre robustes aux d\u00e9calages, rotations et d\u00e9formations dans les images, ce qui am\u00e9liore leur capacit\u00e9 \u00e0 reconna\u00eetre des objets dans diff\u00e9rentes conditions.</p> <p>\u00c9tape 3 : Construction du mod\u00e8le mental (5 min) Sur votre feuille de travail, compl\u00e9tez le sch\u00e9ma simplifi\u00e9 d'un CNN :</p> <p></p> <ol> <li>Identifiez et nommez les trois types principaux de couches</li> <li>Pour chaque type, pr\u00e9cisez bri\u00e8vement sa fonction</li> <li>Listez les trois avantages majeurs des CNN</li> </ol> <p>\u00c9tape 4 : Analogie concr\u00e8te (3 min) Pour comprendre le fonctionnement d'un CNN, voyons comment il pourrait identifier un personnage c\u00e9l\u00e8bre comme Dark Vador :</p> <p></p> <ul> <li>La couche de convolution rep\u00e8re les caract\u00e9ristiques distinctives : \"Je d\u00e9tecte un casque noir, un respirateur, une cape...\"</li> <li>La couche de pooling ignore les d\u00e9tails non pertinents : \"Peu importe l'angle de vue, l'\u00e9clairage, s'il est de face ou de profil...\"</li> <li>La couche fully connected prend la d\u00e9cision finale : \"D'apr\u00e8s toutes ces caract\u00e9ristiques combin\u00e9es, c'est Dark Vador \u00e0 99.8%!\"</li> </ul> <p>Cette analogie montre comment un CNN analyse une image de mani\u00e8re hi\u00e9rarchique, comme notre cerveau le fait naturellement.</p>"},{"location":"seance2/partie1-cnn/#points-importants-a-retenir","title":"Points importants \u00e0 retenir","text":"<p>\u00c0 savoir avant de passer \u00e0 la pratique :</p> <ol> <li> <p>Les CNN sont con\u00e7us sp\u00e9cifiquement pour traiter les donn\u00e9es en grille comme les images.</p> </li> <li> <p>Les filtres de convolution agissent comme des d\u00e9tecteurs de motifs qui s'appliquent \u00e0 toute l'image.</p> </li> <li> <p>Le pooling permet de r\u00e9duire les dimensions tout en conservant l'information importante.</p> </li> <li> <p>Les poids du r\u00e9seau sont ajust\u00e9s automatiquement pendant l'entra\u00eenement.</p> </li> <li> <p>Un CNN profond permet de d\u00e9tecter des motifs de plus en plus complexes et abstraits.</p> </li> <li> <p>Le grand avantage des CNN est qu'ils apprennent automatiquement les caract\u00e9ristiques pertinentes, sans qu'on ait \u00e0 les programmer manuellement.</p> </li> </ol>"},{"location":"seance2/partie1-cnn/#transition-vers-limplementation","title":"Transition vers l'impl\u00e9mentation","text":"<p>Maintenant que vous avez conceptualis\u00e9 l'architecture d'un CNN, passons \u00e0 l'impl\u00e9mentation pratique pour voir ces concepts en action. Gardez votre sch\u00e9ma \u00e0 port\u00e9e de main - vous pourrez le compl\u00e9ter avec des observations pratiques.</p>"},{"location":"seance2/partie1-cnn/#partie-2-implementation-dun-cnn-pour-mnist-40-min","title":"Partie 2: Impl\u00e9mentation d'un CNN pour MNIST (40 min)","text":""},{"location":"seance2/partie1-cnn/#instructions","title":"Instructions","text":"<ol> <li>Ouvrez le notebook Jupyter cnn-classification dans Google Colab</li> <li>Suivez les instructions \u00e9tape par \u00e9tape pour impl\u00e9menter un CNN pour la classification des chiffres manuscrits (MNIST)</li> <li>Ex\u00e9cutez chaque cellule et observez les r\u00e9sultats</li> <li> <p>Portez une attention particuli\u00e8re aux sections suivantes :</p> </li> <li> <p>Architecture du mod\u00e8le CNN</p> </li> <li>Processus d'entra\u00eenement</li> <li>Visualisation des filtres et feature maps</li> <li>Analyse des performances et des erreurs</li> </ol>"},{"location":"seance2/partie1-cnn/#points-cles-a-explorer","title":"Points cl\u00e9s \u00e0 explorer","text":"<ul> <li>Comment les couches de convolution extraient-elles des caract\u00e9ristiques de plus en plus abstraites ?</li> <li>Quel est l'impact du nombre de filtres et de couches sur les performances ?</li> <li>Comment les feature maps r\u00e9v\u00e8lent-elles ce que \"voit\" le r\u00e9seau ?</li> <li>Quelles sont les limites du mod\u00e8le face \u00e0 des donn\u00e9es bruit\u00e9es ou d\u00e9form\u00e9es ?</li> </ul>"},{"location":"seance2/partie1-cnn/#partie-3-integration-dans-une-application-web-30-min","title":"Partie 3: Int\u00e9gration dans une application web (30 min)","text":"<p>Dans cette partie, vous allez d\u00e9couvrir comment int\u00e9grer un mod\u00e8le CNN pr\u00e9-entra\u00een\u00e9 dans une application web interactive.</p>"},{"location":"seance2/partie1-cnn/#mini-projet-reconnaissance-de-chiffres-manuscrits","title":"Mini-projet : Reconnaissance de chiffres manuscrits","text":""},{"location":"seance2/partie1-cnn/#contexte-professionnel","title":"Contexte professionnel","text":"<p>Vous \u00eates stagiaire dans une PME o\u00f9 les employ\u00e9s doivent r\u00e9guli\u00e8rement saisir manuellement des codes \u00e0 partir de documents papier (bons de commande, formulaires clients, etc.). Votre responsable informatique souhaite explorer des solutions d'automatisation et vous demande de tester une application de reconnaissance de chiffres manuscrits.</p>"},{"location":"seance2/partie1-cnn/#etape-1-preparation-de-lenvironnement-8-minutes","title":"\u00c9tape 1: Pr\u00e9paration de l'environnement (8 minutes)","text":"<p>Pour la partie web, vous aurez besoin d'un fichier <code>mnist_cnn_model.h5</code> contenant votre mod\u00e8le CNN entra\u00een\u00e9. Ce fichier doit \u00eatre g\u00e9n\u00e9r\u00e9 sur Google Colab en suivant ces \u00e9tapes:</p>"},{"location":"seance2/partie1-cnn/#generation-du-modele-sur-google-colab","title":"G\u00e9n\u00e9ration du mod\u00e8le sur Google Colab","text":"<ol> <li> <p>Coller dans un nouveau notebook Google Colab le code du fichier suivant :<code>create_model.py</code>.</p> </li> <li> <p>Ex\u00e9cutez la cellule en cliquant sur le bouton de lecture \u25b6\ufe0f \u00e0 gauche de la cellule, ou en appuyant sur Shift+Enter</p> </li> </ol>"},{"location":"seance2/partie1-cnn/#attendre-lentrainement-et-telecharger-le-modele","title":"Attendre l'entra\u00eenement et t\u00e9l\u00e9charger le mod\u00e8le","text":"<ol> <li>L'ex\u00e9cution durera environ 3-5 minutes sur Google Colab (qui utilise des GPU/TPU)</li> <li>Vous verrez la progression de l'entra\u00eenement pour chaque \u00e9poque</li> <li>\u00c0 la fin, votre navigateur d\u00e9marrera automatiquement le t\u00e9l\u00e9chargement du fichier <code>mnist_cnn_model.h5</code></li> <li>Enregistrez ce fichier dans le dossier de votre projet web</li> </ol>"},{"location":"seance2/partie1-cnn/#avantages-de-cette-approche","title":"Avantages de cette approche:","text":"<ul> <li>Aucune installation locale requise</li> <li>Utilisation gratuite des ressources GPU de Google</li> <li>Ex\u00e9cution plus rapide que sur un ordinateur standard</li> <li>Interface famili\u00e8re et intuitive</li> <li>Pas de probl\u00e8me d'installation ou de performance de TensorFlow sur sa machine locale.</li> </ul>"},{"location":"seance2/partie1-cnn/#etape-2-configuration-5-minutes","title":"\u00c9tape 2 : Configuration (5 minutes)","text":"<p>1.Pr\u00e9paration de l'environnement VS Code</p> <ul> <li>Ouvrez Visual Studio Code</li> <li>Cr\u00e9ez un nouveau dossier pour le projet: <code>File &gt; Open Folder</code> et cr\u00e9ez un dossier nomm\u00e9 <code>reconnaissance-chiffres</code></li> <li>Dans VS Code, cr\u00e9ez la structure de dossiers suivante via l'explorateur:<ul> <li>Cr\u00e9ez un dossier <code>templates</code></li> <li>Cr\u00e9ez un dossier <code>static</code></li> <li>Dans <code>static</code>, cr\u00e9ez les sous-dossiers <code>css</code> et <code>js</code>Problpro</li> </ul> </li> </ul> <p>2.\ud83d\udce5T\u00e9l\u00e9chargement des fichiers de l'application web</p> <p>T\u00e9l\u00e9chargez les fichiers suivants et placez-les dans les dossiers indiqu\u00e9s :</p> <ul> <li>code-app-web.zip </li> </ul> <p>\ud83d\udccc Clic droit sur le lien \u2192 \"Enregistrer le lien sous...\" pour t\u00e9l\u00e9charger chaque fichier.</p> <ol> <li>Structure des dossiers :</li> </ol> <p>*Assurez-vous que votre structure de dossiers est la suivante:</p> <pre><code>votre_dossier_de_travail/\n\u251c\u2500\u2500 mnist_cnn_model.h5      # Votre mod\u00e8le sauvegard\u00e9 ou le mod\u00e8le fourni\n\u251c\u2500\u2500 web-integration.py      # Script principal Flask\n\u251c\u2500\u2500 templates/              \n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 static/                 # Dossier pour CSS, JS, images\n    \u251c\u2500\u2500 css/\n    \u2502   \u2514\u2500\u2500 style.css\n    \u2514\u2500\u2500 js/\n        \u2514\u2500\u2500 app.js\n</code></pre>"},{"location":"seance2/partie1-cnn/#etape-3-installation-et-lancement-5-minutes","title":"\u00c9tape 3 : Installation et lancement (5 minutes)","text":"<p>1.Cr\u00e9ation du mod\u00e8le via Google Colab</p> <ul> <li>Suivez les instructions pour cr\u00e9er le mod\u00e8le avec Google Colab (voir sections pr\u00e9c\u00e9dentes)</li> <li>Une fois le fichier <code>mnist_cnn_model.h5</code> t\u00e9l\u00e9charg\u00e9, d\u00e9placez-le dans le dossier racine de votre projet</li> </ul> <p>2.Ouverture du Terminal int\u00e9gr\u00e9 \u00e0 VS Code</p> <ul> <li>Dans VS Code, ouvrez un terminal en allant dans <code>Terminal &gt; New Terminal</code></li> <li>Vous verrez un terminal s'ouvrir en bas de la fen\u00eatre</li> </ul> <p>3.Installation des d\u00e9pendances</p> <ul> <li>Dans le terminal VS Code, tapez la commande suivante:    <pre><code>pip install flask tensorflow pillow numpy\n</code></pre><ul> <li>Attendez que l'installation se termine</li> </ul> </li> </ul> <p>4.Lancement de l'application</p> <ul> <li>Dans le m\u00eame terminal, tapez:      <pre><code>python web-integration.py\n</code></pre><ul> <li>Vous devriez voir un message indiquant que l'application est en cours d'ex\u00e9cution</li> <li>VS Code peut vous proposer d'ouvrir le lien - cliquez dessus, ou</li> <li>Ouvrez votre navigateur et acc\u00e9dez \u00e0 http://localhost:5001</li> </ul> </li> </ul>"},{"location":"seance2/partie1-cnn/#etape-4-tests-pratiques-10-minutes","title":"\u00c9tape 4 : Tests pratiques (10 minutes)","text":"<ol> <li> <p>Test avec dessins \u00e0 la souris</p> <ul> <li>Dans l'interface web, dessinez clairement un chiffre (de 0 \u00e0 9) dans la zone pr\u00e9vue</li> <li> <p>Cliquez sur le bouton \"Pr\u00e9dire\"</p> </li> <li> <p>Notez la pr\u00e9diction et le niveau de confiance</p> </li> <li>R\u00e9p\u00e9tez ce processus avec 5 chiffres diff\u00e9rents</li> <li>Gardez une trace de vos r\u00e9sultats (tableau simple : chiffre r\u00e9el / pr\u00e9diction / confiance)</li> </ul> </li> </ol> <p>2.Test avec image import\u00e9e</p> <pre><code>- Sur une feuille de papier, \u00e9crivez clairement un chiffre\n- Prenez une photo de ce chiffre avec votre smartphone ou appareil photo\n- Transf\u00e9rez l'image sur votre ordinateur (par email, cloud, c\u00e2ble USB, etc.)\n- Dans l'application, cliquez sur \"Charger une image\"\n- S\u00e9lectionnez l'image que vous venez de prendre\n- Observez la pr\u00e9diction et le niveau de confiance\n</code></pre> <ol> <li> <p>Test avec feature maps (optionnel)</p> </li> <li> <p>Cochez la case \"Visualiser les feature maps\"</p> </li> <li>Dessinez un nouveau chiffre et cliquez sur \"Pr\u00e9dire\"</li> <li>Observez les feature maps qui s'affichent (repr\u00e9sentations visuelles de ce que \"voit\" le r\u00e9seau)</li> </ol>"},{"location":"seance2/partie1-cnn/#etape-5-evaluation-et-rapport-10-minutes","title":"\u00c9tape 5 : \u00c9valuation et rapport (10 minutes)","text":"<p>1.Remplissage du formulaire d'\u00e9valuation</p> <ul> <li>Ouvrez le document evaluation fourni par votre formateur</li> <li>Remplissez les sections suivantes :<ul> <li>Nombre de pr\u00e9dictions correctes/incorrectes</li> <li>Chiffres les mieux reconnus</li> <li>Chiffres les plus difficiles \u00e0 reconna\u00eetre</li> <li>Niveau de confiance moyen observ\u00e9</li> </ul> </li> </ul> <p>2.Analyse critique</p> <ul> <li>Dans le formulaire, notez au moins 3 points forts de l'application</li> <li>Notez \u00e9galement au moins 3 limitations ou probl\u00e8mes rencontr\u00e9s</li> </ul> <p>3.Propositions d'am\u00e9lioration</p> <ul> <li>Proposez 2-3 id\u00e9es concr\u00e8tes pour am\u00e9liorer l'outil dans un contexte professionnel</li> <li>Exemple : \"Ajouter une fonction pour traiter plusieurs chiffres \u00e0 la fois\"</li> </ul> <p>4.Conclusion professionnelle</p> <ul> <li>R\u00e9digez une br\u00e8ve conclusion (2-3 phrases) sur l'utilit\u00e9 potentielle de cet outil dans l'entreprise</li> </ul>"},{"location":"seance2/partie1-cnn/#livrable-a-rendre","title":"Livrable \u00e0 rendre","text":"<p>\u00c0 la fin de la session (30 minutes), veuillez :</p> <ol> <li>Copier et Compl\u00e9ter enti\u00e8rement ce formulaire d'\u00e9valuation</li> <li>Enregistrer  le document  sous le nom \"Eval_CNN_NOM_Prenom.doc\"</li> <li>Partager votre \u00e9valuation avec l'enseignant sur l'espace de cours:</li> </ol> <p>IMPORTANT : La remise de ce document compl\u00e9t\u00e9 est obligatoire et fait partie de l'\u00e9valuation du mini-projet.</p> <p>Date limite de remise : \u00c0 la fin de la s\u00e9ance</p>"},{"location":"seance2/partie1-cnn/#pour-aller-plus-loin-si-vous-terminez-en-avance","title":"Pour aller plus loin (si vous terminez en avance)","text":"<p>Si vous avez termin\u00e9 avant la fin du temps imparti, vous pouvez explorer ces pistes : - Testez les limites du mod\u00e8le en dessinant des chiffres de diff\u00e9rentes tailles/styles - Observez comment le bruit ou les distorsions affectent la pr\u00e9cision - Essayez de comprendre le code source dans <code>web-integration.py</code> pour voir comment l'application fonctionne</p>"},{"location":"seance2/partie1-cnn/#ressources-complementaires","title":"Ressources compl\u00e9mentaires","text":"<ul> <li>Tutoriel TensorFlow sur les CNN - Guide officiel de TensorFlow sur l'impl\u00e9mentation des r\u00e9seaux de neurones convolutifs</li> <li>Visualisation de CNN (Distill.pub) - Article interactif sur la visualisation et l'interpr\u00e9tation des r\u00e9seaux convolutifs</li> <li>Documentation Flask - Documentation officielle du framework Flask pour le d\u00e9veloppement web</li> </ul> <p>Retour \u00e0 la S\u00e9ance 2{ .md-button } Continuer vers la Phase 2: RNN{ .md-button .md-button--primary }</p>"},{"location":"seance2/partie2-rnn/","title":"Phase 2 : Mini-projet RNN pour le traitement du langage","text":""},{"location":"seance2/partie2-rnn/#objectifs-de-la-phase","title":"Objectifs de la phase","text":"<p>Dans cette phase, vous allez :</p> <ul> <li>Comprendre les principes des r\u00e9seaux r\u00e9currents (RNN) et de leurs variantes (LSTM, GRU)</li> <li>Impl\u00e9menter un mod\u00e8le LSTM pour l'analyse de sentiment</li> <li>Visualiser et interpr\u00e9ter le fonctionnement interne d'un RNN</li> <li>Exp\u00e9rimenter avec l'API Mistral AI pour la g\u00e9n\u00e9ration de texte</li> <li>\u00c9tablir les bases pour le projet de chatbot p\u00e9dagogique</li> </ul>"},{"location":"seance2/partie2-rnn/#partie-1-principes-des-rnn-20-min","title":"Partie 1: Principes des RNN (20 min)","text":""},{"location":"seance2/partie2-rnn/#architecture-et-fonctionnement-des-rnn","title":"Architecture et fonctionnement des RNN","text":"<p>Les r\u00e9seaux de neurones r\u00e9currents (RNN) sont sp\u00e9cialement con\u00e7us pour traiter des donn\u00e9es s\u00e9quentielles, comme du texte, des s\u00e9ries temporelles ou des signaux audio. Leur architecture inclut des connections r\u00e9currentes qui leur permettent de \"m\u00e9moriser\" les informations pr\u00e9c\u00e9dentes :</p> <ol> <li>Principe de base : contrairement aux r\u00e9seaux feed-forward, les RNN poss\u00e8dent des boucles de r\u00e9troaction</li> <li>M\u00e9moire \u00e0 court terme : chaque \u00e9tat cach\u00e9 d\u00e9pend de l'\u00e9tat pr\u00e9c\u00e9dent et de l'entr\u00e9e actuelle</li> <li>Probl\u00e8me de la disparition du gradient : difficult\u00e9 \u00e0 capturer les d\u00e9pendances \u00e0 long terme</li> <li>Architectures avanc\u00e9es : LSTM (Long Short-Term Memory) et GRU (Gated Recurrent Unit) qui r\u00e9solvent ce probl\u00e8me</li> </ol> <p>Avantages pour un d\u00e9veloppeur d'applications :</p> <ul> <li>Traitement de s\u00e9quences de longueur variable</li> <li>Capacit\u00e9 \u00e0 \"m\u00e9moriser\" des informations importantes</li> <li>Applications diverses : analyse de texte, traduction, g\u00e9n\u00e9ration de contenu</li> </ul>"},{"location":"seance2/partie2-rnn/#partie-2-implementation-dun-lstm-pour-lanalyse-de-sentiment-40-min","title":"Partie 2: Impl\u00e9mentation d'un LSTM pour l'analyse de sentiment (40 min)","text":""},{"location":"seance2/partie2-rnn/#instructions","title":"Instructions","text":"<ol> <li>Ouvrez le notebook Jupyter rnn-sequence.ipynb dans Google Colab</li> <li>Suivez les instructions \u00e9tape par \u00e9tape pour impl\u00e9menter un mod\u00e8le LSTM pour l'analyse de sentiment</li> <li>Ex\u00e9cutez chaque cellule et observez les r\u00e9sultats</li> <li>Portez une attention particuli\u00e8re aux sections suivantes :</li> <li>Pr\u00e9traitement du texte (tokenisation)</li> <li>Architecture du mod\u00e8le LSTM</li> <li>Visualisation des embeddings de mots</li> <li>Analyse des performances et des erreurs</li> </ol>"},{"location":"seance2/partie2-rnn/#points-cles-a-explorer","title":"Points cl\u00e9s \u00e0 explorer","text":"<ul> <li>Comment le texte est-il transform\u00e9 en entr\u00e9es num\u00e9riques pour le r\u00e9seau ?</li> <li>Comment les cellules LSTM g\u00e8rent-elles l'information \u00e0 long terme ?</li> <li>Quelle est la diff\u00e9rence entre les embeddings de mots positifs et n\u00e9gatifs ?</li> <li>Comment le mod\u00e8le LSTM peut-il comprendre le contexte d'une phrase ?</li> <li>Quelles sont les limitations de cette approche pour l'analyse de sentiment ?</li> <li>Comment pourriez-vous am\u00e9liorer ce mod\u00e8le pour des t\u00e2ches plus complexes ?</li> </ul>"},{"location":"seance2/partie2-rnn/#partie-3-integration-avec-lapi-mistral-ai-30-min","title":"Partie 3: Int\u00e9gration avec l'API Mistral AI (30 min)","text":""},{"location":"seance2/partie2-rnn/#introduction-a-mistral-ai","title":"Introduction \u00e0 Mistral AI","text":"<p>Mistral AI est une plateforme avanc\u00e9e d'intelligence artificielle sp\u00e9cialis\u00e9e dans le traitement du langage naturel (NLP). Contrairement \u00e0 nos mod\u00e8les LSTM simples, Mistral utilise des architectures de type transformer, beaucoup plus puissantes pour comprendre et g\u00e9n\u00e9rer du texte.</p> <p>Avantages de l'API Mistral AI: - Mod\u00e8les pr\u00e9-entra\u00een\u00e9s sur d'immenses corpus de texte - Compr\u00e9hension contextuelle profonde - Capacit\u00e9s multilingues - Flexibilit\u00e9 pour diff\u00e9rents cas d'usage NLP</p>"},{"location":"seance2/partie2-rnn/#instructions_1","title":"Instructions","text":"<ol> <li>Ouvrez le script mistral-integration.py</li> <li>Examinez comment l'API Mistral AI est int\u00e9gr\u00e9e pour am\u00e9liorer les capacit\u00e9s de traitement du langage</li> <li>Vous aurez besoin d'une cl\u00e9 API (une cl\u00e9 de d\u00e9monstration sera fournie pendant la s\u00e9ance)</li> <li>Pour ex\u00e9cuter le script:</li> </ol> <pre><code># Installer les d\u00e9pendances\npip install requests pandas matplotlib \n\n# Configurer votre cl\u00e9 API (sur Windows)\nset MISTRAL_API_KEY=votre_cl\u00e9_api_ici\n\n# Ex\u00e9cuter le script\npython mistral-integration.py\n</code></pre>"},{"location":"seance2/partie2-rnn/#structure-du-script-mistral-ai","title":"Structure du script Mistral AI","text":"<p>Le script est organis\u00e9 en plusieurs sections:</p> <pre><code># Structure du script mistral-integration.py\n\n# 1. Configuration et imports\nimport requests\nimport json\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 2. Configuration de l'API\nAPI_KEY = os.environ.get(\"MISTRAL_API_KEY\")\nAPI_URL = \"https://api.mistral.ai/v1/chat/completions\"\n\n# 3. Fonction pour envoyer des requ\u00eates \u00e0 l'API\ndef query_mistral(prompt, system_message=None, temperature=0.7, max_tokens=256):\n    \"\"\"\n    Envoie une requ\u00eate \u00e0 l'API Mistral et retourne la r\u00e9ponse.\n\n    Args:\n        prompt (str): Le message utilisateur\n        system_message (str): Instructions syst\u00e8me pour guider le mod\u00e8le\n        temperature (float): Contr\u00f4le la cr\u00e9ativit\u00e9 (0.0-1.0)\n        max_tokens (int): Limite de tokens pour la r\u00e9ponse\n\n    Returns:\n        str: R\u00e9ponse du mod\u00e8le\n    \"\"\"\n    # Construction des messages\n    messages = []\n    if system_message:\n        messages.append({\"role\": \"system\", \"content\": system_message})\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    # Configuration de la requ\u00eate\n    headers = {\n        \"Authorization\": f\"Bearer {API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    payload = {\n        \"model\": \"mistral-small\",\n        \"messages\": messages,\n        \"temperature\": temperature,\n        \"max_tokens\": max_tokens\n    }\n\n    # Envoi de la requ\u00eate\n    try:\n        response = requests.post(API_URL, headers=headers, json=payload)\n        response.raise_for_status()\n        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n    except Exception as e:\n        print(f\"Erreur lors de la requ\u00eate \u00e0 l'API: {e}\")\n        return None\n\n# 4. D\u00e9monstrations d'applications\n# 4.1 Analyse de sentiment avanc\u00e9e\ndef analyze_sentiment_mistral(text):\n    \"\"\"Analyse le sentiment d'un texte avec Mistral AI\"\"\"\n    system_message = \"\"\"\n    Vous \u00eates un expert en analyse de sentiment.\n    Analysez le sentiment du texte fourni et r\u00e9pondez uniquement par:\n    POSITIF, N\u00c9GATIF ou NEUTRE, suivi d'un score de -1.0 \u00e0 1.0 entre parenth\u00e8ses.\n    Exemple: POSITIF (0.8) ou N\u00c9GATIF (-0.5)\n    \"\"\"\n    prompt = f\"Analysez le sentiment du texte suivant: '{text}'\"\n    return query_mistral(prompt, system_message, temperature=0.3)\n\n# 4.2 G\u00e9n\u00e9ration de texte contr\u00f4l\u00e9e\ndef generate_continuation(text, style=\"informatif\"):\n    \"\"\"G\u00e9n\u00e8re une continuation de texte dans un style sp\u00e9cifi\u00e9\"\"\"\n    style_instructions = {\n        \"informatif\": \"Continuez ce texte dans un style informatif et factuel.\",\n        \"persuasif\": \"Continuez ce texte dans un style persuasif et convaincant.\",\n        \"narratif\": \"Continuez ce texte dans un style narratif engageant.\"\n    }\n\n    system_message = f\"\"\"\n    Vous \u00eates un expert en r\u00e9daction et g\u00e9n\u00e9ration de texte.\n    {style_instructions.get(style, style_instructions['informatif'])}\n    \"\"\"\n\n    prompt = f\"Voici le d\u00e9but d'un texte. Continuez-le de mani\u00e8re coh\u00e9rente:\\n\\n{text}\"\n    return query_mistral(prompt, system_message, temperature=0.7, max_tokens=150)\n\n# 4.3 Question-r\u00e9ponse contextuelle\ndef answer_question(context, question):\n    \"\"\"R\u00e9pond \u00e0 une question bas\u00e9e sur un contexte donn\u00e9\"\"\"\n    system_message = \"\"\"\n    Vous \u00eates un assistant sp\u00e9cialis\u00e9 en compr\u00e9hension de texte et question-r\u00e9ponse.\n    R\u00e9pondez \u00e0 la question uniquement \u00e0 partir des informations fournies dans le contexte.\n    Si la r\u00e9ponse n'est pas dans le contexte, r\u00e9pondez \"Je ne peux pas r\u00e9pondre \u00e0 cette question d'apr\u00e8s le contexte fourni.\"\n    \"\"\"\n\n    prompt = f\"\"\"\n    Contexte: {context}\n\n    Question: {question}\n    \"\"\"\n\n    return query_mistral(prompt, system_message, temperature=0.3)\n\n# 5. D\u00e9monstration interactive\ndef run_demos():\n    print(\"=== D\u00e9monstration d'int\u00e9gration de Mistral AI ===\\n\")\n\n    # Test d'analyse de sentiment\n    print(\"1. Analyse de sentiment avanc\u00e9e\")\n    sample_texts = [\n        \"J'ai ador\u00e9 ce film, c'\u00e9tait vraiment captivant !\",\n        \"Le service \u00e9tait m\u00e9diocre et la nourriture froide.\",\n        \"Cette application semble int\u00e9ressante mais manque de fonctionnalit\u00e9s.\"\n    ]\n\n    results = []\n    for text in sample_texts:\n        sentiment = analyze_sentiment_mistral(text)\n        print(f\"Texte: '{text}'\")\n        print(f\"Sentiment: {sentiment}\\n\")\n\n        # Extraction du score pour visualisation\n        if sentiment:\n            score_str = sentiment.split('(')[1].split(')')[0]\n            try:\n                score = float(score_str)\n                results.append({\"text\": text, \"score\": score})\n            except:\n                pass\n\n    # Visualisation des scores de sentiment\n    if results:\n        df = pd.DataFrame(results)\n        plt.figure(figsize=(10, 6))\n        bars = plt.barh(df['text'], df['score'], color=['green' if x &gt; 0 else 'red' for x in df['score']])\n        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n        plt.xlim(-1, 1)\n        plt.xlabel('Score de sentiment')\n        plt.title('Analyse de sentiment avec Mistral AI')\n        plt.tight_layout()\n        plt.savefig('sentiment_analysis.png')\n        print(\"Visualisation sauvegard\u00e9e dans 'sentiment_analysis.png'\\n\")\n\n    # Test de g\u00e9n\u00e9ration de texte\n    print(\"2. G\u00e9n\u00e9ration de texte contr\u00f4l\u00e9e\")\n    sample_text = \"L'intelligence artificielle transforme notre mani\u00e8re de travailler.\"\n    for style in [\"informatif\", \"persuasif\", \"narratif\"]:\n        continuation = generate_continuation(sample_text, style)\n        print(f\"Style: {style}\")\n        print(f\"Texte initial: {sample_text}\")\n        print(f\"Continuation: {continuation}\\n\")\n\n    # Test de Q&amp;A contextuel\n    print(\"3. Question-r\u00e9ponse contextuelle\")\n    context = \"\"\"\n    Les r\u00e9seaux de neurones r\u00e9currents (RNN) sont une classe de r\u00e9seaux de neurones artificiels\n    o\u00f9 les connexions entre les n\u0153uds forment un graphe orient\u00e9 le long d'une s\u00e9quence temporelle.\n    Contrairement aux r\u00e9seaux de neurones \u00e0 propagation avant, les RNN peuvent utiliser leur \u00e9tat interne\n    (m\u00e9moire) pour traiter des s\u00e9quences d'entr\u00e9es, ce qui les rend applicables \u00e0 des t\u00e2ches\n    comme la reconnaissance vocale ou la traduction automatique.\n\n    Les LSTM (Long Short-Term Memory) sont une architecture sp\u00e9ciale de RNN capable d'apprendre\n    les d\u00e9pendances \u00e0 long terme. Les LSTM ont \u00e9t\u00e9 con\u00e7us pour r\u00e9soudre le probl\u00e8me de la disparition\n    du gradient qui peut \u00eatre rencontr\u00e9 lors de l'entra\u00eenement des RNN traditionnels.\n    \"\"\"\n\n    questions = [\n        \"Quelle est la principale diff\u00e9rence entre les RNN et les r\u00e9seaux \u00e0 propagation avant?\",\n        \"Pourquoi les LSTM ont-ils \u00e9t\u00e9 d\u00e9velopp\u00e9s?\",\n        \"Qui a invent\u00e9 les r\u00e9seaux de neurones r\u00e9currents?\"\n    ]\n\n    for question in questions:\n        answer = answer_question(context, question)\n        print(f\"Question: {question}\")\n        print(f\"R\u00e9ponse: {answer}\\n\")\n\n# 6. Point d'entr\u00e9e principal\nif __name__ == \"__main__\":\n    if not API_KEY:\n        print(\"Erreur: Cl\u00e9 API Mistral non configur\u00e9e. Utilisez 'set MISTRAL_API_KEY=votre_cl\u00e9' (Windows) ou 'export MISTRAL_API_KEY=votre_cl\u00e9' (Linux/Mac)\")\n    else:\n        run_demos()\n</code></pre>"},{"location":"seance2/partie2-rnn/#exercices-pratiques-avec-mistral-ai","title":"Exercices pratiques avec Mistral AI","text":"<ol> <li> <p>Modifiez le script pour ajouter une fonction qui g\u00e9n\u00e8re des explications p\u00e9dagogiques sur les concepts du Deep Learning</p> </li> <li> <p>Impl\u00e9mentez une fonction simple de dialogue qui permettrait \u00e0 un utilisateur de poser des questions sur un sujet sp\u00e9cifique</p> </li> <li> <p>Comparez les r\u00e9sultats d'analyse de sentiment entre votre mod\u00e8le LSTM et Mistral AI:</p> </li> <li>Quelles sont les diff\u00e9rences principales?</li> <li>Dans quels cas Mistral AI semble-t-il plus performant?</li> <li>Pourquoi les grands mod\u00e8les de langage comme Mistral surpassent-ils les architectures LSTM simples?</li> </ol>"},{"location":"seance2/partie2-rnn/#partie-4-comparaison-et-synthese-20-min","title":"Partie 4: Comparaison et synth\u00e8se (20 min)","text":""},{"location":"seance2/partie2-rnn/#tableau-comparatif-cnn-vs-rnn","title":"Tableau comparatif CNN vs RNN","text":"Caract\u00e9ristique CNN RNN/LSTM Type de donn\u00e9es Structur\u00e9es spatialement (images) S\u00e9quentielles (texte, s\u00e9ries temporelles) Avantage principal Extraction automatique de caract\u00e9ristiques M\u00e9moire contextuelle Architecture cl\u00e9 Convolution + Pooling Cellules r\u00e9currentes + portes Applications Vision par ordinateur, classification d'images NLP, traduction, g\u00e9n\u00e9ration de texte D\u00e9fi principal Invariance \u00e0 la rotation/\u00e9chelle D\u00e9pendances \u00e0 long terme Interpr\u00e9tabilit\u00e9 Visualisation des feature maps \u00c9tats cach\u00e9s et portes d'oubli"},{"location":"seance2/partie2-rnn/#discussion-collective","title":"Discussion collective","text":"<p>En groupes de 3-4, discutez des questions suivantes et pr\u00e9parez une synth\u00e8se \u00e0 partager:</p> <ol> <li>Quelles sont les forces et faiblesses respectives des CNN et RNN?</li> <li>Comment les deux types d'architectures pourraient-ils \u00eatre combin\u00e9s?</li> <li>Quels d\u00e9fis sp\u00e9cifiques posent les donn\u00e9es textuelles par rapport aux images?</li> <li>Comment l'architecture Transformer (utilis\u00e9e par Mistral) a-t-elle r\u00e9volutionn\u00e9 le NLP?</li> <li>Comment ces technologies pourraient-elles \u00eatre int\u00e9gr\u00e9es dans notre projet de chatbot p\u00e9dagogique?</li> </ol>"},{"location":"seance2/partie2-rnn/#ressources-complementaires","title":"Ressources compl\u00e9mentaires","text":"<ul> <li>Understanding LSTM Networks</li> <li>The Illustrated Transformer</li> <li>Documentation de l'API Mistral AI</li> </ul> <p>Retour \u00e0 la S\u00e9ance 2 Continuer vers la Phase 3: Am\u00e9lioration des mod\u00e8les</p>"},{"location":"seance2/partie3-amelioration/","title":"Challenge d'am\u00e9lioration de mod\u00e8le pour une application m\u00e9tier","text":""},{"location":"seance2/partie3-amelioration/#objectif-du-challenge","title":"Objectif du challenge","text":"<p>Lors de cette phase, vous allez travailler en \u00e9quipe sur un challenge d'am\u00e9lioration de mod\u00e8le qui simule une situation r\u00e9elle en entreprise. Vous prendrez le r\u00f4le d'un stagiaire en informatique charg\u00e9 d'optimiser un syst\u00e8me de pr\u00e9diction existant mais sous-performant.</p> <p>Dur\u00e9e: 45 minutes</p> <p>Contexte professionnel: Vous effectuez un stage dans une entreprise de commerce en ligne qui souhaite am\u00e9liorer son syst\u00e8me de pr\u00e9vision des ventes. Un mod\u00e8le de base a \u00e9t\u00e9 d\u00e9velopp\u00e9, mais ses performances sont insuffisantes. Votre mission est de diagnostiquer les probl\u00e8mes et d'am\u00e9liorer ce mod\u00e8le.</p>"},{"location":"seance2/partie3-amelioration/#telechargement-des-ressources","title":"T\u00e9l\u00e9chargement des ressources","text":"<p>Tous les fichiers n\u00e9cessaires pour r\u00e9aliser ce challenge sont disponibles ci-dessous :</p> <p>Structure du projet</p> <pre><code>challenge-prevision-ventes/\n\u251c\u2500\u2500 sales_prediction_improvement.ipynb    # Notebook principal \u00e0 compl\u00e9ter\n\u251c\u2500\u2500 data_utils.py                         # Utilitaires pour le traitement des donn\u00e9es\n\u251c\u2500\u2500 visualization.py                      # Fonctions de visualisation\n\u2514\u2500\u2500 visualize_utils.py                    # Fonctions auxiliaires pour les visualisations\n</code></pre>"},{"location":"seance2/partie3-amelioration/#fichiers-a-telecharger","title":"Fichiers \u00e0 t\u00e9l\u00e9charger","text":"<ul> <li> sales_prediction_improvement.ipynb - Notebook principal contenant le mod\u00e8le \u00e0 am\u00e9liorer</li> <li> data_utils.py - Utilitaires pour le traitement des donn\u00e9es</li> <li> visualization.py - Fonctions de visualisation des r\u00e9sultats</li> <li> visualize_utils.py - Fonctions auxiliaires pour les visualisations</li> </ul>"},{"location":"seance2/partie3-amelioration/#instructions","title":"Instructions","text":""},{"location":"seance2/partie3-amelioration/#etape-1-briefing-et-prise-en-main-5-min","title":"\u00c9tape 1: Briefing et prise en main (5 min)","text":"<ol> <li>Formez des \u00e9quipes de 1-2 personnes</li> <li>T\u00e9l\u00e9chargez les fichiers individuellement ou le dossier ZIP complet</li> <li>Ouvrez le notebook Jupyter <code>sales_prediction_improvement.ipynb</code> dans Google Colab</li> <li>Vous pouvez l'ouvrir directement via ce lien : Ouvrir dans Colab</li> <li>Ou le t\u00e9l\u00e9charger puis l'importer dans Colab</li> </ol> <p>Le dataset contient les donn\u00e9es de ventes historiques avec les caract\u00e9ristiques suivantes:</p> <ul> <li>Date de la vente</li> <li>Cat\u00e9gorie de produit</li> <li>Prix</li> <li>Remise appliqu\u00e9e</li> <li>Quantit\u00e9 vendue</li> <li>Jour de la semaine</li> <li>Indicateur de p\u00e9riode promotionnelle</li> <li>Donn\u00e9es m\u00e9t\u00e9orologiques du jour</li> </ul>"},{"location":"seance2/partie3-amelioration/#etape-2-diagnostic-du-modele-existant-10-min","title":"\u00c9tape 2: Diagnostic du mod\u00e8le existant (10 min)","text":"<p>Le notebook contient un mod\u00e8le de pr\u00e9vision des ventes sous-optimal avec une erreur moyenne (RMSE) trop \u00e9lev\u00e9e. Votre t\u00e2che consiste \u00e0:</p> <ol> <li>Ex\u00e9cuter les cellules d'initialisation et d'analyse exploratoire des donn\u00e9es</li> <li>Examiner l'architecture du mod\u00e8le de base (un r\u00e9seau de neurones simple)</li> <li>Analyser les m\u00e9triques d'erreur et les pr\u00e9dictions sur le graphique</li> <li>Identifier les p\u00e9riodes ou cat\u00e9gories o\u00f9 le mod\u00e8le performe le moins bien</li> <li>Documenter vos observations dans la section \"Diagnostic\" du notebook</li> </ol>"},{"location":"seance2/partie3-amelioration/#etape-3-amelioration-du-modele-20-min","title":"\u00c9tape 3: Am\u00e9lioration du mod\u00e8le (20 min)","text":"<p>Modifiez le mod\u00e8le pour am\u00e9liorer ses performances en appliquant vos connaissances de Deep Learning. Vous pouvez:</p> <ul> <li>Transformer le probl\u00e8me pour utiliser une architecture CNN ou RNN plus adapt\u00e9e</li> <li>Ajouter des caract\u00e9ristiques temporelles pertinentes (tendance, saisonnalit\u00e9)</li> <li>Modifier l'architecture du r\u00e9seau (couches, neurones, fonctions d'activation)</li> <li>Impl\u00e9menter des techniques d'apprentissage avanc\u00e9es (r\u00e9gularisation, normalisation)</li> <li>Ajuster les hyperparam\u00e8tres (taux d'apprentissage, batch size, optimiseur)</li> </ul> <p>Pour chaque modification:</p> <ol> <li>Documentez votre hypoth\u00e8se (\"Nous pensons que cette modification am\u00e9liorera les performances car...\")</li> <li>Impl\u00e9mentez le changement et mesurez l'impact</li> <li>Analysez les r\u00e9sultats (RMSE globale et par cat\u00e9gorie de produit)</li> </ol>"},{"location":"seance2/partie3-amelioration/#etape-4-preparation-dune-solution-professionnelle-10-min","title":"\u00c9tape 4: Pr\u00e9paration d'une solution professionnelle (10 min)","text":"<p>Votre responsable vous demande une pr\u00e9sentation concise de vos r\u00e9sultats pour la r\u00e9union de service. Pr\u00e9parez:</p> <ol> <li>Un r\u00e9sum\u00e9 des probl\u00e8mes identifi\u00e9s dans le mod\u00e8le initial</li> <li>Une explication des am\u00e9liorations apport\u00e9es (avec justifications techniques)</li> <li>Une d\u00e9monstration des gains de performance (avec visualisations)</li> <li>Une estimation de l'impact business (r\u00e9duction des stocks, meilleure planification)</li> <li>Des recommandations pour am\u00e9liorer davantage le syst\u00e8me</li> </ol>"},{"location":"seance2/partie3-amelioration/#livrables-attendus","title":"Livrables attendus","text":"<p>Votre \u00e9quipe doit produire:</p> <ol> <li>Le notebook compl\u00e9t\u00e9 avec vos am\u00e9liorations et leur justification</li> <li>Une pr\u00e9sentation de 3 minutes maximum (3-4 slides) destin\u00e9e \u00e0 l'\u00e9quipe m\u00e9tier</li> <li>Un tableau r\u00e9capitulatif des exp\u00e9rimentations et de leurs r\u00e9sultats</li> </ol>"},{"location":"seance2/partie3-amelioration/#criteres-devaluation","title":"Crit\u00e8res d'\u00e9valuation","text":"Crit\u00e8re Description Pertinence du diagnostic Avez-vous correctement identifi\u00e9 les faiblesses du mod\u00e8le initial? Qualit\u00e9 technique des am\u00e9liorations Les modifications apport\u00e9es sont-elles pertinentes et bien impl\u00e9ment\u00e9es? Am\u00e9lioration effective des performances Dans quelle mesure avez-vous r\u00e9duit l'erreur de pr\u00e9diction? Pr\u00e9sentation professionnelle Clart\u00e9 de l'explication et pertinence business des r\u00e9sultats Travail d'\u00e9quipe Organisation, r\u00e9partition des t\u00e2ches et collaboration"},{"location":"seance2/partie3-amelioration/#applications-en-stage-de-1ere-annee","title":"Applications en stage de 1\u00e8re ann\u00e9e","text":"<p>Ce challenge simule des situations courantes que vous pourriez rencontrer en stage:</p> <ul> <li>Analyse de donn\u00e9es commerciales: De nombreuses entreprises cherchent \u00e0 mieux comprendre leurs donn\u00e9es de vente</li> <li>Optimisation de syst\u00e8mes existants: Il est fr\u00e9quent qu'on vous demande d'am\u00e9liorer un syst\u00e8me plut\u00f4t que d'en cr\u00e9er un nouveau</li> <li>Communication des r\u00e9sultats techniques: La capacit\u00e9 \u00e0 expliquer clairement des concepts techniques \u00e0 une audience non technique est tr\u00e8s valoris\u00e9e</li> <li>\u00c9valuation de l'impact business: Comprendre comment une am\u00e9lioration technique se traduit en avantage concurrentiel ou \u00e9conomique</li> </ul>"},{"location":"seance2/partie3-amelioration/#conseils-pour-reussir","title":"Conseils pour r\u00e9ussir","text":"<ul> <li>Commencez par l'analyse des donn\u00e9es: Comprendre les patterns dans les donn\u00e9es est souvent plus important que d'avoir un mod\u00e8le complexe</li> <li>Privil\u00e9giez l'approche m\u00e9tier: Concentrez-vous sur les aspects qui ont le plus d'impact business (ex: pr\u00e9voir correctement les pics de vente)</li> <li>Documentez votre d\u00e9marche: En entreprise, la compr\u00e9hension et la maintenance du code sont essentielles</li> <li>\u00c9quilibrez th\u00e9orie et pratique: Les meilleures solutions combinent les connaissances th\u00e9oriques du Deep Learning avec des consid\u00e9rations pratiques du monde r\u00e9el</li> <li>Pensez \u00e0 l'exploitation r\u00e9elle: Comment votre mod\u00e8le s'int\u00e9grerait-il dans les syst\u00e8mes d'information de l'entreprise?</li> </ul> <p>Collaboration</p> <p>Pendant ce challenge, n'h\u00e9sitez pas \u00e0 collaborer au sein de votre \u00e9quipe en vous r\u00e9partissant les t\u00e2ches : par exemple, une personne peut se concentrer sur la pr\u00e9paration des donn\u00e9es pendant que l'autre travaille sur l'architecture du mod\u00e8le.</p> <p>Bonne chance \u00e0 toutes les \u00e9quipes!</p> <p>Retour \u00e0 la vue d'ensemble de la S\u00e9ance 2 Continuer vers la S\u00e9ance 3</p>"},{"location":"seance2/test/","title":"Carte de progression","text":""},{"location":"seance2/test/#gps-pedagogique-votre-itineraire-dapprentissage-du-deep-learning","title":"GPS p\u00e9dagogique : votre itin\u00e9raire d'apprentissage du Deep Learning","text":"<p>Cette carte de progression vous permettra de visualiser clairement les objectifs, les activit\u00e9s et les comp\u00e9tences d\u00e9velopp\u00e9es \u00e0 chaque \u00e9tape de votre formation en Deep Learning.</p>"},{"location":"seance2/test/#parcours-global","title":"Parcours global","text":"<pre><code>gantt\n    title Parcours d'apprentissage du Deep Learning\n    dateFormat  D\n    axisFormat %d\n    tickInterval 1day\n\n    section Fondamentaux\n    S\u00e9ance 1: Introduction au Deep Learning    :s1, 0, 1d\n\n    section Architectures\n    S\u00e9ance 2: Types de r\u00e9seaux et applications    :s2, after s1, 1d\n\n    section Outils &amp; Techniques\n    S\u00e9ance 3: Frameworks et pr\u00e9paration du projet    :s3, after s2, 1d\n\n    section Projet Final\n    S\u00e9ance 4: D\u00e9veloppement et pr\u00e9sentation du chatbot    :s4, after s3, 1d\n\n    section Comp\u00e9tences\n    Concepts fondamentaux    :crit, active, c1, 0, 4d\n    Programmation TensorFlow/Keras    :active, c2, 0, 4d\n    Int\u00e9gration API    :active, c3, after s2, 2d\n    D\u00e9veloppement web    :active, c4, after s1, 3d</code></pre>"},{"location":"seance2/test/#progression-detaillee-des-competences","title":"Progression d\u00e9taill\u00e9e des comp\u00e9tences","text":"<p>Le tableau ci-dessous d\u00e9taille l'\u00e9volution des comp\u00e9tences techniques et conceptuelles que vous d\u00e9velopperez au cours de cette formation :</p> Comp\u00e9tence S\u00e9ance 1 S\u00e9ance 2 S\u00e9ance 3 S\u00e9ance 4 Compr\u00e9hension des r\u00e9seaux de neurones Programmation avec TensorFlow/Keras Visualisation et analyse de donn\u00e9es Traitement d'images Traitement du texte D\u00e9veloppement web Conception et architecture D\u00e9veloppement collaboratif"},{"location":"seance2/test/#parcours-dapprentissage-visuel","title":"Parcours d'apprentissage visuel","text":"<pre><code>flowchart LR\n    subgraph S1[\"S\u00e9ance 1: Introduction\"]\n        F[\"Fondamentaux\\nR\u00e9seaux de neurones\"]\n        DL[\"ML classique vs\\nDeep Learning\"]\n        E[\"Exp\u00e9rimentation\\nnotebook MNIST\"]\n    end\n\n    subgraph S2[\"S\u00e9ance 2: Architectures\"]\n        CNN[\"R\u00e9seaux convolutifs\\nVision par ordinateur\"]\n        RNN[\"R\u00e9seaux r\u00e9currents\\nTraitement du texte\"]\n        AM[\"Challenge\\nd'am\u00e9lioration\"]\n    end\n\n    subgraph S3[\"S\u00e9ance 3: Outils\"]\n        FW[\"Frameworks\\nTensorFlow/Keras\"]\n        OP[\"Optimisation\\ndes performances\"]\n        API[\"API Mistral\\nPrompt engineering\"]\n    end\n\n    subgraph S4[\"S\u00e9ance 4: Projet\"]\n        DEV[\"D\u00e9veloppement\\ndu chatbot\"]\n        TEST[\"Finalisation\\net tests\"]\n        PRES[\"Pr\u00e9sentation\\nfinale\"]\n    end\n\n    S1 --&gt; S2\n    S2 --&gt; S3\n    S3 --&gt; S4\n\n    style S1 fill:#e1f5fe,stroke:#0288d1\n    style S2 fill:#e8f5e9,stroke:#388e3c\n    style S3 fill:#fff8e1,stroke:#ffa000\n    style S4 fill:#ffebee,stroke:#d32f2f</code></pre>"},{"location":"seance2/test/#detail-des-seances-et-objectifs-pedagogiques","title":"D\u00e9tail des s\u00e9ances et objectifs p\u00e9dagogiques","text":""},{"location":"seance2/test/#seance-1-introduction-au-deep-learning-par-lexperimentation","title":"S\u00e9ance 1 : Introduction au Deep Learning par l'exp\u00e9rimentation","text":"<p>Objectifs p\u00e9dagogiques :</p> <ul> <li>D\u00e9couvrir le Deep Learning par des manipulations concr\u00e8tes</li> <li>Comprendre les diff\u00e9rences fondamentales entre ML classique et DL</li> <li>Explorer le fonctionnement interne d'un r\u00e9seau de neurones simple</li> <li>Acqu\u00e9rir le vocabulaire technique de base</li> </ul> <p>Activit\u00e9s :</p> <ul> <li>Mise en situation pratique : d\u00e9monstrations et notebook \"Hello World\"</li> <li>D\u00e9couverte comparative : ML classique vs Deep Learning</li> <li>Exploration guid\u00e9e : anatomie d'un r\u00e9seau de neurones</li> <li>Synth\u00e8se et auto-\u00e9valuation</li> </ul> <p>Comp\u00e9tences BTS SIO d\u00e9velopp\u00e9es :</p> <ul> <li>B1.3 : Gestion des donn\u00e9es d'images pour les mod\u00e8les</li> <li>B2.2 : Conception de mod\u00e8les simples</li> <li>B3.2 : \u00c9valuation de la performance des mod\u00e8les</li> </ul> <p>Livrables :</p> <ul> <li>Fiche d'observations compl\u00e9t\u00e9e</li> <li>Tableau comparatif ML vs DL</li> <li>Sch\u00e9ma annot\u00e9 d'un r\u00e9seau de neurones</li> </ul>"},{"location":"seance2/test/#seance-2-types-de-reseaux-et-leurs-applications","title":"S\u00e9ance 2 : Types de r\u00e9seaux et leurs applications","text":"<p>Objectifs p\u00e9dagogiques :</p> <ul> <li>Ma\u00eetriser les principes des r\u00e9seaux convolutifs (CNN)</li> <li>Comprendre le fonctionnement des r\u00e9seaux r\u00e9currents (RNN)</li> <li>Impl\u00e9menter des mod\u00e8les pour diff\u00e9rents types de donn\u00e9es</li> <li>Visualiser et interpr\u00e9ter le fonctionnement des mod\u00e8les</li> </ul> <p>Activit\u00e9s :</p> <ul> <li>Mini-projet CNN : classification d'images et visualisation</li> <li>Mini-projet RNN : traitement de texte et pr\u00e9diction de s\u00e9quences</li> <li>Challenge d'am\u00e9lioration : optimisation collaborative d'un mod\u00e8le</li> </ul> <p>Comp\u00e9tences BTS SIO d\u00e9velopp\u00e9es :</p> <ul> <li>B1.3 : Traitement de donn\u00e9es complexes (images, textes)</li> <li>B2.2 : D\u00e9veloppement de mod\u00e8les sp\u00e9cialis\u00e9s</li> <li>B2.3 : Cr\u00e9ation d'interfaces simples pour les mod\u00e8les</li> <li>B3.2 : Analyse comparative des performances</li> </ul> <p>Livrables :</p> <ul> <li>Mod\u00e8le CNN fonctionnel avec visualisations</li> <li>Mod\u00e8le RNN pour analyse de texte</li> <li>Rapport d'am\u00e9lioration document\u00e9</li> </ul>"},{"location":"seance2/test/#seance-3-frameworks-pratiques-et-preparation-du-projet","title":"S\u00e9ance 3 : Frameworks pratiques et pr\u00e9paration du projet","text":"<p>Objectifs p\u00e9dagogiques :</p> <ul> <li>Ma\u00eetriser les frameworks de Deep Learning courants</li> <li>Optimiser les performances des mod\u00e8les</li> <li>D\u00e9couvrir l'API Mistral AI pour les applications conversationnelles</li> <li>Pr\u00e9parer le projet de chatbot p\u00e9dagogique</li> </ul> <p>Activit\u00e9s :</p> <ul> <li>Frameworks en pratique : utilisation efficace de TensorFlow/Keras</li> <li>Optimisation de mod\u00e8les : techniques d'am\u00e9lioration des performances</li> <li>Introduction \u00e0 Mistral AI : premiers pas avec l'API</li> <li>Conception du chatbot : pr\u00e9paration de l'architecture et des fonctionnalit\u00e9s</li> </ul> <p>Comp\u00e9tences BTS SIO d\u00e9velopp\u00e9es :</p> <ul> <li>B1.4 : Exploitation des API et interfaces de programmation</li> <li>B2.2 : Optimisation de solutions applicatives</li> <li>B3.2 : V\u00e9rification et am\u00e9lioration des performances</li> </ul> <p>Livrables :</p> <ul> <li>Applications fonctionnelles avec TensorFlow/Keras</li> <li>Premier test d'int\u00e9gration avec l'API Mistral</li> <li>Document de conception du chatbot</li> </ul>"},{"location":"seance2/test/#seance-4-projet-integrateur-chatbot-pedagogique","title":"S\u00e9ance 4 : Projet int\u00e9grateur - Chatbot p\u00e9dagogique","text":"<p>Objectifs p\u00e9dagogiques :</p> <ul> <li>Mettre en \u0153uvre l'ensemble des connaissances acquises</li> <li>D\u00e9velopper une application conversationnelle compl\u00e8te</li> <li>Structurer une base de connaissances p\u00e9dagogique</li> <li>Pr\u00e9senter et d\u00e9fendre un projet technique</li> </ul> <p>Activit\u00e9s :</p> <ul> <li>D\u00e9veloppement du chatbot : interface et backend</li> <li>Int\u00e9gration de l'API Mistral et de la base de connaissances</li> <li>Tests et optimisation de l'exp\u00e9rience utilisateur</li> <li>Pr\u00e9paration et r\u00e9alisation de la pr\u00e9sentation finale</li> </ul> <p>Comp\u00e9tences BTS SIO d\u00e9velopp\u00e9es :</p> <ul> <li>B1.4 : Exploitation avanc\u00e9e des API</li> <li>B2.2/B2.3 : D\u00e9veloppement d'une solution applicative compl\u00e8te</li> <li>B3.2/B3.3 : Tests, documentation et pr\u00e9sentation technique</li> </ul> <p>Livrables :</p> <ul> <li>Code source complet du chatbot</li> <li>Documentation technique et guide utilisateur</li> <li>Pr\u00e9sentation et d\u00e9monstration du projet</li> </ul>"},{"location":"seance2/test/#points-de-controle-de-votre-progression","title":"Points de contr\u00f4le de votre progression","text":""},{"location":"seance2/test/#apres-la-seance-1","title":"Apr\u00e8s la S\u00e9ance 1","text":"<ul> <li> Je comprends ce qu'est un r\u00e9seau de neurones</li> <li> Je sais expliquer la diff\u00e9rence entre ML classique et Deep Learning</li> <li> J'ai r\u00e9ussi \u00e0 manipuler un mod\u00e8le simple sur MNIST</li> </ul>"},{"location":"seance2/test/#apres-la-seance-2","title":"Apr\u00e8s la S\u00e9ance 2","text":"<ul> <li> Je comprends ce qu'est un CNN et son application en vision par ordinateur</li> <li> Je comprends ce qu'est un RNN et son application en traitement de texte</li> <li> J'ai r\u00e9ussi \u00e0 am\u00e9liorer un mod\u00e8le existant</li> </ul>"},{"location":"seance2/test/#apres-la-seance-3","title":"Apr\u00e8s la S\u00e9ance 3","text":"<ul> <li> Je sais utiliser TensorFlow/Keras pour cr\u00e9er un mod\u00e8le simple</li> <li> Je connais les techniques d'optimisation des performances</li> <li> J'ai r\u00e9ussi \u00e0 faire un premier test avec l'API Mistral</li> </ul>"},{"location":"seance2/test/#apres-la-seance-4","title":"Apr\u00e8s la S\u00e9ance 4","text":"<ul> <li> J'ai d\u00e9velopp\u00e9 un chatbot p\u00e9dagogique fonctionnel</li> <li> J'ai int\u00e9gr\u00e9 une base de connaissances structur\u00e9e</li> <li> J'ai pr\u00e9sent\u00e9 mon projet de mani\u00e8re claire et convaincante</li> </ul>"},{"location":"seance2/test/#conseil-pour-optimiser-votre-apprentissage","title":"Conseil pour optimiser votre apprentissage","text":"<ol> <li>Exp\u00e9rimentez r\u00e9guli\u00e8rement avec les exemples de code fournis</li> <li>Posez des questions d\u00e8s qu'un concept n'est pas clair</li> <li>Collaborez avec vos pairs pour r\u00e9soudre les probl\u00e8mes complexes</li> <li>Documentez votre progression et vos d\u00e9couvertes</li> <li>\u00c9tablissez des liens entre les diff\u00e9rents concepts et technologies</li> </ol> <p>Cette formation est con\u00e7ue comme un parcours progressif o\u00f9 chaque s\u00e9ance s'appuie sur les acquis des pr\u00e9c\u00e9dentes. Suivez le chemin balis\u00e9 tout en explorant les possibilit\u00e9s qui vous int\u00e9ressent particuli\u00e8rement.</p> <p>Retour \u00e0 l'accueil Voir la pr\u00e9sentation du projet Commencer la S\u00e9ance 1</p>"},{"location":"seance2/challenge-prevision-ventes/data_utils/","title":"Data utils","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\ndata_utils.py\nFonctions utilitaires pour le traitement des donn\u00e9es de ventes pour le challenge de pr\u00e9vision.\n\"\"\"\n</pre> \"\"\" data_utils.py Fonctions utilitaires pour le traitement des donn\u00e9es de ventes pour le challenge de pr\u00e9vision. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n</pre> import numpy as np import pandas as pd from sklearn.preprocessing import MinMaxScaler, OneHotEncoder import matplotlib.pyplot as plt from datetime import datetime, timedelta In\u00a0[\u00a0]: Copied! <pre>def load_sales_data(filepath=\"sales_data.csv\"):\n    \"\"\"\n    Charge les donn\u00e9es de ventes \u00e0 partir d'un fichier CSV.\n    \n    Args:\n        filepath (str): Chemin vers le fichier CSV des donn\u00e9es de ventes\n        \n    Returns:\n        pandas.DataFrame: DataFrame contenant les donn\u00e9es de ventes\n    \"\"\"\n    try:\n        df = pd.read_csv(filepath)\n        # Convertir la colonne date en datetime\n        df['date'] = pd.to_datetime(df['date'])\n        print(f\"Donn\u00e9es charg\u00e9es avec succ\u00e8s: {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n        return df\n    except Exception as e:\n        print(f\"Erreur lors du chargement des donn\u00e9es: {e}\")\n        # Cr\u00e9er des donn\u00e9es synth\u00e9tiques si le fichier n'existe pas\n        return generate_synthetic_data()\n</pre> def load_sales_data(filepath=\"sales_data.csv\"):     \"\"\"     Charge les donn\u00e9es de ventes \u00e0 partir d'un fichier CSV.          Args:         filepath (str): Chemin vers le fichier CSV des donn\u00e9es de ventes              Returns:         pandas.DataFrame: DataFrame contenant les donn\u00e9es de ventes     \"\"\"     try:         df = pd.read_csv(filepath)         # Convertir la colonne date en datetime         df['date'] = pd.to_datetime(df['date'])         print(f\"Donn\u00e9es charg\u00e9es avec succ\u00e8s: {df.shape[0]} lignes et {df.shape[1]} colonnes\")         return df     except Exception as e:         print(f\"Erreur lors du chargement des donn\u00e9es: {e}\")         # Cr\u00e9er des donn\u00e9es synth\u00e9tiques si le fichier n'existe pas         return generate_synthetic_data() In\u00a0[\u00a0]: Copied! <pre>def generate_synthetic_data(n_samples=365*2):\n    \"\"\"\n    G\u00e9n\u00e8re des donn\u00e9es de ventes synth\u00e9tiques pour le challenge.\n    \n    Args:\n        n_samples (int): Nombre d'\u00e9chantillons \u00e0 g\u00e9n\u00e9rer\n        \n    Returns:\n        pandas.DataFrame: DataFrame contenant les donn\u00e9es synth\u00e9tiques\n    \"\"\"\n    # Date de d\u00e9but (2 ans de donn\u00e9es)\n    start_date = datetime(2022, 1, 1)\n    dates = [start_date + timedelta(days=i) for i in range(n_samples)]\n    \n    # Cat\u00e9gories de produits\n    categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports']\n    \n    # Cr\u00e9ation du DataFrame\n    data = []\n    for date in dates:\n        for _ in range(np.random.randint(3, 8)):  # 3-7 transactions par jour\n            category = np.random.choice(categories)\n            \n            # Prix de base selon la cat\u00e9gorie\n            base_prices = {\n                'Electronics': np.random.uniform(100, 1000),\n                'Clothing': np.random.uniform(20, 200),\n                'Home': np.random.uniform(50, 500),\n                'Books': np.random.uniform(10, 50),\n                'Sports': np.random.uniform(30, 300)\n            }\n            price = base_prices[category]\n            \n            # Remise\n            discount = np.random.choice([0, 0, 0, 5, 10, 15, 20, 25], p=[0.6, 0.1, 0.1, 0.05, 0.05, 0.05, 0.03, 0.02])\n            \n            # Jour de la semaine (0=lundi, 6=dimanche)\n            weekday = date.weekday()\n            \n            # P\u00e9riode promotionnelle (plus fr\u00e9quente en fin de mois)\n            is_promotional = 1 if (date.day &gt; 25 or date.day &lt; 5) or np.random.random() &lt; 0.1 else 0\n            \n            # Temp\u00e9rature (saisonnalit\u00e9)\n            month = date.month\n            base_temp = 15 + 10 * np.sin((month - 1) * np.pi / 6)  # Plus chaud en \u00e9t\u00e9\n            temperature = base_temp + np.random.uniform(-5, 5)\n            \n            # Pr\u00e9cipitations (plus \u00e9lev\u00e9es en automne/hiver)\n            rainfall_prob = 0.3 + 0.2 * np.sin((month - 7) * np.pi / 6)\n            rainfall = np.random.exponential(5) if np.random.random() &lt; rainfall_prob else 0\n            \n            # Quantit\u00e9 vendue (influenc\u00e9e par divers facteurs)\n            base_quantity = np.random.poisson(5)\n            \n            # Facteurs d'influence\n            # - Les weekends (5,6) ont plus de ventes\n            weekend_factor = 1.5 if weekday &gt;= 5 else 1.0\n            # - Les p\u00e9riodes promotionnelles augmentent les ventes\n            promo_factor = 1.8 if is_promotional else 1.0\n            # - Les ventes sont saisonni\u00e8res (pics avant No\u00ebl, \u00e9t\u00e9 pour certaines cat\u00e9gories)\n            seasonal_factor = 1.0\n            if category == 'Electronics' and (month == 11 or month == 12):\n                seasonal_factor = 1.7  # Plus d'\u00e9lectronique avant No\u00ebl\n            elif category == 'Clothing' and (month &gt;= 3 and month &lt;= 5):\n                seasonal_factor = 1.4  # Plus de v\u00eatements au printemps\n            elif category == 'Sports' and (month &gt;= 4 and month &lt;= 8):\n                seasonal_factor = 1.6  # Plus d'articles de sport en \u00e9t\u00e9\n            \n            # Appliquer tous les facteurs\n            quantity_sold = max(1, int(base_quantity * weekend_factor * promo_factor * seasonal_factor))\n            \n            # Calculer le chiffre d'affaires\n            revenue = price * quantity_sold * (1 - discount/100)\n            \n            data.append({\n                'date': date,\n                'product_category': category,\n                'price': round(price, 2),\n                'discount': discount,\n                'quantity_sold': quantity_sold,\n                'weekday': weekday,\n                'is_promotional_period': is_promotional,\n                'temperature': round(temperature, 1),\n                'rainfall': round(rainfall, 1),\n                'revenue': round(revenue, 2)\n            })\n    \n    df = pd.DataFrame(data)\n    print(f\"Donn\u00e9es synth\u00e9tiques g\u00e9n\u00e9r\u00e9es: {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n    return df\n</pre> def generate_synthetic_data(n_samples=365*2):     \"\"\"     G\u00e9n\u00e8re des donn\u00e9es de ventes synth\u00e9tiques pour le challenge.          Args:         n_samples (int): Nombre d'\u00e9chantillons \u00e0 g\u00e9n\u00e9rer              Returns:         pandas.DataFrame: DataFrame contenant les donn\u00e9es synth\u00e9tiques     \"\"\"     # Date de d\u00e9but (2 ans de donn\u00e9es)     start_date = datetime(2022, 1, 1)     dates = [start_date + timedelta(days=i) for i in range(n_samples)]          # Cat\u00e9gories de produits     categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Sports']          # Cr\u00e9ation du DataFrame     data = []     for date in dates:         for _ in range(np.random.randint(3, 8)):  # 3-7 transactions par jour             category = np.random.choice(categories)                          # Prix de base selon la cat\u00e9gorie             base_prices = {                 'Electronics': np.random.uniform(100, 1000),                 'Clothing': np.random.uniform(20, 200),                 'Home': np.random.uniform(50, 500),                 'Books': np.random.uniform(10, 50),                 'Sports': np.random.uniform(30, 300)             }             price = base_prices[category]                          # Remise             discount = np.random.choice([0, 0, 0, 5, 10, 15, 20, 25], p=[0.6, 0.1, 0.1, 0.05, 0.05, 0.05, 0.03, 0.02])                          # Jour de la semaine (0=lundi, 6=dimanche)             weekday = date.weekday()                          # P\u00e9riode promotionnelle (plus fr\u00e9quente en fin de mois)             is_promotional = 1 if (date.day &gt; 25 or date.day &lt; 5) or np.random.random() &lt; 0.1 else 0                          # Temp\u00e9rature (saisonnalit\u00e9)             month = date.month             base_temp = 15 + 10 * np.sin((month - 1) * np.pi / 6)  # Plus chaud en \u00e9t\u00e9             temperature = base_temp + np.random.uniform(-5, 5)                          # Pr\u00e9cipitations (plus \u00e9lev\u00e9es en automne/hiver)             rainfall_prob = 0.3 + 0.2 * np.sin((month - 7) * np.pi / 6)             rainfall = np.random.exponential(5) if np.random.random() &lt; rainfall_prob else 0                          # Quantit\u00e9 vendue (influenc\u00e9e par divers facteurs)             base_quantity = np.random.poisson(5)                          # Facteurs d'influence             # - Les weekends (5,6) ont plus de ventes             weekend_factor = 1.5 if weekday &gt;= 5 else 1.0             # - Les p\u00e9riodes promotionnelles augmentent les ventes             promo_factor = 1.8 if is_promotional else 1.0             # - Les ventes sont saisonni\u00e8res (pics avant No\u00ebl, \u00e9t\u00e9 pour certaines cat\u00e9gories)             seasonal_factor = 1.0             if category == 'Electronics' and (month == 11 or month == 12):                 seasonal_factor = 1.7  # Plus d'\u00e9lectronique avant No\u00ebl             elif category == 'Clothing' and (month &gt;= 3 and month &lt;= 5):                 seasonal_factor = 1.4  # Plus de v\u00eatements au printemps             elif category == 'Sports' and (month &gt;= 4 and month &lt;= 8):                 seasonal_factor = 1.6  # Plus d'articles de sport en \u00e9t\u00e9                          # Appliquer tous les facteurs             quantity_sold = max(1, int(base_quantity * weekend_factor * promo_factor * seasonal_factor))                          # Calculer le chiffre d'affaires             revenue = price * quantity_sold * (1 - discount/100)                          data.append({                 'date': date,                 'product_category': category,                 'price': round(price, 2),                 'discount': discount,                 'quantity_sold': quantity_sold,                 'weekday': weekday,                 'is_promotional_period': is_promotional,                 'temperature': round(temperature, 1),                 'rainfall': round(rainfall, 1),                 'revenue': round(revenue, 2)             })          df = pd.DataFrame(data)     print(f\"Donn\u00e9es synth\u00e9tiques g\u00e9n\u00e9r\u00e9es: {df.shape[0]} lignes et {df.shape[1]} colonnes\")     return df In\u00a0[\u00a0]: Copied! <pre>def preprocess_data(df, target_col='revenue', test_size=0.2):\n    \"\"\"\n    Pr\u00e9traite les donn\u00e9es pour les mod\u00e8les de Deep Learning.\n    \n    Args:\n        df (pandas.DataFrame): DataFrame contenant les donn\u00e9es de ventes\n        target_col (str): Nom de la colonne cible \u00e0 pr\u00e9dire\n        test_size (float): Proportion des donn\u00e9es \u00e0 r\u00e9server pour le test\n        \n    Returns:\n        tuple: (X_train, X_test, y_train, y_test, scalers) o\u00f9 scalers est un dictionnaire \n               contenant les objets de mise \u00e0 l'\u00e9chelle pour inverser les transformations\n    \"\"\"\n    # Copie du DataFrame pour \u00e9viter les modifications inplace\n    data = df.copy()\n    \n    # Ajouter des caract\u00e9ristiques temporelles\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n    data['day'] = data['date'].dt.day\n    data['day_of_year'] = data['date'].dt.dayofyear\n    data['quarter'] = data['date'].dt.quarter\n    \n    # Encodage one-hot des cat\u00e9gories de produits\n    encoder = OneHotEncoder(sparse=False)\n    category_encoded = encoder.fit_transform(data[['product_category']])\n    category_cols = [f\"cat_{cat}\" for cat in encoder.categories_[0]]\n    category_df = pd.DataFrame(category_encoded, columns=category_cols)\n    \n    # Concat\u00e9ner au DataFrame original\n    data = pd.concat([data.reset_index(drop=True), category_df.reset_index(drop=True)], axis=1)\n    \n    # S\u00e9lectionner les colonnes num\u00e9riques pour la normalisation\n    numeric_cols = ['price', 'discount', 'quantity_sold', 'weekday', \n                    'is_promotional_period', 'temperature', 'rainfall',\n                    'year', 'month', 'day', 'day_of_year', 'quarter']\n    \n    # Normaliser les caract\u00e9ristiques num\u00e9riques\n    scaler_X = MinMaxScaler()\n    data[numeric_cols] = scaler_X.fit_transform(data[numeric_cols])\n    \n    # Normaliser la cible\n    scaler_y = MinMaxScaler()\n    data[target_col] = scaler_y.fit_transform(data[[target_col]])\n    \n    # S\u00e9lectionner les caract\u00e9ristiques et la cible\n    features = numeric_cols + category_cols\n    X = data[features].values\n    y = data[target_col].values\n    \n    # Division temporelle en ensembles d'entra\u00eenement et de test\n    split_idx = int(len(data) * (1 - test_size))\n    X_train, X_test = X[:split_idx], X[split_idx:]\n    y_train, y_test = y[:split_idx], y[split_idx:]\n    \n    # Stocker les scalers pour inverser les transformations plus tard\n    scalers = {\n        'X': scaler_X,\n        'y': scaler_y,\n        'features': features,\n        'target': target_col\n    }\n    \n    print(f\"Donn\u00e9es pr\u00e9trait\u00e9es: {len(X_train)} \u00e9chantillons d'entra\u00eenement, {len(X_test)} \u00e9chantillons de test\")\n    return X_train, X_test, y_train, y_test, scalers\n</pre> def preprocess_data(df, target_col='revenue', test_size=0.2):     \"\"\"     Pr\u00e9traite les donn\u00e9es pour les mod\u00e8les de Deep Learning.          Args:         df (pandas.DataFrame): DataFrame contenant les donn\u00e9es de ventes         target_col (str): Nom de la colonne cible \u00e0 pr\u00e9dire         test_size (float): Proportion des donn\u00e9es \u00e0 r\u00e9server pour le test              Returns:         tuple: (X_train, X_test, y_train, y_test, scalers) o\u00f9 scalers est un dictionnaire                 contenant les objets de mise \u00e0 l'\u00e9chelle pour inverser les transformations     \"\"\"     # Copie du DataFrame pour \u00e9viter les modifications inplace     data = df.copy()          # Ajouter des caract\u00e9ristiques temporelles     data['year'] = data['date'].dt.year     data['month'] = data['date'].dt.month     data['day'] = data['date'].dt.day     data['day_of_year'] = data['date'].dt.dayofyear     data['quarter'] = data['date'].dt.quarter          # Encodage one-hot des cat\u00e9gories de produits     encoder = OneHotEncoder(sparse=False)     category_encoded = encoder.fit_transform(data[['product_category']])     category_cols = [f\"cat_{cat}\" for cat in encoder.categories_[0]]     category_df = pd.DataFrame(category_encoded, columns=category_cols)          # Concat\u00e9ner au DataFrame original     data = pd.concat([data.reset_index(drop=True), category_df.reset_index(drop=True)], axis=1)          # S\u00e9lectionner les colonnes num\u00e9riques pour la normalisation     numeric_cols = ['price', 'discount', 'quantity_sold', 'weekday',                      'is_promotional_period', 'temperature', 'rainfall',                     'year', 'month', 'day', 'day_of_year', 'quarter']          # Normaliser les caract\u00e9ristiques num\u00e9riques     scaler_X = MinMaxScaler()     data[numeric_cols] = scaler_X.fit_transform(data[numeric_cols])          # Normaliser la cible     scaler_y = MinMaxScaler()     data[target_col] = scaler_y.fit_transform(data[[target_col]])          # S\u00e9lectionner les caract\u00e9ristiques et la cible     features = numeric_cols + category_cols     X = data[features].values     y = data[target_col].values          # Division temporelle en ensembles d'entra\u00eenement et de test     split_idx = int(len(data) * (1 - test_size))     X_train, X_test = X[:split_idx], X[split_idx:]     y_train, y_test = y[:split_idx], y[split_idx:]          # Stocker les scalers pour inverser les transformations plus tard     scalers = {         'X': scaler_X,         'y': scaler_y,         'features': features,         'target': target_col     }          print(f\"Donn\u00e9es pr\u00e9trait\u00e9es: {len(X_train)} \u00e9chantillons d'entra\u00eenement, {len(X_test)} \u00e9chantillons de test\")     return X_train, X_test, y_train, y_test, scalers In\u00a0[\u00a0]: Copied! <pre>def create_sequences(X, y, seq_length=7):\n    \"\"\"\n    Cr\u00e9e des s\u00e9quences pour l'entra\u00eenement de mod\u00e8les RNN/LSTM.\n    \n    Args:\n        X (numpy.ndarray): Caract\u00e9ristiques\n        y (numpy.ndarray): Cible\n        seq_length (int): Longueur de la s\u00e9quence (nombre de jours pr\u00e9c\u00e9dents \u00e0 consid\u00e9rer)\n        \n    Returns:\n        tuple: (X_seq, y_seq) o\u00f9 X_seq contient des s\u00e9quences de caract\u00e9ristiques et \n               y_seq contient les valeurs cibles correspondantes\n    \"\"\"\n    X_seq, y_seq = [], []\n    \n    for i in range(len(X) - seq_length):\n        X_seq.append(X[i:i+seq_length])\n        y_seq.append(y[i+seq_length])\n    \n    return np.array(X_seq), np.array(y_seq)\n</pre> def create_sequences(X, y, seq_length=7):     \"\"\"     Cr\u00e9e des s\u00e9quences pour l'entra\u00eenement de mod\u00e8les RNN/LSTM.          Args:         X (numpy.ndarray): Caract\u00e9ristiques         y (numpy.ndarray): Cible         seq_length (int): Longueur de la s\u00e9quence (nombre de jours pr\u00e9c\u00e9dents \u00e0 consid\u00e9rer)              Returns:         tuple: (X_seq, y_seq) o\u00f9 X_seq contient des s\u00e9quences de caract\u00e9ristiques et                 y_seq contient les valeurs cibles correspondantes     \"\"\"     X_seq, y_seq = [], []          for i in range(len(X) - seq_length):         X_seq.append(X[i:i+seq_length])         y_seq.append(y[i+seq_length])          return np.array(X_seq), np.array(y_seq) In\u00a0[\u00a0]: Copied! <pre>def inverse_transform_predictions(y_pred, scaler_y):\n    \"\"\"\n    Inverse la normalisation des pr\u00e9dictions.\n    \n    Args:\n        y_pred (numpy.ndarray): Pr\u00e9dictions normalis\u00e9es\n        scaler_y (sklearn.preprocessing.MinMaxScaler): Scaler utilis\u00e9 pour la cible\n        \n    Returns:\n        numpy.ndarray: Pr\u00e9dictions dans l'\u00e9chelle originale\n    \"\"\"\n    # Reshape si n\u00e9cessaire\n    if len(y_pred.shape) == 1:\n        y_pred = y_pred.reshape(-1, 1)\n    \n    # Inverse la normalisation\n    return scaler_y.inverse_transform(y_pred)\n</pre> def inverse_transform_predictions(y_pred, scaler_y):     \"\"\"     Inverse la normalisation des pr\u00e9dictions.          Args:         y_pred (numpy.ndarray): Pr\u00e9dictions normalis\u00e9es         scaler_y (sklearn.preprocessing.MinMaxScaler): Scaler utilis\u00e9 pour la cible              Returns:         numpy.ndarray: Pr\u00e9dictions dans l'\u00e9chelle originale     \"\"\"     # Reshape si n\u00e9cessaire     if len(y_pred.shape) == 1:         y_pred = y_pred.reshape(-1, 1)          # Inverse la normalisation     return scaler_y.inverse_transform(y_pred) In\u00a0[\u00a0]: Copied! <pre>def plot_predictions_vs_actual(y_true, y_pred, dates=None, title=\"Pr\u00e9dictions vs Valeurs R\u00e9elles\"):\n    \"\"\"\n    Trace un graphique des pr\u00e9dictions vs valeurs r\u00e9elles.\n    \n    Args:\n        y_true (numpy.ndarray): Valeurs r\u00e9elles\n        y_pred (numpy.ndarray): Pr\u00e9dictions\n        dates (list, optional): Liste des dates correspondant aux donn\u00e9es\n        title (str): Titre du graphique\n    \"\"\"\n    plt.figure(figsize=(12, 6))\n    \n    if dates is not None:\n        plt.plot(dates, y_true, 'b-', label='Valeurs R\u00e9elles')\n        plt.plot(dates, y_pred, 'r--', label='Pr\u00e9dictions')\n        plt.xticks(rotation=45)\n    else:\n        plt.plot(y_true, 'b-', label='Valeurs R\u00e9elles')\n        plt.plot(y_pred, 'r--', label='Pr\u00e9dictions')\n    \n    plt.title(title)\n    plt.xlabel('Temps')\n    plt.ylabel('Chiffre d\\'affaires')\n    plt.legend()\n    plt.tight_layout()\n    plt.grid(True, alpha=0.3)\n    \n    # Calculer et afficher les m\u00e9triques\n    mse = np.mean((y_true - y_pred)**2)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(y_true - y_pred))\n    \n    plt.figtext(0.5, 0.01, f'RMSE: {rmse:.2f}, MAE: {mae:.2f}', \n                ha='center', fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.2, \"pad\":5})\n    \n    plt.show()\n</pre> def plot_predictions_vs_actual(y_true, y_pred, dates=None, title=\"Pr\u00e9dictions vs Valeurs R\u00e9elles\"):     \"\"\"     Trace un graphique des pr\u00e9dictions vs valeurs r\u00e9elles.          Args:         y_true (numpy.ndarray): Valeurs r\u00e9elles         y_pred (numpy.ndarray): Pr\u00e9dictions         dates (list, optional): Liste des dates correspondant aux donn\u00e9es         title (str): Titre du graphique     \"\"\"     plt.figure(figsize=(12, 6))          if dates is not None:         plt.plot(dates, y_true, 'b-', label='Valeurs R\u00e9elles')         plt.plot(dates, y_pred, 'r--', label='Pr\u00e9dictions')         plt.xticks(rotation=45)     else:         plt.plot(y_true, 'b-', label='Valeurs R\u00e9elles')         plt.plot(y_pred, 'r--', label='Pr\u00e9dictions')          plt.title(title)     plt.xlabel('Temps')     plt.ylabel('Chiffre d\\'affaires')     plt.legend()     plt.tight_layout()     plt.grid(True, alpha=0.3)          # Calculer et afficher les m\u00e9triques     mse = np.mean((y_true - y_pred)**2)     rmse = np.sqrt(mse)     mae = np.mean(np.abs(y_true - y_pred))          plt.figtext(0.5, 0.01, f'RMSE: {rmse:.2f}, MAE: {mae:.2f}',                  ha='center', fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.2, \"pad\":5})          plt.show() In\u00a0[\u00a0]: Copied! <pre>def calculate_metrics(y_true, y_pred):\n    \"\"\"\n    Calcule diverses m\u00e9triques d'\u00e9valuation.\n    \n    Args:\n        y_true (numpy.ndarray): Valeurs r\u00e9elles\n        y_pred (numpy.ndarray): Pr\u00e9dictions\n        \n    Returns:\n        dict: Dictionnaire contenant les m\u00e9triques d'\u00e9valuation\n    \"\"\"\n    mse = np.mean((y_true - y_pred)**2)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(y_true - y_pred))\n    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100\n    \n    metrics = {\n        'mse': mse,\n        'rmse': rmse,\n        'mae': mae,\n        'mape': mape\n    }\n    \n    return metrics\n</pre> def calculate_metrics(y_true, y_pred):     \"\"\"     Calcule diverses m\u00e9triques d'\u00e9valuation.          Args:         y_true (numpy.ndarray): Valeurs r\u00e9elles         y_pred (numpy.ndarray): Pr\u00e9dictions              Returns:         dict: Dictionnaire contenant les m\u00e9triques d'\u00e9valuation     \"\"\"     mse = np.mean((y_true - y_pred)**2)     rmse = np.sqrt(mse)     mae = np.mean(np.abs(y_true - y_pred))     mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100          metrics = {         'mse': mse,         'rmse': rmse,         'mae': mae,         'mape': mape     }          return metrics In\u00a0[\u00a0]: Copied! <pre>def plot_metrics_by_category(df, y_true, y_pred, category_col='product_category'):\n    \"\"\"\n    Trace les m\u00e9triques d'erreur par cat\u00e9gorie de produit.\n    \n    Args:\n        df (pandas.DataFrame): DataFrame contenant les donn\u00e9es\n        y_true (numpy.ndarray): Valeurs r\u00e9elles\n        y_pred (numpy.ndarray): Pr\u00e9dictions\n        category_col (str): Nom de la colonne de cat\u00e9gorie\n    \"\"\"\n    # S'assurer que les donn\u00e9es ont la m\u00eame longueur\n    n_samples = min(len(df), len(y_true), len(y_pred))\n    df_subset = df.iloc[-n_samples:].copy()\n    df_subset['true'] = y_true[-n_samples:]\n    df_subset['pred'] = y_pred[-n_samples:]\n    df_subset['abs_error'] = np.abs(df_subset['true'] - df_subset['pred'])\n    \n    # Calculer l'erreur moyenne par cat\u00e9gorie\n    error_by_category = df_subset.groupby(category_col)['abs_error'].mean().sort_values(ascending=False)\n    \n    # Tracer le graphique\n    plt.figure(figsize=(10, 6))\n    error_by_category.plot(kind='bar', color='coral')\n    plt.title('Erreur Moyenne Absolue par Cat\u00e9gorie de Produit')\n    plt.ylabel('Erreur Moyenne Absolue')\n    plt.xlabel('Cat\u00e9gorie de Produit')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.grid(True, axis='y', alpha=0.3)\n    plt.show()\n</pre> def plot_metrics_by_category(df, y_true, y_pred, category_col='product_category'):     \"\"\"     Trace les m\u00e9triques d'erreur par cat\u00e9gorie de produit.          Args:         df (pandas.DataFrame): DataFrame contenant les donn\u00e9es         y_true (numpy.ndarray): Valeurs r\u00e9elles         y_pred (numpy.ndarray): Pr\u00e9dictions         category_col (str): Nom de la colonne de cat\u00e9gorie     \"\"\"     # S'assurer que les donn\u00e9es ont la m\u00eame longueur     n_samples = min(len(df), len(y_true), len(y_pred))     df_subset = df.iloc[-n_samples:].copy()     df_subset['true'] = y_true[-n_samples:]     df_subset['pred'] = y_pred[-n_samples:]     df_subset['abs_error'] = np.abs(df_subset['true'] - df_subset['pred'])          # Calculer l'erreur moyenne par cat\u00e9gorie     error_by_category = df_subset.groupby(category_col)['abs_error'].mean().sort_values(ascending=False)          # Tracer le graphique     plt.figure(figsize=(10, 6))     error_by_category.plot(kind='bar', color='coral')     plt.title('Erreur Moyenne Absolue par Cat\u00e9gorie de Produit')     plt.ylabel('Erreur Moyenne Absolue')     plt.xlabel('Cat\u00e9gorie de Produit')     plt.xticks(rotation=45)     plt.tight_layout()     plt.grid(True, axis='y', alpha=0.3)     plt.show() In\u00a0[\u00a0]: Copied! <pre>def analyze_residuals(y_true, y_pred):\n    \"\"\"\n    Analyse les r\u00e9sidus du mod\u00e8le.\n    \n    Args:\n        y_true (numpy.ndarray): Valeurs r\u00e9elles\n        y_pred (numpy.ndarray): Pr\u00e9dictions\n    \"\"\"\n    residuals = y_true - y_pred\n    \n    plt.figure(figsize=(15, 5))\n    \n    # Histogramme des r\u00e9sidus\n    plt.subplot(1, 2, 1)\n    plt.hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n    plt.axvline(x=0, color='red', linestyle='--')\n    plt.title('Distribution des R\u00e9sidus')\n    plt.xlabel('R\u00e9sidu')\n    plt.ylabel('Fr\u00e9quence')\n    \n    # Graphique des r\u00e9sidus par rapport aux valeurs pr\u00e9dites\n    plt.subplot(1, 2, 2)\n    plt.scatter(y_pred, residuals, alpha=0.5, color='blue')\n    plt.axhline(y=0, color='red', linestyle='--')\n    plt.title('R\u00e9sidus vs Valeurs Pr\u00e9dites')\n    plt.xlabel('Valeurs Pr\u00e9dites')\n    plt.ylabel('R\u00e9sidus')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Statistiques des r\u00e9sidus\n    print(\"Statistiques des r\u00e9sidus:\")\n    print(f\"Moyenne: {np.mean(residuals):.4f}\")\n    print(f\"\u00c9cart-type: {np.std(residuals):.4f}\")\n    print(f\"Min: {np.min(residuals):.4f}\")\n    print(f\"Max: {np.max(residuals):.4f}\")\n</pre> def analyze_residuals(y_true, y_pred):     \"\"\"     Analyse les r\u00e9sidus du mod\u00e8le.          Args:         y_true (numpy.ndarray): Valeurs r\u00e9elles         y_pred (numpy.ndarray): Pr\u00e9dictions     \"\"\"     residuals = y_true - y_pred          plt.figure(figsize=(15, 5))          # Histogramme des r\u00e9sidus     plt.subplot(1, 2, 1)     plt.hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')     plt.axvline(x=0, color='red', linestyle='--')     plt.title('Distribution des R\u00e9sidus')     plt.xlabel('R\u00e9sidu')     plt.ylabel('Fr\u00e9quence')          # Graphique des r\u00e9sidus par rapport aux valeurs pr\u00e9dites     plt.subplot(1, 2, 2)     plt.scatter(y_pred, residuals, alpha=0.5, color='blue')     plt.axhline(y=0, color='red', linestyle='--')     plt.title('R\u00e9sidus vs Valeurs Pr\u00e9dites')     plt.xlabel('Valeurs Pr\u00e9dites')     plt.ylabel('R\u00e9sidus')          plt.tight_layout()     plt.show()          # Statistiques des r\u00e9sidus     print(\"Statistiques des r\u00e9sidus:\")     print(f\"Moyenne: {np.mean(residuals):.4f}\")     print(f\"\u00c9cart-type: {np.std(residuals):.4f}\")     print(f\"Min: {np.min(residuals):.4f}\")     print(f\"Max: {np.max(residuals):.4f}\")"},{"location":"seance2/challenge-prevision-ventes/sales_prediction_improvement/","title":"Sales prediction improvement","text":"In\u00a0[\u00a0]: Copied! <pre>{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Challenge d'am\u00e9lioration de mod\u00e8le de pr\u00e9vision des ventes\\n\",\n    \"\\n\",\n    \"## Introduction\\n\",\n    \"\\n\",\n    \"Bienvenue dans ce challenge professionnel ! Vous \u00eates stagiaire data scientist dans une entreprise de commerce en ligne, et on vous a confi\u00e9 la mission d'am\u00e9liorer le syst\u00e8me de pr\u00e9vision des ventes existant.\\n\",\n    \"\\n\",\n    \"Le mod\u00e8le actuel ne donne pas de r\u00e9sultats satisfaisants, ce qui entra\u00eene des probl\u00e8mes de gestion des stocks et de planification. Votre objectif est d'analyser ce mod\u00e8le, d'identifier ses faiblesses, et de proposer des am\u00e9liorations concr\u00e8tes.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 1. Configuration et importation des biblioth\u00e8ques\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import numpy as np\\n\",\n    \"import pandas as pd\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import seaborn as sns\\n\",\n    \"from sklearn.model_selection import train_test_split\\n\",\n    \"from sklearn.preprocessing import StandardScaler, OneHotEncoder\\n\",\n    \"from sklearn.compose import ColumnTransformer\\n\",\n    \"from sklearn.pipeline import Pipeline\\n\",\n    \"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\n\",\n    \"import tensorflow as tf\\n\",\n    \"from tensorflow.keras.models import Sequential\\n\",\n    \"from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten\\n\",\n    \"from tensorflow.keras.callbacks import EarlyStopping\\n\",\n    \"import sys\\n\",\n    \"import warnings\\n\",\n    \"\\n\",\n    \"# Import des utilitaires personnalis\u00e9s\\n\",\n    \"sys.path.append('./utils')\\n\",\n    \"from visualization import plot_actual_vs_predicted, plot_error_distribution, plot_category_performance\\n\",\n    \"from model_evaluation import evaluate_model, calculate_metrics_by_category\\n\",\n    \"from data_preprocessing import create_time_features, prepare_sequences\\n\",\n    \"\\n\",\n    \"# Configuration pour la reproductibilit\u00e9\\n\",\n    \"np.random.seed(42)\\n\",\n    \"tf.random.set_seed(42)\\n\",\n    \"warnings.filterwarnings('ignore')\\n\",\n    \"\\n\",\n    \"# Style des graphiques\\n\",\n    \"plt.style.use('seaborn-v0_8-whitegrid')\\n\",\n    \"sns.set_palette('colorblind')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 2. Chargement et exploration des donn\u00e9es\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Chargement des donn\u00e9es\\n\",\n    \"data = pd.read_csv('./data/sales_data.csv')\\n\",\n    \"\\n\",\n    \"# Convertir la colonne de date en datetime\\n\",\n    \"data['date'] = pd.to_datetime(data['date'])\\n\",\n    \"\\n\",\n    \"# Afficher les premi\u00e8res lignes\\n\",\n    \"print(\\\"Aper\u00e7u des donn\u00e9es:\\\")\\n\",\n    \"display(data.head())\\n\",\n    \"\\n\",\n    \"# Informations sur le jeu de donn\u00e9es\\n\",\n    \"print(\\\"\\\\nInformations sur le jeu de donn\u00e9es:\\\")\\n\",\n    \"print(f\\\"Nombre d'enregistrements: {data.shape[0]}\\\")\\n\",\n    \"print(f\\\"P\u00e9riode couverte: de {data['date'].min().date()} \u00e0 {data['date'].max().date()}\\\")\\n\",\n    \"print(f\\\"Cat\u00e9gories de produits: {', '.join(data['product_category'].unique())}\\\")\\n\",\n    \"\\n\",\n    \"# Statistiques descriptives\\n\",\n    \"print(\\\"\\\\nStatistiques descriptives:\\\")\\n\",\n    \"display(data.describe())\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Visualisation des ventes au fil du temps\\n\",\n    \"plt.figure(figsize=(15, 6))\\n\",\n    \"data_grouped = data.groupby('date')['total_sales'].sum().reset_index()\\n\",\n    \"plt.plot(data_grouped['date'], data_grouped['total_sales'])\\n\",\n    \"plt.title('\u00c9volution des ventes totales au fil du temps')\\n\",\n    \"plt.xlabel('Date')\\n\",\n    \"plt.ylabel('Ventes totales')\\n\",\n    \"plt.grid(True)\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\\n\",\n    \"\\n\",\n    \"# Visualisation des ventes par cat\u00e9gorie de produit\\n\",\n    \"plt.figure(figsize=(12, 6))\\n\",\n    \"category_sales = data.groupby('product_category')['total_sales'].sum().sort_values(ascending=False)\\n\",\n    \"ax = category_sales.plot(kind='bar', color='skyblue')\\n\",\n    \"plt.title('Ventes totales par cat\u00e9gorie de produit')\\n\",\n    \"plt.xlabel('Cat\u00e9gorie de produit')\\n\",\n    \"plt.ylabel('Ventes totales')\\n\",\n    \"plt.xticks(rotation=45)\\n\",\n    \"\\n\",\n    \"# Ajouter les valeurs sur les barres\\n\",\n    \"for i, v in enumerate(category_sales):\\n\",\n    \"    ax.text(i, v + 0.1, f'{v:.1f}', ha='center')\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\\n\",\n    \"\\n\",\n    \"# Corr\u00e9lation entre les variables\\n\",\n    \"plt.figure(figsize=(10, 8))\\n\",\n    \"numeric_data = data.select_dtypes(include=[np.number])\\n\",\n    \"correlation_matrix = numeric_data.corr()\\n\",\n    \"mask = np.triu(np.ones_like(correlation_matrix))\\n\",\n    \"sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', cbar_kws={'shrink': .8})\\n\",\n    \"plt.title('Matrice de corr\u00e9lation des variables num\u00e9riques')\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 3. Pr\u00e9paration des donn\u00e9es\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cr\u00e9ation de caract\u00e9ristiques temporelles\\n\",\n    \"data = create_time_features(data)\\n\",\n    \"\\n\",\n    \"# S\u00e9paration des variables explicatives et de la cible\\n\",\n    \"X = data.drop(['date', 'total_sales'], axis=1)\\n\",\n    \"y = data['total_sales']\\n\",\n    \"\\n\",\n    \"# S\u00e9paration en ensembles d'entra\u00eenement et de test (80-20)\\n\",\n    \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\",\n    \"\\n\",\n    \"# Identification des colonnes cat\u00e9gorielles et num\u00e9riques\\n\",\n    \"categorical_cols = ['product_category', 'weekday']\\n\",\n    \"numerical_cols = [col for col in X.columns if col not in categorical_cols]\\n\",\n    \"\\n\",\n    \"# Pr\u00e9processeur pour transformer les donn\u00e9es\\n\",\n    \"preprocessor = ColumnTransformer(\\n\",\n    \"    transformers=[\\n\",\n    \"        ('num', StandardScaler(), numerical_cols),\\n\",\n    \"        ('cat', OneHotEncoder(drop='first'), categorical_cols)\\n\",\n    \"    ])\\n\",\n    \"\\n\",\n    \"# Application du pr\u00e9processeur\\n\",\n    \"X_train_processed = preprocessor.fit_transform(X_train)\\n\",\n    \"X_test_processed = preprocessor.transform(X_test)\\n\",\n    \"\\n\",\n    \"print(f\\\"Forme des donn\u00e9es d'entra\u00eenement apr\u00e8s pr\u00e9traitement: {X_train_processed.shape}\\\")\\n\",\n    \"print(f\\\"Forme des donn\u00e9es de test apr\u00e8s pr\u00e9traitement: {X_test_processed.shape}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 4. Mod\u00e8le de base (sous-optimal)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def create_baseline_model(input_shape):\\n\",\n    \"    \\\"\\\"\\\"Cr\u00e9ation du mod\u00e8le de base sous-optimal\\\"\\\"\\\"\\n\",\n    \"    model = Sequential([\\n\",\n    \"        Dense(64, activation='relu', input_shape=(input_shape,)),\\n\",\n    \"        Dense(32, activation='relu'),\\n\",\n    \"        Dense(1)  # R\u00e9gression: pas d'activation sur la couche de sortie\\n\",\n    \"    ])\\n\",\n    \"    \\n\",\n    \"    model.compile(optimizer='sgd', loss='mse', metrics=['mae'])\\n\",\n    \"    return model\\n\",\n    \"\\n\",\n    \"# Cr\u00e9ation et entra\u00eenement du mod\u00e8le de base\\n\",\n    \"baseline_model = create_baseline_model(X_train_processed.shape[1])\\n\",\n    \"baseline_model.summary()\\n\",\n    \"\\n\",\n    \"# Configuration des callbacks\\n\",\n    \"early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\\n\",\n    \"\\n\",\n    \"# Entra\u00eenement du mod\u00e8le\\n\",\n    \"baseline_history = baseline_model.fit(\\n\",\n    \"    X_train_processed, y_train,\\n\",\n    \"    epochs=50,\\n\",\n    \"    batch_size=32,\\n\",\n    \"    validation_split=0.2,\\n\",\n    \"    callbacks=[early_stopping],\\n\",\n    \"    verbose=1\\n\",\n    \")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \u00c9valuation du mod\u00e8le de base\\n\",\n    \"y_pred_baseline = baseline_model.predict(X_test_processed).flatten()\\n\",\n    \"\\n\",\n    \"# M\u00e9triques globales\\n\",\n    \"baseline_metrics = evaluate_model(y_test, y_pred_baseline)\\n\",\n    \"print(\\\"\\\\nM\u00e9triques du mod\u00e8le de base:\\\")\\n\",\n    \"print(f\\\"RMSE: {baseline_metrics['rmse']:.2f}\\\")\\n\",\n    \"print(f\\\"MAE: {baseline_metrics['mae']:.2f}\\\")\\n\",\n    \"print(f\\\"R\u00b2: {baseline_metrics['r2']:.4f}\\\")\\n\",\n    \"print(f\\\"MAPE: {baseline_metrics['mape']:.2f}%\\\")\\n\",\n    \"\\n\",\n    \"# Visualisation des pr\u00e9dictions vs. valeurs r\u00e9elles\\n\",\n    \"plot_actual_vs_predicted(y_test, y_pred_baseline, 'Mod\u00e8le de base')\\n\",\n    \"\\n\",\n    \"# Distribution des erreurs\\n\",\n    \"plot_error_distribution(y_test, y_pred_baseline, 'Mod\u00e8le de base')\\n\",\n    \"\\n\",\n    \"# Performance par cat\u00e9gorie\\n\",\n    \"X_test_with_categories = X_test.reset_index(drop=True)\\n\",\n    \"category_performance = calculate_metrics_by_category(X_test_with_categories, y_test, y_pred_baseline)\\n\",\n    \"plot_category_performance(category_performance, 'Mod\u00e8le de base')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 5. Diagnostic du mod\u00e8le de base\\n\",\n    \"\\n\",\n    \"Analysez les r\u00e9sultats ci-dessus et identifiez les faiblesses du mod\u00e8le de base. Notez vos observations dans cette cellule.\\n\",\n    \"\\n\",\n    \"### Probl\u00e8mes identifi\u00e9s\\n\",\n    \"1. ...\\n\",\n    \"2. ...\\n\",\n    \"3. ...\\n\",\n    \"\\n\",\n    \"### Pistes d'am\u00e9lioration\\n\",\n    \"1. ...\\n\",\n    \"2. ...\\n\",\n    \"3. ...\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 6. Am\u00e9lioration du mod\u00e8le\\n\",\n    \"\\n\",\n    \"Impl\u00e9mentez vos am\u00e9liorations ici. Pour chaque modification, documentez votre hypoth\u00e8se, le changement apport\u00e9, et les r\u00e9sultats obtenus.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Am\u00e9lioration 1: [Nom de la modification]\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"**Hypoth\u00e8se**: [Expliquez pourquoi vous pensez que cette modification am\u00e9liorera les performances]\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def create_improved_model_1(input_shape):\\n\",\n    \"    \\\"\\\"\\\"Cr\u00e9ation d'un mod\u00e8le am\u00e9lior\u00e9 - Version 1\\\"\\\"\\\"\\n\",\n    \"    # Impl\u00e9mentez votre mod\u00e8le am\u00e9lior\u00e9 ici\\n\",\n    \"    model = Sequential([\\n\",\n    \"        # Modifiez cette architecture selon votre hypoth\u00e8se\\n\",\n    \"        Dense(128, activation='relu', input_shape=(input_shape,)),\\n\",\n    \"        Dropout(0.3),  # Exemple d'ajout de dropout pour r\u00e9duire le surapprentissage\\n\",\n    \"        Dense(64, activation='relu'),\\n\",\n    \"        Dropout(0.2),\\n\",\n    \"        Dense(32, activation='relu'),\\n\",\n    \"        Dense(1)\\n\",\n    \"    ])\\n\",\n    \"    \\n\",\n    \"    # Modifiez l'optimiseur et les param\u00e8tres selon vos besoins\\n\",\n    \"    model.compile(\\n\",\n    \"        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\\n\",\n    \"        loss='mse',\\n\",\n    \"        metrics=['mae']\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    return model\\n\",\n    \"\\n\",\n    \"# Cr\u00e9ation et entra\u00eenement du mod\u00e8le am\u00e9lior\u00e9\\n\",\n    \"improved_model_1 = create_improved_model_1(X_train_processed.shape[1])\\n\",\n    \"improved_model_1.summary()\\n\",\n    \"\\n\",\n    \"# Entra\u00eenement du mod\u00e8le\\n\",\n    \"improved_history_1 = improved_model_1.fit(\\n\",\n    \"    X_train_processed, y_train,\\n\",\n    \"    epochs=50,\\n\",\n    \"    batch_size=32,  # Vous pouvez modifier le batch size\\n\",\n    \"    validation_split=0.2,\\n\",\n    \"    callbacks=[early_stopping],\\n\",\n    \"    verbose=1\\n\",\n    \")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# \u00c9valuation du mod\u00e8le am\u00e9lior\u00e9 1\\n\",\n    \"y_pred_improved_1 = improved_model_1.predict(X_test_processed).flatten()\\n\",\n    \"\\n\",\n    \"# M\u00e9triques globales\\n\",\n    \"improved_metrics_1 = evaluate_model(y_test, y_pred_improved_1)\\n\",\n    \"print(\\\"\\\\nM\u00e9triques du mod\u00e8le am\u00e9lior\u00e9 1:\\\")\\n\",\n    \"print(f\\\"RMSE: {improved_metrics_1['rmse']:.2f}\\\")\\n\",\n    \"print(f\\\"MAE: {improved_metrics_1['mae']:.2f}\\\")\\n\",\n    \"print(f\\\"R\u00b2: {improved_metrics_1['r2']:.4f}\\\")\\n\",\n    \"print(f\\\"MAPE: {improved_metrics_1['mape']:.2f}%\\\")\\n\",\n    \"\\n\",\n    \"# Am\u00e9lioration par rapport au mod\u00e8le de base\\n\",\n    \"rmse_improvement = (baseline_metrics['rmse'] - improved_metrics_1['rmse']) / baseline_metrics['rmse'] * 100\\n\",\n    \"print(f\\\"Am\u00e9lioration de la RMSE: {rmse_improvement:.2f}%\\\")\\n\",\n    \"\\n\",\n    \"# Visualisation des pr\u00e9dictions vs. valeurs r\u00e9elles\\n\",\n    \"plot_actual_vs_predicted(y_test, y_pred_improved_1, 'Mod\u00e8le am\u00e9lior\u00e9 1')\\n\",\n    \"\\n\",\n    \"# Performance par cat\u00e9gorie\\n\",\n    \"category_performance_improved_1 = calculate_metrics_by_category(X_test_with_categories, y_test, y_pred_improved_1)\\n\",\n    \"plot_category_performance(category_performance_improved_1, 'Mod\u00e8le am\u00e9lior\u00e9 1')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"**R\u00e9sultats et analyse**: [Commentez les r\u00e9sultats obtenus. L'am\u00e9lioration est-elle significative? Votre hypoth\u00e8se est-elle confirm\u00e9e?]\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Am\u00e9lioration 2: Utilisation d'un mod\u00e8le RNN (LSTM) pour capturer les tendances temporelles\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"**Hypoth\u00e8se**: Les donn\u00e9es de ventes sont s\u00e9quentielles par nature et contiennent probablement des patterns temporels complexes. Un mod\u00e8le LSTM devrait mieux capturer ces d\u00e9pendances temporelles et am\u00e9liorer les pr\u00e9dictions, particuli\u00e8rement pour les p\u00e9riodes de ventes fluctuantes.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Pr\u00e9paration des donn\u00e9es pour LSTM (s\u00e9quences)\\n\",\n    \"# Nous allons utiliser des s\u00e9quences de 7 jours pour pr\u00e9dire le jour suivant\\n\",\n    \"sequence_length = 7\\n\",\n    \"\\n\",\n    \"# Pr\u00e9paration des s\u00e9quences pour LSTM\\n\",\n    \"X_train_seq, y_train_seq, X_test_seq, y_test_seq = prepare_sequences(\\n\",\n    \"    data, sequence_length, train_size=0.8, random_state=42\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"print(f\\\"Forme des s\u00e9quences d'entra\u00eenement: {X_train_seq.shape}\\\")\\n\",\n    \"print(f\\\"Forme des s\u00e9quences de test: {X_test_seq.shape}\\\")\\n\",\n    \"\\n\",\n    \"def create_lstm_model(input_shape):\\n\",\n    \"    \\\"\\\"\\\"Cr\u00e9ation d'un mod\u00e8le LSTM pour les s\u00e9quences temporelles\\\"\\\"\\\"\\n\",\n    \"    model = Sequential([\\n\",\n    \"        LSTM(64, return_sequences=True, input_shape=input_shape),\\n\",\n    \"        Dropout(0.2),\\n\",\n    \"        LSTM(32),\\n\",\n    \"        Dropout(0.2),\\n\",\n    \"        Dense(16, activation='relu'),\\n\",\n    \"        Dense(1)\\n\",\n    \"    ])\\n\",\n    \"    \\n\",\n    \"    model.compile(\\n\",\n    \"        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\\n\",\n    \"        loss='mse',\\n\",\n    \"        metrics=['mae']\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    return model\\n\",\n    \"\\n\",\n    \"# Cr\u00e9ation du mod\u00e8le LSTM\\n\",\n    \"lstm_model = create_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]))\\n\",\n    \"lstm_model.summary()\\n\",\n    \"\\n\",\n    \"# Entra\u00eenement du mod\u00e8le\\n\",\n    \"lstm_history = lstm_model.fit(\\n\",\n    \"    X_train_seq, y_train_seq,\\n\",\n    \"    epochs=50,\\n\",\n    \"    batch_size=32,\\n\",\n    \"    validation_split=0.2,\\n\",\n    \"    callbacks=[early_stopping],\\n\",\n    \"    verbose=1\\n\",\n    \")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Le code pour \u00e9valuer le mod\u00e8le LSTM serait mis ici\\n\",\n    \"# Nous l'adaptons pour utiliser les s\u00e9quences et comparer avec les mod\u00e8les pr\u00e9c\u00e9dents\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Am\u00e9lioration 3: [Autre approche de votre choix]\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"**Hypoth\u00e8se**: [Votre hypoth\u00e8se pour cette troisi\u00e8me approche]\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Impl\u00e9mentez votre troisi\u00e8me approche ici\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 7. Comparaison des mod\u00e8les\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Tableau r\u00e9capitulatif des performances\\n\",\n    \"models = ['Baseline', 'Improved 1', 'LSTM']\\n\",\n    \"rmse_values = [baseline_metrics['rmse'], improved_metrics_1['rmse'], 0]  # Compl\u00e9tez avec les r\u00e9sultats LSTM\\n\",\n    \"r2_values = [baseline_metrics['r2'], improved_metrics_1['r2'], 0]  # Compl\u00e9tez avec les r\u00e9sultats LSTM\\n\",\n    \"\\n\",\n    \"# Visualisation comparative\\n\",\n    \"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\\n\",\n    \"\\n\",\n    \"# RMSE (plus petit est meilleur)\\n\",\n    \"ax1.bar(models, rmse_values, color=['lightcoral', 'lightgreen', 'lightblue'])\\n\",\n    \"ax1.set_title('Comparaison de la RMSE (plus bas = meilleur)')\\n\",\n    \"ax1.set_ylabel('RMSE')\\n\",\n    \"for i, v in enumerate(rmse_values):\\n\",\n    \"    if v &gt; 0:\\n\",\n    \"        ax1.text(i, v + 0.5, f\\\"{v:.2f}\\\", ha='center')\\n\",\n    \"\\n\",\n    \"# R\u00b2 (plus grand est meilleur)\\n\",\n    \"ax2.bar(models, r2_values, color=['lightcoral', 'lightgreen', 'lightblue'])\\n\",\n    \"ax2.set_title('Comparaison du R\u00b2 (plus haut = meilleur)')\\n\",\n    \"ax2.set_ylabel('R\u00b2')\\n\",\n    \"for i, v in enumerate(r2_values):\\n\",\n    \"    if v &gt; 0:\\n\",\n    \"        ax2.text(i, v + 0.01, f\\\"{v:.4f}\\\", ha='center')\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 8. Impact business\\n\",\n    \"\\n\",\n    \"Analysez l'impact business des am\u00e9liorations apport\u00e9es au mod\u00e8le. Quantifiez les avantages pour l'entreprise.\\n\",\n    \"\\n\",\n    \"### Estimation de l'impact\\n\",\n    \"\\n\",\n    \"1. **R\u00e9duction des erreurs de pr\u00e9vision**:\\n\",\n    \"   - Mod\u00e8le de base: RMSE de X\\n\",\n    \"   - Meilleur mod\u00e8le: RMSE de Y\\n\",\n    \"   - Am\u00e9lioration: Z%\\n\",\n    \"\\n\",\n    \"2. **Avantages op\u00e9rationnels estim\u00e9s**:\\n\",\n    \"   - R\u00e9duction potentielle des stocks exc\u00e9dentaires: ...\\n\",\n    \"   - Am\u00e9lioration de la disponibilit\u00e9 des produits: ...\\n\",\n    \"   - Impact sur la satisfaction client: ...\\n\",\n    \"   - Estimation des \u00e9conomies annuelles: ...\\n\",\n    \"\\n\",\n    \"3. **Cat\u00e9gories de produits les plus impact\u00e9es**:\\n\",\n    \"   - ...\\n\",\n    \"   - ...\\n\",\n    \"   - ...\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 9. Conclusions et recommandations\\n\",\n    \"\\n\",\n    \"### Synth\u00e8se des am\u00e9liorations\\n\",\n    \"1. ...\\n\",\n    \"2. ...\\n\",\n    \"3. ...\\n\",\n    \"\\n\",\n    \"### Recommandations techniques\\n\",\n    \"1. ...\\n\",\n    \"2. ...\\n\",\n    \"3. ...\\n\",\n    \"\\n\",\n    \"### Recommandations m\u00e9tier\\n\",\n    \"1. ...\\n\",\n    \"2. ...\\n\",\n    \"3. ...\\n\",\n    \"\\n\",\n    \"### Prochaines \u00e9tapes\\n\",\n    \"1. ...\\n\",\n    \"2. ...\\n\",\n    \"3. ...\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.10\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n</pre> {  \"cells\": [   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"# Challenge d'am\u00e9lioration de mod\u00e8le de pr\u00e9vision des ventes\\n\",     \"\\n\",     \"## Introduction\\n\",     \"\\n\",     \"Bienvenue dans ce challenge professionnel ! Vous \u00eates stagiaire data scientist dans une entreprise de commerce en ligne, et on vous a confi\u00e9 la mission d'am\u00e9liorer le syst\u00e8me de pr\u00e9vision des ventes existant.\\n\",     \"\\n\",     \"Le mod\u00e8le actuel ne donne pas de r\u00e9sultats satisfaisants, ce qui entra\u00eene des probl\u00e8mes de gestion des stocks et de planification. Votre objectif est d'analyser ce mod\u00e8le, d'identifier ses faiblesses, et de proposer des am\u00e9liorations concr\u00e8tes.\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"## 1. Configuration et importation des biblioth\u00e8ques\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"outputs\": [],    \"source\": [     \"import numpy as np\\n\",     \"import pandas as pd\\n\",     \"import matplotlib.pyplot as plt\\n\",     \"import seaborn as sns\\n\",     \"from sklearn.model_selection import train_test_split\\n\",     \"from sklearn.preprocessing import StandardScaler, OneHotEncoder\\n\",     \"from sklearn.compose import ColumnTransformer\\n\",     \"from sklearn.pipeline import Pipeline\\n\",     \"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\n\",     \"import tensorflow as tf\\n\",     \"from tensorflow.keras.models import Sequential\\n\",     \"from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten\\n\",     \"from tensorflow.keras.callbacks import EarlyStopping\\n\",     \"import sys\\n\",     \"import warnings\\n\",     \"\\n\",     \"# Import des utilitaires personnalis\u00e9s\\n\",     \"sys.path.append('./utils')\\n\",     \"from visualization import plot_actual_vs_predicted, plot_error_distribution, plot_category_performance\\n\",     \"from model_evaluation import evaluate_model, calculate_metrics_by_category\\n\",     \"from data_preprocessing import create_time_features, prepare_sequences\\n\",     \"\\n\",     \"# Configuration pour la reproductibilit\u00e9\\n\",     \"np.random.seed(42)\\n\",     \"tf.random.set_seed(42)\\n\",     \"warnings.filterwarnings('ignore')\\n\",     \"\\n\",     \"# Style des graphiques\\n\",     \"plt.style.use('seaborn-v0_8-whitegrid')\\n\",     \"sns.set_palette('colorblind')\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"## 2. Chargement et exploration des donn\u00e9es\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"outputs\": [],    \"source\": [     \"# Chargement des donn\u00e9es\\n\",     \"data = pd.read_csv('./data/sales_data.csv')\\n\",     \"\\n\",     \"# Convertir la colonne de date en datetime\\n\",     \"data['date'] = pd.to_datetime(data['date'])\\n\",     \"\\n\",     \"# Afficher les premi\u00e8res lignes\\n\",     \"print(\\\"Aper\u00e7u des donn\u00e9es:\\\")\\n\",     \"display(data.head())\\n\",     \"\\n\",     \"# Informations sur le jeu de donn\u00e9es\\n\",     \"print(\\\"\\\\nInformations sur le jeu de donn\u00e9es:\\\")\\n\",     \"print(f\\\"Nombre d'enregistrements: {data.shape[0]}\\\")\\n\",     \"print(f\\\"P\u00e9riode couverte: de {data['date'].min().date()} \u00e0 {data['date'].max().date()}\\\")\\n\",     \"print(f\\\"Cat\u00e9gories de produits: {', '.join(data['product_category'].unique())}\\\")\\n\",     \"\\n\",     \"# Statistiques descriptives\\n\",     \"print(\\\"\\\\nStatistiques descriptives:\\\")\\n\",     \"display(data.describe())\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"outputs\": [],    \"source\": [     \"# Visualisation des ventes au fil du temps\\n\",     \"plt.figure(figsize=(15, 6))\\n\",     \"data_grouped = data.groupby('date')['total_sales'].sum().reset_index()\\n\",     \"plt.plot(data_grouped['date'], data_grouped['total_sales'])\\n\",     \"plt.title('\u00c9volution des ventes totales au fil du temps')\\n\",     \"plt.xlabel('Date')\\n\",     \"plt.ylabel('Ventes totales')\\n\",     \"plt.grid(True)\\n\",     \"plt.tight_layout()\\n\",     \"plt.show()\\n\",     \"\\n\",     \"# Visualisation des ventes par cat\u00e9gorie de produit\\n\",     \"plt.figure(figsize=(12, 6))\\n\",     \"category_sales = data.groupby('product_category')['total_sales'].sum().sort_values(ascending=False)\\n\",     \"ax = category_sales.plot(kind='bar', color='skyblue')\\n\",     \"plt.title('Ventes totales par cat\u00e9gorie de produit')\\n\",     \"plt.xlabel('Cat\u00e9gorie de produit')\\n\",     \"plt.ylabel('Ventes totales')\\n\",     \"plt.xticks(rotation=45)\\n\",     \"\\n\",     \"# Ajouter les valeurs sur les barres\\n\",     \"for i, v in enumerate(category_sales):\\n\",     \"    ax.text(i, v + 0.1, f'{v:.1f}', ha='center')\\n\",     \"\\n\",     \"plt.tight_layout()\\n\",     \"plt.show()\\n\",     \"\\n\",     \"# Corr\u00e9lation entre les variables\\n\",     \"plt.figure(figsize=(10, 8))\\n\",     \"numeric_data = data.select_dtypes(include=[np.number])\\n\",     \"correlation_matrix = numeric_data.corr()\\n\",     \"mask = np.triu(np.ones_like(correlation_matrix))\\n\",     \"sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', cbar_kws={'shrink': .8})\\n\",     \"plt.title('Matrice de corr\u00e9lation des variables num\u00e9riques')\\n\",     \"plt.tight_layout()\\n\",     \"plt.show()\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"## 3. Pr\u00e9paration des donn\u00e9es\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"outputs\": [],    \"source\": [     \"# Cr\u00e9ation de caract\u00e9ristiques temporelles\\n\",     \"data = create_time_features(data)\\n\",     \"\\n\",     \"# S\u00e9paration des variables explicatives et de la cible\\n\",     \"X = data.drop(['date', 'total_sales'], axis=1)\\n\",     \"y = data['total_sales']\\n\",     \"\\n\",     \"# S\u00e9paration en ensembles d'entra\u00eenement et de test (80-20)\\n\",     \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\",     \"\\n\",     \"# Identification des colonnes cat\u00e9gorielles et num\u00e9riques\\n\",     \"categorical_cols = ['product_category', 'weekday']\\n\",     \"numerical_cols = [col for col in X.columns if col not in categorical_cols]\\n\",     \"\\n\",     \"# Pr\u00e9processeur pour transformer les donn\u00e9es\\n\",     \"preprocessor = ColumnTransformer(\\n\",     \"    transformers=[\\n\",     \"        ('num', StandardScaler(), numerical_cols),\\n\",     \"        ('cat', OneHotEncoder(drop='first'), categorical_cols)\\n\",     \"    ])\\n\",     \"\\n\",     \"# Application du pr\u00e9processeur\\n\",     \"X_train_processed = preprocessor.fit_transform(X_train)\\n\",     \"X_test_processed = preprocessor.transform(X_test)\\n\",     \"\\n\",     \"print(f\\\"Forme des donn\u00e9es d'entra\u00eenement apr\u00e8s pr\u00e9traitement: {X_train_processed.shape}\\\")\\n\",     \"print(f\\\"Forme des donn\u00e9es de test apr\u00e8s pr\u00e9traitement: {X_test_processed.shape}\\\")\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"## 4. Mod\u00e8le de base (sous-optimal)\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"outputs\": [],    \"source\": [     \"def create_baseline_model(input_shape):\\n\",     \"    \\\"\\\"\\\"Cr\u00e9ation du mod\u00e8le de base sous-optimal\\\"\\\"\\\"\\n\",     \"    model = Sequential([\\n\",     \"        Dense(64, activation='relu', input_shape=(input_shape,)),\\n\",     \"        Dense(32, activation='relu'),\\n\",     \"        Dense(1)  # R\u00e9gression: pas d'activation sur la couche de sortie\\n\",     \"    ])\\n\",     \"    \\n\",     \"    model.compile(optimizer='sgd', loss='mse', metrics=['mae'])\\n\",     \"    return model\\n\",     \"\\n\",     \"# Cr\u00e9ation et entra\u00eenement du mod\u00e8le de base\\n\",     \"baseline_model = create_baseline_model(X_train_processed.shape[1])\\n\",     \"baseline_model.summary()\\n\",     \"\\n\",     \"# Configuration des callbacks\\n\",     \"early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\\n\",     \"\\n\",     \"# Entra\u00eenement du mod\u00e8le\\n\",     \"baseline_history = baseline_model.fit(\\n\",     \"    X_train_processed, y_train,\\n\",     \"    epochs=50,\\n\",     \"    batch_size=32,\\n\",     \"    validation_split=0.2,\\n\",     \"    callbacks=[early_stopping],\\n\",     \"    verbose=1\\n\",     \")\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"outputs\": [],    \"source\": [     \"# \u00c9valuation du mod\u00e8le de base\\n\",     \"y_pred_baseline = baseline_model.predict(X_test_processed).flatten()\\n\",     \"\\n\",     \"# M\u00e9triques globales\\n\",     \"baseline_metrics = evaluate_model(y_test, y_pred_baseline)\\n\",     \"print(\\\"\\\\nM\u00e9triques du mod\u00e8le de base:\\\")\\n\",     \"print(f\\\"RMSE: {baseline_metrics['rmse']:.2f}\\\")\\n\",     \"print(f\\\"MAE: {baseline_metrics['mae']:.2f}\\\")\\n\",     \"print(f\\\"R\u00b2: {baseline_metrics['r2']:.4f}\\\")\\n\",     \"print(f\\\"MAPE: {baseline_metrics['mape']:.2f}%\\\")\\n\",     \"\\n\",     \"# Visualisation des pr\u00e9dictions vs. valeurs r\u00e9elles\\n\",     \"plot_actual_vs_predicted(y_test, y_pred_baseline, 'Mod\u00e8le de base')\\n\",     \"\\n\",     \"# Distribution des erreurs\\n\",     \"plot_error_distribution(y_test, y_pred_baseline, 'Mod\u00e8le de base')\\n\",     \"\\n\",     \"# Performance par cat\u00e9gorie\\n\",     \"X_test_with_categories = X_test.reset_index(drop=True)\\n\",     \"category_performance = calculate_metrics_by_category(X_test_with_categories, y_test, y_pred_baseline)\\n\",     \"plot_category_performance(category_performance, 'Mod\u00e8le de base')\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"## 5. Diagnostic du mod\u00e8le de base\\n\",     \"\\n\",     \"Analysez les r\u00e9sultats ci-dessus et identifiez les faiblesses du mod\u00e8le de base. Notez vos observations dans cette cellule.\\n\",     \"\\n\",     \"### Probl\u00e8mes identifi\u00e9s\\n\",     \"1. ...\\n\",     \"2. ...\\n\",     \"3. ...\\n\",     \"\\n\",     \"### Pistes d'am\u00e9lioration\\n\",     \"1. ...\\n\",     \"2. ...\\n\",     \"3. ...\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"## 6. Am\u00e9lioration du mod\u00e8le\\n\",     \"\\n\",     \"Impl\u00e9mentez vos am\u00e9liorations ici. Pour chaque modification, documentez votre hypoth\u00e8se, le changement apport\u00e9, et les r\u00e9sultats obtenus.\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"### Am\u00e9lioration 1: [Nom de la modification]\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"**Hypoth\u00e8se**: [Expliquez pourquoi vous pensez que cette modification am\u00e9liorera les performances]\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"outputs\": [],    \"source\": [     \"def create_improved_model_1(input_shape):\\n\",     \"    \\\"\\\"\\\"Cr\u00e9ation d'un mod\u00e8le am\u00e9lior\u00e9 - Version 1\\\"\\\"\\\"\\n\",     \"    # Impl\u00e9mentez votre mod\u00e8le am\u00e9lior\u00e9 ici\\n\",     \"    model = Sequential([\\n\",     \"        # Modifiez cette architecture selon votre hypoth\u00e8se\\n\",     \"        Dense(128, activation='relu', input_shape=(input_shape,)),\\n\",     \"        Dropout(0.3),  # Exemple d'ajout de dropout pour r\u00e9duire le surapprentissage\\n\",     \"        Dense(64, activation='relu'),\\n\",     \"        Dropout(0.2),\\n\",     \"        Dense(32, activation='relu'),\\n\",     \"        Dense(1)\\n\",     \"    ])\\n\",     \"    \\n\",     \"    # Modifiez l'optimiseur et les param\u00e8tres selon vos besoins\\n\",     \"    model.compile(\\n\",     \"        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\\n\",     \"        loss='mse',\\n\",     \"        metrics=['mae']\\n\",     \"    )\\n\",     \"    \\n\",     \"    return model\\n\",     \"\\n\",     \"# Cr\u00e9ation et entra\u00eenement du mod\u00e8le am\u00e9lior\u00e9\\n\",     \"improved_model_1 = create_improved_model_1(X_train_processed.shape[1])\\n\",     \"improved_model_1.summary()\\n\",     \"\\n\",     \"# Entra\u00eenement du mod\u00e8le\\n\",     \"improved_history_1 = improved_model_1.fit(\\n\",     \"    X_train_processed, y_train,\\n\",     \"    epochs=50,\\n\",     \"    batch_size=32,  # Vous pouvez modifier le batch size\\n\",     \"    validation_split=0.2,\\n\",     \"    callbacks=[early_stopping],\\n\",     \"    verbose=1\\n\",     \")\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"outputs\": [],    \"source\": [     \"# \u00c9valuation du mod\u00e8le am\u00e9lior\u00e9 1\\n\",     \"y_pred_improved_1 = improved_model_1.predict(X_test_processed).flatten()\\n\",     \"\\n\",     \"# M\u00e9triques globales\\n\",     \"improved_metrics_1 = evaluate_model(y_test, y_pred_improved_1)\\n\",     \"print(\\\"\\\\nM\u00e9triques du mod\u00e8le am\u00e9lior\u00e9 1:\\\")\\n\",     \"print(f\\\"RMSE: {improved_metrics_1['rmse']:.2f}\\\")\\n\",     \"print(f\\\"MAE: {improved_metrics_1['mae']:.2f}\\\")\\n\",     \"print(f\\\"R\u00b2: {improved_metrics_1['r2']:.4f}\\\")\\n\",     \"print(f\\\"MAPE: {improved_metrics_1['mape']:.2f}%\\\")\\n\",     \"\\n\",     \"# Am\u00e9lioration par rapport au mod\u00e8le de base\\n\",     \"rmse_improvement = (baseline_metrics['rmse'] - improved_metrics_1['rmse']) / baseline_metrics['rmse'] * 100\\n\",     \"print(f\\\"Am\u00e9lioration de la RMSE: {rmse_improvement:.2f}%\\\")\\n\",     \"\\n\",     \"# Visualisation des pr\u00e9dictions vs. valeurs r\u00e9elles\\n\",     \"plot_actual_vs_predicted(y_test, y_pred_improved_1, 'Mod\u00e8le am\u00e9lior\u00e9 1')\\n\",     \"\\n\",     \"# Performance par cat\u00e9gorie\\n\",     \"category_performance_improved_1 = calculate_metrics_by_category(X_test_with_categories, y_test, y_pred_improved_1)\\n\",     \"plot_category_performance(category_performance_improved_1, 'Mod\u00e8le am\u00e9lior\u00e9 1')\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"**R\u00e9sultats et analyse**: [Commentez les r\u00e9sultats obtenus. L'am\u00e9lioration est-elle significative? Votre hypoth\u00e8se est-elle confirm\u00e9e?]\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"### Am\u00e9lioration 2: Utilisation d'un mod\u00e8le RNN (LSTM) pour capturer les tendances temporelles\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"**Hypoth\u00e8se**: Les donn\u00e9es de ventes sont s\u00e9quentielles par nature et contiennent probablement des patterns temporels complexes. Un mod\u00e8le LSTM devrait mieux capturer ces d\u00e9pendances temporelles et am\u00e9liorer les pr\u00e9dictions, particuli\u00e8rement pour les p\u00e9riodes de ventes fluctuantes.\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"outputs\": [],    \"source\": [     \"# Pr\u00e9paration des donn\u00e9es pour LSTM (s\u00e9quences)\\n\",     \"# Nous allons utiliser des s\u00e9quences de 7 jours pour pr\u00e9dire le jour suivant\\n\",     \"sequence_length = 7\\n\",     \"\\n\",     \"# Pr\u00e9paration des s\u00e9quences pour LSTM\\n\",     \"X_train_seq, y_train_seq, X_test_seq, y_test_seq = prepare_sequences(\\n\",     \"    data, sequence_length, train_size=0.8, random_state=42\\n\",     \")\\n\",     \"\\n\",     \"print(f\\\"Forme des s\u00e9quences d'entra\u00eenement: {X_train_seq.shape}\\\")\\n\",     \"print(f\\\"Forme des s\u00e9quences de test: {X_test_seq.shape}\\\")\\n\",     \"\\n\",     \"def create_lstm_model(input_shape):\\n\",     \"    \\\"\\\"\\\"Cr\u00e9ation d'un mod\u00e8le LSTM pour les s\u00e9quences temporelles\\\"\\\"\\\"\\n\",     \"    model = Sequential([\\n\",     \"        LSTM(64, return_sequences=True, input_shape=input_shape),\\n\",     \"        Dropout(0.2),\\n\",     \"        LSTM(32),\\n\",     \"        Dropout(0.2),\\n\",     \"        Dense(16, activation='relu'),\\n\",     \"        Dense(1)\\n\",     \"    ])\\n\",     \"    \\n\",     \"    model.compile(\\n\",     \"        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\\n\",     \"        loss='mse',\\n\",     \"        metrics=['mae']\\n\",     \"    )\\n\",     \"    \\n\",     \"    return model\\n\",     \"\\n\",     \"# Cr\u00e9ation du mod\u00e8le LSTM\\n\",     \"lstm_model = create_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]))\\n\",     \"lstm_model.summary()\\n\",     \"\\n\",     \"# Entra\u00eenement du mod\u00e8le\\n\",     \"lstm_history = lstm_model.fit(\\n\",     \"    X_train_seq, y_train_seq,\\n\",     \"    epochs=50,\\n\",     \"    batch_size=32,\\n\",     \"    validation_split=0.2,\\n\",     \"    callbacks=[early_stopping],\\n\",     \"    verbose=1\\n\",     \")\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"outputs\": [],    \"source\": [     \"# Le code pour \u00e9valuer le mod\u00e8le LSTM serait mis ici\\n\",     \"# Nous l'adaptons pour utiliser les s\u00e9quences et comparer avec les mod\u00e8les pr\u00e9c\u00e9dents\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"### Am\u00e9lioration 3: [Autre approche de votre choix]\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"**Hypoth\u00e8se**: [Votre hypoth\u00e8se pour cette troisi\u00e8me approche]\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"outputs\": [],    \"source\": [     \"# Impl\u00e9mentez votre troisi\u00e8me approche ici\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"## 7. Comparaison des mod\u00e8les\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"outputs\": [],    \"source\": [     \"# Tableau r\u00e9capitulatif des performances\\n\",     \"models = ['Baseline', 'Improved 1', 'LSTM']\\n\",     \"rmse_values = [baseline_metrics['rmse'], improved_metrics_1['rmse'], 0]  # Compl\u00e9tez avec les r\u00e9sultats LSTM\\n\",     \"r2_values = [baseline_metrics['r2'], improved_metrics_1['r2'], 0]  # Compl\u00e9tez avec les r\u00e9sultats LSTM\\n\",     \"\\n\",     \"# Visualisation comparative\\n\",     \"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\\n\",     \"\\n\",     \"# RMSE (plus petit est meilleur)\\n\",     \"ax1.bar(models, rmse_values, color=['lightcoral', 'lightgreen', 'lightblue'])\\n\",     \"ax1.set_title('Comparaison de la RMSE (plus bas = meilleur)')\\n\",     \"ax1.set_ylabel('RMSE')\\n\",     \"for i, v in enumerate(rmse_values):\\n\",     \"    if v &gt; 0:\\n\",     \"        ax1.text(i, v + 0.5, f\\\"{v:.2f}\\\", ha='center')\\n\",     \"\\n\",     \"# R\u00b2 (plus grand est meilleur)\\n\",     \"ax2.bar(models, r2_values, color=['lightcoral', 'lightgreen', 'lightblue'])\\n\",     \"ax2.set_title('Comparaison du R\u00b2 (plus haut = meilleur)')\\n\",     \"ax2.set_ylabel('R\u00b2')\\n\",     \"for i, v in enumerate(r2_values):\\n\",     \"    if v &gt; 0:\\n\",     \"        ax2.text(i, v + 0.01, f\\\"{v:.4f}\\\", ha='center')\\n\",     \"\\n\",     \"plt.tight_layout()\\n\",     \"plt.show()\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"## 8. Impact business\\n\",     \"\\n\",     \"Analysez l'impact business des am\u00e9liorations apport\u00e9es au mod\u00e8le. Quantifiez les avantages pour l'entreprise.\\n\",     \"\\n\",     \"### Estimation de l'impact\\n\",     \"\\n\",     \"1. **R\u00e9duction des erreurs de pr\u00e9vision**:\\n\",     \"   - Mod\u00e8le de base: RMSE de X\\n\",     \"   - Meilleur mod\u00e8le: RMSE de Y\\n\",     \"   - Am\u00e9lioration: Z%\\n\",     \"\\n\",     \"2. **Avantages op\u00e9rationnels estim\u00e9s**:\\n\",     \"   - R\u00e9duction potentielle des stocks exc\u00e9dentaires: ...\\n\",     \"   - Am\u00e9lioration de la disponibilit\u00e9 des produits: ...\\n\",     \"   - Impact sur la satisfaction client: ...\\n\",     \"   - Estimation des \u00e9conomies annuelles: ...\\n\",     \"\\n\",     \"3. **Cat\u00e9gories de produits les plus impact\u00e9es**:\\n\",     \"   - ...\\n\",     \"   - ...\\n\",     \"   - ...\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"## 9. Conclusions et recommandations\\n\",     \"\\n\",     \"### Synth\u00e8se des am\u00e9liorations\\n\",     \"1. ...\\n\",     \"2. ...\\n\",     \"3. ...\\n\",     \"\\n\",     \"### Recommandations techniques\\n\",     \"1. ...\\n\",     \"2. ...\\n\",     \"3. ...\\n\",     \"\\n\",     \"### Recommandations m\u00e9tier\\n\",     \"1. ...\\n\",     \"2. ...\\n\",     \"3. ...\\n\",     \"\\n\",     \"### Prochaines \u00e9tapes\\n\",     \"1. ...\\n\",     \"2. ...\\n\",     \"3. ...\"    ]   }  ],  \"metadata\": {   \"kernelspec\": {    \"display_name\": \"Python 3\",    \"language\": \"python\",    \"name\": \"python3\"   },   \"language_info\": {    \"codemirror_mode\": {     \"name\": \"ipython\",     \"version\": 3    },    \"file_extension\": \".py\",    \"mimetype\": \"text/x-python\",    \"name\": \"python\",    \"nbconvert_exporter\": \"python\",    \"pygments_lexer\": \"ipython3\",    \"version\": \"3.8.10\"   }  },  \"nbformat\": 4,  \"nbformat_minor\": 4 }"},{"location":"seance2/challenge-prevision-ventes/visualization/","title":"Visualization","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nvisualization.py\nFonctions de visualisation pour l'analyse des mod\u00e8les et des r\u00e9sultats du challenge de pr\u00e9vision des ventes.\n\"\"\"\n</pre> \"\"\" visualization.py Fonctions de visualisation pour l'analyse des mod\u00e8les et des r\u00e9sultats du challenge de pr\u00e9vision des ventes. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n</pre> import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.metrics import confusion_matrix In\u00a0[\u00a0]: Copied! <pre>def plot_training_history(history):\n    \"\"\"\n    Visualise l'\u00e9volution de la pr\u00e9cision et de la perte pendant l'entra\u00eenement.\n    \n    Args:\n        history: Historique retourn\u00e9 par la m\u00e9thode fit() de Keras\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # \u00c9volution de la pr\u00e9cision\n    if 'accuracy' in history.history:\n        ax1.plot(history.history['accuracy'], label='Entra\u00eenement')\n        if 'val_accuracy' in history.history:\n            ax1.plot(history.history['val_accuracy'], label='Validation')\n        ax1.set_title('\u00c9volution de la pr\u00e9cision')\n        ax1.set_xlabel('\u00c9poque')\n        ax1.set_ylabel('Pr\u00e9cision')\n        ax1.legend()\n        ax1.grid(True, linestyle='--', alpha=0.7)\n    else:\n        # Si pas d'accuracy (r\u00e9gression), on peut utiliser une autre m\u00e9trique comme MSE\n        for metric in history.history:\n            if metric != 'loss' and metric != 'val_loss' and not metric.startswith('val_'):\n                ax1.plot(history.history[metric], label=f'Entra\u00eenement ({metric})')\n                if f'val_{metric}' in history.history:\n                    ax1.plot(history.history[f'val_{metric}'], label=f'Validation ({metric})')\n                ax1.set_title(f'\u00c9volution de {metric}')\n                ax1.set_xlabel('\u00c9poque')\n                ax1.set_ylabel(metric)\n                ax1.legend()\n                ax1.grid(True, linestyle='--', alpha=0.7)\n                break\n    \n    # \u00c9volution de la perte\n    ax2.plot(history.history['loss'], label='Entra\u00eenement')\n    if 'val_loss' in history.history:\n        ax2.plot(history.history['val_loss'], label='Validation')\n    ax2.set_title('\u00c9volution de la perte')\n    ax2.set_xlabel('\u00c9poque')\n    ax2.set_ylabel('Perte')\n    ax2.legend()\n    ax2.grid(True, linestyle='--', alpha=0.7)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Analyse des courbes\n    if 'accuracy' in history.history and 'val_accuracy' in history.history:\n        train_acc = history.history['accuracy'][-1]\n        val_acc = history.history['val_accuracy'][-1]\n        gap = train_acc - val_acc\n        \n        print(f\"Pr\u00e9cision finale sur l'ensemble d'entra\u00eenement: {train_acc*100:.2f}%\")\n        print(f\"Pr\u00e9cision finale sur l'ensemble de validation: {val_acc*100:.2f}%\")\n        print(f\"\u00c9cart entre entra\u00eenement et validation: {gap*100:.2f}%\")\n        \n        if gap &gt; 0.05:\n            print(\"\u26a0\ufe0f Potentiel surapprentissage: l'\u00e9cart entre les pr\u00e9cisions d'entra\u00eenement et de validation est important.\")\n        elif val_acc &lt; 0.75:\n            print(\"\u26a0\ufe0f Potentiel sous-apprentissage: la pr\u00e9cision de validation est relativement faible.\")\n        else:\n            print(\"\u2705 Bon \u00e9quilibre: pas de signe \u00e9vident de sur ou sous-apprentissage.\")\n    elif 'loss' in history.history and 'val_loss' in history.history:\n        # Pour les t\u00e2ches de r\u00e9gression\n        train_loss = history.history['loss'][-1]\n        val_loss = history.history['val_loss'][-1]\n        loss_ratio = val_loss / (train_loss + 1e-10)\n        \n        print(f\"Perte finale sur l'ensemble d'entra\u00eenement: {train_loss:.4f}\")\n        print(f\"Perte finale sur l'ensemble de validation: {val_loss:.4f}\")\n        print(f\"Rapport validation/entra\u00eenement: {loss_ratio:.2f}\")\n        \n        if loss_ratio &gt; 1.3:\n            print(\"\u26a0\ufe0f Potentiel surapprentissage: la perte de validation est significativement plus \u00e9lev\u00e9e que celle d'entra\u00eenement.\")\n        elif train_loss &gt; 0.1 and val_loss &gt; 0.1:  # Seuil arbitraire \u00e0 adapter selon votre cas\n            print(\"\u26a0\ufe0f Potentiel sous-apprentissage: les pertes d'entra\u00eenement et de validation sont relativement \u00e9lev\u00e9es.\")\n        else:\n            print(\"\u2705 Bon \u00e9quilibre: pas de signe \u00e9vident de sur ou sous-apprentissage.\")\n</pre> def plot_training_history(history):     \"\"\"     Visualise l'\u00e9volution de la pr\u00e9cision et de la perte pendant l'entra\u00eenement.          Args:         history: Historique retourn\u00e9 par la m\u00e9thode fit() de Keras     \"\"\"     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))          # \u00c9volution de la pr\u00e9cision     if 'accuracy' in history.history:         ax1.plot(history.history['accuracy'], label='Entra\u00eenement')         if 'val_accuracy' in history.history:             ax1.plot(history.history['val_accuracy'], label='Validation')         ax1.set_title('\u00c9volution de la pr\u00e9cision')         ax1.set_xlabel('\u00c9poque')         ax1.set_ylabel('Pr\u00e9cision')         ax1.legend()         ax1.grid(True, linestyle='--', alpha=0.7)     else:         # Si pas d'accuracy (r\u00e9gression), on peut utiliser une autre m\u00e9trique comme MSE         for metric in history.history:             if metric != 'loss' and metric != 'val_loss' and not metric.startswith('val_'):                 ax1.plot(history.history[metric], label=f'Entra\u00eenement ({metric})')                 if f'val_{metric}' in history.history:                     ax1.plot(history.history[f'val_{metric}'], label=f'Validation ({metric})')                 ax1.set_title(f'\u00c9volution de {metric}')                 ax1.set_xlabel('\u00c9poque')                 ax1.set_ylabel(metric)                 ax1.legend()                 ax1.grid(True, linestyle='--', alpha=0.7)                 break          # \u00c9volution de la perte     ax2.plot(history.history['loss'], label='Entra\u00eenement')     if 'val_loss' in history.history:         ax2.plot(history.history['val_loss'], label='Validation')     ax2.set_title('\u00c9volution de la perte')     ax2.set_xlabel('\u00c9poque')     ax2.set_ylabel('Perte')     ax2.legend()     ax2.grid(True, linestyle='--', alpha=0.7)          plt.tight_layout()     plt.show()          # Analyse des courbes     if 'accuracy' in history.history and 'val_accuracy' in history.history:         train_acc = history.history['accuracy'][-1]         val_acc = history.history['val_accuracy'][-1]         gap = train_acc - val_acc                  print(f\"Pr\u00e9cision finale sur l'ensemble d'entra\u00eenement: {train_acc*100:.2f}%\")         print(f\"Pr\u00e9cision finale sur l'ensemble de validation: {val_acc*100:.2f}%\")         print(f\"\u00c9cart entre entra\u00eenement et validation: {gap*100:.2f}%\")                  if gap &gt; 0.05:             print(\"\u26a0\ufe0f Potentiel surapprentissage: l'\u00e9cart entre les pr\u00e9cisions d'entra\u00eenement et de validation est important.\")         elif val_acc &lt; 0.75:             print(\"\u26a0\ufe0f Potentiel sous-apprentissage: la pr\u00e9cision de validation est relativement faible.\")         else:             print(\"\u2705 Bon \u00e9quilibre: pas de signe \u00e9vident de sur ou sous-apprentissage.\")     elif 'loss' in history.history and 'val_loss' in history.history:         # Pour les t\u00e2ches de r\u00e9gression         train_loss = history.history['loss'][-1]         val_loss = history.history['val_loss'][-1]         loss_ratio = val_loss / (train_loss + 1e-10)                  print(f\"Perte finale sur l'ensemble d'entra\u00eenement: {train_loss:.4f}\")         print(f\"Perte finale sur l'ensemble de validation: {val_loss:.4f}\")         print(f\"Rapport validation/entra\u00eenement: {loss_ratio:.2f}\")                  if loss_ratio &gt; 1.3:             print(\"\u26a0\ufe0f Potentiel surapprentissage: la perte de validation est significativement plus \u00e9lev\u00e9e que celle d'entra\u00eenement.\")         elif train_loss &gt; 0.1 and val_loss &gt; 0.1:  # Seuil arbitraire \u00e0 adapter selon votre cas             print(\"\u26a0\ufe0f Potentiel sous-apprentissage: les pertes d'entra\u00eenement et de validation sont relativement \u00e9lev\u00e9es.\")         else:             print(\"\u2705 Bon \u00e9quilibre: pas de signe \u00e9vident de sur ou sous-apprentissage.\") In\u00a0[\u00a0]: Copied! <pre>def plot_predictions(y_true, y_pred, dates=None, title=\"Pr\u00e9dictions vs Valeurs R\u00e9elles\"):\n    \"\"\"\n    Trace un graphique des pr\u00e9dictions vs valeurs r\u00e9elles pour des donn\u00e9es de s\u00e9ries temporelles.\n    \n    Args:\n        y_true: Valeurs r\u00e9elles\n        y_pred: Pr\u00e9dictions\n        dates: Dates correspondantes (optionnel)\n        title: Titre du graphique\n    \"\"\"\n    plt.figure(figsize=(12, 6))\n    \n    if dates is not None:\n        plt.plot(dates, y_true, 'b-', label='Valeurs R\u00e9elles')\n        plt.plot(dates, y_pred, 'r--', label='Pr\u00e9dictions')\n        plt.xticks(rotation=45)\n    else:\n        plt.plot(y_true, 'b-', label='Valeurs R\u00e9elles')\n        plt.plot(y_pred, 'r--', label='Pr\u00e9dictions')\n    \n    plt.title(title)\n    plt.xlabel('Temps')\n    plt.ylabel('Valeur')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Calculer et afficher les m\u00e9triques\n    mse = np.mean((y_true - y_pred)**2)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(y_true - y_pred))\n    \n    plt.figtext(0.5, 0.01, f'RMSE: {rmse:.2f}, MAE: {mae:.2f}', \n                ha='center', fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.2, \"pad\":5})\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return {\"rmse\": rmse, \"mae\": mae}\n</pre> def plot_predictions(y_true, y_pred, dates=None, title=\"Pr\u00e9dictions vs Valeurs R\u00e9elles\"):     \"\"\"     Trace un graphique des pr\u00e9dictions vs valeurs r\u00e9elles pour des donn\u00e9es de s\u00e9ries temporelles.          Args:         y_true: Valeurs r\u00e9elles         y_pred: Pr\u00e9dictions         dates: Dates correspondantes (optionnel)         title: Titre du graphique     \"\"\"     plt.figure(figsize=(12, 6))          if dates is not None:         plt.plot(dates, y_true, 'b-', label='Valeurs R\u00e9elles')         plt.plot(dates, y_pred, 'r--', label='Pr\u00e9dictions')         plt.xticks(rotation=45)     else:         plt.plot(y_true, 'b-', label='Valeurs R\u00e9elles')         plt.plot(y_pred, 'r--', label='Pr\u00e9dictions')          plt.title(title)     plt.xlabel('Temps')     plt.ylabel('Valeur')     plt.legend()     plt.grid(True, alpha=0.3)          # Calculer et afficher les m\u00e9triques     mse = np.mean((y_true - y_pred)**2)     rmse = np.sqrt(mse)     mae = np.mean(np.abs(y_true - y_pred))          plt.figtext(0.5, 0.01, f'RMSE: {rmse:.2f}, MAE: {mae:.2f}',                  ha='center', fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.2, \"pad\":5})          plt.tight_layout()     plt.show()          return {\"rmse\": rmse, \"mae\": mae} In\u00a0[\u00a0]: Copied! <pre>def plot_residuals(y_true, y_pred):\n    \"\"\"\n    Analyse les r\u00e9sidus (erreurs) entre les pr\u00e9dictions et les valeurs r\u00e9elles.\n    \n    Args:\n        y_true: Valeurs r\u00e9elles\n        y_pred: Pr\u00e9dictions\n    \"\"\"\n    residuals = y_true - y_pred\n    \n    plt.figure(figsize=(15, 5))\n    \n    # Distribution des r\u00e9sidus\n    plt.subplot(1, 3, 1)\n    sns.histplot(residuals, kde=True, color='skyblue')\n    plt.title('Distribution des R\u00e9sidus')\n    plt.xlabel('Erreur')\n    plt.axvline(x=0, color='red', linestyle='--')\n    \n    # R\u00e9sidus vs valeurs pr\u00e9dites\n    plt.subplot(1, 3, 2)\n    plt.scatter(y_pred, residuals, alpha=0.5, color='blue')\n    plt.title('R\u00e9sidus vs Valeurs Pr\u00e9dites')\n    plt.xlabel('Valeurs Pr\u00e9dites')\n    plt.ylabel('R\u00e9sidus')\n    plt.axhline(y=0, color='red', linestyle='--')\n    plt.grid(True, alpha=0.3)\n    \n    # QQ plot pour normalit\u00e9\n    plt.subplot(1, 3, 3)\n    from scipy import stats\n    stats.probplot(residuals, dist=\"norm\", plot=plt)\n    plt.title('Q-Q Plot des R\u00e9sidus')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Statistiques des r\u00e9sidus\n    print(\"Statistiques des r\u00e9sidus:\")\n    print(f\"Moyenne: {np.mean(residuals):.4f}\")\n    print(f\"\u00c9cart-type: {np.std(residuals):.4f}\")\n    print(f\"Min: {np.min(residuals):.4f}\")\n    print(f\"Max: {np.max(residuals):.4f}\")\n    \n    # Test de normalit\u00e9\n    from scipy.stats import shapiro\n    stat, p = shapiro(residuals)\n    print(f\"Test de Shapiro-Wilk pour la normalit\u00e9: p-value = {p:.4f}\")\n    if p &lt; 0.05:\n        print(\"\u26a0\ufe0f Les r\u00e9sidus ne semblent pas suivre une distribution normale.\")\n    else:\n        print(\"\u2705 Les r\u00e9sidus semblent suivre une distribution normale.\")\n</pre> def plot_residuals(y_true, y_pred):     \"\"\"     Analyse les r\u00e9sidus (erreurs) entre les pr\u00e9dictions et les valeurs r\u00e9elles.          Args:         y_true: Valeurs r\u00e9elles         y_pred: Pr\u00e9dictions     \"\"\"     residuals = y_true - y_pred          plt.figure(figsize=(15, 5))          # Distribution des r\u00e9sidus     plt.subplot(1, 3, 1)     sns.histplot(residuals, kde=True, color='skyblue')     plt.title('Distribution des R\u00e9sidus')     plt.xlabel('Erreur')     plt.axvline(x=0, color='red', linestyle='--')          # R\u00e9sidus vs valeurs pr\u00e9dites     plt.subplot(1, 3, 2)     plt.scatter(y_pred, residuals, alpha=0.5, color='blue')     plt.title('R\u00e9sidus vs Valeurs Pr\u00e9dites')     plt.xlabel('Valeurs Pr\u00e9dites')     plt.ylabel('R\u00e9sidus')     plt.axhline(y=0, color='red', linestyle='--')     plt.grid(True, alpha=0.3)          # QQ plot pour normalit\u00e9     plt.subplot(1, 3, 3)     from scipy import stats     stats.probplot(residuals, dist=\"norm\", plot=plt)     plt.title('Q-Q Plot des R\u00e9sidus')          plt.tight_layout()     plt.show()          # Statistiques des r\u00e9sidus     print(\"Statistiques des r\u00e9sidus:\")     print(f\"Moyenne: {np.mean(residuals):.4f}\")     print(f\"\u00c9cart-type: {np.std(residuals):.4f}\")     print(f\"Min: {np.min(residuals):.4f}\")     print(f\"Max: {np.max(residuals):.4f}\")          # Test de normalit\u00e9     from scipy.stats import shapiro     stat, p = shapiro(residuals)     print(f\"Test de Shapiro-Wilk pour la normalit\u00e9: p-value = {p:.4f}\")     if p &lt; 0.05:         print(\"\u26a0\ufe0f Les r\u00e9sidus ne semblent pas suivre une distribution normale.\")     else:         print(\"\u2705 Les r\u00e9sidus semblent suivre une distribution normale.\") In\u00a0[\u00a0]: Copied! <pre>def plot_feature_importance(model, feature_names):\n    \"\"\"\n    Visualise l'importance des caract\u00e9ristiques pour un mod\u00e8le qui fournit feature_importances_.\n    \n    Args:\n        model: Mod\u00e8le entra\u00een\u00e9 (doit avoir un attribut feature_importances_)\n        feature_names: Liste des noms des caract\u00e9ristiques\n    \"\"\"\n    if not hasattr(model, 'feature_importances_'):\n        print(\"Ce mod\u00e8le ne fournit pas d'attribut feature_importances_.\")\n        return\n    \n    # Obtenir les importances et les trier\n    importances = model.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    \n    plt.figure(figsize=(12, 6))\n    plt.title('Importance des Caract\u00e9ristiques')\n    plt.bar(range(len(importances)), importances[indices], align='center', color='skyblue')\n    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)\n    plt.tight_layout()\n    plt.show()\n    \n    # Afficher les 10 caract\u00e9ristiques les plus importantes\n    print(\"Les 10 caract\u00e9ristiques les plus importantes:\")\n    for i in range(min(10, len(feature_names))):\n        print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n</pre> def plot_feature_importance(model, feature_names):     \"\"\"     Visualise l'importance des caract\u00e9ristiques pour un mod\u00e8le qui fournit feature_importances_.          Args:         model: Mod\u00e8le entra\u00een\u00e9 (doit avoir un attribut feature_importances_)         feature_names: Liste des noms des caract\u00e9ristiques     \"\"\"     if not hasattr(model, 'feature_importances_'):         print(\"Ce mod\u00e8le ne fournit pas d'attribut feature_importances_.\")         return          # Obtenir les importances et les trier     importances = model.feature_importances_     indices = np.argsort(importances)[::-1]          plt.figure(figsize=(12, 6))     plt.title('Importance des Caract\u00e9ristiques')     plt.bar(range(len(importances)), importances[indices], align='center', color='skyblue')     plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)     plt.tight_layout()     plt.show()          # Afficher les 10 caract\u00e9ristiques les plus importantes     print(\"Les 10 caract\u00e9ristiques les plus importantes:\")     for i in range(min(10, len(feature_names))):         print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\") In\u00a0[\u00a0]: Copied! <pre>def plot_comparison_metrics(models_results, metrics=['rmse', 'mae']):\n    \"\"\"\n    Compare les performances de diff\u00e9rents mod\u00e8les selon plusieurs m\u00e9triques.\n    \n    Args:\n        models_results: Dictionnaire {nom_mod\u00e8le: {m\u00e9trique1: valeur1, m\u00e9trique2: valeur2, ...}}\n        metrics: Liste des m\u00e9triques \u00e0 comparer\n    \"\"\"\n    n_models = len(models_results)\n    n_metrics = len(metrics)\n    \n    plt.figure(figsize=(12, 5 * n_metrics))\n    \n    for i, metric in enumerate(metrics):\n        plt.subplot(n_metrics, 1, i+1)\n        \n        model_names = list(models_results.keys())\n        metric_values = [models_results[model].get(metric, np.nan) for model in model_names]\n        \n        # Trier par performance (valeurs plus basses sont meilleures pour rmse, mae)\n        sorted_indices = np.argsort(metric_values)\n        sorted_names = [model_names[i] for i in sorted_indices]\n        sorted_values = [metric_values[i] for i in sorted_indices]\n        \n        bars = plt.bar(sorted_names, sorted_values, color='skyblue')\n        \n        # Ajouter les valeurs au-dessus des barres\n        for bar, value in zip(bars, sorted_values):\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n                    f'{value:.4f}', ha='center', va='bottom', fontsize=11)\n        \n        plt.title(f'Comparaison des Mod\u00e8les - {metric.upper()}')\n        plt.ylabel(metric.upper())\n        plt.xticks(rotation=45)\n        plt.grid(axis='y', alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n</pre> def plot_comparison_metrics(models_results, metrics=['rmse', 'mae']):     \"\"\"     Compare les performances de diff\u00e9rents mod\u00e8les selon plusieurs m\u00e9triques.          Args:         models_results: Dictionnaire {nom_mod\u00e8le: {m\u00e9trique1: valeur1, m\u00e9trique2: valeur2, ...}}         metrics: Liste des m\u00e9triques \u00e0 comparer     \"\"\"     n_models = len(models_results)     n_metrics = len(metrics)          plt.figure(figsize=(12, 5 * n_metrics))          for i, metric in enumerate(metrics):         plt.subplot(n_metrics, 1, i+1)                  model_names = list(models_results.keys())         metric_values = [models_results[model].get(metric, np.nan) for model in model_names]                  # Trier par performance (valeurs plus basses sont meilleures pour rmse, mae)         sorted_indices = np.argsort(metric_values)         sorted_names = [model_names[i] for i in sorted_indices]         sorted_values = [metric_values[i] for i in sorted_indices]                  bars = plt.bar(sorted_names, sorted_values, color='skyblue')                  # Ajouter les valeurs au-dessus des barres         for bar, value in zip(bars, sorted_values):             plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,                     f'{value:.4f}', ha='center', va='bottom', fontsize=11)                  plt.title(f'Comparaison des Mod\u00e8les - {metric.upper()}')         plt.ylabel(metric.upper())         plt.xticks(rotation=45)         plt.grid(axis='y', alpha=0.3)          plt.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre>def plot_sales_by_category(df, target_col='revenue', category_col='product_category'):\n    \"\"\"\n    Visualise les ventes par cat\u00e9gorie de produit.\n    \n    Args:\n        df: DataFrame contenant les donn\u00e9es\n        target_col: Colonne cible (ex: 'revenue')\n        category_col: Colonne de cat\u00e9gorie\n    \"\"\"\n    # Agr\u00e9ger par cat\u00e9gorie\n    category_sales = df.groupby(category_col)[target_col].agg(['sum', 'mean']).sort_values('sum', ascending=False)\n    \n    plt.figure(figsize=(14, 7))\n    \n    # Ventes totales par cat\u00e9gorie\n    plt.subplot(1, 2, 1)\n    sns.barplot(x=category_sales.index, y=category_sales['sum'], palette='viridis')\n    plt.title('Ventes Totales par Cat\u00e9gorie')\n    plt.xlabel('Cat\u00e9gorie')\n    plt.ylabel(f'Total des {target_col}')\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', alpha=0.3)\n    \n    # Ventes moyennes par cat\u00e9gorie\n    plt.subplot(1, 2, 2)\n    sns.barplot(x=category_sales.index, y=category_sales['mean'], palette='viridis')\n    plt.title('Ventes Moyennes par Cat\u00e9gorie')\n    plt.xlabel('Cat\u00e9gorie')\n    plt.ylabel(f'Moyenne des {target_col}')\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n</pre> def plot_sales_by_category(df, target_col='revenue', category_col='product_category'):     \"\"\"     Visualise les ventes par cat\u00e9gorie de produit.          Args:         df: DataFrame contenant les donn\u00e9es         target_col: Colonne cible (ex: 'revenue')         category_col: Colonne de cat\u00e9gorie     \"\"\"     # Agr\u00e9ger par cat\u00e9gorie     category_sales = df.groupby(category_col)[target_col].agg(['sum', 'mean']).sort_values('sum', ascending=False)          plt.figure(figsize=(14, 7))          # Ventes totales par cat\u00e9gorie     plt.subplot(1, 2, 1)     sns.barplot(x=category_sales.index, y=category_sales['sum'], palette='viridis')     plt.title('Ventes Totales par Cat\u00e9gorie')     plt.xlabel('Cat\u00e9gorie')     plt.ylabel(f'Total des {target_col}')     plt.xticks(rotation=45)     plt.grid(axis='y', alpha=0.3)          # Ventes moyennes par cat\u00e9gorie     plt.subplot(1, 2, 2)     sns.barplot(x=category_sales.index, y=category_sales['mean'], palette='viridis')     plt.title('Ventes Moyennes par Cat\u00e9gorie')     plt.xlabel('Cat\u00e9gorie')     plt.ylabel(f'Moyenne des {target_col}')     plt.xticks(rotation=45)     plt.grid(axis='y', alpha=0.3)          plt.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre>def plot_seasonal_patterns(df, date_col='date', target_col='revenue'):\n    \"\"\"\n    Visualise les patterns saisonniers dans les ventes.\n    \n    Args:\n        df: DataFrame contenant les donn\u00e9es\n        date_col: Colonne de date\n        target_col: Colonne cible (ex: 'revenue')\n    \"\"\"\n    # S'assurer que la colonne date est au format datetime\n    df[date_col] = pd.to_datetime(df[date_col])\n    \n    # Extraire les composantes temporelles\n    df['year'] = df[date_col].dt.year\n    df['month'] = df[date_col].dt.month\n    df['day'] = df[date_col].dt.day\n    df['weekday'] = df[date_col].dt.weekday\n    \n    plt.figure(figsize=(15, 10))\n    \n    # Ventes par mois\n    plt.subplot(2, 2, 1)\n    monthly_avg = df.groupby('month')[target_col].mean()\n    sns.barplot(x=monthly_avg.index, y=monthly_avg.values, palette='coolwarm')\n    plt.title('Ventes Moyennes par Mois')\n    plt.xlabel('Mois')\n    plt.ylabel(f'Moyenne des {target_col}')\n    plt.xticks(range(0, 12), ['Jan', 'F\u00e9v', 'Mar', 'Avr', 'Mai', 'Juin', 'Juil', 'Ao\u00fbt', 'Sep', 'Oct', 'Nov', 'D\u00e9c'])\n    plt.grid(axis='y', alpha=0.3)\n    \n    # Ventes par jour de la semaine\n    plt.subplot(2, 2, 2)\n    weekday_avg = df.groupby('weekday')[target_col].mean()\n    sns.barplot(x=weekday_avg.index, y=weekday_avg.values, palette='coolwarm')\n    plt.title('Ventes Moyennes par Jour de la Semaine')\n    plt.xlabel('Jour de la Semaine')\n    plt.ylabel(f'Moyenne des {target_col}')\n    plt.xticks(range(0, 7), ['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam', 'Dim'])\n    plt.grid(axis='y', alpha=0.3)\n    \n    # Tendance au fil du temps\n    plt.subplot(2, 1, 2)\n    df_grouped = df.groupby(pd.Grouper(key=date_col, freq='M'))[target_col].sum().reset_index()\n    plt.plot(df_grouped[date_col], df_grouped[target_col], marker='o', linestyle='-')\n    plt.title('Tendance des Ventes au Fil du Temps')\n    plt.xlabel('Date')\n    plt.ylabel(f'Total des {target_col}')\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n</pre> def plot_seasonal_patterns(df, date_col='date', target_col='revenue'):     \"\"\"     Visualise les patterns saisonniers dans les ventes.          Args:         df: DataFrame contenant les donn\u00e9es         date_col: Colonne de date         target_col: Colonne cible (ex: 'revenue')     \"\"\"     # S'assurer que la colonne date est au format datetime     df[date_col] = pd.to_datetime(df[date_col])          # Extraire les composantes temporelles     df['year'] = df[date_col].dt.year     df['month'] = df[date_col].dt.month     df['day'] = df[date_col].dt.day     df['weekday'] = df[date_col].dt.weekday          plt.figure(figsize=(15, 10))          # Ventes par mois     plt.subplot(2, 2, 1)     monthly_avg = df.groupby('month')[target_col].mean()     sns.barplot(x=monthly_avg.index, y=monthly_avg.values, palette='coolwarm')     plt.title('Ventes Moyennes par Mois')     plt.xlabel('Mois')     plt.ylabel(f'Moyenne des {target_col}')     plt.xticks(range(0, 12), ['Jan', 'F\u00e9v', 'Mar', 'Avr', 'Mai', 'Juin', 'Juil', 'Ao\u00fbt', 'Sep', 'Oct', 'Nov', 'D\u00e9c'])     plt.grid(axis='y', alpha=0.3)          # Ventes par jour de la semaine     plt.subplot(2, 2, 2)     weekday_avg = df.groupby('weekday')[target_col].mean()     sns.barplot(x=weekday_avg.index, y=weekday_avg.values, palette='coolwarm')     plt.title('Ventes Moyennes par Jour de la Semaine')     plt.xlabel('Jour de la Semaine')     plt.ylabel(f'Moyenne des {target_col}')     plt.xticks(range(0, 7), ['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam', 'Dim'])     plt.grid(axis='y', alpha=0.3)          # Tendance au fil du temps     plt.subplot(2, 1, 2)     df_grouped = df.groupby(pd.Grouper(key=date_col, freq='M'))[target_col].sum().reset_index()     plt.plot(df_grouped[date_col], df_grouped[target_col], marker='o', linestyle='-')     plt.title('Tendance des Ventes au Fil du Temps')     plt.xlabel('Date')     plt.ylabel(f'Total des {target_col}')     plt.grid(True, alpha=0.3)          plt.tight_layout()     plt.show()"},{"location":"seance2/challenge-prevision-ventes/visualize_utils/","title":"Visualize utils","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n</pre> import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.metrics import confusion_matrix In\u00a0[\u00a0]: Copied! <pre>def plot_training_history(history):\n    \"\"\"\n    Visualise l'\u00e9volution de la pr\u00e9cision et de la perte pendant l'entra\u00eenement.\n    \n    Args:\n        history: Historique retourn\u00e9 par la m\u00e9thode fit() de Keras\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # \u00c9volution de la pr\u00e9cision\n    ax1.plot(history.history['accuracy'], label='Entra\u00eenement')\n    ax1.plot(history.history['val_accuracy'], label='Validation')\n    ax1.set_title('\u00c9volution de la pr\u00e9cision')\n    ax1.set_xlabel('\u00c9poque')\n    ax1.set_ylabel('Pr\u00e9cision')\n    ax1.legend()\n    ax1.grid(True, linestyle='--', alpha=0.7)\n    \n    # \u00c9volution de la perte\n    ax2.plot(history.history['loss'], label='Entra\u00eenement')\n    ax2.plot(history.history['val_loss'], label='Validation')\n    ax2.set_title('\u00c9volution de la perte')\n    ax2.set_xlabel('\u00c9poque')\n    ax2.set_ylabel('Perte')\n    ax2.legend()\n    ax2.grid(True, linestyle='--', alpha=0.7)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Analyse des courbes\n    train_acc = history.history['accuracy'][-1]\n    val_acc = history.history['val_accuracy'][-1]\n    gap = train_acc - val_acc\n    \n    print(f\"Pr\u00e9cision finale sur l'ensemble d'entra\u00eenement: {train_acc*100:.2f}%\")\n    print(f\"Pr\u00e9cision finale sur l'ensemble de validation: {val_acc*100:.2f}%\")\n    print(f\"\u00c9cart entre entra\u00eenement et validation: {gap*100:.2f}%\")\n    \n    if gap &gt; 0.05:\n        print(\"\u26a0\ufe0f Potentiel surapprentissage: l'\u00e9cart entre les pr\u00e9cisions d'entra\u00eenement et de validation est important.\")\n    elif val_acc &lt; 0.75:\n        print(\"\u26a0\ufe0f Potentiel sous-apprentissage: la pr\u00e9cision de validation est relativement faible.\")\n    else:\n        print(\"\u2705 Bon \u00e9quilibre: pas de signe \u00e9vident de sur ou sous-apprentissage.\")\n</pre> def plot_training_history(history):     \"\"\"     Visualise l'\u00e9volution de la pr\u00e9cision et de la perte pendant l'entra\u00eenement.          Args:         history: Historique retourn\u00e9 par la m\u00e9thode fit() de Keras     \"\"\"     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))          # \u00c9volution de la pr\u00e9cision     ax1.plot(history.history['accuracy'], label='Entra\u00eenement')     ax1.plot(history.history['val_accuracy'], label='Validation')     ax1.set_title('\u00c9volution de la pr\u00e9cision')     ax1.set_xlabel('\u00c9poque')     ax1.set_ylabel('Pr\u00e9cision')     ax1.legend()     ax1.grid(True, linestyle='--', alpha=0.7)          # \u00c9volution de la perte     ax2.plot(history.history['loss'], label='Entra\u00eenement')     ax2.plot(history.history['val_loss'], label='Validation')     ax2.set_title('\u00c9volution de la perte')     ax2.set_xlabel('\u00c9poque')     ax2.set_ylabel('Perte')     ax2.legend()     ax2.grid(True, linestyle='--', alpha=0.7)          plt.tight_layout()     plt.show()          # Analyse des courbes     train_acc = history.history['accuracy'][-1]     val_acc = history.history['val_accuracy'][-1]     gap = train_acc - val_acc          print(f\"Pr\u00e9cision finale sur l'ensemble d'entra\u00eenement: {train_acc*100:.2f}%\")     print(f\"Pr\u00e9cision finale sur l'ensemble de validation: {val_acc*100:.2f}%\")     print(f\"\u00c9cart entre entra\u00eenement et validation: {gap*100:.2f}%\")          if gap &gt; 0.05:         print(\"\u26a0\ufe0f Potentiel surapprentissage: l'\u00e9cart entre les pr\u00e9cisions d'entra\u00eenement et de validation est important.\")     elif val_acc &lt; 0.75:         print(\"\u26a0\ufe0f Potentiel sous-apprentissage: la pr\u00e9cision de validation est relativement faible.\")     else:         print(\"\u2705 Bon \u00e9quilibre: pas de signe \u00e9vident de sur ou sous-apprentissage.\") In\u00a0[\u00a0]: Copied! <pre>def plot_confusion_matrix(y_true, y_pred, class_names):\n    \"\"\"\n    Affiche la matrice de confusion avec des annotations.\n    \n    Args:\n        y_true: \u00c9tiquettes r\u00e9elles (classes num\u00e9riques)\n        y_pred: Pr\u00e9dictions (classes num\u00e9riques)\n        class_names: Liste des noms de classes\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Pr\u00e9dictions')\n    plt.ylabel('Valeurs r\u00e9elles')\n    plt.title('Matrice de confusion')\n    plt.tight_layout()\n    plt.show()\n    \n    # Analyse des classes les plus confuses\n    np.fill_diagonal(cm, 0)  # Ignorer la diagonale pour trouver les confusions\n    max_confusions = []\n    for i in range(len(class_names)):\n        max_confusion_idx = np.argmax(cm[i])\n        if cm[i, max_confusion_idx] &gt; 0:\n            max_confusions.append((class_names[i], class_names[max_confusion_idx], cm[i, max_confusion_idx]))\n    \n    if max_confusions:\n        print(\"Classes les plus souvent confondues:\")\n        for real, pred, count in sorted(max_confusions, key=lambda x: x[2], reverse=True)[:5]:\n            print(f\"  \u2022 {real} confondu avec {pred}: {count} fois\")\n</pre> def plot_confusion_matrix(y_true, y_pred, class_names):     \"\"\"     Affiche la matrice de confusion avec des annotations.          Args:         y_true: \u00c9tiquettes r\u00e9elles (classes num\u00e9riques)         y_pred: Pr\u00e9dictions (classes num\u00e9riques)         class_names: Liste des noms de classes     \"\"\"     cm = confusion_matrix(y_true, y_pred)     plt.figure(figsize=(10, 8))     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)     plt.xlabel('Pr\u00e9dictions')     plt.ylabel('Valeurs r\u00e9elles')     plt.title('Matrice de confusion')     plt.tight_layout()     plt.show()          # Analyse des classes les plus confuses     np.fill_diagonal(cm, 0)  # Ignorer la diagonale pour trouver les confusions     max_confusions = []     for i in range(len(class_names)):         max_confusion_idx = np.argmax(cm[i])         if cm[i, max_confusion_idx] &gt; 0:             max_confusions.append((class_names[i], class_names[max_confusion_idx], cm[i, max_confusion_idx]))          if max_confusions:         print(\"Classes les plus souvent confondues:\")         for real, pred, count in sorted(max_confusions, key=lambda x: x[2], reverse=True)[:5]:             print(f\"  \u2022 {real} confondu avec {pred}: {count} fois\") In\u00a0[\u00a0]: Copied! <pre>def plot_misclassified_images(X_test, y_true, y_pred, class_names, num_images=10):\n    \"\"\"\n    Affiche des exemples d'images mal class\u00e9es.\n    \n    Args:\n        X_test: Images de test\n        y_true: \u00c9tiquettes r\u00e9elles (classes num\u00e9riques)\n        y_pred: Pr\u00e9dictions (classes num\u00e9riques)\n        class_names: Liste des noms de classes\n        num_images: Nombre d'images \u00e0 afficher\n    \"\"\"\n    # Trouver les indices des images mal class\u00e9es\n    misclassified_indices = np.where(y_true != y_pred)[0]\n    \n    if len(misclassified_indices) == 0:\n        print(\"Aucune image mal class\u00e9e trouv\u00e9e!\")\n        return\n    \n    # S\u00e9lectionner un sous-ensemble al\u00e9atoire d'images mal class\u00e9es\n    selected_indices = np.random.choice(misclassified_indices, \n                                        size=min(num_images, len(misclassified_indices)), \n                                        replace=False)\n    \n    # Afficher les images\n    fig = plt.figure(figsize=(15, num_images * 2 // 5))\n    for i, idx in enumerate(selected_indices):\n        plt.subplot(num_images // 5 + 1, 5, i + 1)\n        plt.imshow(X_test[idx])\n        plt.title(f\"R\u00e9el: {class_names[y_true[idx]]}\\nPr\u00e9dit: {class_names[y_pred[idx]]}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.suptitle(\"Exemples d'images mal class\u00e9es\", y=1.02, fontsize=16)\n    plt.show()\n</pre> def plot_misclassified_images(X_test, y_true, y_pred, class_names, num_images=10):     \"\"\"     Affiche des exemples d'images mal class\u00e9es.          Args:         X_test: Images de test         y_true: \u00c9tiquettes r\u00e9elles (classes num\u00e9riques)         y_pred: Pr\u00e9dictions (classes num\u00e9riques)         class_names: Liste des noms de classes         num_images: Nombre d'images \u00e0 afficher     \"\"\"     # Trouver les indices des images mal class\u00e9es     misclassified_indices = np.where(y_true != y_pred)[0]          if len(misclassified_indices) == 0:         print(\"Aucune image mal class\u00e9e trouv\u00e9e!\")         return          # S\u00e9lectionner un sous-ensemble al\u00e9atoire d'images mal class\u00e9es     selected_indices = np.random.choice(misclassified_indices,                                          size=min(num_images, len(misclassified_indices)),                                          replace=False)          # Afficher les images     fig = plt.figure(figsize=(15, num_images * 2 // 5))     for i, idx in enumerate(selected_indices):         plt.subplot(num_images // 5 + 1, 5, i + 1)         plt.imshow(X_test[idx])         plt.title(f\"R\u00e9el: {class_names[y_true[idx]]}\\nPr\u00e9dit: {class_names[y_pred[idx]]}\")         plt.axis('off')     plt.tight_layout()     plt.suptitle(\"Exemples d'images mal class\u00e9es\", y=1.02, fontsize=16)     plt.show()"},{"location":"seance2/code-app-web/web-integration/","title":"Web integration","text":"<p>Int\u00e9gration d'un mod\u00e8le CNN dans une application web</p> <ul> <li>S\u00e9ance 2: Types de r\u00e9seaux et applications</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from flask import Flask, request, jsonify, render_template, send_from_directory\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport base64\nfrom io import BytesIO\nfrom PIL import Image, ImageOps\nimport matplotlib.pyplot as plt\nimport time\n</pre> from flask import Flask, request, jsonify, render_template, send_from_directory import tensorflow as tf import numpy as np import os import base64 from io import BytesIO from PIL import Image, ImageOps import matplotlib.pyplot as plt import time In\u00a0[\u00a0]: Copied! <pre># Cr\u00e9er l'application Flask\napp = Flask(__name__)\n</pre> # Cr\u00e9er l'application Flask app = Flask(__name__) In\u00a0[\u00a0]: Copied! <pre># Configuration\nMODEL_PATH = 'cnn_mnist_model.h5'  # Chemin vers le mod\u00e8le entra\u00een\u00e9\nUPLOAD_FOLDER = 'uploads'\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\n</pre> # Configuration MODEL_PATH = 'cnn_mnist_model.h5'  # Chemin vers le mod\u00e8le entra\u00een\u00e9 UPLOAD_FOLDER = 'uploads' os.makedirs(UPLOAD_FOLDER, exist_ok=True) In\u00a0[\u00a0]: Copied! <pre># Charger le mod\u00e8le CNN\nprint(\"Chargement du mod\u00e8le...\")\ntry:\n    model = tf.keras.models.load_model(MODEL_PATH)\n    print(\"Mod\u00e8le charg\u00e9 avec succ\u00e8s!\")\nexcept Exception as e:\n    print(f\"Erreur lors du chargement du mod\u00e8le: {e}\")\n    print(\"Un mod\u00e8le par d\u00e9faut sera utilis\u00e9\")\n    # Si le mod\u00e8le n'est pas trouv\u00e9, cr\u00e9er un mod\u00e8le simple pour d\u00e9monstration\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n        tf.keras.layers.MaxPooling2D((2, 2)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n</pre> # Charger le mod\u00e8le CNN print(\"Chargement du mod\u00e8le...\") try:     model = tf.keras.models.load_model(MODEL_PATH)     print(\"Mod\u00e8le charg\u00e9 avec succ\u00e8s!\") except Exception as e:     print(f\"Erreur lors du chargement du mod\u00e8le: {e}\")     print(\"Un mod\u00e8le par d\u00e9faut sera utilis\u00e9\")     # Si le mod\u00e8le n'est pas trouv\u00e9, cr\u00e9er un mod\u00e8le simple pour d\u00e9monstration     model = tf.keras.Sequential([         tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),         tf.keras.layers.MaxPooling2D((2, 2)),         tf.keras.layers.Flatten(),         tf.keras.layers.Dense(128, activation='relu'),         tf.keras.layers.Dense(10, activation='softmax')     ])     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) In\u00a0[\u00a0]: Copied! <pre># Fonction de pr\u00e9traitement des images\ndef preprocess_image(image_data):\n    \"\"\"\n    Pr\u00e9traite l'image pour qu'elle corresponde au format attendu par le mod\u00e8le\n    \"\"\"\n    # Convertir l'image en niveaux de gris\n    image = ImageOps.grayscale(image_data)\n    \n    # Redimensionner \u00e0 28x28 pixels\n    image = image.resize((28, 28))\n    \n    # Normaliser les pixels entre 0 et 1\n    image_array = np.array(image) / 255.0\n    \n    # Ajouter les dimensions de batch et de canal\n    image_array = image_array.reshape(1, 28, 28, 1)\n    \n    return image_array\n</pre> # Fonction de pr\u00e9traitement des images def preprocess_image(image_data):     \"\"\"     Pr\u00e9traite l'image pour qu'elle corresponde au format attendu par le mod\u00e8le     \"\"\"     # Convertir l'image en niveaux de gris     image = ImageOps.grayscale(image_data)          # Redimensionner \u00e0 28x28 pixels     image = image.resize((28, 28))          # Normaliser les pixels entre 0 et 1     image_array = np.array(image) / 255.0          # Ajouter les dimensions de batch et de canal     image_array = image_array.reshape(1, 28, 28, 1)          return image_array In\u00a0[\u00a0]: Copied! <pre># Fonction pour g\u00e9n\u00e9rer la visualisation des pr\u00e9dictions\ndef generate_prediction_visualization(image_array, prediction):\n    \"\"\"\n    G\u00e9n\u00e8re une visualisation des probabilit\u00e9s de pr\u00e9diction pour chaque chiffre\n    \"\"\"\n    plt.figure(figsize=(10, 4))\n    \n    # Afficher l'image originale\n    plt.subplot(1, 2, 1)\n    plt.imshow(image_array.reshape(28, 28), cmap='gray')\n    plt.title('Image d\\'entr\u00e9e')\n    plt.axis('off')\n    \n    # Afficher les probabilit\u00e9s\n    plt.subplot(1, 2, 2)\n    classes = range(10)\n    plt.bar(classes, prediction[0])\n    plt.xticks(classes)\n    plt.xlabel('Chiffre')\n    plt.ylabel('Probabilit\u00e9')\n    plt.title('Pr\u00e9diction du mod\u00e8le')\n    \n    # Sauvegarder l'image en m\u00e9moire\n    buf = BytesIO()\n    plt.tight_layout()\n    plt.savefig(buf, format='png')\n    buf.seek(0)\n    plt.close()\n    \n    # Convertir en base64 pour l'affichage HTML\n    img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n    return img_base64\n</pre> # Fonction pour g\u00e9n\u00e9rer la visualisation des pr\u00e9dictions def generate_prediction_visualization(image_array, prediction):     \"\"\"     G\u00e9n\u00e8re une visualisation des probabilit\u00e9s de pr\u00e9diction pour chaque chiffre     \"\"\"     plt.figure(figsize=(10, 4))          # Afficher l'image originale     plt.subplot(1, 2, 1)     plt.imshow(image_array.reshape(28, 28), cmap='gray')     plt.title('Image d\\'entr\u00e9e')     plt.axis('off')          # Afficher les probabilit\u00e9s     plt.subplot(1, 2, 2)     classes = range(10)     plt.bar(classes, prediction[0])     plt.xticks(classes)     plt.xlabel('Chiffre')     plt.ylabel('Probabilit\u00e9')     plt.title('Pr\u00e9diction du mod\u00e8le')          # Sauvegarder l'image en m\u00e9moire     buf = BytesIO()     plt.tight_layout()     plt.savefig(buf, format='png')     buf.seek(0)     plt.close()          # Convertir en base64 pour l'affichage HTML     img_base64 = base64.b64encode(buf.read()).decode('utf-8')     return img_base64 In\u00a0[\u00a0]: Copied! <pre># Routes Flask\n@app.route('/')\ndef home():\n    \"\"\"Page d'accueil\"\"\"\n    return render_template('index.html')\n</pre> # Routes Flask @app.route('/') def home():     \"\"\"Page d'accueil\"\"\"     return render_template('index.html') In\u00a0[\u00a0]: Copied! <pre>@app.route('/predict', methods=['POST'])\ndef predict():\n    \"\"\"Endpoint pour pr\u00e9dire le chiffre \u00e0 partir d'une image\"\"\"\n    # V\u00e9rifier si une image a \u00e9t\u00e9 envoy\u00e9e\n    if 'image' not in request.files:\n        return jsonify({'error': 'Aucune image fournie'}), 400\n    \n    file = request.files['image']\n    \n    # V\u00e9rifier si le fichier est vide\n    if file.filename == '':\n        return jsonify({'error': 'Nom de fichier vide'}), 400\n    \n    try:\n        # Ouvrir et pr\u00e9traiter l'image\n        image = Image.open(file)\n        processed_image = preprocess_image(image)\n        \n        # Mesurer le temps d'inf\u00e9rence\n        start_time = time.time()\n        prediction = model.predict(processed_image)\n        inference_time = time.time() - start_time\n        \n        # D\u00e9terminer le chiffre avec la plus haute probabilit\u00e9\n        predicted_digit = np.argmax(prediction[0])\n        confidence = float(prediction[0][predicted_digit] * 100)\n        \n        # G\u00e9n\u00e9rer la visualisation\n        viz_base64 = generate_prediction_visualization(processed_image, prediction)\n        \n        # Pr\u00e9paration de la r\u00e9ponse\n        result = {\n            'predicted_digit': int(predicted_digit),\n            'confidence': confidence,\n            'inference_time_ms': inference_time * 1000,\n            'visualization': viz_base64,\n            'probabilities': prediction[0].tolist()\n        }\n        \n        return jsonify(result)\n        \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n</pre> @app.route('/predict', methods=['POST']) def predict():     \"\"\"Endpoint pour pr\u00e9dire le chiffre \u00e0 partir d'une image\"\"\"     # V\u00e9rifier si une image a \u00e9t\u00e9 envoy\u00e9e     if 'image' not in request.files:         return jsonify({'error': 'Aucune image fournie'}), 400          file = request.files['image']          # V\u00e9rifier si le fichier est vide     if file.filename == '':         return jsonify({'error': 'Nom de fichier vide'}), 400          try:         # Ouvrir et pr\u00e9traiter l'image         image = Image.open(file)         processed_image = preprocess_image(image)                  # Mesurer le temps d'inf\u00e9rence         start_time = time.time()         prediction = model.predict(processed_image)         inference_time = time.time() - start_time                  # D\u00e9terminer le chiffre avec la plus haute probabilit\u00e9         predicted_digit = np.argmax(prediction[0])         confidence = float(prediction[0][predicted_digit] * 100)                  # G\u00e9n\u00e9rer la visualisation         viz_base64 = generate_prediction_visualization(processed_image, prediction)                  # Pr\u00e9paration de la r\u00e9ponse         result = {             'predicted_digit': int(predicted_digit),             'confidence': confidence,             'inference_time_ms': inference_time * 1000,             'visualization': viz_base64,             'probabilities': prediction[0].tolist()         }                  return jsonify(result)              except Exception as e:         return jsonify({'error': str(e)}), 500 In\u00a0[\u00a0]: Copied! <pre>@app.route('/uploads/&lt;filename&gt;')\ndef uploaded_file(filename):\n    \"\"\"Servir les fichiers t\u00e9l\u00e9charg\u00e9s\"\"\"\n    return send_from_directory(UPLOAD_FOLDER, filename)\n</pre> @app.route('/uploads/') def uploaded_file(filename):     \"\"\"Servir les fichiers t\u00e9l\u00e9charg\u00e9s\"\"\"     return send_from_directory(UPLOAD_FOLDER, filename) In\u00a0[\u00a0]: Copied! <pre># Cr\u00e9ation des templates\ndef create_templates():\n    \"\"\"Cr\u00e9e les templates n\u00e9cessaires pour l'application\"\"\"\n    os.makedirs('templates', exist_ok=True)\n    \n    # Index.html\n    with open('templates/index.html', 'w') as f:\n        f.write(\"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Reconnaissance de chiffres - CNN MNIST&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n            line-height: 1.6;\n            max-width: 900px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #f5f7fa;\n            color: #333;\n        }\n        h1 {\n            color: #2c3e50;\n            border-bottom: 2px solid #3498db;\n            padding-bottom: 10px;\n        }\n        .container {\n            background-color: white;\n            border-radius: 8px;\n            padding: 20px;\n            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n            margin-bottom: 20px;\n        }\n        .upload-container {\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n            gap: 15px;\n            margin: 20px 0;\n            padding: 20px;\n            border: 2px dashed #bdc3c7;\n            border-radius: 5px;\n            background-color: #f9f9f9;\n        }\n        #drop-area {\n            width: 100%;\n            height: 200px;\n            display: flex;\n            flex-direction: column;\n            justify-content: center;\n            align-items: center;\n            cursor: pointer;\n        }\n        #file-input {\n            display: none;\n        }\n        .button {\n            background-color: #3498db;\n            color: white;\n            border: none;\n            padding: 10px 20px;\n            border-radius: 4px;\n            cursor: pointer;\n            font-weight: bold;\n            transition: background-color 0.3s;\n        }\n        .button:hover {\n            background-color: #2980b9;\n        }\n        .results-container {\n            margin-top: 20px;\n            display: none;\n        }\n        .loading {\n            text-align: center;\n            margin: 20px 0;\n            display: none;\n        }\n        .prediction-result {\n            font-size: 24px;\n            font-weight: bold;\n            text-align: center;\n            margin: 20px 0;\n        }\n        .info-box {\n            background-color: #e8f4fc;\n            border-left: 4px solid #3498db;\n            padding: 10px 15px;\n            margin: 15px 0;\n            border-radius: 4px;\n        }\n        .visualization {\n            max-width: 100%;\n            height: auto;\n            margin: 20px 0;\n        }\n        @media (max-width: 600px) {\n            .upload-container {\n                padding: 10px;\n            }\n            #drop-area {\n                height: 150px;\n            }\n        }\n        table {\n            width: 100%;\n            border-collapse: collapse;\n            margin: 20px 0;\n        }\n        th, td {\n            padding: 10px;\n            text-align: left;\n            border-bottom: 1px solid #ddd;\n        }\n        th {\n            background-color: #f2f2f2;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;h1&gt;Reconnaissance de chiffres avec un CNN&lt;/h1&gt;\n        &lt;p&gt;Cette application utilise un r\u00e9seau de neurones convolutif (CNN) entra\u00een\u00e9 sur le dataset MNIST pour reconna\u00eetre des chiffres manuscrits.&lt;/p&gt;\n        \n        &lt;div class=\"info-box\"&gt;\n            &lt;p&gt;&lt;strong&gt;Comment utiliser :&lt;/strong&gt; Uploadez une image d'un chiffre manuscrit (pr\u00e9f\u00e9rablement sur fond blanc). Le mod\u00e8le analysera l'image et pr\u00e9dira le chiffre.&lt;/p&gt;\n        &lt;/div&gt;\n        \n        &lt;div class=\"upload-container\"&gt;\n            &lt;div id=\"drop-area\"&gt;\n                &lt;p&gt;Glissez-d\u00e9posez une image ici&lt;/p&gt;\n                &lt;p&gt;ou&lt;/p&gt;\n                &lt;label for=\"file-input\" class=\"button\"&gt;Choisir une image&lt;/label&gt;\n                &lt;input type=\"file\" id=\"file-input\" accept=\"image/*\"&gt;\n            &lt;/div&gt;\n            &lt;div id=\"preview-container\" style=\"display: none;\"&gt;\n                &lt;img id=\"preview-image\" style=\"max-width: 200px; max-height: 200px;\"&gt;\n                &lt;button id=\"predict-button\" class=\"button\"&gt;Analyser l'image&lt;/button&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n        \n        &lt;div id=\"loading\" class=\"loading\"&gt;\n            &lt;p&gt;Analyse de l'image en cours...&lt;/p&gt;\n        &lt;/div&gt;\n        \n        &lt;div id=\"results\" class=\"results-container\"&gt;\n            &lt;div class=\"prediction-result\"&gt;\n                Chiffre pr\u00e9dit : &lt;span id=\"predicted-digit\"&gt;-&lt;/span&gt;\n                &lt;span id=\"confidence\"&gt;&lt;/span&gt;\n            &lt;/div&gt;\n            \n            &lt;div id=\"visualization-container\"&gt;\n                &lt;img id=\"visualization\" class=\"visualization\"&gt;\n            &lt;/div&gt;\n            \n            &lt;div class=\"info-box\"&gt;\n                &lt;h3&gt;Informations techniques&lt;/h3&gt;\n                &lt;table&gt;\n                    &lt;tr&gt;\n                        &lt;th&gt;M\u00e9trique&lt;/th&gt;\n                        &lt;th&gt;Valeur&lt;/th&gt;\n                    &lt;/tr&gt;\n                    &lt;tr&gt;\n                        &lt;td&gt;Temps d'inf\u00e9rence&lt;/td&gt;\n                        &lt;td id=\"inference-time\"&gt;-&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr&gt;\n                        &lt;td&gt;Confiance&lt;/td&gt;\n                        &lt;td id=\"confidence-value\"&gt;-&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr&gt;\n                        &lt;td&gt;Mod\u00e8le&lt;/td&gt;\n                        &lt;td&gt;CNN (TensorFlow/Keras)&lt;/td&gt;\n                    &lt;/tr&gt;\n                &lt;/table&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n    \n    &lt;div class=\"container\"&gt;\n        &lt;h2&gt;\u00c0 propos de cette application&lt;/h2&gt;\n        &lt;p&gt;Cette application d\u00e9montre l'int\u00e9gration d'un mod\u00e8le de Deep Learning (CNN) dans une application web. Les r\u00e9seaux de neurones convolutifs sont particuli\u00e8rement efficaces pour les t\u00e2ches de vision par ordinateur comme la reconnaissance d'images.&lt;/p&gt;\n        \n        &lt;h3&gt;Comment fonctionne un CNN?&lt;/h3&gt;\n        &lt;p&gt;Un CNN (Convolutional Neural Network) utilise des couches de convolution pour d\u00e9tecter automatiquement des caract\u00e9ristiques (features) dans les images :&lt;/p&gt;\n        &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;Couches de convolution&lt;/strong&gt; : d\u00e9tectent des motifs locaux comme des bords, des textures&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Pooling&lt;/strong&gt; : r\u00e9duit la dimensionnalit\u00e9 et rend le mod\u00e8le plus robuste aux variations&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Couches denses&lt;/strong&gt; : effectuent la classification finale&lt;/li&gt;\n        &lt;/ul&gt;\n        \n        &lt;h3&gt;Technologies utilis\u00e9es&lt;/h3&gt;\n        &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt; : Flask (Python), TensorFlow/Keras&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt; : HTML, CSS, JavaScript&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt; : MNIST (chiffres manuscrits)&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/div&gt;\n    \n    &lt;script&gt;\n        // \u00c9l\u00e9ments DOM\n        const dropArea = document.getElementById('drop-area');\n        const fileInput = document.getElementById('file-input');\n        const previewContainer = document.getElementById('preview-container');\n        const previewImage = document.getElementById('preview-image');\n        const predictButton = document.getElementById('predict-button');\n        const loadingIndicator = document.getElementById('loading');\n        const resultsContainer = document.getElementById('results');\n        const predictedDigit = document.getElementById('predicted-digit');\n        const confidenceSpan = document.getElementById('confidence');\n        const confidenceValue = document.getElementById('confidence-value');\n        const inferenceTime = document.getElementById('inference-time');\n        const visualization = document.getElementById('visualization');\n        \n        // Prevent default drag behaviors\n        ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName =&gt; {\n            dropArea.addEventListener(eventName, preventDefaults, false);\n        });\n        \n        function preventDefaults(e) {\n            e.preventDefault();\n            e.stopPropagation();\n        }\n        \n        // Highlight drop area when item is dragged over it\n        ['dragenter', 'dragover'].forEach(eventName =&gt; {\n            dropArea.addEventListener(eventName, highlight, false);\n        });\n        \n        ['dragleave', 'drop'].forEach(eventName =&gt; {\n            dropArea.addEventListener(eventName, unhighlight, false);\n        });\n        \n        function highlight() {\n            dropArea.style.backgroundColor = '#e8f4fc';\n            dropArea.style.borderColor = '#3498db';\n        }\n        \n        function unhighlight() {\n            dropArea.style.backgroundColor = '#f9f9f9';\n            dropArea.style.borderColor = '#bdc3c7';\n        }\n        \n        // Handle dropped files\n        dropArea.addEventListener('drop', handleDrop, false);\n        \n        function handleDrop(e) {\n            const dt = e.dataTransfer;\n            const files = dt.files;\n            handleFiles(files);\n        }\n        \n        // Handle file input change\n        fileInput.addEventListener('change', function() {\n            handleFiles(this.files);\n        });\n        \n        function handleFiles(files) {\n            if (files.length &gt; 0) {\n                const file = files[0];\n                \n                // Check if the file is an image\n                if (!file.type.match('image.*')) {\n                    alert('Veuillez s\u00e9lectionner une image');\n                    return;\n                }\n                \n                // Preview the image\n                const reader = new FileReader();\n                reader.onload = function(e) {\n                    previewImage.src = e.target.result;\n                    previewContainer.style.display = 'block';\n                    \n                    // Hide previous results if any\n                    resultsContainer.style.display = 'none';\n                };\n                reader.readAsDataURL(file);\n            }\n        }\n        \n        // Handle prediction button click\n        predictButton.addEventListener('click', function() {\n            // Show loading indicator\n            loadingIndicator.style.display = 'block';\n            \n            // Create form data\n            const formData = new FormData();\n            formData.append('image', fileInput.files[0]);\n            \n            // Send request to the server\n            fetch('/predict', {\n                method: 'POST',\n                body: formData\n            })\n            .then(response =&gt; response.json())\n            .then(data =&gt; {\n                // Hide loading indicator\n                loadingIndicator.style.display = 'none';\n                \n                // Show results\n                resultsContainer.style.display = 'block';\n                \n                if (data.error) {\n                    predictedDigit.textContent = 'Erreur';\n                    confidenceSpan.textContent = '';\n                    alert('Erreur: ' + data.error);\n                } else {\n                    // Display prediction results\n                    predictedDigit.textContent = data.predicted_digit;\n                    confidenceSpan.textContent = ` (${data.confidence.toFixed(2)}% de confiance)`;\n                    confidenceValue.textContent = `${data.confidence.toFixed(2)}%`;\n                    inferenceTime.textContent = `${data.inference_time_ms.toFixed(2)} ms`;\n                    \n                    // Display visualization\n                    visualization.src = 'data:image/png;base64,' + data.visualization;\n                }\n            })\n            .catch(error =&gt; {\n                loadingIndicator.style.display = 'none';\n                alert('Erreur lors de la communication avec le serveur: ' + error);\n            });\n        });\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n        \"\"\")\n    \n    print(\"Template index.html cr\u00e9\u00e9 avec succ\u00e8s!\")\n</pre> # Cr\u00e9ation des templates def create_templates():     \"\"\"Cr\u00e9e les templates n\u00e9cessaires pour l'application\"\"\"     os.makedirs('templates', exist_ok=True)          # Index.html     with open('templates/index.html', 'w') as f:         f.write(\"\"\"  Reconnaissance de chiffres - CNN MNIST Reconnaissance de chiffres avec un CNN <p>Cette application utilise un r\u00e9seau de neurones convolutif (CNN) entra\u00een\u00e9 sur le dataset MNIST pour reconna\u00eetre des chiffres manuscrits.</p> <p>Comment utiliser : Uploadez une image d'un chiffre manuscrit (pr\u00e9f\u00e9rablement sur fond blanc). Le mod\u00e8le analysera l'image et pr\u00e9dira le chiffre.</p> <p>Glissez-d\u00e9posez une image ici</p> <p>ou</p> Choisir une image Analyser l'image <p>Analyse de l'image en cours...</p>                  Chiffre pr\u00e9dit : - Informations techniques M\u00e9trique Valeur Temps d'inf\u00e9rence - Confiance - Mod\u00e8le CNN (TensorFlow/Keras) \u00c0 propos de cette application <p>Cette application d\u00e9montre l'int\u00e9gration d'un mod\u00e8le de Deep Learning (CNN) dans une application web. Les r\u00e9seaux de neurones convolutifs sont particuli\u00e8rement efficaces pour les t\u00e2ches de vision par ordinateur comme la reconnaissance d'images.</p> Comment fonctionne un CNN? <p>Un CNN (Convolutional Neural Network) utilise des couches de convolution pour d\u00e9tecter automatiquement des caract\u00e9ristiques (features) dans les images :</p> <ul> <li>Couches de convolution : d\u00e9tectent des motifs locaux comme des bords, des textures</li> <li>Pooling : r\u00e9duit la dimensionnalit\u00e9 et rend le mod\u00e8le plus robuste aux variations</li> <li>Couches denses : effectuent la classification finale</li> </ul> Technologies utilis\u00e9es <ul> <li>Backend : Flask (Python), TensorFlow/Keras</li> <li>Frontend : HTML, CSS, JavaScript</li> <li>Dataset : MNIST (chiffres manuscrits)</li> </ul>          \"\"\")          print(\"Template index.html cr\u00e9\u00e9 avec succ\u00e8s!\") In\u00a0[\u00a0]: Copied! <pre># Fonction principale\ndef main():\n    # Cr\u00e9er les templates\n    create_templates()\n    \n    # D\u00e9marrer l'application Flask\n    print(\"\\nD\u00e9marrage de l'application web de reconnaissance de chiffres...\")\n    print(\"URL: http://localhost:5001\")\n    app.run(host='0.0.0.0', port=5001, debug=True)\n</pre> # Fonction principale def main():     # Cr\u00e9er les templates     create_templates()          # D\u00e9marrer l'application Flask     print(\"\\nD\u00e9marrage de l'application web de reconnaissance de chiffres...\")     print(\"URL: http://localhost:5001\")     app.run(host='0.0.0.0', port=5001, debug=True) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"seance3/","title":"S\u00e9ance 3 : Frameworks pratiques et pr\u00e9paration du projet","text":""},{"location":"seance3/#objectifs-de-la-seance","title":"Objectifs de la s\u00e9ance","text":"<p>Cette troisi\u00e8me s\u00e9ance vous permettra de :</p> <ul> <li>D\u00e9couvrir les frameworks de Deep Learning utilis\u00e9s en entreprise</li> <li>Acqu\u00e9rir des comp\u00e9tences pratiques avec TensorFlow/Keras</li> <li>Apprendre \u00e0 am\u00e9liorer les performances de vos mod\u00e8les simplement</li> <li>Pr\u00e9parer la conception du chatbot p\u00e9dagogique pour votre projet</li> <li>S'initier \u00e0 l'API Mistral AI pour cr\u00e9er des applications concr\u00e8tes</li> </ul>"},{"location":"seance3/#approche-pedagogique","title":"Approche p\u00e9dagogique","text":"<p>Cette s\u00e9ance est r\u00e9solument orient\u00e9e vers les comp\u00e9tences pratiques. Vous d\u00e9couvrirez les outils industriels de Deep Learning et apprendrez \u00e0 les utiliser dans des situations concr\u00e8tes. Les exercices sont con\u00e7us pour d\u00e9velopper des comp\u00e9tences directement transf\u00e9rables en milieu professionnel.</p>"},{"location":"seance3/#structure-de-la-seance-4h","title":"Structure de la s\u00e9ance (4h)","text":"<pre><code>gantt\n    dateFormat  HH:mm\n    axisFormat %H:%M\n    section Planning\n    Frameworks pour d\u00e9butants        :milestone, 00:00, 1h30m\n    Am\u00e9lioration des performances    :milestone, 01:30, 1h30m\n    Pause                        :milestone, 03:00, 15m\n    Pr\u00e9paration du projet final  :milestone, 03:15, 45m</code></pre>"},{"location":"seance3/#trois-phases-dapprentissage","title":"Trois phases d'apprentissage","text":"<p>Cette s\u00e9ance est structur\u00e9e en trois phases compl\u00e9mentaires :</p>"},{"location":"seance3/#phase-1-frameworks-pour-debutants-1h30","title":"Phase 1 : Frameworks pour d\u00e9butants (1h30)","text":"<p>D\u00e9couvrez les outils qui vous permettront de cr\u00e9er des applications d'IA sans complexit\u00e9 excessive.</p> <ul> <li>Installation et configuration de base de TensorFlow/Keras</li> <li>Utilisation de mod\u00e8les pr\u00e9-entra\u00een\u00e9s pour des t\u00e2ches courantes</li> <li>Mini-projet : d\u00e9velopper une API de reconnaissance d'images simple</li> </ul>"},{"location":"seance3/#phase-2-amelioration-des-performances-1h30","title":"Phase 2 : Am\u00e9lioration des performances (1h30)","text":"<p>Apprenez \u00e0 am\u00e9liorer vos mod\u00e8les d'IA sans avoir besoin de connaissances math\u00e9matiques avanc\u00e9es.</p> <ul> <li>Techniques simples d'optimisation des performances</li> <li>Bonnes pratiques pour \u00e9viter les erreurs courantes</li> <li>TP pratique : am\u00e9liorer un mod\u00e8le existant pour une application web</li> </ul>"},{"location":"seance3/#phase-3-preparation-au-projet-final-45min","title":"Phase 3 : Pr\u00e9paration au projet final (45min)","text":"<p>Commencez \u00e0 d\u00e9velopper votre chatbot p\u00e9dagogique en utilisant des technologies accessibles.</p> <ul> <li>Pr\u00e9sentation d'un cahier des charges adapt\u00e9 au niveau BTS SIO</li> <li>\u00c9tude de cas r\u00e9els d'entreprises utilisant des chatbots similaires</li> <li>Premiers pas avec l'API Mistral AI pour cr\u00e9er un prototype fonctionnel</li> </ul>"},{"location":"seance3/#ressources-necessaires","title":"Ressources n\u00e9cessaires","text":"<p>Pour cette s\u00e9ance, vous aurez besoin de :</p> <ul> <li>Un ordinateur avec une connexion internet</li> <li>Un compte Google pour acc\u00e9der \u00e0 Google Colab (pas d'installation complexe requise)</li> <li>Les notebooks pr\u00e9par\u00e9s avec des exemples de code comment\u00e9s \u00e9tape par \u00e9tape</li> <li>Les templates de projets \u00e0 compl\u00e9ter (approche \"fill in the blanks\")</li> <li>Une cl\u00e9 API Mistral gratuite (instructions de cr\u00e9ation fournies)</li> </ul>"},{"location":"seance3/#livrables-attendus","title":"Livrables attendus","text":"<p>\u00c0 l'issue de cette s\u00e9ance, vous devrez avoir produit :</p> <ul> <li>Une API simple de reconnaissance d'images fonctionnelle (utilisable en contexte professionnel)</li> <li>Un mod\u00e8le am\u00e9lior\u00e9 pour une application web concr\u00e8te (document\u00e9 pour un transfert \u00e0 un coll\u00e8gue)</li> <li>Un document de conception pour votre chatbot avec les fonctionnalit\u00e9s essentielles prioris\u00e9es</li> </ul> <p>Ces livrables serviront de base pour la derni\u00e8re s\u00e9ance et sont \u00e0 d\u00e9poser dans l'espace pr\u00e9vu avant la date limite.</p>"},{"location":"seance3/#pret-a-commencer","title":"Pr\u00eat \u00e0 commencer ?","text":"<p>Commencez par la premi\u00e8re phase pour d\u00e9couvrir comment utiliser les frameworks de Deep Learning de mani\u00e8re accessible et orient\u00e9e projet.</p> <p>Commencer la Phase 1 ```</p>"},{"location":"seance3/partie1-frameworks-debutants/","title":"Phase 1 : Frameworks pour d\u00e9butants (1h30)","text":""},{"location":"seance3/partie1-frameworks-debutants/#introduction-aux-frameworks-dans-un-contexte-professionnel-15-min","title":"Introduction aux frameworks dans un contexte professionnel (15 min)","text":"<p>Objectif: Comprendre l'utilit\u00e9 des frameworks de Deep Learning pour un d\u00e9veloppeur en entreprise et identifier ceux qui sont r\u00e9ellement utilis\u00e9s sur le terrain.</p>"},{"location":"seance3/partie1-frameworks-debutants/#les-frameworks-en-entreprise","title":"Les frameworks en entreprise","text":"<p>Avant de plonger dans le code, prenons un moment pour comprendre pourquoi les frameworks de Deep Learning sont si importants en contexte professionnel:</p> <ul> <li>Productivit\u00e9: Ils permettent de d\u00e9velopper des applications d'IA sans repartir de z\u00e9ro</li> <li>Maintenabilit\u00e9: Code plus standard, plus facile \u00e0 comprendre par d'autres d\u00e9veloppeurs</li> <li>Performances: Optimisations int\u00e9gr\u00e9es qui seraient complexes \u00e0 d\u00e9velopper soi-m\u00eame</li> <li>D\u00e9ploiement: Outils int\u00e9gr\u00e9s pour mettre en production les mod\u00e8les</li> </ul> <p>Dans le monde professionnel actuel, plusieurs frameworks de Deep Learning sont couramment utilis\u00e9s:</p> Framework Principaux cas d'usage TensorFlow/Keras Applications web/mobile, syst\u00e8mes en production PyTorch Recherche, prototypage, startups Hugging Face NLP, chatbots, traitement de texte Scikit-learn Pr\u00e9traitement, ML classique, pipeline de donn\u00e9es <p>\"Pour un stage, la capacit\u00e9 \u00e0 utiliser efficacement des frameworks existants est recherch\u00e9e davantage que l'expertise th\u00e9orique approfondie en Deep Learning.\"  </p>"},{"location":"seance3/partie1-frameworks-debutants/#tensorflowkeras-la-solution-pragmatique","title":"TensorFlow/Keras: la solution pragmatique","text":"<p>Pour cette s\u00e9ance, nous allons nous concentrer sur TensorFlow/Keras pour plusieurs raisons:</p> <ol> <li>Interface simple: Keras offre une API haut niveau, parfaite pour d\u00e9buter</li> <li>D\u00e9ploiement facile: Solutions int\u00e9gr\u00e9es pour mettre en production (TF Serving, TFLite)</li> <li>Documentation riche: Ressources abondantes en fran\u00e7ais</li> <li>Mod\u00e8les pr\u00e9-entra\u00een\u00e9s: Large biblioth\u00e8que de mod\u00e8les pr\u00eats \u00e0 l'emploi</li> <li>Demande professionnelle: Le plus mentionn\u00e9 dans les offres de stage</li> </ol>"},{"location":"seance3/partie1-frameworks-debutants/#demonstration-applications-reelles-en-entreprise","title":"D\u00e9monstration: Applications r\u00e9elles en entreprise","text":"<p>Voici quelques exemples concrets d\u00e9velopp\u00e9s par des entreprises locales employant des anciens \u00e9tudiants:</p> <ul> <li>PME de logistique: Application de reconnaissance de documents (bons de livraison, factures) permettant d'automatiser la saisie \u2192 \u00c9conomie de 15h/semaine</li> <li>Agence web: Syst\u00e8me de d\u00e9tection de contenu inappropri\u00e9 dans les commentaires de sites e-commerce</li> <li>Cabinet m\u00e9dical: Application de classification d'images pour le tri pr\u00e9liminaire de photos de l\u00e9sions cutan\u00e9es</li> </ul>"},{"location":"seance3/partie1-frameworks-debutants/#conclusion-et-transition","title":"Conclusion et transition","text":"<p>Cette introduction vous a permis de comprendre l'importance des frameworks de Deep Learning en contexte professionnel. Dans la prochaine partie, nous allons mettre en pratique ces connaissances en r\u00e9alisant un atelier sur la prise en main de TensorFlow/Keras.</p> <p>Passer \u00e0 la Phase 2</p>"},{"location":"seance3/partie2-atelier-tensorflow/","title":"Phase 2 : Atelier pratique TensorFlow/Keras (40 min)","text":""},{"location":"seance3/partie2-atelier-tensorflow/#objectif","title":"Objectif","text":"<p>D\u00e9couvrir par la pratique l'utilisation de TensorFlow/Keras pour cr\u00e9er rapidement un mod\u00e8le fonctionnel.</p>"},{"location":"seance3/partie2-atelier-tensorflow/#installation-et-configuration-5-min","title":"Installation et configuration (5 min)","text":"<p>Bonne nouvelle: nous allons utiliser Google Colab, donc aucune installation n\u00e9cessaire!</p> <ol> <li>Ouvrez le notebook \"TensorFlow/Keras pour d\u00e9butants\" dans Google Colab</li> <li>Le lien est disponible ici: tensorflow-keras-debutants.ipynb</li> <li>Tout est pr\u00e9configur\u00e9, il suffit d'ex\u00e9cuter les cellules</li> </ol> <p>Si vous souhaitez installer localement plus tard: <pre><code># Avec pip\npip install tensorflow\n\n# V\u00e9rification\nimport tensorflow as tf\nprint(tf.__version__)\n</code></pre></p>"},{"location":"seance3/partie2-atelier-tensorflow/#structure-dun-projet-tensorflowkeras-10-min","title":"Structure d'un projet TensorFlow/Keras (10 min)","text":"<p>Explorons ensemble la structure typique d'un projet avec TensorFlow/Keras:</p> <ol> <li> <p>Importation des biblioth\u00e8ques <pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre></p> </li> <li> <p>Pr\u00e9paration des donn\u00e9es <pre><code># Chargement de donn\u00e9es (ex: MNIST - chiffres manuscrits)\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n\n# Normalisation (ramener les valeurs entre 0 et 1)\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n</code></pre></p> </li> <li> <p>Cr\u00e9ation du mod\u00e8le <pre><code>model = models.Sequential([\n    layers.Flatten(input_shape=(28, 28)),      # Aplatit l'image 28x28 en un vecteur\n    layers.Dense(128, activation='relu'),      # Couche cach\u00e9e avec 128 neurones\n    layers.Dense(10, activation='softmax')     # Couche de sortie (10 chiffres)\n])\n</code></pre></p> </li> <li> <p>Compilation du mod\u00e8le <pre><code>model.compile(\n    optimizer='adam',                           # Algorithme d'optimisation\n    loss='sparse_categorical_crossentropy',     # Fonction de perte\n    metrics=['accuracy']                        # M\u00e9trique \u00e0 suivre\n)\n</code></pre></p> </li> <li> <p>Entra\u00eenement <pre><code>model.fit(\n    train_images, \n    train_labels, \n    epochs=5,                  # Nombre de passages sur les donn\u00e9es\n    batch_size=32,             # Taille des lots\n    validation_split=0.2       # 20% des donn\u00e9es pour la validation\n)\n</code></pre></p> </li> <li> <p>\u00c9valuation <pre><code>test_loss, test_acc = model.evaluate(test_images, test_labels)\nprint(f'Pr\u00e9cision sur donn\u00e9es de test: {test_acc:.2f}')\n</code></pre></p> </li> <li> <p>Pr\u00e9diction et utilisation <pre><code>predictions = model.predict(test_images[:5])\nprint(\"Pr\u00e9dictions:\", np.argmax(predictions, axis=1))\nprint(\"Valeurs r\u00e9elles:\", test_labels[:5])\n</code></pre></p> </li> <li> <p>Sauvegarde du mod\u00e8le <pre><code>model.save('mon_modele.h5')  # Format HDF5\n# ou\nmodel.save('dossier_modele')  # Format SavedModel (pour TF Serving)\n</code></pre></p> </li> </ol>"},{"location":"seance3/partie2-atelier-tensorflow/#exercice-guide-creer-votre-premier-modele-25-min","title":"Exercice guid\u00e9: Cr\u00e9er votre premier mod\u00e8le (25 min)","text":"<p>Suivez les instructions dans le notebook Colab pour:</p> <ol> <li> <p>Explorer le jeu de donn\u00e9es Fashion MNIST</p> <ul> <li>10 cat\u00e9gories de v\u00eatements (t-shirts, pantalons, etc.)</li> <li>Images 28x28 en niveaux de gris</li> <li>Parfait pour commencer avec la classification d'images</li> </ul> </li> <li> <p>Construire un mod\u00e8le simple</p> <ul> <li>Mod\u00e8le s\u00e9quentiel avec 2-3 couches</li> <li>Visualiser l'architecture avec <code>model.summary()</code></li> </ul> </li> <li> <p>Entra\u00eener et \u00e9valuer le mod\u00e8le</p> <ul> <li>Observer les courbes d'apprentissage (accuracy, loss)</li> <li>Comprendre le ph\u00e9nom\u00e8ne de surapprentissage</li> </ul> </li> <li> <p>Faire des pr\u00e9dictions</p> <ul> <li>Tester le mod\u00e8le sur de nouvelles images</li> <li>Visualiser les r\u00e9sultats avec des graphiques</li> </ul> </li> <li> <p>Modifier le mod\u00e8le pour l'am\u00e9liorer</p> <ul> <li>Ajouter/supprimer des couches</li> <li>Changer le nombre de neurones</li> <li>Observer l'impact sur les performances</li> </ul> </li> </ol>"},{"location":"seance3/partie2-atelier-tensorflow/#conclusion","title":"Conclusion","text":"<p>Cet atelier vous a permis de prendre en main TensorFlow/Keras et de cr\u00e9er votre premier mod\u00e8le de Deep Learning. Dans la prochaine phase, nous allons voir comment transformer ce mod\u00e8le en une API utilisable dans un projet r\u00e9el.</p> <p>Passer \u00e0 la Phase 3</p>"},{"location":"seance3/partie3-api-reconnaissance/","title":"Phase 4 : Am\u00e9lioration des performances (1h30)","text":""},{"location":"seance3/partie3-api-reconnaissance/#introduction-a-loptimisation-pratique-des-modeles-15-min","title":"Introduction \u00e0 l'optimisation pratique des mod\u00e8les (15 min)","text":"<p>Objectif : Comprendre les approches simples et efficaces pour am\u00e9liorer les performances d'un mod\u00e8le de Deep Learning sans expertise math\u00e9matique approfondie.</p>"},{"location":"seance3/partie3-api-reconnaissance/#au-dela-de-larchitecture-facteurs-cles-de-performance","title":"Au-del\u00e0 de l'architecture : facteurs cl\u00e9s de performance","text":"<p>Les performances d'un mod\u00e8le de Deep Learning ne d\u00e9pendent pas seulement de son architecture, mais aussi de nombreux autres facteurs que nous pouvons optimiser :</p> <ol> <li>Qualit\u00e9 des donn\u00e9es : Souvent plus importante que la complexit\u00e9 du mod\u00e8le</li> <li>Pr\u00e9traitement appropri\u00e9 : Normalisation, augmentation de donn\u00e9es, etc.</li> <li>Hyperparam\u00e8tres : Taux d'apprentissage, taille de batch, etc.</li> <li>Technique d'entra\u00eenement : R\u00e9gularisation, early stopping, etc.</li> </ol>"},{"location":"seance3/partie3-api-reconnaissance/#les-erreurs-les-plus-courantes-en-deep-learning","title":"Les erreurs les plus courantes en Deep Learning","text":"<p>Avant d'am\u00e9liorer, il faut comprendre ce qui peut poser probl\u00e8me :</p> Erreur courante Sympt\u00f4mes Impact Donn\u00e9es non normalis\u00e9es Apprentissage lent ou instable Convergence difficile Surapprentissage (overfitting) Bonnes performances sur donn\u00e9es d'entra\u00eenement, mauvaises sur donn\u00e9es de test Mauvaise g\u00e9n\u00e9ralisation Sous-apprentissage (underfitting) Mauvaises performances partout Capacit\u00e9 insuffisante Fuite de donn\u00e9es (data leakage) Performances irr\u00e9alistes Mod\u00e8le inutilisable en production Mauvaise division train/test \u00c9valuation incorrecte Fausse confiance dans le mod\u00e8le"},{"location":"seance3/partie3-api-reconnaissance/#approche-pragmatique-de-lamelioration","title":"Approche pragmatique de l'am\u00e9lioration","text":"<p>Nous allons suivre une approche m\u00e9thodique pour am\u00e9liorer nos mod\u00e8les :</p> <ol> <li>Diagnostiquer les probl\u00e8mes : Identifier ce qui limite les performances</li> <li>Appliquer des solutions cibl\u00e9es : Choisir les techniques adapt\u00e9es au probl\u00e8me identifi\u00e9</li> <li>\u00c9valuer correctement : Mesurer l'impact des modifications de mani\u00e8re objective</li> <li>It\u00e9rer rapidement : Tester, apprendre, ajuster, recommencer</li> </ol>"},{"location":"seance3/partie3-api-reconnaissance/#atelier-diagnostiquer-les-problemes-de-performance-30-min","title":"Atelier : Diagnostiquer les probl\u00e8mes de performance (30 min)","text":"<p>Objectif : Apprendre \u00e0 identifier et diagnostiquer les probl\u00e8mes courants dans les mod\u00e8les de Deep Learning.</p> <p>Mat\u00e9riel et ressources : * Notebook Colab \"Diagnostic des mod\u00e8les\" * Mod\u00e8les pr\u00e9-configur\u00e9s avec diff\u00e9rents probl\u00e8mes</p>"},{"location":"seance3/partie3-api-reconnaissance/#instructions","title":"Instructions","text":"<p>Partie 1 : Reconna\u00eetre les signes de surapprentissage (15 min)</p> <ol> <li>Ouvrez le doc Diagnostic des mod\u00e8les dans Google Colab.</li> <li>Acc\u00e9dez \u00e0 la section \"Cas 1 : Surapprentissage\".</li> <li>Ex\u00e9cutez les cellules pour entra\u00eener le mod\u00e8le et afficher les courbes d'apprentissage.</li> <li>Observez les signes caract\u00e9ristiques du surapprentissage :</li> <li>\u00c9cart grandissant entre la performance sur donn\u00e9es d'entra\u00eenement et de validation</li> <li>Courbe de perte de validation qui remonte apr\u00e8s avoir diminu\u00e9</li> <li> <p>Performances parfaites sur l'entra\u00eenement mais m\u00e9diocres sur la validation</p> </li> <li> <p>Appliquez ces techniques pour r\u00e9soudre le probl\u00e8me :</p> </li> <li>Ajout de dropout (d\u00e9sactivation al\u00e9atoire de neurones)</li> <li>R\u00e9gularisation L2 (weight decay)</li> <li>Augmentation de donn\u00e9es</li> <li> <p>Early stopping (arr\u00eat anticip\u00e9)</p> </li> <li> <p>Pour chaque technique, notez l'impact sur les courbes d'apprentissage et les performances.</p> </li> </ol> <p>Partie 2 : D\u00e9tecter les probl\u00e8mes de donn\u00e9es et de pr\u00e9traitement (15 min)</p> <ol> <li>Passez \u00e0 la section \"Cas 2 : Probl\u00e8mes de donn\u00e9es\".</li> <li>Analysez les visualisations des donn\u00e9es d'entr\u00e9e.</li> <li>Identifiez les probl\u00e8mes potentiels :</li> <li>Donn\u00e9es non normalis\u00e9es (valeurs extr\u00eames)</li> <li>Distributions diff\u00e9rentes entre entra\u00eenement et test</li> <li> <p>Classes d\u00e9s\u00e9quilibr\u00e9es</p> </li> <li> <p>Appliquez ces corrections :</p> </li> <li>Normalisation appropri\u00e9e (Z-score, min-max, etc.)</li> <li>R\u00e9\u00e9chantillonnage pour \u00e9quilibrer les classes</li> <li> <p>Stratification de la division train/test</p> </li> <li> <p>Observez comment ces corrections am\u00e9liorent l'entra\u00eenement et les performances du mod\u00e8le.</p> </li> </ol>"},{"location":"seance3/partie3-api-reconnaissance/#tp-pratique-ameliorer-un-modele-pour-une-application-web-40-min","title":"TP pratique : Am\u00e9liorer un mod\u00e8le pour une application web (40 min)","text":"<p>Objectif : Appliquer les techniques d'am\u00e9lioration \u00e0 un mod\u00e8le existant pour une application web r\u00e9elle.</p> <p>Mat\u00e9riel et ressources :  * Notebook \"Am\u00e9lioration pour application web\"  * Application web simple de reconnaissance d'objets  * Mod\u00e8le de base fonctionnel mais limit\u00e9</p>"},{"location":"seance3/partie3-api-reconnaissance/#instructions_1","title":"Instructions","text":"<p>Partie 1 : Analyse du mod\u00e8le existant (10 min)</p> <ol> <li>Ouvrez le notebook \"Am\u00e9lioration pour application web\".</li> <li>Ex\u00e9cutez les cellules qui chargent et \u00e9valuent le mod\u00e8le existant.</li> <li> <p>Analysez ses performances et limitations :</p> <ul> <li>Pr\u00e9cision globale et par classe</li> <li>Matrice de confusion</li> <li>Temps d'inf\u00e9rence</li> <li>Robustesse aux variations (rotation, luminosit\u00e9, etc.)</li> </ul> </li> <li> <p>Documentez au moins trois probl\u00e8mes sp\u00e9cifiques \u00e0 r\u00e9soudre.</p> </li> </ol> <p>Partie 2 : Am\u00e9lioration des performances (15 min)</p> <p>Choisissez et impl\u00e9mentez au moins trois techniques d'am\u00e9lioration parmi :</p> <ol> <li> <p>Pr\u00e9traitement am\u00e9lior\u00e9 :</p> <ul> <li>Normalisation adapt\u00e9e au dataset</li> <li>D\u00e9tection et recadrage automatique des objets</li> <li>Correction de luminosit\u00e9 et de contraste</li> </ul> </li> <li> <p>Augmentation de donn\u00e9es :</p> <ul> <li>Rotations et translations</li> <li>Modifications de luminosit\u00e9 et contraste</li> <li>Zoom et recadrage al\u00e9atoire</li> </ul> </li> <li> <p>Optimisation du mod\u00e8le :</p> <ul> <li>Ajustement de l'architecture (nombre de couches, neurones)</li> <li>Techniques de r\u00e9gularisation (dropout, batch normalization)</li> <li>Transfer learning avec un mod\u00e8le pr\u00e9-entra\u00een\u00e9</li> </ul> </li> <li> <p>Optimisation du temps d'inf\u00e9rence :</p> <ul> <li>Quantification du mod\u00e8le (r\u00e9duction de pr\u00e9cision)</li> <li>\u00c9lagage (pruning) des connexions peu importantes</li> <li>Optimisation pour CPU ou mobile</li> </ul> </li> </ol> <p>Partie 3 : Tests et documentation (15 min)</p> <ol> <li>\u00c9valuez le mod\u00e8le am\u00e9lior\u00e9 sur le jeu de test.</li> <li> <p>Comparez les performances avec le mod\u00e8le original :</p> <ul> <li>Pr\u00e9cision globale (avant vs apr\u00e8s)</li> <li>Temps d'inf\u00e9rence (avant vs apr\u00e8s)</li> <li>Taille du mod\u00e8le (avant vs apr\u00e8s)</li> <li>Robustesse aux variations</li> </ul> </li> <li> <p>Cr\u00e9ez un rapport concis au format suivant :</p> </li> </ol> <pre><code># Rapport d'am\u00e9lioration du mod\u00e8le\n\n## Probl\u00e8mes identifi\u00e9s\n1. [Probl\u00e8me 1]\n2. [Probl\u00e8me 2]\n3. [Probl\u00e8me 3]\n\n## Solutions impl\u00e9ment\u00e9es\n1. [Solution 1] \u2192 Impact: [r\u00e9sultat]\n2. [Solution 2] \u2192 Impact: [r\u00e9sultat]\n3. [Solution 3] \u2192 Impact: [r\u00e9sultat]\n\n## Performances compar\u00e9es\n| M\u00e9trique | Avant | Apr\u00e8s | Am\u00e9lioration |\n|----------|-------|-------|--------------|\n| Pr\u00e9cision | x% | y% | +z% |\n| Temps d'inf\u00e9rence | x ms | y ms | -z% |\n| Taille du mod\u00e8le | x MB | y MB | -z% |\n\n## Conclusion\n[R\u00e9sum\u00e9 des am\u00e9liorations et recommandations]\n</code></pre> <ol> <li>Exportez le mod\u00e8le am\u00e9lior\u00e9 pour l'int\u00e9gration dans l'application web.</li> </ol>"},{"location":"seance3/partie3-api-reconnaissance/#bonnes-pratiques-pour-loptimisation-en-production-5-min","title":"Bonnes pratiques pour l'optimisation en production (5 min)","text":""},{"location":"seance3/partie3-api-reconnaissance/#checklist-doptimisation-pour-vos-futurs-projets","title":"Checklist d'optimisation pour vos futurs projets","text":"<p>\u2705 Donn\u00e9es :  - Normalisation adapt\u00e9e au probl\u00e8me  - Augmentation pertinente (pas excessive)  - Division train/validation/test stratifi\u00e9e  - V\u00e9rification de la qualit\u00e9 des donn\u00e9es (valeurs manquantes, erreurs)</p> <p>\u2705 Mod\u00e8le :  - Architecture adapt\u00e9e \u00e0 la complexit\u00e9 du probl\u00e8me  - R\u00e9gularisation appropri\u00e9e (dropout, L1/L2)  - Batch normalization pour stabiliser l'entra\u00eenement  - Learning rate adaptatif (Adam, RMSprop)</p> <p>\u2705 Entra\u00eenement :  - Early stopping avec patience appropri\u00e9e  - R\u00e9duction du learning rate sur plateau  - Validation crois\u00e9e pour les petits datasets  - Sauvegarde du meilleur mod\u00e8le, pas forc\u00e9ment le dernier</p> <p>\u2705 D\u00e9ploiement :  - Quantification pour r\u00e9duire la taille (TFLite, ONNX)  - Tests sur les plateformes cibles  - Monitoring des performances  - Re-entra\u00eenement p\u00e9riodique avec de nouvelles donn\u00e9es</p>"},{"location":"seance3/partie3-api-reconnaissance/#conclusion-et-transition-vers-la-phase-finale-5-min","title":"Conclusion et transition vers la phase finale (5 min)","text":"<p>Ce que vous avez appris :     - Identifier les probl\u00e8mes limitant les performances d'un mod\u00e8le     - Appliquer des techniques cibl\u00e9es pour r\u00e9soudre ces probl\u00e8mes     - \u00c9valuer objectivement l'impact des am\u00e9liorations     - Optimiser un mod\u00e8le pour un d\u00e9ploiement en production</p> <p>Prochaine \u00e9tape : Pr\u00e9parer le d\u00e9veloppement de votre chatbot p\u00e9dagogique en appliquant ces techniques d'optimisation.</p> <p>Passer \u00e0 la Phase 5</p>"},{"location":"seance3/partie4-amelioration/","title":"Phase 2 : Am\u00e9lioration des performances (1h30)","text":""},{"location":"seance3/partie4-amelioration/#introduction-a-loptimisation-pratique-des-modeles-15-min","title":"Introduction \u00e0 l'optimisation pratique des mod\u00e8les (15 min)","text":"<p>Objectif : Comprendre les approches simples et efficaces pour am\u00e9liorer les performances d'un mod\u00e8le de Deep Learning sans expertise math\u00e9matique approfondie.</p>"},{"location":"seance3/partie4-amelioration/#au-dela-de-larchitecture-facteurs-cles-de-performance","title":"Au-del\u00e0 de l'architecture : facteurs cl\u00e9s de performance","text":"<p>Les performances d'un mod\u00e8le de Deep Learning ne d\u00e9pendent pas seulement de son architecture, mais aussi de nombreux autres facteurs que nous pouvons optimiser :</p> <ol> <li>Qualit\u00e9 des donn\u00e9es : Souvent plus importante que la complexit\u00e9 du mod\u00e8le</li> <li>Pr\u00e9traitement appropri\u00e9 : Normalisation, augmentation de donn\u00e9es, etc.</li> <li>Hyperparam\u00e8tres : Taux d'apprentissage, taille de batch, etc.</li> <li>Technique d'entra\u00eenement : R\u00e9gularisation, early stopping, etc.</li> </ol>"},{"location":"seance3/partie4-amelioration/#les-erreurs-les-plus-courantes-en-deep-learning","title":"Les erreurs les plus courantes en Deep Learning","text":"<p>Avant d'am\u00e9liorer, il faut comprendre ce qui peut poser probl\u00e8me :</p> Erreur courante Sympt\u00f4mes Impact Donn\u00e9es non normalis\u00e9es Apprentissage lent ou instable Convergence difficile Surapprentissage (overfitting) Bonnes performances sur donn\u00e9es d'entra\u00eenement, mauvaises sur donn\u00e9es de test Mauvaise g\u00e9n\u00e9ralisation Sous-apprentissage (underfitting) Mauvaises performances partout Capacit\u00e9 insuffisante Fuite de donn\u00e9es (data leakage) Performances irr\u00e9alistes Mod\u00e8le inutilisable en production Mauvaise division train/test \u00c9valuation incorrecte Fausse confiance dans le mod\u00e8le"},{"location":"seance3/partie4-amelioration/#approche-pragmatique-de-lamelioration","title":"Approche pragmatique de l'am\u00e9lioration","text":"<p>Nous allons suivre une approche m\u00e9thodique pour am\u00e9liorer nos mod\u00e8les :</p> <ol> <li>Diagnostiquer les probl\u00e8mes : Identifier ce qui limite les performances</li> <li>Appliquer des solutions cibl\u00e9es : Choisir les techniques adapt\u00e9es au probl\u00e8me identifi\u00e9</li> <li>\u00c9valuer correctement : Mesurer l'impact des modifications de mani\u00e8re objective</li> <li>It\u00e9rer rapidement : Tester, apprendre, ajuster, recommencer</li> </ol>"},{"location":"seance3/partie4-amelioration/#atelier-diagnostiquer-les-problemes-de-performance-30-min","title":"Atelier : Diagnostiquer les probl\u00e8mes de performance (30 min)","text":"<p>Objectif : Apprendre \u00e0 identifier et diagnostiquer les probl\u00e8mes courants dans les mod\u00e8les de Deep Learning.</p> <p>Mat\u00e9riel et ressources : * Notebook Colab \"Diagnostic des mod\u00e8les\" * Mod\u00e8les pr\u00e9-configur\u00e9s avec diff\u00e9rents probl\u00e8mes</p>"},{"location":"seance3/partie4-amelioration/#instructions","title":"Instructions","text":"<p>Partie 1 : Reconna\u00eetre les signes de surapprentissage (15 min)</p> <ol> <li>Ouvrez le doc Diagnostic des mod\u00e8les dans Google Colab.</li> <li>Acc\u00e9dez \u00e0 la section \"Cas 1 : Surapprentissage\".</li> <li>Ex\u00e9cutez les cellules pour entra\u00eener le mod\u00e8le et afficher les courbes d'apprentissage.</li> <li>Observez les signes caract\u00e9ristiques du surapprentissage :</li> <li>\u00c9cart grandissant entre la performance sur donn\u00e9es d'entra\u00eenement et de validation</li> <li>Courbe de perte de validation qui remonte apr\u00e8s avoir diminu\u00e9</li> <li> <p>Performances parfaites sur l'entra\u00eenement mais m\u00e9diocres sur la validation</p> </li> <li> <p>Appliquez ces techniques pour r\u00e9soudre le probl\u00e8me :</p> </li> <li>Ajout de dropout (d\u00e9sactivation al\u00e9atoire de neurones)</li> <li>R\u00e9gularisation L2 (weight decay)</li> <li>Augmentation de donn\u00e9es</li> <li> <p>Early stopping (arr\u00eat anticip\u00e9)</p> </li> <li> <p>Pour chaque technique, notez l'impact sur les courbes d'apprentissage et les performances.</p> </li> </ol> <p>Partie 2 : D\u00e9tecter les probl\u00e8mes de donn\u00e9es et de pr\u00e9traitement (15 min)</p> <ol> <li>Passez \u00e0 la section \"Cas 2 : Probl\u00e8mes de donn\u00e9es\".</li> <li>Analysez les visualisations des donn\u00e9es d'entr\u00e9e.</li> <li>Identifiez les probl\u00e8mes potentiels :</li> <li>Donn\u00e9es non normalis\u00e9es (valeurs extr\u00eames)</li> <li>Distributions diff\u00e9rentes entre entra\u00eenement et test</li> <li> <p>Classes d\u00e9s\u00e9quilibr\u00e9es</p> </li> <li> <p>Appliquez ces corrections :</p> </li> <li>Normalisation appropri\u00e9e (Z-score, min-max, etc.)</li> <li>R\u00e9\u00e9chantillonnage pour \u00e9quilibrer les classes</li> <li> <p>Stratification de la division train/test</p> </li> <li> <p>Observez comment ces corrections am\u00e9liorent l'entra\u00eenement et les performances du mod\u00e8le.</p> </li> </ol>"},{"location":"seance3/partie4-amelioration/#tp-pratique-ameliorer-un-modele-pour-une-application-web-40-min","title":"TP pratique : Am\u00e9liorer un mod\u00e8le pour une application web (40 min)","text":"<p>Objectif : Appliquer les techniques d'am\u00e9lioration \u00e0 un mod\u00e8le existant pour une application web r\u00e9elle.</p> <p>Mat\u00e9riel et ressources :  * Notebook \"Am\u00e9lioration pour application web\"  * Application web simple de reconnaissance d'objets  * Mod\u00e8le de base fonctionnel mais limit\u00e9</p>"},{"location":"seance3/partie4-amelioration/#instructions_1","title":"Instructions","text":"<p>Partie 1 : Analyse du mod\u00e8le existant (10 min)</p> <ol> <li>Ouvrez le notebook \"Am\u00e9lioration pour application web\".</li> <li>Ex\u00e9cutez les cellules qui chargent et \u00e9valuent le mod\u00e8le existant.</li> <li> <p>Analysez ses performances et limitations :</p> <ul> <li>Pr\u00e9cision globale et par classe</li> <li>Matrice de confusion</li> <li>Temps d'inf\u00e9rence</li> <li>Robustesse aux variations (rotation, luminosit\u00e9, etc.)</li> </ul> </li> <li> <p>Documentez au moins trois probl\u00e8mes sp\u00e9cifiques \u00e0 r\u00e9soudre.</p> </li> </ol> <p>Partie 2 : Am\u00e9lioration des performances (15 min)</p> <p>Choisissez et impl\u00e9mentez au moins trois techniques d'am\u00e9lioration parmi :</p> <ol> <li> <p>Pr\u00e9traitement am\u00e9lior\u00e9 :</p> <ul> <li>Normalisation adapt\u00e9e au dataset</li> <li>D\u00e9tection et recadrage automatique des objets</li> <li>Correction de luminosit\u00e9 et de contraste</li> </ul> </li> <li> <p>Augmentation de donn\u00e9es :</p> <ul> <li>Rotations et translations</li> <li>Modifications de luminosit\u00e9 et contraste</li> <li>Zoom et recadrage al\u00e9atoire</li> </ul> </li> <li> <p>Optimisation du mod\u00e8le :</p> <ul> <li>Ajustement de l'architecture (nombre de couches, neurones)</li> <li>Techniques de r\u00e9gularisation (dropout, batch normalization)</li> <li>Transfer learning avec un mod\u00e8le pr\u00e9-entra\u00een\u00e9</li> </ul> </li> <li> <p>Optimisation du temps d'inf\u00e9rence :</p> <ul> <li>Quantification du mod\u00e8le (r\u00e9duction de pr\u00e9cision)</li> <li>\u00c9lagage (pruning) des connexions peu importantes</li> <li>Optimisation pour CPU ou mobile</li> </ul> </li> </ol> <p>Partie 3 : Tests et documentation (15 min)</p> <ol> <li>\u00c9valuez le mod\u00e8le am\u00e9lior\u00e9 sur le jeu de test.</li> <li> <p>Comparez les performances avec le mod\u00e8le original :</p> <ul> <li>Pr\u00e9cision globale (avant vs apr\u00e8s)</li> <li>Temps d'inf\u00e9rence (avant vs apr\u00e8s)</li> <li>Taille du mod\u00e8le (avant vs apr\u00e8s)</li> <li>Robustesse aux variations</li> </ul> </li> <li> <p>Cr\u00e9ez un rapport concis au format suivant :</p> </li> </ol> <pre><code># Rapport d'am\u00e9lioration du mod\u00e8le\n\n## Probl\u00e8mes identifi\u00e9s\n1. [Probl\u00e8me 1]\n2. [Probl\u00e8me 2]\n3. [Probl\u00e8me 3]\n\n## Solutions impl\u00e9ment\u00e9es\n1. [Solution 1] \u2192 Impact: [r\u00e9sultat]\n2. [Solution 2] \u2192 Impact: [r\u00e9sultat]\n3. [Solution 3] \u2192 Impact: [r\u00e9sultat]\n\n## Performances compar\u00e9es\n| M\u00e9trique | Avant | Apr\u00e8s | Am\u00e9lioration |\n|----------|-------|-------|--------------|\n| Pr\u00e9cision | x% | y% | +z% |\n| Temps d'inf\u00e9rence | x ms | y ms | -z% |\n| Taille du mod\u00e8le | x MB | y MB | -z% |\n\n## Conclusion\n[R\u00e9sum\u00e9 des am\u00e9liorations et recommandations]\n</code></pre> <ol> <li>Exportez le mod\u00e8le am\u00e9lior\u00e9 pour l'int\u00e9gration dans l'application web.</li> </ol>"},{"location":"seance3/partie4-amelioration/#bonnes-pratiques-pour-loptimisation-en-production-5-min","title":"Bonnes pratiques pour l'optimisation en production (5 min)","text":""},{"location":"seance3/partie4-amelioration/#checklist-doptimisation-pour-vos-futurs-projets","title":"Checklist d'optimisation pour vos futurs projets","text":"<p>\u2705 Donn\u00e9es :  - Normalisation adapt\u00e9e au probl\u00e8me  - Augmentation pertinente (pas excessive)  - Division train/validation/test stratifi\u00e9e  - V\u00e9rification de la qualit\u00e9 des donn\u00e9es (valeurs manquantes, erreurs)</p> <p>\u2705 Mod\u00e8le :  - Architecture adapt\u00e9e \u00e0 la complexit\u00e9 du probl\u00e8me  - R\u00e9gularisation appropri\u00e9e (dropout, L1/L2)  - Batch normalization pour stabiliser l'entra\u00eenement  - Learning rate adaptatif (Adam, RMSprop)</p> <p>\u2705 Entra\u00eenement :  - Early stopping avec patience appropri\u00e9e  - R\u00e9duction du learning rate sur plateau  - Validation crois\u00e9e pour les petits datasets  - Sauvegarde du meilleur mod\u00e8le, pas forc\u00e9ment le dernier</p> <p>\u2705 D\u00e9ploiement :  - Quantification pour r\u00e9duire la taille (TFLite, ONNX)  - Tests sur les plateformes cibles  - Monitoring des performances  - Re-entra\u00eenement p\u00e9riodique avec de nouvelles donn\u00e9es</p>"},{"location":"seance3/partie4-amelioration/#conclusion-et-transition-vers-la-phase-finale-5-min","title":"Conclusion et transition vers la phase finale (5 min)","text":"<p>Ce que vous avez appris :     - Identifier les probl\u00e8mes limitant les performances d'un mod\u00e8le     - Appliquer des techniques cibl\u00e9es pour r\u00e9soudre ces probl\u00e8mes     - \u00c9valuer objectivement l'impact des am\u00e9liorations     - Optimiser un mod\u00e8le pour un d\u00e9ploiement en production</p> <p>Prochaine \u00e9tape : Pr\u00e9parer le d\u00e9veloppement de votre chatbot p\u00e9dagogique en appliquant ces techniques d'optimisation.</p> <p>Passer \u00e0 la Phase 3</p>"},{"location":"seance3/partie5-preparation-projet/","title":"Phase 3 : Pr\u00e9paration au projet final (45min)","text":""},{"location":"seance3/partie5-preparation-projet/#presentation-du-projet-chatbot-pedagogique-15-min","title":"Pr\u00e9sentation du projet chatbot p\u00e9dagogique (15 min)","text":"<p>Objectif : Comprendre les attentes concr\u00e8tes pour le projet final et voir comment les technologies vues jusqu'\u00e0 pr\u00e9sent s'int\u00e8grent dans ce contexte.</p>"},{"location":"seance3/partie5-preparation-projet/#vue-densemble-du-projet","title":"Vue d'ensemble du projet","text":"<p>Le projet consiste \u00e0 d\u00e9velopper un chatbot p\u00e9dagogique capable d'expliquer les concepts du Deep Learning \u00e0 des \u00e9tudiants de BTS SIO.</p> <p></p> <p>Les composants principaux du projet sont :</p> <ol> <li>Interface conversationnelle : Interface simple permettant de dialoguer avec le chatbot</li> <li>API Mistral AI : Moteur de g\u00e9n\u00e9ration de r\u00e9ponses intelligentes</li> <li>Base de connaissances : Structure contenant les informations sur le Deep Learning</li> <li>Syst\u00e8me de gestion de contexte : Maintien de la coh\u00e9rence dans les conversations</li> </ol>"},{"location":"seance3/partie5-preparation-projet/#objectifs-pedagogiques-du-projet","title":"Objectifs p\u00e9dagogiques du projet","text":"<p>Ce projet vous permettra de :</p> <ul> <li>Appliquer vos connaissances en frameworks IA dans un contexte concret</li> <li>D\u00e9velopper des comp\u00e9tences pratiques en int\u00e9gration d'API</li> <li>Cr\u00e9er une application web avec fonctionnalit\u00e9s d'IA</li> <li>Structurer des connaissances pour un usage p\u00e9dagogique</li> </ul>"},{"location":"seance3/partie5-preparation-projet/#exemple-de-chatbot-fonctionnel","title":"Exemple de chatbot fonctionnel","text":"<p>D\u00e9monstration d'un exemple de chatbot similaire \u00e0 celui que vous d\u00e9velopperez, montrant :</p> <ul> <li>La qualit\u00e9 des r\u00e9ponses attendues</li> <li>La structure de l'interface</li> <li>Les fonctionnalit\u00e9s de base</li> </ul>"},{"location":"seance3/partie5-preparation-projet/#niveau-de-complexite-attendu","title":"Niveau de complexit\u00e9 attendu","text":"<p>Ce projet est intentionnellement dimensionn\u00e9 pour \u00eatre r\u00e9alisable avec vos comp\u00e9tences actuelles :</p> Aspect Niveau attendu Remarque Interface Simple mais fonctionnelle Pas besoin de design complexe Base de connaissances Couvrant les concepts principaux Qualit\u00e9 &gt; Quantit\u00e9 Int\u00e9gration API Fonctionnelle avec gestion d'erreurs Pas besoin d'optimisations avanc\u00e9es Fonctionnalit\u00e9s 3-4 fonctionnalit\u00e9s bien impl\u00e9ment\u00e9es Mieux vaut peu mais bien"},{"location":"seance3/partie5-preparation-projet/#etude-de-cas-reels-dutilisation-des-chatbots-15-min","title":"\u00c9tude de cas r\u00e9els d'utilisation des chatbots (15 min)","text":"<p>Objectif : Comprendre comment les chatbots sont utilis\u00e9s en entreprise pour mieux orienter votre projet.</p>"},{"location":"seance3/partie5-preparation-projet/#cas-detude-1-chatbot-dassistance-technique","title":"Cas d'\u00e9tude 1 : Chatbot d'assistance technique","text":"<p>Entreprise : PME de services informatiques (20 employ\u00e9s) Probl\u00e9matique : Support technique de premier niveau surcharg\u00e9</p> <p>Solution mise en place : - Chatbot capable de g\u00e9rer les questions fr\u00e9quentes - Base de connaissances structur\u00e9e par cat\u00e9gories de probl\u00e8mes - Redirection vers un technicien humain pour les cas complexes - Disponible 24/7, r\u00e9duisant les temps d'attente</p> <p>R\u00e9sultats : - 45% des demandes trait\u00e9es sans intervention humaine - Temps de r\u00e9ponse initial r\u00e9duit de 4h \u00e0 5min - Meilleure satisfaction client - Techniciens concentr\u00e9s sur des probl\u00e8mes \u00e0 plus forte valeur ajout\u00e9e</p> <p>Technologies utilis\u00e9es : - API LLM similaire \u00e0 Mistral AI - Base de donn\u00e9es MongoDB pour la base de connaissances - Interface web React avec design responsive</p>"},{"location":"seance3/partie5-preparation-projet/#cas-detude-2-chatbot-pedagogique-pour-formation-interne","title":"Cas d'\u00e9tude 2 : Chatbot p\u00e9dagogique pour formation interne","text":"<p>Entreprise : Centre de formation professionnelle Probl\u00e9matique : Besoin d'assistance pour les apprenants en dehors des heures de cours</p> <p>Solution mise en place : - Chatbot sp\u00e9cialis\u00e9 dans les formations techniques - Capacit\u00e9 \u00e0 expliquer des concepts, fournir des exemples et proposer des exercices - Suivi de la progression des apprenants - Disponible sur la plateforme e-learning existante</p> <p>R\u00e9sultats : - Augmentation de 30% du taux de compl\u00e9tion des formations - R\u00e9duction du nombre de questions basiques aux formateurs - Meilleure adaptation aux diff\u00e9rents rythmes d'apprentissage - Collecte de donn\u00e9es sur les concepts les plus difficiles \u00e0 assimiler</p> <p>Technologies utilis\u00e9es : - API GPT (similaire \u00e0 Mistral AI) - Structure JSON pour la base de connaissances - Interface web int\u00e9gr\u00e9e \u00e0 la plateforme existante</p>"},{"location":"seance3/partie5-preparation-projet/#enseignements-a-retenir-pour-votre-projet","title":"Enseignements \u00e0 retenir pour votre projet","text":"<ol> <li>Clart\u00e9 du p\u00e9rim\u00e8tre : D\u00e9finir pr\u00e9cis\u00e9ment ce que le chatbot peut et ne peut pas faire</li> <li>Structure de l'information : Organiser la base de connaissances de mani\u00e8re logique</li> <li>Exp\u00e9rience utilisateur : Privil\u00e9gier la fluidit\u00e9 et la simplicit\u00e9</li> <li>Gestion des limites : Pr\u00e9voir comment r\u00e9agir quand le chatbot ne conna\u00eet pas la r\u00e9ponse</li> </ol>"},{"location":"seance3/partie5-preparation-projet/#premiers-pas-avec-lapi-mistral-ai-15-min","title":"Premiers pas avec l'API Mistral AI (15 min)","text":"<p>Objectif : D\u00e9couvrir l'API Mistral AI et r\u00e9aliser un premier test fonctionnel.</p>"},{"location":"seance3/partie5-preparation-projet/#introduction-a-mistral-ai","title":"Introduction \u00e0 Mistral AI","text":"<p>Mistral AI est une entreprise fran\u00e7aise qui d\u00e9veloppe des mod\u00e8les de langage performants. Son API permet d'acc\u00e9der \u00e0 ces mod\u00e8les pour cr\u00e9er des applications conversationnelles.</p> <p>Avantages pour votre projet : - Mod\u00e8les performants, y compris en fran\u00e7ais - API simple \u00e0 utiliser - Quota gratuit suffisant pour le d\u00e9veloppement - Entreprise europ\u00e9enne (conformit\u00e9 RGPD)</p>"},{"location":"seance3/partie5-preparation-projet/#configuration-de-lacces-a-lapi","title":"Configuration de l'acc\u00e8s \u00e0 l'API","text":"<ol> <li>Cr\u00e9ation d'un compte :</li> <li>Rendez-vous sur https://console.mistral.ai/</li> <li>Inscrivez-vous avec votre adresse email</li> <li> <p>Confirmez votre compte</p> </li> <li> <p>Obtention d'une cl\u00e9 API :</p> </li> <li>Dans le tableau de bord, acc\u00e9dez \u00e0 \"API Keys\"</li> <li>Cliquez sur \"Create API Key\"</li> <li>Donnez un nom \u00e0 votre cl\u00e9 (ex: \"projet-chatbot-bts\")</li> <li> <p>Copiez la cl\u00e9 g\u00e9n\u00e9r\u00e9e (elle ne sera plus accessible ensuite)</p> </li> <li> <p>Conservation de la cl\u00e9 :</p> </li> <li>Cr\u00e9ez un fichier <code>.env</code> pour stocker votre cl\u00e9</li> <li>Ne partagez jamais votre cl\u00e9 dans vos d\u00e9p\u00f4ts Git (utilisez .gitignore)</li> <li>Format du fichier <code>.env</code> :      <pre><code>MISTRAL_API_KEY=votre_cl\u00e9_api_ici\n</code></pre></li> </ol>"},{"location":"seance3/partie5-preparation-projet/#premier-test-avec-lapi","title":"Premier test avec l'API","text":"<p>Nous allons r\u00e9aliser un test simple pour v\u00e9rifier que l'API fonctionne correctement :</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\n# Charger la cl\u00e9 API depuis le fichier .env\nload_dotenv()\napi_key = os.getenv(\"MISTRAL_API_KEY\")\n\n# Initialiser le client Mistral\nclient = MistralClient(api_key=api_key)\n\n# Cr\u00e9er une liste de messages (incluant un prompt syst\u00e8me et un message utilisateur)\nmessages = [\n    ChatMessage(role=\"system\", content=\"Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans le Deep Learning pour des \u00e9tudiants de BTS SIO.\"),\n    ChatMessage(role=\"user\", content=\"Peux-tu m'expliquer ce qu'est un r\u00e9seau de neurones convolutif (CNN) en termes simples ?\")\n]\n\n# Envoyer la requ\u00eate \u00e0 l'API\nchat_response = client.chat(\n    model=\"mistral-tiny\",  # Mod\u00e8le de base, \u00e9conomique et rapide\n    messages=messages\n)\n\n# Afficher la r\u00e9ponse\nprint(chat_response.choices[0].message.content)\n</code></pre>"},{"location":"seance3/partie5-preparation-projet/#structure-de-base-de-votre-chatbot","title":"Structure de base de votre chatbot","text":"<p>Voici une structure de base pour votre futur chatbot :</p> <pre><code># Structure simplifi\u00e9e du chatbot\ndef chatbot_educatif(question, historique=None):\n    \"\"\"\n    R\u00e9pond \u00e0 une question sur le Deep Learning\n\n    Args:\n        question (str): Question de l'utilisateur\n        historique (list, optional): Historique de la conversation\n\n    Returns:\n        str: R\u00e9ponse du chatbot\n        list: Historique mis \u00e0 jour\n    \"\"\"\n    # Initialiser l'historique si n\u00e9cessaire\n    if historique is None:\n        historique = [\n            ChatMessage(role=\"system\", content=\"Tu es un assistant p\u00e9dagogique...\")\n        ]\n\n    # Ajouter la question \u00e0 l'historique\n    historique.append(ChatMessage(role=\"user\", content=question))\n\n    # Obtenir la r\u00e9ponse via l'API\n    response = client.chat(\n        model=\"mistral-tiny\",\n        messages=historique\n    )\n\n    # Extraire le contenu de la r\u00e9ponse\n    reponse_texte = response.choices[0].message.content\n\n    # Ajouter la r\u00e9ponse \u00e0 l'historique\n    historique.append(ChatMessage(role=\"assistant\", content=reponse_texte))\n\n    return reponse_texte, historique\n</code></pre>"},{"location":"seance3/partie5-preparation-projet/#presentation-du-cahier-des-charges-15-min","title":"Pr\u00e9sentation du cahier des charges (15 min)","text":""},{"location":"seance3/partie5-preparation-projet/#fonctionnalites-minimales-requises","title":"Fonctionnalit\u00e9s minimales requises","text":"<p>Votre chatbot doit impl\u00e9menter au minimum :</p> <ol> <li>Interface conversationnelle :</li> <li>Zone de saisie et d'affichage des messages</li> <li>Historique de conversation visible</li> <li> <p>Indication visuelle pendant le chargement des r\u00e9ponses</p> </li> <li> <p>Int\u00e9gration de l'API Mistral AI :</p> </li> <li>Gestion du contexte de conversation</li> <li>Configuration des param\u00e8tres de g\u00e9n\u00e9ration</li> <li> <p>Gestion des erreurs d'API</p> </li> <li> <p>Base de connaissances :</p> </li> <li>Structuration des informations sur le Deep Learning</li> <li>Couverture des concepts principaux du cours</li> <li> <p>Format JSON ou similaire</p> </li> <li> <p>Fonctionnalit\u00e9s p\u00e9dagogiques :</p> </li> <li>Possibilit\u00e9 de demander des explications sur les concepts</li> <li>G\u00e9n\u00e9ration d'exemples concrets</li> <li>Au moins une fonctionnalit\u00e9 suppl\u00e9mentaire au choix</li> </ol>"},{"location":"seance3/partie5-preparation-projet/#livrables-attendus","title":"Livrables attendus","text":"<ol> <li>Code source du chatbot (fichiers Python, HTML/CSS/JS)</li> <li>Base de connaissances structur\u00e9e</li> <li>Documentation technique expliquant :</li> <li>L'architecture de la solution</li> <li>Les choix d'impl\u00e9mentation</li> <li>Les instructions d'installation et d'utilisation</li> <li>Les possibilit\u00e9s d'\u00e9volution</li> <li>Pr\u00e9sentation pour la d\u00e9monstration finale</li> </ol>"},{"location":"seance3/partie5-preparation-projet/#planning-et-jalons","title":"Planning et jalons","text":"Horaire Jalon Livrable interm\u00e9diaire D\u00e9but s\u00e9ance 4 Document de conception Structure du projet et maquette +1h Prototype initial Interface basique + API connect\u00e9e +2h Version fonctionnelle Principales fonctionnalit\u00e9s impl\u00e9ment\u00e9es +3h Version finale Solution compl\u00e8te et documentation +3h30 Pr\u00e9paration pr\u00e9sentation Support visuel et d\u00e9monstration"},{"location":"seance3/partie5-preparation-projet/#conseils-pour-reussir-votre-projet","title":"Conseils pour r\u00e9ussir votre projet","text":"<ol> <li>Commencez simple et ajoutez des fonctionnalit\u00e9s progressivement</li> <li>Travaillez en \u00e9quipe en r\u00e9partissant clairement les t\u00e2ches</li> <li>Testez r\u00e9guli\u00e8rement pour identifier les probl\u00e8mes t\u00f4t</li> <li>Documentez au fur et \u00e0 mesure pour \u00e9viter la surcharge \u00e0 la fin</li> <li>Demandez de l'aide si vous rencontrez des blocages</li> </ol>"},{"location":"seance3/partie5-preparation-projet/#conclusion-de-la-seance-5-min","title":"Conclusion de la s\u00e9ance (5 min)","text":""},{"location":"seance3/partie5-preparation-projet/#recapitulatif-des-apprentissages","title":"R\u00e9capitulatif des apprentissages","text":"<p>Au cours de cette s\u00e9ance, vous avez :</p> <ul> <li>D\u00e9couvert les frameworks de Deep Learning utilis\u00e9s en entreprise</li> <li>Appris \u00e0 am\u00e9liorer les performances des mod\u00e8les</li> <li>Explor\u00e9 des cas d'utilisation r\u00e9els de chatbots</li> <li>Fait vos premiers pas avec l'API Mistral AI</li> <li>Pris connaissance du cahier des charges du projet final</li> </ul>"},{"location":"seance3/partie5-preparation-projet/#prochaines-etapes","title":"Prochaines \u00e9tapes","text":"<p>Pour la prochaine s\u00e9ance, vous devrez :</p> <ol> <li>Former votre \u00e9quipe projet (1-2 personnes)</li> <li>R\u00e9diger un document de conception initial</li> <li>Cr\u00e9er un d\u00e9p\u00f4t Git pour votre projet</li> <li>Pr\u00e9parer une maquette simple de l'interface</li> </ol>"},{"location":"seance3/partie5-preparation-projet/#questions-et-clarifications","title":"Questions et clarifications","text":"<p>Profitez de ces derni\u00e8res minutes pour poser vos questions sur :</p> <ul> <li>Le format du projet</li> <li>Les attentes techniques</li> <li>Les ressources disponibles</li> <li>Les prochaines \u00e9tapes</li> </ul> <p>Retour \u00e0 l'index de la s\u00e9ance 3</p>"},{"location":"seance3/test/","title":"Proposition de restructuration du document \"Frameworks pour d\u00e9butants\"","text":"<p>Pour am\u00e9liorer la clart\u00e9 et la progression du document \"partie1-frameworks-debutants.md\", je propose une restructuration avec une progression plus explicite et les fichiers/ressources manquants. Voici ma proposition:</p>"},{"location":"seance3/test/#frameworks-pour-debutants-1h30","title":"FRAMEWORKS POUR D\u00c9BUTANTS (1h30)","text":""},{"location":"seance3/test/#apercu-de-la-session","title":"APER\u00c7U DE LA SESSION","text":"<pre><code>\ud83c\udfaf OBJECTIFS\n- Comprendre l'utilit\u00e9 des frameworks de Deep Learning en entreprise\n- Ma\u00eetriser les bases de TensorFlow/Keras\n- Cr\u00e9er et d\u00e9ployer un mod\u00e8le via une API\n\n\ud83d\udccb PLAN DE LA SESSION\n1. Introduction aux frameworks en contexte professionnel (15 min)\n2. Atelier pratique: D\u00e9velopper un mod\u00e8le avec TensorFlow/Keras (40 min)\n3. Mini-projet: Cr\u00e9ation d'une API de reconnaissance d'images (35 min)\n</code></pre>"},{"location":"seance3/test/#1-introduction-aux-frameworks-15-min","title":"1. INTRODUCTION AUX FRAMEWORKS (15 min)","text":""},{"location":"seance3/test/#11-pourquoi-utiliser-des-frameworks-en-entreprise","title":"1.1 Pourquoi utiliser des frameworks en entreprise?","text":"<ul> <li>Productivit\u00e9: D\u00e9veloppement plus rapide</li> <li>Maintenabilit\u00e9: Code standardis\u00e9</li> <li>Performances: Optimisations int\u00e9gr\u00e9es </li> <li>D\u00e9ploiement: Outils pour la mise en production</li> </ul>"},{"location":"seance3/test/#12-comparatif-des-frameworks-courants","title":"1.2 Comparatif des frameworks courants","text":"Framework Cas d'usage Avantages Inconv\u00e9nients TensorFlow/Keras Applications web/mobile, production D\u00e9ploiement facile, bonne documentation Peut \u00eatre lourd pour le prototypage PyTorch Recherche, prototypage Flexibilit\u00e9, d\u00e9bogage facile D\u00e9ploiement plus complexe Hugging Face NLP, chatbots Mod\u00e8les pr\u00e9-entra\u00een\u00e9s pour le texte Sp\u00e9cialis\u00e9 en NLP principalement Scikit-learn Pr\u00e9traitement, ML classique Simple, int\u00e9gration facile Limit\u00e9 pour le Deep Learning"},{"location":"seance3/test/#13-exemples-reels-dentreprises","title":"1.3 Exemples r\u00e9els d'entreprises","text":"<ul> <li>Cas d'\u00e9tude 1: Syst\u00e8me de tri automatique de documents (PME de logistique)</li> <li>Cas d'\u00e9tude 2: Mod\u00e9ration de contenu pour site e-commerce (Agence web)</li> <li>Cas d'\u00e9tude 3: Assistance diagnostic m\u00e9dical (Cabinet m\u00e9dical)</li> </ul>"},{"location":"seance3/test/#2-atelier-pratique-tensorflowkeras-40-min","title":"2. ATELIER PRATIQUE: TENSORFLOW/KERAS (40 min)","text":""},{"location":"seance3/test/#21-configuration-de-lenvironnement-5-min","title":"2.1 Configuration de l'environnement (5 min)","text":"<p>Option A: Google Colab (recommand\u00e9e) 1. Acc\u00e9dez au notebook via ce lien: TensorFlow/Keras pour d\u00e9butants 2. Cr\u00e9ez une copie dans votre Drive: <code>Fichier &gt; Enregistrer une copie dans Drive</code></p> <p>Option B: Configuration locale <pre><code># Installation de TensorFlow\npip install tensorflow matplotlib numpy pandas\n\n# V\u00e9rification\nimport tensorflow as tf\nprint(tf.__version__)\n</code></pre></p>"},{"location":"seance3/test/#22-structure-dun-projet-tensorflowkeras-10-min","title":"2.2 Structure d'un projet TensorFlow/Keras (10 min)","text":"<p>Les \u00e9tapes essentielles d'un projet sont toujours les m\u00eames:</p> <pre><code># 1. Importation des biblioth\u00e8ques\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 2. Chargement et pr\u00e9paration des donn\u00e9es\n(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\ntrain_images = train_images / 255.0  # Normalisation\ntest_images = test_images / 255.0\n\n# 3. Cr\u00e9ation du mod\u00e8le\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\n\n# 4. Compilation du mod\u00e8le\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# 5. Entra\u00eenement\nmodel.fit(train_images, train_labels, epochs=5, validation_split=0.2)\n\n# 6. \u00c9valuation\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\nprint(f\"Pr\u00e9cision sur donn\u00e9es de test: {test_acc:.2f}\")\n\n# 7. Pr\u00e9diction et utilisation\npredictions = model.predict(test_images[:5])\n</code></pre>"},{"location":"seance3/test/#23-exercice-guide-fashion-mnist-25-min","title":"2.3 Exercice guid\u00e9: Fashion MNIST (25 min)","text":"<p>Objectif: Cr\u00e9er un mod\u00e8le de classification d'images de v\u00eatements</p> <p>\u00c9tapes: 1. Explorez le jeu de donn\u00e9es Fashion MNIST 2. Construisez et entra\u00eenez un mod\u00e8le simple 3. Am\u00e9liorez le mod\u00e8le en modifiant l'architecture 4. Visualisez les r\u00e9sultats</p> <p>Fichier \u00e0 utiliser: fashion_mnist_classification.ipynb</p> <p>Points de contr\u00f4le: - \u2713 Le mod\u00e8le initial atteint &gt;80% de pr\u00e9cision - \u2713 Les pr\u00e9dictions sur les exemples de test sont majoritairement correctes - \u2713 Vous comprenez l'impact de vos modifications sur les performances</p>"},{"location":"seance3/test/#3-mini-projet-api-de-reconnaissance-dimages-35-min","title":"3. MINI-PROJET: API DE RECONNAISSANCE D'IMAGES (35 min)","text":""},{"location":"seance3/test/#31-concepts-cles-des-apis-5-min","title":"3.1 Concepts cl\u00e9s des APIs (5 min)","text":"<ul> <li>API REST: Interface permettant \u00e0 diff\u00e9rentes applications de communiquer</li> <li>Avantages en entreprise:</li> <li>S\u00e9paration frontend/backend</li> <li>R\u00e9utilisation du mod\u00e8le par plusieurs applications</li> <li>Mise \u00e0 jour du mod\u00e8le sans toucher aux applications clientes</li> </ul>"},{"location":"seance3/test/#32-structure-dune-api-flaskfastapi-10-min","title":"3.2 Structure d'une API Flask/FastAPI (10 min)","text":"<pre><code># Exemple avec Flask\nfrom flask import Flask, request, jsonify\nimport tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nimport io\n\napp = Flask(__name__)\n\n# Charger le mod\u00e8le pr\u00e9-entra\u00een\u00e9\nmodel = tf.keras.models.load_model('mon_modele.h5')\n\n# Classes pour Fashion MNIST\nclass_names = ['T-shirt/top', 'Pantalon', 'Pull', 'Robe', 'Manteau',\n               'Sandale', 'Chemise', 'Basket', 'Sac', 'Bottine']\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if 'image' not in request.files:\n        return jsonify({'error': 'Pas d\\'image fournie'}), 400\n\n    file = request.files['image']\n    image = Image.open(file.stream).convert('L')\n    image = image.resize((28, 28))\n    image_array = np.array(image) / 255.0\n    image_array = image_array.reshape(1, 28, 28)\n\n    predictions = model.predict(image_array)\n    predicted_class = np.argmax(predictions[0])\n\n    return jsonify({\n        'class': class_names[predicted_class],\n        'confidence': float(predictions[0][predicted_class])\n    })\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"seance3/test/#33-travail-pratique-developpement-dune-api-20-min","title":"3.3 Travail pratique: D\u00e9veloppement d'une API (20 min)","text":"<p>Objectif: D\u00e9velopper une API qui expose votre mod\u00e8le de classification Fashion MNIST</p> <p>\u00c9tapes: 1. Sauvegarder le mod\u00e8le entra\u00een\u00e9 pr\u00e9c\u00e9demment 2. Impl\u00e9menter l'API avec Flask ou FastAPI 3. Tester l'API avec des requ\u00eates d'images 4. Ajouter des fonctionnalit\u00e9s suppl\u00e9mentaires</p> <p>Fichier \u00e0 utiliser: fashion_mnist_api.ipynb</p> <p>Points de contr\u00f4le: - \u2713 L'API r\u00e9pond correctement aux requ\u00eates d'images - \u2713 Le pr\u00e9traitement des images fonctionne bien - \u2713 Les r\u00e9ponses incluent la classe pr\u00e9dite et le niveau de confiance</p>"},{"location":"seance3/test/#fichiers-a-ajouter-au-projet","title":"FICHIERS \u00c0 AJOUTER AU PROJET","text":""},{"location":"seance3/test/#1-notebook-tensorflowkeras-pour-debutants-tensorflow_keras_debutantsipynb","title":"1. Notebook: TensorFlow/Keras pour d\u00e9butants (tensorflow_keras_debutants.ipynb)","text":"<pre><code># TensorFlow/Keras pour d\u00e9butants\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"Version de TensorFlow:\", tf.__version__)\n\n# 1. Chargement des donn\u00e9es Fashion MNIST\nprint(\"Chargement du jeu de donn\u00e9es Fashion MNIST...\")\n(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n\n# Noms des classes\nclass_names = ['T-shirt/Top', 'Pantalon', 'Pull', 'Robe', 'Manteau',\n               'Sandale', 'Chemise', 'Basket', 'Sac', 'Bottine']\n\n# 2. Exploration des donn\u00e9es\nprint(f\"Forme des donn\u00e9es d'entra\u00eenement: {train_images.shape}\")\nprint(f\"Nombre d'\u00e9chantillons d'entra\u00eenement: {len(train_labels)}\")\nprint(f\"Forme des donn\u00e9es de test: {test_images.shape}\")\n\n# Affichage de quelques exemples\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap='gray')\n    plt.xlabel(class_names[train_labels[i]])\nplt.show()\n\n# 3. Pr\u00e9traitement des donn\u00e9es\n# Normalisation des valeurs de pixels entre 0 et 1\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n\n# 4. Cr\u00e9ation d'un mod\u00e8le simple\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),  # Conversion 28x28 -&gt; 784\n    keras.layers.Dense(128, activation='relu'),   # Couche cach\u00e9e avec 128 neurones\n    keras.layers.Dense(10, activation='softmax')  # Couche de sortie avec 10 neurones (10 classes)\n])\n\n# Affichage du r\u00e9sum\u00e9 du mod\u00e8le\nmodel.summary()\n\n# 5. Compilation du mod\u00e8le\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# 6. Entra\u00eenement du mod\u00e8le\nprint(\"Entra\u00eenement du mod\u00e8le...\")\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=5,\n    validation_split=0.2\n)\n\n# 7. Visualisation de l'apprentissage\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training')\nplt.plot(history.history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# 8. \u00c9valuation du mod\u00e8le\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\nprint(f\"\\nPr\u00e9cision sur les donn\u00e9es de test: {test_acc*100:.2f}%\")\n\n# 9. Faire des pr\u00e9dictions\npredictions = model.predict(test_images)\n\n# Afficher les pr\u00e9dictions pour quelques images\nplt.figure(figsize=(12, 12))\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    plt.imshow(test_images[i], cmap='gray')\n\n    predicted_label = np.argmax(predictions[i])\n    true_label = test_labels[i]\n\n    color = 'blue' if predicted_label == true_label else 'red'\n\n    plt.xlabel(f\"Pr\u00e9dit: {class_names[predicted_label]}\\nR\u00e9el: {class_names[true_label]}\", color=color)\n    plt.xticks([])\n    plt.yticks([])\nplt.tight_layout()\nplt.show()\n\n# 10. Exercice: Am\u00e9liorer le mod\u00e8le\n# TODO: Modifiez l'architecture du mod\u00e8le pour am\u00e9liorer les performances\n# Suggestions:\n# - Ajouter plus de couches Dense\n# - Ajouter du Dropout pour r\u00e9duire le surapprentissage\n# - Essayer diff\u00e9rentes fonctions d'activation\n# - Modifier le nombre de neurones\n\n# 11. Sauvegarde du mod\u00e8le\nmodel.save('fashion_mnist_model.h5')\nprint(\"Mod\u00e8le sauvegard\u00e9 avec succ\u00e8s dans 'fashion_mnist_model.h5'\")\n</code></pre>"},{"location":"seance3/test/#2-notebook-api-de-reconnaissance-dimages-fashion_mnist_apiipynb","title":"2. Notebook: API de reconnaissance d'images (fashion_mnist_api.ipynb)","text":"<pre><code># API de reconnaissance d'images avec Flask\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport io\nimport base64\nfrom IPython.display import HTML, display\nimport json\n\n# 1. Cr\u00e9ation ou chargement du mod\u00e8le\n# Option 1: Entra\u00eener un nouveau mod\u00e8le\ndef train_model():\n    print(\"Entra\u00eenement d'un nouveau mod\u00e8le...\")\n\n    # Chargement des donn\u00e9es\n    (train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n\n    # Normalisation\n    train_images = train_images / 255.0\n    test_images = test_images / 255.0\n\n    # Cr\u00e9ation du mod\u00e8le\n    model = keras.Sequential([\n        keras.layers.Flatten(input_shape=(28, 28)),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dense(10, activation='softmax')\n    ])\n\n    # Compilation\n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Entra\u00eenement\n    model.fit(train_images, train_labels, epochs=5, validation_split=0.2)\n\n    # Sauvegarde\n    model.save('fashion_mnist_model.h5')\n    print(\"Mod\u00e8le sauvegard\u00e9 dans 'fashion_mnist_model.h5'\")\n\n    return model, test_images, test_labels\n\n# Option 2: Charger un mod\u00e8le existant\ndef load_model():\n    try:\n        print(\"Chargement du mod\u00e8le existant...\")\n        model = keras.models.load_model('fashion_mnist_model.h5')\n\n        # Chargement des donn\u00e9es de test\n        (_, _), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n        test_images = test_images / 255.0\n\n        return model, test_images, test_labels\n    except:\n        print(\"Mod\u00e8le existant non trouv\u00e9. Entra\u00eenement d'un nouveau mod\u00e8le...\")\n        return train_model()\n\n# Choix de l'option (d\u00e9commentez celle que vous souhaitez utiliser)\n# model, test_images, test_labels = train_model()\nmodel, test_images, test_labels = load_model()\n\n# Classes pour Fashion MNIST\nclass_names = ['T-shirt/Top', 'Pantalon', 'Pull', 'Robe', 'Manteau',\n               'Sandale', 'Chemise', 'Basket', 'Sac', 'Bottine']\n\n# 2. Pr\u00e9paration des fonctions de l'API\ndef preprocess_image(image):\n    \"\"\"Pr\u00e9traite une image pour la pr\u00e9diction\"\"\"\n    # Redimensionnement et conversion en niveaux de gris\n    image = image.convert('L').resize((28, 28))\n\n    # Conversion en array et normalisation\n    image_array = np.array(image) / 255.0\n\n    # Ajout de la dimension batch\n    image_array = image_array.reshape(1, 28, 28)\n\n    return image_array\n\ndef predict_image(image_array):\n    \"\"\"Pr\u00e9dit la classe d'une image\"\"\"\n    predictions = model.predict(image_array)\n    predicted_class = np.argmax(predictions[0])\n    confidence = float(predictions[0][predicted_class])\n\n    return {\n        'class': class_names[predicted_class],\n        'class_id': int(predicted_class),\n        'confidence': confidence * 100  # En pourcentage\n    }\n\n# 3. Cr\u00e9ation d'une API Flask simplifi\u00e9e\n# Note: Dans Colab, nous ne pouvons pas ex\u00e9cuter un vrai serveur Flask,\n# mais nous pouvons simuler son comportement\n\ndef simulate_api_call(image):\n    \"\"\"Simule un appel \u00e0 l'API Flask\"\"\"\n    try:\n        # Pr\u00e9traitement\n        image_array = preprocess_image(image)\n\n        # Pr\u00e9diction\n        result = predict_image(image_array)\n\n        return {\n            'success': True,\n            'prediction': result\n        }\n    except Exception as e:\n        return {\n            'success': False,\n            'error': str(e)\n        }\n\n# 4. Interface de test dans Colab\nfrom google.colab import files\nfrom IPython.display import display, clear_output\n\ndef test_with_upload():\n    \"\"\"Interface pour tester avec un upload d'image\"\"\"\n    clear_output()\n    print(\"T\u00e9l\u00e9chargez une image de v\u00eatement...\")\n    uploaded = files.upload()\n\n    for filename in uploaded.keys():\n        image = Image.open(io.BytesIO(uploaded[filename]))\n\n        # Afficher l'image\n        plt.figure(figsize=(4, 4))\n        plt.imshow(image)\n        plt.axis('off')\n        plt.title(\"Image t\u00e9l\u00e9charg\u00e9e\")\n        plt.show()\n\n        # Simuler l'appel API\n        result = simulate_api_call(image)\n\n        if result['success']:\n            pred = result['prediction']\n            print(f\"Classe pr\u00e9dite: {pred['class']}\")\n            print(f\"Confiance: {pred['confidence']:.2f}%\")\n\n            # Graphique des probabilit\u00e9s\n            predictions = model.predict(preprocess_image(image))[0]\n            plt.figure(figsize=(10, 3))\n            plt.bar(range(len(predictions)), predictions)\n            plt.xticks(range(len(predictions)), class_names, rotation=45)\n            plt.xlabel('Classe')\n            plt.ylabel('Probabilit\u00e9')\n            plt.title('Probabilit\u00e9s par classe')\n            plt.show()\n        else:\n            print(f\"Erreur: {result['error']}\")\n\n# 5. Test avec des exemples de Fashion MNIST\ndef test_with_samples():\n    \"\"\"Test avec des exemples du jeu de donn\u00e9es\"\"\"\n    # S\u00e9lectionner quelques exemples\n    sample_indices = np.random.choice(len(test_images), 3, replace=False)\n\n    for idx in sample_indices:\n        # Convertir l'array en image\n        image = Image.fromarray((test_images[idx] * 255).astype(np.uint8))\n\n        # Afficher l'image\n        plt.figure(figsize=(4, 4))\n        plt.imshow(image, cmap='gray')\n        plt.axis('off')\n        plt.title(f\"Vrai classe: {class_names[test_labels[idx]]}\")\n        plt.show()\n\n        # Simuler l'appel API\n        result = simulate_api_call(image)\n\n        if result['success']:\n            pred = result['prediction']\n            print(f\"Classe pr\u00e9dite: {pred['class']}\")\n            print(f\"Confiance: {pred['confidence']:.2f}%\")\n        else:\n            print(f\"Erreur: {result['error']}\")\n\n        print(\"-\" * 50)\n\n# 6. Impl\u00e9mentation compl\u00e8te de l'API Flask (code \u00e0 ex\u00e9cuter localement)\nflask_code = \"\"\"\nfrom flask import Flask, request, jsonify\nimport tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nimport io\n\napp = Flask(__name__)\n\n# Charger le mod\u00e8le\nmodel = tf.keras.models.load_model('fashion_mnist_model.h5')\n\n# Classes\nclass_names = ['T-shirt/top', 'Pantalon', 'Pull', 'Robe', 'Manteau',\n               'Sandale', 'Chemise', 'Basket', 'Sac', 'Bottine']\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if 'image' not in request.files:\n        return jsonify({'error': 'Pas d\\\\'image fournie'}), 400\n\n    file = request.files['image']\n\n    # Lire et pr\u00e9traiter l'image\n    image = Image.open(file.stream).convert('L')\n    image = image.resize((28, 28))\n    image_array = np.array(image) / 255.0\n    image_array = image_array.reshape(1, 28, 28)\n\n    # Faire la pr\u00e9diction\n    predictions = model.predict(image_array)\n    predicted_class = np.argmax(predictions[0])\n    confidence = float(predictions[0][predicted_class])\n\n    return jsonify({\n        'class': class_names[predicted_class],\n        'class_id': int(predicted_class),\n        'confidence': confidence * 100  # En pourcentage\n    })\n\n@app.route('/info', methods=['GET'])\ndef model_info():\n    return jsonify({\n        'name': 'Fashion MNIST Classifier',\n        'classes': class_names,\n        'version': '1.0'\n    })\n\nif __name__ == '__main__':\n    app.run(debug=True, host='0.0.0.0', port=5000)\n\"\"\"\n\nprint(\"Code Flask pour l'API (\u00e0 utiliser localement):\")\nprint(\"-\" * 80)\nprint(flask_code)\nprint(\"-\" * 80)\n\n# Affichage du menu\nprint(\"\\nOptions de test:\")\nprint(\"1. Tester avec vos propres images\")\nprint(\"2. Tester avec des exemples du jeu de donn\u00e9es\")\n\nchoice = input(\"Entrez votre choix (1 ou 2): \")\nif choice == \"1\":\n    test_with_upload()\nelif choice == \"2\":\n    test_with_samples()\nelse:\n    print(\"Choix non valide\")\n</code></pre>"},{"location":"seance3/test/#ressources-supplementaires","title":"RESSOURCES SUPPL\u00c9MENTAIRES","text":"<ol> <li>Guide d'installation locale</li> <li>Instructions d\u00e9taill\u00e9es pour installer TensorFlow en local</li> <li>Solutions aux probl\u00e8mes courants</li> <li> <p>Configurations recommand\u00e9es</p> </li> <li> <p>Cheat Sheet TensorFlow/Keras</p> </li> <li>R\u00e9sum\u00e9 des fonctions et m\u00e9thodes principales</li> <li>Exemples d'utilisation communs</li> <li> <p>Bonnes pratiques</p> </li> <li> <p>Liste de v\u00e9rification du projet API</p> </li> <li>Points \u00e0 v\u00e9rifier avant de finaliser votre API</li> <li>Conseils pour un d\u00e9ploiement r\u00e9ussi</li> <li>Ressources pour aller plus loin</li> </ol> <p>Cette structure r\u00e9vis\u00e9e offre une progression plus claire, ajoute les fichiers manquants et fournit des points de contr\u00f4le pour que les \u00e9tudiants puissent suivre leur progression. Les ressources suppl\u00e9mentaires aident \u00e9galement \u00e0 r\u00e9soudre les probl\u00e8mes courants et \u00e0 approfondir leurs connaissances.</p>"},{"location":"seance3/ressources/amelioration-performances/","title":"Amelioration performances","text":"In\u00a0[\u00a0]: Copied! <pre># 2.6 Solution 2: R\u00e9\u00e9chantillonnage\nprint(\"\\n--- SOLUTION 2: R\u00c9\u00c9CHANTILLONNAGE ---\")\n\n# Sous-\u00e9chantillonnage des classes majoritaires et sur-\u00e9chantillonnage des classes minoritaires\nfrom sklearn.utils import resample\n\n# S\u00e9parer les exemples par classe\nclass_samples = []\nfor i in range(n_classes):\n    class_samples.append(imbalanced_images[imbalanced_labels == i])\n\n# D\u00e9finir la taille cible (moyenne)\ntarget_size = int(total_samples / n_classes)\nprint(f\"Taille cible par classe apr\u00e8s r\u00e9\u00e9chantillonnage: {target_size}\")\n\n# R\u00e9\u00e9chantillonner chaque classe\nresampled_images = []\nresampled_labels = []\n\nfor i in range(n_classes):\n    if len(class_samples[i]) == 0:\n        continue\n        \n    # Sur-\u00e9chantillonnage pour les classes minoritaires\n    if len(class_samples[i]) &lt; target_size:\n        resampled = resample(class_samples[i], \n                            replace=True,        # Avec remplacement\n                            n_samples=target_size,\n                            random_state=42)\n    # Sous-\u00e9chantillonnage pour les classes majoritaires\n    elif len(class_samples[i]) &gt; target_size:\n        resampled = resample(class_samples[i],\n                            replace=False,       # Sans remplacement\n                            n_samples=target_size,\n                            random_state=42)\n    else:\n        resampled = class_samples[i]\n    \n    resampled_images.append(resampled)\n    resampled_labels.extend([i] * len(resampled))\n\n# Convertir en arrays numpy\nresampled_images = np.vstack([img for img in resampled_images if len(img) &gt; 0])\nresampled_labels = np.array(resampled_labels)\n\n# V\u00e9rifier la nouvelle distribution\nplt.figure(figsize=(10, 6))\nresampled_class_counts = np.bincount(resampled_labels, minlength=n_classes)\nplt.bar(range(len(class_names)), resampled_class_counts)\nplt.xticks(range(len(class_names)), class_names, rotation=90)\nplt.title(\"Distribution apr\u00e8s r\u00e9\u00e9chantillonnage\")\nplt.xlabel(\"Classe\")\nplt.ylabel(\"Nombre d'\u00e9chantillons\")\nplt.tight_layout()\nplt.show()\n\n# Diviser en ensembles d'entra\u00eenement et de validation\nX_train_res, X_val_res, y_train_res, y_val_res = train_test_split(\n    resampled_images, resampled_labels, test_size=0.2, random_state=42\n)\n\n# Entra\u00eener sur les donn\u00e9es r\u00e9\u00e9chantillonn\u00e9es\nresampled_model = create_base_model()\nhistory_resampled = resampled_model.fit(\n    X_train_res, y_train_res,\n    epochs=10,\n    batch_size=32,\n    validation_data=(X_val_res, y_val_res),\n    verbose=1\n)\n\n# \u00c9valuation du mod\u00e8le r\u00e9\u00e9chantillonn\u00e9\nresampled_predictions = resampled_model.predict(test_images)\nresampled_pred_classes = np.argmax(resampled_predictions, axis=1)\n\n# Rapport de classification\nprint(\"\\nRapport de classification - Mod\u00e8le avec r\u00e9\u00e9chantillonnage:\")\nprint(classification_report(test_labels, resampled_pred_classes, target_names=class_names))\n\n# 2.7 Comparaison des solutions pour les probl\u00e8mes de donn\u00e9es\nprint(\"\\n--- COMPARAISON DES SOLUTIONS POUR LES PROBL\u00c8MES DE DONN\u00c9ES ---\")\n\n# Comparaison des performances par classe\nmodels_data = {\n    \"D\u00e9s\u00e9quilibr\u00e9\": imbalanced_pred_classes,\n    \"Pond\u00e9ration\": weighted_pred_classes,\n    \"R\u00e9\u00e9chantillonnage\": resampled_pred_classes\n}\n\n# Calculer la pr\u00e9cision par classe pour chaque mod\u00e8le\nclass_accuracies = {}\n\nfor model_name, predictions in models_data.items():\n    accuracies = []\n    for class_idx in range(n_classes):\n        # Indices des exemples de cette classe\n        class_indices = np.where(test_labels == class_idx)[0]\n        # Pr\u00e9cision sur cette classe\n        class_acc = np.mean(predictions[class_indices] == test_labels[class_indices])\n        accuracies.append(class_acc)\n    class_accuracies[model_name] = accuracies\n\n# Visualisation des pr\u00e9cisions par classe\nplt.figure(figsize=(12, 6))\nx = np.arange(len(class_names))\nwidth = 0.25\nmultiplier = 0\n\nfor model_name, accuracies in class_accuracies.items():\n    offset = width * multiplier\n    plt.bar(x + offset, accuracies, width, label=model_name)\n    multiplier += 1\n\nplt.xlabel('Classe')\nplt.ylabel('Pr\u00e9cision')\nplt.title('Pr\u00e9cision par classe et par mod\u00e8le')\nplt.xticks(x + width, class_names, rotation=90)\nplt.legend(loc='lower center')\nplt.grid(True, axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 3. AM\u00c9LIORATION POUR APPLICATION WEB\n# ===================================\n\nprint(\"\\n\\n--- AM\u00c9LIORATION D'UN MOD\u00c8LE POUR APPLICATION WEB ---\")\n\n# 3.1 D\u00e9finition des contraintes d'une application web\nprint(\"Dans un contexte d'application web, un mod\u00e8le doit \u00eatre:\")\nprint(\"1. Rapide (temps d'inf\u00e9rence court)\")\nprint(\"2. L\u00e9ger (taille r\u00e9duite)\")\nprint(\"3. Pr\u00e9cis sur les cas d'utilisation r\u00e9els\")\nprint(\"4. Robuste aux variations (rotations, luminosit\u00e9, etc.)\")\n\n# 3.2 Mod\u00e8le de base pour une application de reconnaissance de v\u00eatements\nprint(\"\\nCr\u00e9ation d'un mod\u00e8le de base...\")\n\ndef create_web_model():\n    \"\"\"Cr\u00e9e un mod\u00e8le CNN simple pour la classification de v\u00eatements\"\"\"\n    model = models.Sequential([\n        # Reshaping pour CNN\n        layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n        \n        # Premi\u00e8re couche de convolution\n        layers.Conv2D(32, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        \n        # Deuxi\u00e8me couche de convolution\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        \n        # Aplatissement et couches denses\n        layers.Flatten(),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(10, activation='softmax')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# Entra\u00eenement du mod\u00e8le de base\nbase_web_model = create_web_model()\n\n# Reshape pour CNN\ntrain_images_reshaped = train_images.reshape(-1, 28, 28, 1)\ntest_images_reshaped = test_images.reshape(-1, 28, 28, 1)\n\nhistory_base_web = base_web_model.fit(\n    train_images_reshaped, train_labels,\n    epochs=5,\n    batch_size=64,\n    validation_split=0.2,\n    verbose=1\n)\n\n# 3.3 Mesure des performances initiales\nprint(\"\\n\u00c9valuation des performances initiales...\")\n\n# Pr\u00e9cision\ntest_loss, test_acc = base_web_model.evaluate(test_images_reshaped, test_labels, verbose=0)\nprint(f\"Pr\u00e9cision sur test: {test_acc*100:.2f}%\")\n\n# Temps d'inf\u00e9rence\nstart_time = time.time()\nbase_web_model.predict(test_images_reshaped[:100])\ninference_time = (time.time() - start_time) / 100  # Temps moyen par image\nprint(f\"Temps d'inf\u00e9rence moyen: {inference_time*1000:.2f} ms par image\")\n\n# Taille du mod\u00e8le\nbase_web_model.save(\"base_web_model.h5\")\nimport os\nmodel_size_mb = os.path.getsize(\"base_web_model.h5\") / (1024 * 1024)\nprint(f\"Taille du mod\u00e8le: {model_size_mb:.2f} MB\")\n\n# 3.4 Am\u00e9lioration 1: Data Augmentation\nprint(\"\\n--- AM\u00c9LIORATION 1: DATA AUGMENTATION ---\")\n\n# Cr\u00e9er un g\u00e9n\u00e9rateur de donn\u00e9es avec augmentation\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomTranslation(0.1, 0.1)\n])\n\ndef create_augmented_model():\n    \"\"\"Cr\u00e9e un mod\u00e8le avec data augmentation int\u00e9gr\u00e9e\"\"\"\n    model = models.Sequential([\n        # Reshaping pour CNN\n        layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n        \n        # Couche d'augmentation de donn\u00e9es (active uniquement pendant l'entra\u00eenement)\n        data_augmentation,\n        \n        # Premi\u00e8re couche de convolution\n        layers.Conv2D(32, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        \n        # Deuxi\u00e8me couche de convolution\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        \n        # Aplatissement et couches denses\n        layers.Flatten(),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(10, activation='softmax')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\naugmented_model = create_augmented_model()\n\n# Visualiser des exemples d'augmentation\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    augmented_image = data_augmentation(train_images_reshaped[i:i+1])\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(augmented_image[0, :, :, 0], cmap='gray')\n    plt.title(class_names[train_labels[i]])\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n# Entra\u00eenement avec augmentation\nhistory_augmented = augmented_model.fit(\n    train_images_reshaped, train_labels,\n    epochs=10,\n    batch_size=64,\n    validation_split=0.2,\n    verbose=1\n)\n\n# 3.5 Am\u00e9lioration 2: Architecture plus l\u00e9g\u00e8re\nprint(\"\\n--- AM\u00c9LIORATION 2: ARCHITECTURE PLUS L\u00c9G\u00c8RE ---\")\n\ndef create_lightweight_model():\n    \"\"\"Cr\u00e9e un mod\u00e8le plus l\u00e9ger avec s\u00e9parable convolutions\"\"\"\n    model = models.Sequential([\n        # Reshaping pour CNN\n        layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n        \n        # Premi\u00e8re couche de convolution s\u00e9parable (moins de param\u00e8tres)\n        layers.SeparableConv2D(32, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        \n        # Deuxi\u00e8me couche de convolution s\u00e9parable\n        layers.SeparableConv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        \n        # Aplatissement et couches denses\n        layers.Flatten(),\n        layers.Dense(64, activation='relu'),  # Plus petite que l'original\n        layers.Dense(10, activation='softmax')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\nlightweight_model = create_lightweight_model()\nlightweight_model.summary()  # Afficher le r\u00e9sum\u00e9 pour voir la r\u00e9duction de param\u00e8tres\n\n# Entra\u00eenement du mod\u00e8le l\u00e9ger\nhistory_lightweight = lightweight_model.fit(\n    train_images_reshaped, train_labels,\n    epochs=10,\n    batch_size=64,\n    validation_split=0.2,\n    verbose=1\n)\n\n# 3.6 Am\u00e9lioration 3: Optimisation pour le d\u00e9ploiement\nprint(\"\\n--- AM\u00c9LIORATION 3: OPTIMISATION POUR LE D\u00c9PLOIEMENT ---\")\n\n# Convertir en TensorFlow Lite pour le d\u00e9ploiement web/mobile\n# (Simul\u00e9 ici pour d\u00e9monstration)\n\nprint(\"Conversion TensorFlow \u2192 TensorFlow Lite...\")\nconverter = tf.lite.TFLiteConverter.from_keras_model(lightweight_model)\ntflite_model = converter.convert()\n\n# \u00c9crire le mod\u00e8le TFLite dans un fichier\nwith open('model_lite.tflite', 'wb') as f:\n    f.write(tflite_model)\n\n# Taille du mod\u00e8le TFLite\ntflite_model_size_mb = os.path.getsize(\"model_lite.tflite\") / (1024 * 1024)\nprint(f\"Taille du mod\u00e8le TFLite: {tflite_model_size_mb:.2f} MB\")\n\n# Quantification (simulation)\nprint(\"\\nQuantification pour r\u00e9duire davantage la taille...\")\nconverter = tf.lite.TFLiteConverter.from_keras_model(lightweight_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nquantized_tflite_model = converter.convert()\n\n# \u00c9crire le mod\u00e8le quantifi\u00e9 dans un fichier\nwith open('model_quantized.tflite', 'wb') as f:\n    f.write(quantized_tflite_model)\n\n# Taille du mod\u00e8le quantifi\u00e9\nquantized_model_size_mb = os.path.getsize(\"model_quantized.tflite\") / (1024 * 1024)\nprint(f\"Taille du mod\u00e8le quantifi\u00e9: {quantized_model_size_mb:.2f} MB\")\n\n# 3.7 Comparaison finale des am\u00e9liorations\nprint(\"\\n--- COMPARAISON FINALE DES AM\u00c9LIORATIONS ---\")\n\n# \u00c9valuation des mod\u00e8les am\u00e9lior\u00e9s\nmodels_web = {\n    \"Base\": base_web_model,\n    \"Augment\u00e9\": augmented_model,\n    \"L\u00e9ger\": lightweight_model\n}\n\n# Tableau comparatif\nweb_results = []\nfor name, model in models_web.items():\n    # Mesurer la pr\u00e9cision\n    test_loss, test_acc = model.evaluate(test_images_reshaped, test_labels, verbose=0)\n    \n    # Mesurer le temps d'inf\u00e9rence\n    start_time = time.time()\n    model.predict(test_images_reshaped[:100])\n    inference_time = (time.time() - start_time) / 100\n    \n    # Sauvegarder et mesurer la taille\n    model_filename = f\"{name.lower()}_model.h5\"\n    model.save(model_filename)\n    model_size_mb = os.path.getsize(model_filename) / (1024 * 1024)\n    \n    web_results.append({\n        \"Mod\u00e8le\": name,\n        \"Pr\u00e9cision\": f\"{test_acc*100:.2f}%\",\n        \"Temps d'inf\u00e9rence\": f\"{inference_time*1000:.2f} ms\",\n        \"Taille\": f\"{model_size_mb:.2f} MB\"\n    })\n\ndf_web_results = pd.DataFrame(web_results)\nprint(df_web_results)\n\n# 3.8 Conseils pour l'int\u00e9gration dans une application web\nprint(\"\\n--- CONSEILS POUR L'INT\u00c9GRATION WEB ---\")\nprint(\"\"\"\nPour int\u00e9grer efficacement votre mod\u00e8le dans une application web:\n\n1. Format de d\u00e9ploiement:\n   - TensorFlow.js pour ex\u00e9cution c\u00f4t\u00e9 client (navigateur)\n   - TensorFlow Serving pour API REST c\u00f4t\u00e9 serveur\n   - TensorFlow Lite pour applications mobiles\n\n2. Optimisations importantes:\n   - Quantification pour r\u00e9duire la taille et acc\u00e9l\u00e9rer l'inf\u00e9rence\n   - Batch processing pour les requ\u00eates multiples\n   - Mise en cache des r\u00e9sultats fr\u00e9quents\n\n3. Architecture recommand\u00e9e:\n   - API Python (Flask/FastAPI) exposant le mod\u00e8le\n   - Frontend JavaScript/React pour l'interface utilisateur\n   - WebSockets pour les pr\u00e9dictions en temps r\u00e9el\n\n4. Consid\u00e9rations de performances:\n   - Limiter la taille des images upload\u00e9es\n   - Pr\u00e9traitement c\u00f4t\u00e9 client quand c'est possible\n   - Monitoring des temps de r\u00e9ponse\n\"\"\")\n\n# 4. EXERCICE FINAL\n# ================\n\nprint(\"\\n\\n--- EXERCICE FINAL: AM\u00c9LIORATION DE MOD\u00c8LE ---\")\nprint(\"\"\"\nVotre mission: Am\u00e9liorer un mod\u00e8le de reconnaissance de chiffres manuscrits pour une application web.\n\nObjectifs:\n1. Atteindre une pr\u00e9cision d'au moins 98% sur MNIST\n2. R\u00e9duire le temps d'inf\u00e9rence \u00e0 moins de 10ms par image\n3. Maintenir la taille du mod\u00e8le sous 1 MB\n\nApproche sugg\u00e9r\u00e9e:\n1. Commencez par analyser les performances du mod\u00e8le de base\n2. Identifiez les points faibles (pr\u00e9cision, vitesse, taille)\n3. Testez diff\u00e9rentes am\u00e9liorations en isolant leur impact\n4. Documentez vos modifications et leurs effets\n5. Pr\u00e9parez le mod\u00e8le final pour le d\u00e9ploiement\n\nRessources:\n- Dataset MNIST int\u00e9gr\u00e9 dans TensorFlow\n- Mod\u00e8le de base fourni\n- Documentation TensorFlow et TensorFlow Lite\n\nBonus:\n- Cr\u00e9ez une interface web simple pour tester le mod\u00e8le\n- Impl\u00e9mentez la reconnaissance en temps r\u00e9el via la webcam\n\"\"\")\n\n# Mod\u00e8le de base pour l'exercice\ndef create_exercise_base_model():\n    \"\"\"Cr\u00e9e un mod\u00e8le de base pour l'exercice final\"\"\"\n    model = models.Sequential([\n        layers.Flatten(input_shape=(28, 28)),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(10, activation='softmax')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# Charger MNIST\n(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = tf.keras.datasets.mnist.load_data()\nmnist_train_images = mnist_train_images / 255.0\nmnist_test_images = mnist_test_images / 255.0\n\n# Entra\u00eener le mod\u00e8le de base\nexercise_model = create_exercise_base_model()\nexercise_model.fit(\n    mnist_train_images, mnist_train_labels,\n    epochs=3,  # Intentionnellement peu d'\u00e9poques\n    batch_size=128,\n    validation_split=0.2,\n    verbose=1\n)\n\n# \u00c9valuer le mod\u00e8le de base\ntest_loss, test_acc = exercise_model.evaluate(mnist_test_images, mnist_test_labels, verbose=0)\nprint(f\"\\nMod\u00e8le de base pour l'exercice:\")\nprint(f\"- Pr\u00e9cision: {test_acc*100:.2f}%\")\n\n# Temps d'inf\u00e9rence\nstart_time = time.time()\nexercise_model.predict(mnist_test_images[:100])\ninference_time = (time.time() - start_time) / 100\nprint(f\"- Temps d'inf\u00e9rence: {inference_time*1000:.2f} ms par image\")\n\n# Taille du mod\u00e8le\nexercise_model.save(\"exercise_base.h5\")\nmodel_size_mb = os.path.getsize(\"exercise_base.h5\") / (1024 * 1024)\nprint(f\"- Taille du mod\u00e8le: {model_size_mb:.2f} MB\")\n\nprint(\"\\n\u00c0 vous de jouer! Am\u00e9liorez ce mod\u00e8le pour atteindre les objectifs.\")\n\n# 5. CONCLUSION\n# ============\n\nprint(\"\\n\\n--- CONCLUSION ---\")\nprint(\"\"\"\nPoints cl\u00e9s \u00e0 retenir:\n\n1. Diagnostic:\n   - Observer les courbes d'apprentissage pour d\u00e9tecter le surapprentissage\n   - Analyser la distribution des donn\u00e9es et les performances par classe\n   - Mesurer le temps d'inf\u00e9rence et la taille du mod\u00e8le pour applications web\n\n2. Solutions au surapprentissage:\n   - R\u00e9gularisation (Dropout, L2)\n   - Early Stopping\n   - Augmentation de donn\u00e9es\n   - Mod\u00e8les plus simples\n\n3. Solutions aux probl\u00e8mes de donn\u00e9es:\n   - Pond\u00e9ration des classes\n   - R\u00e9\u00e9chantillonnage (sur/sous-\u00e9chantillonnage)\n   - Stratification des ensembles d'entra\u00eenement/validation\n\n4. Optimisation pour le web:\n   - Architectures l\u00e9g\u00e8res (SeparableConv2D)\n   - Quantification\n   - TensorFlow Lite ou TensorFlow.js\n   - Pr\u00e9traitement efficace\n\nL'am\u00e9lioration d'un mod\u00e8le est un processus it\u00e9ratif qui demande de:\n1. Identifier pr\u00e9cis\u00e9ment le probl\u00e8me\n2. Tester une solution \u00e0 la fois\n3. Mesurer objectivement l'impact\n4. Documenter les r\u00e9sultats\n\nCes comp\u00e9tences sont directement valorisables en stage et en entreprise!\n\"\"\")\n# Am\u00e9lioration des performances de mod\u00e8les de Deep Learning\n# Notebook pour BTS SIO - S\u00e9ance 3\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport time\n\nprint(\"TensorFlow version:\", tf.__version__)\n\n# 1. DIAGNOSTIC: SURAPPRENTISSAGE\n# ===============================\n\n# 1.1 Chargement du jeu de donn\u00e9es Fashion MNIST\nprint(\"\\n--- CAS 1: DIAGNOSTIC DU SURAPPRENTISSAGE ---\")\nprint(\"Chargement des donn\u00e9es...\")\n\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n\n# Normalisation\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n\n# Noms des classes\nclass_names = ['T-shirt/top', 'Pantalon', 'Pull', 'Robe', 'Manteau',\n               'Sandale', 'Chemise', 'Baskets', 'Sac', 'Bottine']\n\nprint(f\"Dimensions des donn\u00e9es: {train_images.shape[0]} images d'entra\u00eenement, {test_images.shape[0]} images de test\")\n\n# 1.2 Cr\u00e9er un mod\u00e8le volontairement sujet au surapprentissage\ndef create_overfitting_model():\n    \"\"\"Cr\u00e9e un mod\u00e8le intentionnellement sujet au surapprentissage\"\"\"\n    model = models.Sequential([\n        layers.Flatten(input_shape=(28, 28)),\n        layers.Dense(512, activation='relu'),  # Couche tr\u00e8s large\n        layers.Dense(512, activation='relu'),  # Seconde couche large\n        layers.Dense(10, activation='softmax')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\n# 1.3 Entra\u00eenement du mod\u00e8le sujet au surapprentissage\nprint(\"Entra\u00eenement du mod\u00e8le sujet au surapprentissage...\")\nmodel_overfit = create_overfitting_model()\n\n# Utiliser peu de donn\u00e9es pour acc\u00e9l\u00e9rer le surapprentissage\nhistory_overfit = model_overfit.fit(\n    train_images[:6000], train_labels[:6000],\n    epochs=30,\n    batch_size=32,\n    validation_split=0.2,\n    verbose=1\n)\n\n# 1.4 Visualisation du surapprentissage\ndef plot_learning_curves(history, title=\"Courbes d'apprentissage\"):\n    \"\"\"Trace les courbes d'apprentissage \u00e0 partir de l'historique d'entra\u00eenement\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Courbe de pr\u00e9cision\n    ax1.plot(history.history['accuracy'], label='Entra\u00eenement')\n    ax1.plot(history.history['val_accuracy'], label='Validation')\n    ax1.set_xlabel('\u00c9poque')\n    ax1.set_ylabel('Pr\u00e9cision')\n    ax1.set_title('\u00c9volution de la pr\u00e9cision')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Courbe de perte\n    ax2.plot(history.history['loss'], label='Entra\u00eenement')\n    ax2.plot(history.history['val_loss'], label='Validation')\n    ax2.set_xlabel('\u00c9poque')\n    ax2.set_ylabel('Perte')\n    ax2.set_title('\u00c9volution de la fonction de perte')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()\n\nplot_learning_curves(history_overfit, \"Surapprentissage: \u00c9cart croissant entre entra\u00eenement et validation\")\n\n# 1.5 \u00c9valuation sur les donn\u00e9es de test\ntest_loss, test_acc = model_overfit.evaluate(test_images, test_labels, verbose=0)\nprint(f\"Pr\u00e9cision sur les donn\u00e9es d'entra\u00eenement: {history_overfit.history['accuracy'][-1]*100:.2f}%\")\nprint(f\"Pr\u00e9cision sur les donn\u00e9es de validation: {history_overfit.history['val_accuracy'][-1]*100:.2f}%\")\nprint(f\"Pr\u00e9cision sur les donn\u00e9es de test: {test_acc*100:.2f}%\")\nprint(\"\\nNOTE: Un grand \u00e9cart entre entra\u00eenement et validation/test indique un surapprentissage.\")\n\n# 1.6 Solution 1: R\u00e9gularisation L2 (Weight Decay)\nprint(\"\\n--- SOLUTION 1: R\u00c9GULARISATION L2 ---\")\n\ndef create_l2_model():\n    \"\"\"Cr\u00e9e un mod\u00e8le avec r\u00e9gularisation L2\"\"\"\n    model = models.Sequential([\n        layers.Flatten(input_shape=(28, 28)),\n        layers.Dense(512, activation='relu', \n                    kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Ajout de L2\n        layers.Dense(512, activation='relu',\n                    kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Ajout de L2\n        layers.Dense(10, activation='softmax')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\nmodel_l2 = create_l2_model()\n\nhistory_l2 = model_l2.fit(\n    train_images[:6000], train_labels[:6000],\n    epochs=30,\n    batch_size=32,\n    validation_split=0.2,\n    verbose=1\n)\n\nplot_learning_curves(history_l2, \"Avec r\u00e9gularisation L2: R\u00e9duction du surapprentissage\")\n\n# 1.7 Solution 2: Dropout\nprint(\"\\n--- SOLUTION 2: DROPOUT ---\")\n\ndef create_dropout_model():\n    \"\"\"Cr\u00e9e un mod\u00e8le avec Dropout\"\"\"\n    model = models.Sequential([\n        layers.Flatten(input_shape=(28, 28)),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),  # D\u00e9sactive al\u00e9atoirement 50% des neurones\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),  # D\u00e9sactive al\u00e9atoirement 50% des neurones\n        layers.Dense(10, activation='softmax')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\nmodel_dropout = create_dropout_model()\n\nhistory_dropout = model_dropout.fit(\n    train_images[:6000], train_labels[:6000],\n    epochs=30,\n    batch_size=32,\n    validation_split=0.2,\n    verbose=1\n)\n\nplot_learning_curves(history_dropout, \"Avec Dropout: R\u00e9gularisation efficace\")\n\n# 1.8 Solution 3: Early Stopping\nprint(\"\\n--- SOLUTION 3: EARLY STOPPING ---\")\n\ndef create_base_model():\n    \"\"\"Cr\u00e9e un mod\u00e8le de base identique au mod\u00e8le de surapprentissage\"\"\"\n    model = models.Sequential([\n        layers.Flatten(input_shape=(28, 28)),\n        layers.Dense(512, activation='relu'),\n        layers.Dense(512, activation='relu'),\n        layers.Dense(10, activation='softmax')\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\nmodel_early = create_base_model()\n\n# Ajout du callback Early Stopping\nearly_stopping = callbacks.EarlyStopping(\n    monitor='val_loss',    # Surveille la perte sur validation\n    patience=5,            # Nombre d'\u00e9poques sans am\u00e9lioration avant l'arr\u00eat\n    restore_best_weights=True  # Restaure les meilleurs poids\n)\n\nhistory_early = model_early.fit(\n    train_images[:6000], train_labels[:6000],\n    epochs=30,\n    batch_size=32,\n    validation_split=0.2,\n    callbacks=[early_stopping],\n    verbose=1\n)\n\nplot_learning_curves(history_early, \"Avec Early Stopping: Arr\u00eat avant la d\u00e9gradation\")\n\n# 1.9 Comparaison des solutions\nprint(\"\\n--- COMPARAISON DES SOLUTIONS CONTRE LE SURAPPRENTISSAGE ---\")\n\n# \u00c9valuation sur les donn\u00e9es de test\nmodels = {\n    \"Base (surapprentissage)\": model_overfit,\n    \"R\u00e9gularisation L2\": model_l2,\n    \"Dropout\": model_dropout,\n    \"Early Stopping\": model_early\n}\n\n# Tableau comparatif\nresults = []\nfor name, model in models.items():\n    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n    results.append({\n        \"Mod\u00e8le\": name,\n        \"Pr\u00e9cision Test\": f\"{test_acc*100:.2f}%\"\n    })\n\ndf_results = pd.DataFrame(results)\nprint(df_results)\n\n# 2. DIAGNOSTIC: PROBL\u00c8MES DE DONN\u00c9ES\n# ==================================\n\nprint(\"\\n\\n--- CAS 2: DIAGNOSTIC DES PROBL\u00c8MES DE DONN\u00c9ES ---\")\n\n# 2.1 Simulation de probl\u00e8mes de donn\u00e9es\nprint(\"Simulation de probl\u00e8mes de donn\u00e9es...\")\n\n# Cr\u00e9ation d'un jeu de donn\u00e9es d\u00e9s\u00e9quilibr\u00e9\nnp.random.seed(42)\nn_samples = 3000\n\n# S\u00e9lectionner principalement des T-shirts et pantalons\nselected_classes = [0, 1]  # T-shirt et pantalon\nmask_majority = np.isin(train_labels[:n_samples], selected_classes)\n# Ajouter quelques exemples des autres classes\nmask_minority = ~mask_majority\nminority_indices = np.where(mask_minority)[0][:500]  # Seulement 500 exemples des autres classes\n\n# Combiner pour cr\u00e9er un dataset d\u00e9s\u00e9quilibr\u00e9\ncombined_indices = np.concatenate([np.where(mask_majority)[0], minority_indices])\nimbalanced_images = train_images[combined_indices]\nimbalanced_labels = train_labels[combined_indices]\n\n# 2.2 Visualisation du d\u00e9s\u00e9quilibre\nplt.figure(figsize=(10, 6))\nclass_counts = np.bincount(imbalanced_labels)\nplt.bar(range(len(class_names)), class_counts)\nplt.xticks(range(len(class_names)), class_names, rotation=90)\nplt.title(\"Distribution d\u00e9s\u00e9quilibr\u00e9e des classes\")\nplt.xlabel(\"Classe\")\nplt.ylabel(\"Nombre d'\u00e9chantillons\")\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nDistribution des classes:\")\nfor i, count in enumerate(class_counts):\n    print(f\"{class_names[i]}: {count} \u00e9chantillons ({count/len(imbalanced_labels)*100:.1f}%)\")\n\n# 2.3 Entra\u00eenement sur donn\u00e9es d\u00e9s\u00e9quilibr\u00e9es\nprint(\"\\nEntra\u00eenement sur donn\u00e9es d\u00e9s\u00e9quilibr\u00e9es...\")\n\n# Diviser en ensembles d'entra\u00eenement et de validation\nfrom sklearn.model_selection import train_test_split\n\nX_train_imb, X_val_imb, y_train_imb, y_val_imb = train_test_split(\n    imbalanced_images, imbalanced_labels, test_size=0.2, random_state=42\n)\n\nimbalanced_model = create_base_model()\nhistory_imbalanced = imbalanced_model.fit(\n    X_train_imb, y_train_imb,\n    epochs=10,\n    batch_size=32,\n    validation_data=(X_val_imb, y_val_imb),\n    verbose=1\n)\n\n# 2.4 \u00c9valuation du mod\u00e8le d\u00e9s\u00e9quilibr\u00e9\nimbalanced_predictions = imbalanced_model.predict(test_images)\nimbalanced_pred_classes = np.argmax(imbalanced_predictions, axis=1)\n\n# Matrice de confusion\ncm = confusion_matrix(test_labels, imbalanced_pred_classes)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.title(\"Matrice de confusion - Mod\u00e8le entra\u00een\u00e9 sur donn\u00e9es d\u00e9s\u00e9quilibr\u00e9es\")\nplt.ylabel('Valeur r\u00e9elle')\nplt.xlabel('Valeur pr\u00e9dite')\nplt.tight_layout()\nplt.show()\n\n# Rapport de classification\nprint(\"\\nRapport de classification - Mod\u00e8le d\u00e9s\u00e9quilibr\u00e9:\")\nprint(classification_report(test_labels, imbalanced_pred_classes, target_names=class_names))\n\n# 2.5 Solution: Pond\u00e9ration des classes\nprint(\"\\n--- SOLUTION 1: POND\u00c9RATION DES CLASSES ---\")\n\n# Calculer les poids des classes\ntotal_samples = len(imbalanced_labels)\nn_classes = len(np.unique(imbalanced_labels))\nclass_weights = {}\n\nfor i in range(n_classes):\n    class_weights[i] = total_samples / (n_classes * class_counts[i]) if class_counts[i] &gt; 0 else 0\n\nprint(\"Poids des classes:\")\nfor i, weight in class_weights.items():\n    if class_counts[i] &gt; 0:\n        print(f\"{class_names[i]}: {weight:.2f}\")\n\n# Entra\u00eener avec des poids de classe\nweighted_model = create_base_model()\nhistory_weighted = weighted_model.fit(\n    X_train_imb, y_train_imb,\n    epochs=10,\n    batch_size=32,\n    validation_data=(X_val_imb, y_val_imb),\n    class_weight=class_weights,\n    verbose=1\n)\n\n# \u00c9valuation du mod\u00e8le pond\u00e9r\u00e9\nweighted_predictions = weighted_model.predict(test_images)\nweighted_pred_classes = np.argmax(weighted_predictions, axis=1)\n\n# Matrice de confusion\ncm_weighted = confusion_matrix(test_labels, weighted_pred_classes)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm_weighted, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.title(\"Matrice de confusion - Mod\u00e8le avec pond\u00e9ration des classes\")\nplt.ylabel('Valeur r\u00e9elle')\nplt.xlabel('Valeur pr\u00e9dite')\nplt.tight_layout()\nplt.show()\n\n# Rapport de classification\nprint(\"\\nRapport de classification - Mod\u00e8le avec pond\u00e9ration des classes:\")\nprint(classification_report(test_labels, weighted_pred_classes, target_names=class_names))\n</pre> # 2.6 Solution 2: R\u00e9\u00e9chantillonnage print(\"\\n--- SOLUTION 2: R\u00c9\u00c9CHANTILLONNAGE ---\")  # Sous-\u00e9chantillonnage des classes majoritaires et sur-\u00e9chantillonnage des classes minoritaires from sklearn.utils import resample  # S\u00e9parer les exemples par classe class_samples = [] for i in range(n_classes):     class_samples.append(imbalanced_images[imbalanced_labels == i])  # D\u00e9finir la taille cible (moyenne) target_size = int(total_samples / n_classes) print(f\"Taille cible par classe apr\u00e8s r\u00e9\u00e9chantillonnage: {target_size}\")  # R\u00e9\u00e9chantillonner chaque classe resampled_images = [] resampled_labels = []  for i in range(n_classes):     if len(class_samples[i]) == 0:         continue              # Sur-\u00e9chantillonnage pour les classes minoritaires     if len(class_samples[i]) &lt; target_size:         resampled = resample(class_samples[i],                              replace=True,        # Avec remplacement                             n_samples=target_size,                             random_state=42)     # Sous-\u00e9chantillonnage pour les classes majoritaires     elif len(class_samples[i]) &gt; target_size:         resampled = resample(class_samples[i],                             replace=False,       # Sans remplacement                             n_samples=target_size,                             random_state=42)     else:         resampled = class_samples[i]          resampled_images.append(resampled)     resampled_labels.extend([i] * len(resampled))  # Convertir en arrays numpy resampled_images = np.vstack([img for img in resampled_images if len(img) &gt; 0]) resampled_labels = np.array(resampled_labels)  # V\u00e9rifier la nouvelle distribution plt.figure(figsize=(10, 6)) resampled_class_counts = np.bincount(resampled_labels, minlength=n_classes) plt.bar(range(len(class_names)), resampled_class_counts) plt.xticks(range(len(class_names)), class_names, rotation=90) plt.title(\"Distribution apr\u00e8s r\u00e9\u00e9chantillonnage\") plt.xlabel(\"Classe\") plt.ylabel(\"Nombre d'\u00e9chantillons\") plt.tight_layout() plt.show()  # Diviser en ensembles d'entra\u00eenement et de validation X_train_res, X_val_res, y_train_res, y_val_res = train_test_split(     resampled_images, resampled_labels, test_size=0.2, random_state=42 )  # Entra\u00eener sur les donn\u00e9es r\u00e9\u00e9chantillonn\u00e9es resampled_model = create_base_model() history_resampled = resampled_model.fit(     X_train_res, y_train_res,     epochs=10,     batch_size=32,     validation_data=(X_val_res, y_val_res),     verbose=1 )  # \u00c9valuation du mod\u00e8le r\u00e9\u00e9chantillonn\u00e9 resampled_predictions = resampled_model.predict(test_images) resampled_pred_classes = np.argmax(resampled_predictions, axis=1)  # Rapport de classification print(\"\\nRapport de classification - Mod\u00e8le avec r\u00e9\u00e9chantillonnage:\") print(classification_report(test_labels, resampled_pred_classes, target_names=class_names))  # 2.7 Comparaison des solutions pour les probl\u00e8mes de donn\u00e9es print(\"\\n--- COMPARAISON DES SOLUTIONS POUR LES PROBL\u00c8MES DE DONN\u00c9ES ---\")  # Comparaison des performances par classe models_data = {     \"D\u00e9s\u00e9quilibr\u00e9\": imbalanced_pred_classes,     \"Pond\u00e9ration\": weighted_pred_classes,     \"R\u00e9\u00e9chantillonnage\": resampled_pred_classes }  # Calculer la pr\u00e9cision par classe pour chaque mod\u00e8le class_accuracies = {}  for model_name, predictions in models_data.items():     accuracies = []     for class_idx in range(n_classes):         # Indices des exemples de cette classe         class_indices = np.where(test_labels == class_idx)[0]         # Pr\u00e9cision sur cette classe         class_acc = np.mean(predictions[class_indices] == test_labels[class_indices])         accuracies.append(class_acc)     class_accuracies[model_name] = accuracies  # Visualisation des pr\u00e9cisions par classe plt.figure(figsize=(12, 6)) x = np.arange(len(class_names)) width = 0.25 multiplier = 0  for model_name, accuracies in class_accuracies.items():     offset = width * multiplier     plt.bar(x + offset, accuracies, width, label=model_name)     multiplier += 1  plt.xlabel('Classe') plt.ylabel('Pr\u00e9cision') plt.title('Pr\u00e9cision par classe et par mod\u00e8le') plt.xticks(x + width, class_names, rotation=90) plt.legend(loc='lower center') plt.grid(True, axis='y', alpha=0.3) plt.tight_layout() plt.show()  # 3. AM\u00c9LIORATION POUR APPLICATION WEB # ===================================  print(\"\\n\\n--- AM\u00c9LIORATION D'UN MOD\u00c8LE POUR APPLICATION WEB ---\")  # 3.1 D\u00e9finition des contraintes d'une application web print(\"Dans un contexte d'application web, un mod\u00e8le doit \u00eatre:\") print(\"1. Rapide (temps d'inf\u00e9rence court)\") print(\"2. L\u00e9ger (taille r\u00e9duite)\") print(\"3. Pr\u00e9cis sur les cas d'utilisation r\u00e9els\") print(\"4. Robuste aux variations (rotations, luminosit\u00e9, etc.)\")  # 3.2 Mod\u00e8le de base pour une application de reconnaissance de v\u00eatements print(\"\\nCr\u00e9ation d'un mod\u00e8le de base...\")  def create_web_model():     \"\"\"Cr\u00e9e un mod\u00e8le CNN simple pour la classification de v\u00eatements\"\"\"     model = models.Sequential([         # Reshaping pour CNN         layers.Reshape((28, 28, 1), input_shape=(28, 28)),                  # Premi\u00e8re couche de convolution         layers.Conv2D(32, (3, 3), activation='relu'),         layers.MaxPooling2D((2, 2)),                  # Deuxi\u00e8me couche de convolution         layers.Conv2D(64, (3, 3), activation='relu'),         layers.MaxPooling2D((2, 2)),                  # Aplatissement et couches denses         layers.Flatten(),         layers.Dense(128, activation='relu'),         layers.Dense(10, activation='softmax')     ])          model.compile(         optimizer='adam',         loss='sparse_categorical_crossentropy',         metrics=['accuracy']     )          return model  # Entra\u00eenement du mod\u00e8le de base base_web_model = create_web_model()  # Reshape pour CNN train_images_reshaped = train_images.reshape(-1, 28, 28, 1) test_images_reshaped = test_images.reshape(-1, 28, 28, 1)  history_base_web = base_web_model.fit(     train_images_reshaped, train_labels,     epochs=5,     batch_size=64,     validation_split=0.2,     verbose=1 )  # 3.3 Mesure des performances initiales print(\"\\n\u00c9valuation des performances initiales...\")  # Pr\u00e9cision test_loss, test_acc = base_web_model.evaluate(test_images_reshaped, test_labels, verbose=0) print(f\"Pr\u00e9cision sur test: {test_acc*100:.2f}%\")  # Temps d'inf\u00e9rence start_time = time.time() base_web_model.predict(test_images_reshaped[:100]) inference_time = (time.time() - start_time) / 100  # Temps moyen par image print(f\"Temps d'inf\u00e9rence moyen: {inference_time*1000:.2f} ms par image\")  # Taille du mod\u00e8le base_web_model.save(\"base_web_model.h5\") import os model_size_mb = os.path.getsize(\"base_web_model.h5\") / (1024 * 1024) print(f\"Taille du mod\u00e8le: {model_size_mb:.2f} MB\")  # 3.4 Am\u00e9lioration 1: Data Augmentation print(\"\\n--- AM\u00c9LIORATION 1: DATA AUGMENTATION ---\")  # Cr\u00e9er un g\u00e9n\u00e9rateur de donn\u00e9es avec augmentation data_augmentation = tf.keras.Sequential([     layers.RandomRotation(0.1),     layers.RandomZoom(0.1),     layers.RandomFlip(\"horizontal\"),     layers.RandomTranslation(0.1, 0.1) ])  def create_augmented_model():     \"\"\"Cr\u00e9e un mod\u00e8le avec data augmentation int\u00e9gr\u00e9e\"\"\"     model = models.Sequential([         # Reshaping pour CNN         layers.Reshape((28, 28, 1), input_shape=(28, 28)),                  # Couche d'augmentation de donn\u00e9es (active uniquement pendant l'entra\u00eenement)         data_augmentation,                  # Premi\u00e8re couche de convolution         layers.Conv2D(32, (3, 3), activation='relu'),         layers.MaxPooling2D((2, 2)),                  # Deuxi\u00e8me couche de convolution         layers.Conv2D(64, (3, 3), activation='relu'),         layers.MaxPooling2D((2, 2)),                  # Aplatissement et couches denses         layers.Flatten(),         layers.Dense(128, activation='relu'),         layers.Dense(10, activation='softmax')     ])          model.compile(         optimizer='adam',         loss='sparse_categorical_crossentropy',         metrics=['accuracy']     )          return model  augmented_model = create_augmented_model()  # Visualiser des exemples d'augmentation plt.figure(figsize=(10, 10)) for i in range(9):     augmented_image = data_augmentation(train_images_reshaped[i:i+1])     ax = plt.subplot(3, 3, i + 1)     plt.imshow(augmented_image[0, :, :, 0], cmap='gray')     plt.title(class_names[train_labels[i]])     plt.axis(\"off\") plt.tight_layout() plt.show()  # Entra\u00eenement avec augmentation history_augmented = augmented_model.fit(     train_images_reshaped, train_labels,     epochs=10,     batch_size=64,     validation_split=0.2,     verbose=1 )  # 3.5 Am\u00e9lioration 2: Architecture plus l\u00e9g\u00e8re print(\"\\n--- AM\u00c9LIORATION 2: ARCHITECTURE PLUS L\u00c9G\u00c8RE ---\")  def create_lightweight_model():     \"\"\"Cr\u00e9e un mod\u00e8le plus l\u00e9ger avec s\u00e9parable convolutions\"\"\"     model = models.Sequential([         # Reshaping pour CNN         layers.Reshape((28, 28, 1), input_shape=(28, 28)),                  # Premi\u00e8re couche de convolution s\u00e9parable (moins de param\u00e8tres)         layers.SeparableConv2D(32, (3, 3), activation='relu'),         layers.MaxPooling2D((2, 2)),                  # Deuxi\u00e8me couche de convolution s\u00e9parable         layers.SeparableConv2D(64, (3, 3), activation='relu'),         layers.MaxPooling2D((2, 2)),                  # Aplatissement et couches denses         layers.Flatten(),         layers.Dense(64, activation='relu'),  # Plus petite que l'original         layers.Dense(10, activation='softmax')     ])          model.compile(         optimizer='adam',         loss='sparse_categorical_crossentropy',         metrics=['accuracy']     )          return model  lightweight_model = create_lightweight_model() lightweight_model.summary()  # Afficher le r\u00e9sum\u00e9 pour voir la r\u00e9duction de param\u00e8tres  # Entra\u00eenement du mod\u00e8le l\u00e9ger history_lightweight = lightweight_model.fit(     train_images_reshaped, train_labels,     epochs=10,     batch_size=64,     validation_split=0.2,     verbose=1 )  # 3.6 Am\u00e9lioration 3: Optimisation pour le d\u00e9ploiement print(\"\\n--- AM\u00c9LIORATION 3: OPTIMISATION POUR LE D\u00c9PLOIEMENT ---\")  # Convertir en TensorFlow Lite pour le d\u00e9ploiement web/mobile # (Simul\u00e9 ici pour d\u00e9monstration)  print(\"Conversion TensorFlow \u2192 TensorFlow Lite...\") converter = tf.lite.TFLiteConverter.from_keras_model(lightweight_model) tflite_model = converter.convert()  # \u00c9crire le mod\u00e8le TFLite dans un fichier with open('model_lite.tflite', 'wb') as f:     f.write(tflite_model)  # Taille du mod\u00e8le TFLite tflite_model_size_mb = os.path.getsize(\"model_lite.tflite\") / (1024 * 1024) print(f\"Taille du mod\u00e8le TFLite: {tflite_model_size_mb:.2f} MB\")  # Quantification (simulation) print(\"\\nQuantification pour r\u00e9duire davantage la taille...\") converter = tf.lite.TFLiteConverter.from_keras_model(lightweight_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] quantized_tflite_model = converter.convert()  # \u00c9crire le mod\u00e8le quantifi\u00e9 dans un fichier with open('model_quantized.tflite', 'wb') as f:     f.write(quantized_tflite_model)  # Taille du mod\u00e8le quantifi\u00e9 quantized_model_size_mb = os.path.getsize(\"model_quantized.tflite\") / (1024 * 1024) print(f\"Taille du mod\u00e8le quantifi\u00e9: {quantized_model_size_mb:.2f} MB\")  # 3.7 Comparaison finale des am\u00e9liorations print(\"\\n--- COMPARAISON FINALE DES AM\u00c9LIORATIONS ---\")  # \u00c9valuation des mod\u00e8les am\u00e9lior\u00e9s models_web = {     \"Base\": base_web_model,     \"Augment\u00e9\": augmented_model,     \"L\u00e9ger\": lightweight_model }  # Tableau comparatif web_results = [] for name, model in models_web.items():     # Mesurer la pr\u00e9cision     test_loss, test_acc = model.evaluate(test_images_reshaped, test_labels, verbose=0)          # Mesurer le temps d'inf\u00e9rence     start_time = time.time()     model.predict(test_images_reshaped[:100])     inference_time = (time.time() - start_time) / 100          # Sauvegarder et mesurer la taille     model_filename = f\"{name.lower()}_model.h5\"     model.save(model_filename)     model_size_mb = os.path.getsize(model_filename) / (1024 * 1024)          web_results.append({         \"Mod\u00e8le\": name,         \"Pr\u00e9cision\": f\"{test_acc*100:.2f}%\",         \"Temps d'inf\u00e9rence\": f\"{inference_time*1000:.2f} ms\",         \"Taille\": f\"{model_size_mb:.2f} MB\"     })  df_web_results = pd.DataFrame(web_results) print(df_web_results)  # 3.8 Conseils pour l'int\u00e9gration dans une application web print(\"\\n--- CONSEILS POUR L'INT\u00c9GRATION WEB ---\") print(\"\"\" Pour int\u00e9grer efficacement votre mod\u00e8le dans une application web:  1. Format de d\u00e9ploiement:    - TensorFlow.js pour ex\u00e9cution c\u00f4t\u00e9 client (navigateur)    - TensorFlow Serving pour API REST c\u00f4t\u00e9 serveur    - TensorFlow Lite pour applications mobiles  2. Optimisations importantes:    - Quantification pour r\u00e9duire la taille et acc\u00e9l\u00e9rer l'inf\u00e9rence    - Batch processing pour les requ\u00eates multiples    - Mise en cache des r\u00e9sultats fr\u00e9quents  3. Architecture recommand\u00e9e:    - API Python (Flask/FastAPI) exposant le mod\u00e8le    - Frontend JavaScript/React pour l'interface utilisateur    - WebSockets pour les pr\u00e9dictions en temps r\u00e9el  4. Consid\u00e9rations de performances:    - Limiter la taille des images upload\u00e9es    - Pr\u00e9traitement c\u00f4t\u00e9 client quand c'est possible    - Monitoring des temps de r\u00e9ponse \"\"\")  # 4. EXERCICE FINAL # ================  print(\"\\n\\n--- EXERCICE FINAL: AM\u00c9LIORATION DE MOD\u00c8LE ---\") print(\"\"\" Votre mission: Am\u00e9liorer un mod\u00e8le de reconnaissance de chiffres manuscrits pour une application web.  Objectifs: 1. Atteindre une pr\u00e9cision d'au moins 98% sur MNIST 2. R\u00e9duire le temps d'inf\u00e9rence \u00e0 moins de 10ms par image 3. Maintenir la taille du mod\u00e8le sous 1 MB  Approche sugg\u00e9r\u00e9e: 1. Commencez par analyser les performances du mod\u00e8le de base 2. Identifiez les points faibles (pr\u00e9cision, vitesse, taille) 3. Testez diff\u00e9rentes am\u00e9liorations en isolant leur impact 4. Documentez vos modifications et leurs effets 5. Pr\u00e9parez le mod\u00e8le final pour le d\u00e9ploiement  Ressources: - Dataset MNIST int\u00e9gr\u00e9 dans TensorFlow - Mod\u00e8le de base fourni - Documentation TensorFlow et TensorFlow Lite  Bonus: - Cr\u00e9ez une interface web simple pour tester le mod\u00e8le - Impl\u00e9mentez la reconnaissance en temps r\u00e9el via la webcam \"\"\")  # Mod\u00e8le de base pour l'exercice def create_exercise_base_model():     \"\"\"Cr\u00e9e un mod\u00e8le de base pour l'exercice final\"\"\"     model = models.Sequential([         layers.Flatten(input_shape=(28, 28)),         layers.Dense(128, activation='relu'),         layers.Dense(10, activation='softmax')     ])          model.compile(         optimizer='adam',         loss='sparse_categorical_crossentropy',         metrics=['accuracy']     )          return model  # Charger MNIST (mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = tf.keras.datasets.mnist.load_data() mnist_train_images = mnist_train_images / 255.0 mnist_test_images = mnist_test_images / 255.0  # Entra\u00eener le mod\u00e8le de base exercise_model = create_exercise_base_model() exercise_model.fit(     mnist_train_images, mnist_train_labels,     epochs=3,  # Intentionnellement peu d'\u00e9poques     batch_size=128,     validation_split=0.2,     verbose=1 )  # \u00c9valuer le mod\u00e8le de base test_loss, test_acc = exercise_model.evaluate(mnist_test_images, mnist_test_labels, verbose=0) print(f\"\\nMod\u00e8le de base pour l'exercice:\") print(f\"- Pr\u00e9cision: {test_acc*100:.2f}%\")  # Temps d'inf\u00e9rence start_time = time.time() exercise_model.predict(mnist_test_images[:100]) inference_time = (time.time() - start_time) / 100 print(f\"- Temps d'inf\u00e9rence: {inference_time*1000:.2f} ms par image\")  # Taille du mod\u00e8le exercise_model.save(\"exercise_base.h5\") model_size_mb = os.path.getsize(\"exercise_base.h5\") / (1024 * 1024) print(f\"- Taille du mod\u00e8le: {model_size_mb:.2f} MB\")  print(\"\\n\u00c0 vous de jouer! Am\u00e9liorez ce mod\u00e8le pour atteindre les objectifs.\")  # 5. CONCLUSION # ============  print(\"\\n\\n--- CONCLUSION ---\") print(\"\"\" Points cl\u00e9s \u00e0 retenir:  1. Diagnostic:    - Observer les courbes d'apprentissage pour d\u00e9tecter le surapprentissage    - Analyser la distribution des donn\u00e9es et les performances par classe    - Mesurer le temps d'inf\u00e9rence et la taille du mod\u00e8le pour applications web  2. Solutions au surapprentissage:    - R\u00e9gularisation (Dropout, L2)    - Early Stopping    - Augmentation de donn\u00e9es    - Mod\u00e8les plus simples  3. Solutions aux probl\u00e8mes de donn\u00e9es:    - Pond\u00e9ration des classes    - R\u00e9\u00e9chantillonnage (sur/sous-\u00e9chantillonnage)    - Stratification des ensembles d'entra\u00eenement/validation  4. Optimisation pour le web:    - Architectures l\u00e9g\u00e8res (SeparableConv2D)    - Quantification    - TensorFlow Lite ou TensorFlow.js    - Pr\u00e9traitement efficace  L'am\u00e9lioration d'un mod\u00e8le est un processus it\u00e9ratif qui demande de: 1. Identifier pr\u00e9cis\u00e9ment le probl\u00e8me 2. Tester une solution \u00e0 la fois 3. Mesurer objectivement l'impact 4. Documenter les r\u00e9sultats  Ces comp\u00e9tences sont directement valorisables en stage et en entreprise! \"\"\") # Am\u00e9lioration des performances de mod\u00e8les de Deep Learning # Notebook pour BTS SIO - S\u00e9ance 3  import tensorflow as tf from tensorflow.keras import layers, models, optimizers, callbacks import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from sklearn.metrics import confusion_matrix, classification_report import time  print(\"TensorFlow version:\", tf.__version__)  # 1. DIAGNOSTIC: SURAPPRENTISSAGE # ===============================  # 1.1 Chargement du jeu de donn\u00e9es Fashion MNIST print(\"\\n--- CAS 1: DIAGNOSTIC DU SURAPPRENTISSAGE ---\") print(\"Chargement des donn\u00e9es...\")  (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()  # Normalisation train_images = train_images / 255.0 test_images = test_images / 255.0  # Noms des classes class_names = ['T-shirt/top', 'Pantalon', 'Pull', 'Robe', 'Manteau',                'Sandale', 'Chemise', 'Baskets', 'Sac', 'Bottine']  print(f\"Dimensions des donn\u00e9es: {train_images.shape[0]} images d'entra\u00eenement, {test_images.shape[0]} images de test\")  # 1.2 Cr\u00e9er un mod\u00e8le volontairement sujet au surapprentissage def create_overfitting_model():     \"\"\"Cr\u00e9e un mod\u00e8le intentionnellement sujet au surapprentissage\"\"\"     model = models.Sequential([         layers.Flatten(input_shape=(28, 28)),         layers.Dense(512, activation='relu'),  # Couche tr\u00e8s large         layers.Dense(512, activation='relu'),  # Seconde couche large         layers.Dense(10, activation='softmax')     ])          model.compile(         optimizer='adam',         loss='sparse_categorical_crossentropy',         metrics=['accuracy']     )          return model  # 1.3 Entra\u00eenement du mod\u00e8le sujet au surapprentissage print(\"Entra\u00eenement du mod\u00e8le sujet au surapprentissage...\") model_overfit = create_overfitting_model()  # Utiliser peu de donn\u00e9es pour acc\u00e9l\u00e9rer le surapprentissage history_overfit = model_overfit.fit(     train_images[:6000], train_labels[:6000],     epochs=30,     batch_size=32,     validation_split=0.2,     verbose=1 )  # 1.4 Visualisation du surapprentissage def plot_learning_curves(history, title=\"Courbes d'apprentissage\"):     \"\"\"Trace les courbes d'apprentissage \u00e0 partir de l'historique d'entra\u00eenement\"\"\"     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))          # Courbe de pr\u00e9cision     ax1.plot(history.history['accuracy'], label='Entra\u00eenement')     ax1.plot(history.history['val_accuracy'], label='Validation')     ax1.set_xlabel('\u00c9poque')     ax1.set_ylabel('Pr\u00e9cision')     ax1.set_title('\u00c9volution de la pr\u00e9cision')     ax1.legend()     ax1.grid(True, alpha=0.3)          # Courbe de perte     ax2.plot(history.history['loss'], label='Entra\u00eenement')     ax2.plot(history.history['val_loss'], label='Validation')     ax2.set_xlabel('\u00c9poque')     ax2.set_ylabel('Perte')     ax2.set_title('\u00c9volution de la fonction de perte')     ax2.legend()     ax2.grid(True, alpha=0.3)          plt.suptitle(title)     plt.tight_layout()     plt.show()  plot_learning_curves(history_overfit, \"Surapprentissage: \u00c9cart croissant entre entra\u00eenement et validation\")  # 1.5 \u00c9valuation sur les donn\u00e9es de test test_loss, test_acc = model_overfit.evaluate(test_images, test_labels, verbose=0) print(f\"Pr\u00e9cision sur les donn\u00e9es d'entra\u00eenement: {history_overfit.history['accuracy'][-1]*100:.2f}%\") print(f\"Pr\u00e9cision sur les donn\u00e9es de validation: {history_overfit.history['val_accuracy'][-1]*100:.2f}%\") print(f\"Pr\u00e9cision sur les donn\u00e9es de test: {test_acc*100:.2f}%\") print(\"\\nNOTE: Un grand \u00e9cart entre entra\u00eenement et validation/test indique un surapprentissage.\")  # 1.6 Solution 1: R\u00e9gularisation L2 (Weight Decay) print(\"\\n--- SOLUTION 1: R\u00c9GULARISATION L2 ---\")  def create_l2_model():     \"\"\"Cr\u00e9e un mod\u00e8le avec r\u00e9gularisation L2\"\"\"     model = models.Sequential([         layers.Flatten(input_shape=(28, 28)),         layers.Dense(512, activation='relu',                      kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Ajout de L2         layers.Dense(512, activation='relu',                     kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Ajout de L2         layers.Dense(10, activation='softmax')     ])          model.compile(         optimizer='adam',         loss='sparse_categorical_crossentropy',         metrics=['accuracy']     )          return model  model_l2 = create_l2_model()  history_l2 = model_l2.fit(     train_images[:6000], train_labels[:6000],     epochs=30,     batch_size=32,     validation_split=0.2,     verbose=1 )  plot_learning_curves(history_l2, \"Avec r\u00e9gularisation L2: R\u00e9duction du surapprentissage\")  # 1.7 Solution 2: Dropout print(\"\\n--- SOLUTION 2: DROPOUT ---\")  def create_dropout_model():     \"\"\"Cr\u00e9e un mod\u00e8le avec Dropout\"\"\"     model = models.Sequential([         layers.Flatten(input_shape=(28, 28)),         layers.Dense(512, activation='relu'),         layers.Dropout(0.5),  # D\u00e9sactive al\u00e9atoirement 50% des neurones         layers.Dense(512, activation='relu'),         layers.Dropout(0.5),  # D\u00e9sactive al\u00e9atoirement 50% des neurones         layers.Dense(10, activation='softmax')     ])          model.compile(         optimizer='adam',         loss='sparse_categorical_crossentropy',         metrics=['accuracy']     )          return model  model_dropout = create_dropout_model()  history_dropout = model_dropout.fit(     train_images[:6000], train_labels[:6000],     epochs=30,     batch_size=32,     validation_split=0.2,     verbose=1 )  plot_learning_curves(history_dropout, \"Avec Dropout: R\u00e9gularisation efficace\")  # 1.8 Solution 3: Early Stopping print(\"\\n--- SOLUTION 3: EARLY STOPPING ---\")  def create_base_model():     \"\"\"Cr\u00e9e un mod\u00e8le de base identique au mod\u00e8le de surapprentissage\"\"\"     model = models.Sequential([         layers.Flatten(input_shape=(28, 28)),         layers.Dense(512, activation='relu'),         layers.Dense(512, activation='relu'),         layers.Dense(10, activation='softmax')     ])          model.compile(         optimizer='adam',         loss='sparse_categorical_crossentropy',         metrics=['accuracy']     )          return model  model_early = create_base_model()  # Ajout du callback Early Stopping early_stopping = callbacks.EarlyStopping(     monitor='val_loss',    # Surveille la perte sur validation     patience=5,            # Nombre d'\u00e9poques sans am\u00e9lioration avant l'arr\u00eat     restore_best_weights=True  # Restaure les meilleurs poids )  history_early = model_early.fit(     train_images[:6000], train_labels[:6000],     epochs=30,     batch_size=32,     validation_split=0.2,     callbacks=[early_stopping],     verbose=1 )  plot_learning_curves(history_early, \"Avec Early Stopping: Arr\u00eat avant la d\u00e9gradation\")  # 1.9 Comparaison des solutions print(\"\\n--- COMPARAISON DES SOLUTIONS CONTRE LE SURAPPRENTISSAGE ---\")  # \u00c9valuation sur les donn\u00e9es de test models = {     \"Base (surapprentissage)\": model_overfit,     \"R\u00e9gularisation L2\": model_l2,     \"Dropout\": model_dropout,     \"Early Stopping\": model_early }  # Tableau comparatif results = [] for name, model in models.items():     test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)     results.append({         \"Mod\u00e8le\": name,         \"Pr\u00e9cision Test\": f\"{test_acc*100:.2f}%\"     })  df_results = pd.DataFrame(results) print(df_results)  # 2. DIAGNOSTIC: PROBL\u00c8MES DE DONN\u00c9ES # ==================================  print(\"\\n\\n--- CAS 2: DIAGNOSTIC DES PROBL\u00c8MES DE DONN\u00c9ES ---\")  # 2.1 Simulation de probl\u00e8mes de donn\u00e9es print(\"Simulation de probl\u00e8mes de donn\u00e9es...\")  # Cr\u00e9ation d'un jeu de donn\u00e9es d\u00e9s\u00e9quilibr\u00e9 np.random.seed(42) n_samples = 3000  # S\u00e9lectionner principalement des T-shirts et pantalons selected_classes = [0, 1]  # T-shirt et pantalon mask_majority = np.isin(train_labels[:n_samples], selected_classes) # Ajouter quelques exemples des autres classes mask_minority = ~mask_majority minority_indices = np.where(mask_minority)[0][:500]  # Seulement 500 exemples des autres classes  # Combiner pour cr\u00e9er un dataset d\u00e9s\u00e9quilibr\u00e9 combined_indices = np.concatenate([np.where(mask_majority)[0], minority_indices]) imbalanced_images = train_images[combined_indices] imbalanced_labels = train_labels[combined_indices]  # 2.2 Visualisation du d\u00e9s\u00e9quilibre plt.figure(figsize=(10, 6)) class_counts = np.bincount(imbalanced_labels) plt.bar(range(len(class_names)), class_counts) plt.xticks(range(len(class_names)), class_names, rotation=90) plt.title(\"Distribution d\u00e9s\u00e9quilibr\u00e9e des classes\") plt.xlabel(\"Classe\") plt.ylabel(\"Nombre d'\u00e9chantillons\") plt.tight_layout() plt.show()  print(\"\\nDistribution des classes:\") for i, count in enumerate(class_counts):     print(f\"{class_names[i]}: {count} \u00e9chantillons ({count/len(imbalanced_labels)*100:.1f}%)\")  # 2.3 Entra\u00eenement sur donn\u00e9es d\u00e9s\u00e9quilibr\u00e9es print(\"\\nEntra\u00eenement sur donn\u00e9es d\u00e9s\u00e9quilibr\u00e9es...\")  # Diviser en ensembles d'entra\u00eenement et de validation from sklearn.model_selection import train_test_split  X_train_imb, X_val_imb, y_train_imb, y_val_imb = train_test_split(     imbalanced_images, imbalanced_labels, test_size=0.2, random_state=42 )  imbalanced_model = create_base_model() history_imbalanced = imbalanced_model.fit(     X_train_imb, y_train_imb,     epochs=10,     batch_size=32,     validation_data=(X_val_imb, y_val_imb),     verbose=1 )  # 2.4 \u00c9valuation du mod\u00e8le d\u00e9s\u00e9quilibr\u00e9 imbalanced_predictions = imbalanced_model.predict(test_images) imbalanced_pred_classes = np.argmax(imbalanced_predictions, axis=1)  # Matrice de confusion cm = confusion_matrix(test_labels, imbalanced_pred_classes) plt.figure(figsize=(10, 8)) sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names) plt.title(\"Matrice de confusion - Mod\u00e8le entra\u00een\u00e9 sur donn\u00e9es d\u00e9s\u00e9quilibr\u00e9es\") plt.ylabel('Valeur r\u00e9elle') plt.xlabel('Valeur pr\u00e9dite') plt.tight_layout() plt.show()  # Rapport de classification print(\"\\nRapport de classification - Mod\u00e8le d\u00e9s\u00e9quilibr\u00e9:\") print(classification_report(test_labels, imbalanced_pred_classes, target_names=class_names))  # 2.5 Solution: Pond\u00e9ration des classes print(\"\\n--- SOLUTION 1: POND\u00c9RATION DES CLASSES ---\")  # Calculer les poids des classes total_samples = len(imbalanced_labels) n_classes = len(np.unique(imbalanced_labels)) class_weights = {}  for i in range(n_classes):     class_weights[i] = total_samples / (n_classes * class_counts[i]) if class_counts[i] &gt; 0 else 0  print(\"Poids des classes:\") for i, weight in class_weights.items():     if class_counts[i] &gt; 0:         print(f\"{class_names[i]}: {weight:.2f}\")  # Entra\u00eener avec des poids de classe weighted_model = create_base_model() history_weighted = weighted_model.fit(     X_train_imb, y_train_imb,     epochs=10,     batch_size=32,     validation_data=(X_val_imb, y_val_imb),     class_weight=class_weights,     verbose=1 )  # \u00c9valuation du mod\u00e8le pond\u00e9r\u00e9 weighted_predictions = weighted_model.predict(test_images) weighted_pred_classes = np.argmax(weighted_predictions, axis=1)  # Matrice de confusion cm_weighted = confusion_matrix(test_labels, weighted_pred_classes) plt.figure(figsize=(10, 8)) sns.heatmap(cm_weighted, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names) plt.title(\"Matrice de confusion - Mod\u00e8le avec pond\u00e9ration des classes\") plt.ylabel('Valeur r\u00e9elle') plt.xlabel('Valeur pr\u00e9dite') plt.tight_layout() plt.show()  # Rapport de classification print(\"\\nRapport de classification - Mod\u00e8le avec pond\u00e9ration des classes:\") print(classification_report(test_labels, weighted_pred_classes, target_names=class_names))"},{"location":"seance3/ressources/api-fastapi-template/","title":"Api fastapi template","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nTemplate d'API FastAPI pour exposer un mod\u00e8le de Deep Learning\nVersion: 1.0\nPour:  S\u00e9ance 3\n\nFonctionnalit\u00e9s incluses:\n- Structure de base d'une API FastAPI\n- Chargement d'un mod\u00e8le TensorFlow\n- Pr\u00e9traitement des images\n- Endpoint de pr\u00e9diction\n- Documentation automatique des endpoints (Swagger UI)\n- Gestion des erreurs\n- Rate limiting simple\n- Syst\u00e8me de cache basique\n\"\"\"\n</pre> \"\"\" Template d'API FastAPI pour exposer un mod\u00e8le de Deep Learning Version: 1.0 Pour:  S\u00e9ance 3  Fonctionnalit\u00e9s incluses: - Structure de base d'une API FastAPI - Chargement d'un mod\u00e8le TensorFlow - Pr\u00e9traitement des images - Endpoint de pr\u00e9diction - Documentation automatique des endpoints (Swagger UI) - Gestion des erreurs - Rate limiting simple - Syst\u00e8me de cache basique \"\"\" In\u00a0[\u00a0]: Copied! <pre>from fastapi import FastAPI, File, UploadFile, HTTPException, Request, Depends, BackgroundTasks\nfrom fastapi.responses import JSONResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nimport tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nimport io\nimport os\nimport time\nimport hashlib\nimport logging\nfrom functools import wraps\nfrom datetime import datetime, timedelta\nimport json\nimport uvicorn\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Optional, Any\n</pre> from fastapi import FastAPI, File, UploadFile, HTTPException, Request, Depends, BackgroundTasks from fastapi.responses import JSONResponse from fastapi.middleware.cors import CORSMiddleware from fastapi.staticfiles import StaticFiles from fastapi.templating import Jinja2Templates import tensorflow as tf import numpy as np from PIL import Image import io import os import time import hashlib import logging from functools import wraps from datetime import datetime, timedelta import json import uvicorn from pydantic import BaseModel from typing import List, Dict, Optional, Any In\u00a0[\u00a0]: Copied! <pre># Configuration de l'application\napp = FastAPI(\n    title=\"API de Deep Learning\",\n    description=\"API pour exposer un mod\u00e8le de Deep Learning pour la classification d'images\",\n    version=\"1.0.0\"\n)\n</pre> # Configuration de l'application app = FastAPI(     title=\"API de Deep Learning\",     description=\"API pour exposer un mod\u00e8le de Deep Learning pour la classification d'images\",     version=\"1.0.0\" ) In\u00a0[\u00a0]: Copied! <pre># Permettre CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n</pre> # Permettre CORS app.add_middleware(     CORSMiddleware,     allow_origins=[\"*\"],     allow_credentials=True,     allow_methods=[\"*\"],     allow_headers=[\"*\"], ) In\u00a0[\u00a0]: Copied! <pre># Configuration du logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler()]\n)\nlogger = logging.getLogger(\"api-dl\")\n</pre> # Configuration du logging logging.basicConfig(     level=logging.INFO,     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',     handlers=[logging.StreamHandler()] ) logger = logging.getLogger(\"api-dl\") In\u00a0[\u00a0]: Copied! <pre># Variables de configuration (\u00e0 ajuster selon vos besoins)\nMODEL_PATH = \"mon_modele.h5\"  # Chemin vers votre mod\u00e8le sauvegard\u00e9\nCLASS_NAMES = []  # Noms des classes que votre mod\u00e8le peut pr\u00e9dire\nMAX_REQUESTS_PER_MINUTE = 60  # Limite de requ\u00eates par minute\nCACHE_DURATION = 300  # Dur\u00e9e de cache en secondes (5 minutes)\n</pre> # Variables de configuration (\u00e0 ajuster selon vos besoins) MODEL_PATH = \"mon_modele.h5\"  # Chemin vers votre mod\u00e8le sauvegard\u00e9 CLASS_NAMES = []  # Noms des classes que votre mod\u00e8le peut pr\u00e9dire MAX_REQUESTS_PER_MINUTE = 60  # Limite de requ\u00eates par minute CACHE_DURATION = 300  # Dur\u00e9e de cache en secondes (5 minutes) In\u00a0[\u00a0]: Copied! <pre># Syst\u00e8me de limite de requ\u00eates simple\nrequest_counts = {}  # Format: {ip: {'count': n, 'last_reset': timestamp}}\n</pre> # Syst\u00e8me de limite de requ\u00eates simple request_counts = {}  # Format: {ip: {'count': n, 'last_reset': timestamp}} In\u00a0[\u00a0]: Copied! <pre># Syst\u00e8me de cache simple\nprediction_cache = {}  # Format: {image_hash: {'prediction': result, 'timestamp': time}}\n</pre> # Syst\u00e8me de cache simple prediction_cache = {}  # Format: {image_hash: {'prediction': result, 'timestamp': time}} In\u00a0[\u00a0]: Copied! <pre># --- Chargement du mod\u00e8le ---\ndef load_model():\n    \"\"\"Charge le mod\u00e8le TensorFlow depuis le fichier sauvegard\u00e9\"\"\"\n    try:\n        model = tf.keras.models.load_model(MODEL_PATH)\n        logger.info(f\"Mod\u00e8le charg\u00e9 avec succ\u00e8s depuis {MODEL_PATH}\")\n        return model\n    except Exception as e:\n        logger.error(f\"Erreur lors du chargement du mod\u00e8le: {str(e)}\")\n        # Retourner None indique un \u00e9chec de chargement\n        return None\n</pre> # --- Chargement du mod\u00e8le --- def load_model():     \"\"\"Charge le mod\u00e8le TensorFlow depuis le fichier sauvegard\u00e9\"\"\"     try:         model = tf.keras.models.load_model(MODEL_PATH)         logger.info(f\"Mod\u00e8le charg\u00e9 avec succ\u00e8s depuis {MODEL_PATH}\")         return model     except Exception as e:         logger.error(f\"Erreur lors du chargement du mod\u00e8le: {str(e)}\")         # Retourner None indique un \u00e9chec de chargement         return None In\u00a0[\u00a0]: Copied! <pre># Charger le mod\u00e8le au d\u00e9marrage de l'application\nmodel = load_model()\n</pre> # Charger le mod\u00e8le au d\u00e9marrage de l'application model = load_model() In\u00a0[\u00a0]: Copied! <pre># --- Mod\u00e8les de donn\u00e9es avec Pydantic ---\nclass PredictionResult(BaseModel):\n    prediction: int\n    confidence: float\n    class_name: Optional[str] = None\n    processing_time: float\n</pre> # --- Mod\u00e8les de donn\u00e9es avec Pydantic --- class PredictionResult(BaseModel):     prediction: int     confidence: float     class_name: Optional[str] = None     processing_time: float In\u00a0[\u00a0]: Copied! <pre>class ErrorResponse(BaseModel):\n    error: str\n    details: Optional[str] = None\n</pre> class ErrorResponse(BaseModel):     error: str     details: Optional[str] = None In\u00a0[\u00a0]: Copied! <pre># --- Middleware de limiter de requ\u00eates ---\n@app.middleware(\"http\")\nasync def rate_limit_middleware(request: Request, call_next):\n    # Obtenir l'adresse IP du client\n    ip = request.client.host\n    \n    # V\u00e9rifier uniquement les routes API\n    if request.url.path.startswith(\"/api/\"):\n        now = time.time()\n        \n        # Initialiser ou r\u00e9initialiser le compteur si n\u00e9cessaire\n        if ip not in request_counts or now - request_counts[ip][\"last_reset\"] &gt; 60:\n            request_counts[ip] = {\"count\": 0, \"last_reset\": now}\n        \n        # V\u00e9rifier la limite\n        if request_counts[ip][\"count\"] &gt;= MAX_REQUESTS_PER_MINUTE:\n            return JSONResponse(\n                status_code=429,\n                content={\"error\": \"Too many requests\", \"retry_after\": \"60 seconds\"}\n            )\n        \n        # Incr\u00e9menter le compteur\n        request_counts[ip][\"count\"] += 1\n    \n    # Continuer le traitement de la requ\u00eate\n    response = await call_next(request)\n    return response\n</pre> # --- Middleware de limiter de requ\u00eates --- @app.middleware(\"http\") async def rate_limit_middleware(request: Request, call_next):     # Obtenir l'adresse IP du client     ip = request.client.host          # V\u00e9rifier uniquement les routes API     if request.url.path.startswith(\"/api/\"):         now = time.time()                  # Initialiser ou r\u00e9initialiser le compteur si n\u00e9cessaire         if ip not in request_counts or now - request_counts[ip][\"last_reset\"] &gt; 60:             request_counts[ip] = {\"count\": 0, \"last_reset\": now}                  # V\u00e9rifier la limite         if request_counts[ip][\"count\"] &gt;= MAX_REQUESTS_PER_MINUTE:             return JSONResponse(                 status_code=429,                 content={\"error\": \"Too many requests\", \"retry_after\": \"60 seconds\"}             )                  # Incr\u00e9menter le compteur         request_counts[ip][\"count\"] += 1          # Continuer le traitement de la requ\u00eate     response = await call_next(request)     return response In\u00a0[\u00a0]: Copied! <pre># --- Fonctions de pr\u00e9traitement ---\ndef get_image_hash(image_data):\n    \"\"\"\n    G\u00e9n\u00e8re un hash unique pour une image donn\u00e9e\n    \"\"\"\n    return hashlib.md5(image_data).hexdigest()\n</pre> # --- Fonctions de pr\u00e9traitement --- def get_image_hash(image_data):     \"\"\"     G\u00e9n\u00e8re un hash unique pour une image donn\u00e9e     \"\"\"     return hashlib.md5(image_data).hexdigest() In\u00a0[\u00a0]: Copied! <pre>def preprocess_image(image, target_size=(224, 224)):\n    \"\"\"\n    Pr\u00e9traite une image pour la pr\u00e9diction\n    \n    Args:\n        image (PIL.Image): Image \u00e0 pr\u00e9traiter\n        target_size (tuple): Taille cible de l'image\n        \n    Returns:\n        numpy.ndarray: Image pr\u00e9trait\u00e9e pr\u00eate pour la pr\u00e9diction\n    \"\"\"\n    # Redimensionner\n    image = image.resize(target_size)\n    \n    # Convertir en tableau numpy\n    image_array = np.array(image)\n    \n    # Normaliser les valeurs de pixels entre 0 et 1\n    image_array = image_array / 255.0\n    \n    # Ajouter la dimension de batch\n    image_array = np.expand_dims(image_array, axis=0)\n    \n    return image_array\n</pre> def preprocess_image(image, target_size=(224, 224)):     \"\"\"     Pr\u00e9traite une image pour la pr\u00e9diction          Args:         image (PIL.Image): Image \u00e0 pr\u00e9traiter         target_size (tuple): Taille cible de l'image              Returns:         numpy.ndarray: Image pr\u00e9trait\u00e9e pr\u00eate pour la pr\u00e9diction     \"\"\"     # Redimensionner     image = image.resize(target_size)          # Convertir en tableau numpy     image_array = np.array(image)          # Normaliser les valeurs de pixels entre 0 et 1     image_array = image_array / 255.0          # Ajouter la dimension de batch     image_array = np.expand_dims(image_array, axis=0)          return image_array In\u00a0[\u00a0]: Copied! <pre># --- Routes API ---\n@app.get(\"/\")\nasync def root():\n    \"\"\"Page d'accueil de l'API\"\"\"\n    return {\"message\": \"Bienvenue sur l'API de Deep Learning. Acc\u00e9dez \u00e0 /docs pour la documentation.\"}\n</pre> # --- Routes API --- @app.get(\"/\") async def root():     \"\"\"Page d'accueil de l'API\"\"\"     return {\"message\": \"Bienvenue sur l'API de Deep Learning. Acc\u00e9dez \u00e0 /docs pour la documentation.\"} In\u00a0[\u00a0]: Copied! <pre>@app.post(\"/api/predict\", response_model=PredictionResult)\nasync def predict(file: UploadFile = File(...)):\n    \"\"\"\n    Pr\u00e9dit la classe d'une image\n    \n    Args:\n        file: Fichier image \u00e0 classifier\n        \n    Returns:\n        PredictionResult: R\u00e9sultat de la pr\u00e9diction\n    \"\"\"\n    # V\u00e9rifier que le mod\u00e8le est charg\u00e9\n    if model is None:\n        raise HTTPException(status_code=500, detail=\"Mod\u00e8le non disponible\")\n    \n    # Lire l'image\n    try:\n        image_data = await file.read()\n        image = Image.open(io.BytesIO(image_data))\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Erreur lors de la lecture de l'image: {str(e)}\")\n    \n    # V\u00e9rifier si l'image est dans le cache\n    image_hash = get_image_hash(image_data)\n    current_time = time.time()\n    \n    if image_hash in prediction_cache:\n        cache_entry = prediction_cache[image_hash]\n        if current_time - cache_entry[\"timestamp\"] &lt; CACHE_DURATION:\n            logger.info(f\"R\u00e9sultat trouv\u00e9 dans le cache pour {file.filename}\")\n            return cache_entry[\"prediction\"]\n    \n    # Pr\u00e9traiter l'image\n    try:\n        processed_image = preprocess_image(image)\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Erreur lors du pr\u00e9traitement: {str(e)}\")\n    \n    # Faire la pr\u00e9diction\n    try:\n        start_time = time.time()\n        predictions = model.predict(processed_image)\n        end_time = time.time()\n        \n        processing_time = end_time - start_time\n        \n        # Extraire la classe pr\u00e9dite et la confiance\n        predicted_class = np.argmax(predictions[0])\n        confidence = float(predictions[0][predicted_class])\n        \n        # Ajouter le nom de classe s'il est disponible\n        class_name = CLASS_NAMES[predicted_class] if predicted_class &lt; len(CLASS_NAMES) else None\n        \n        # Cr\u00e9er le r\u00e9sultat\n        result = PredictionResult(\n            prediction=int(predicted_class),\n            confidence=float(confidence * 100),  # En pourcentage\n            class_name=class_name,\n            processing_time=processing_time\n        )\n        \n        # Mettre en cache le r\u00e9sultat\n        prediction_cache[image_hash] = {\n            \"prediction\": result,\n            \"timestamp\": current_time\n        }\n        \n        # Nettoyer le cache si n\u00e9cessaire\n        if len(prediction_cache) &gt; 100:  # Limiter la taille du cache\n            oldest_keys = sorted(prediction_cache.keys(), \n                               key=lambda k: prediction_cache[k][\"timestamp\"])[:10]\n            for key in oldest_keys:\n                prediction_cache.pop(key, None)\n        \n        logger.info(f\"Pr\u00e9diction r\u00e9ussie pour {file.filename}: classe {predicted_class} avec {confidence*100:.2f}% de confiance\")\n        return result\n        \n    except Exception as e:\n        logger.error(f\"Erreur lors de la pr\u00e9diction: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Erreur lors de la pr\u00e9diction: {str(e)}\")\n</pre> @app.post(\"/api/predict\", response_model=PredictionResult) async def predict(file: UploadFile = File(...)):     \"\"\"     Pr\u00e9dit la classe d'une image          Args:         file: Fichier image \u00e0 classifier              Returns:         PredictionResult: R\u00e9sultat de la pr\u00e9diction     \"\"\"     # V\u00e9rifier que le mod\u00e8le est charg\u00e9     if model is None:         raise HTTPException(status_code=500, detail=\"Mod\u00e8le non disponible\")          # Lire l'image     try:         image_data = await file.read()         image = Image.open(io.BytesIO(image_data))     except Exception as e:         raise HTTPException(status_code=400, detail=f\"Erreur lors de la lecture de l'image: {str(e)}\")          # V\u00e9rifier si l'image est dans le cache     image_hash = get_image_hash(image_data)     current_time = time.time()          if image_hash in prediction_cache:         cache_entry = prediction_cache[image_hash]         if current_time - cache_entry[\"timestamp\"] &lt; CACHE_DURATION:             logger.info(f\"R\u00e9sultat trouv\u00e9 dans le cache pour {file.filename}\")             return cache_entry[\"prediction\"]          # Pr\u00e9traiter l'image     try:         processed_image = preprocess_image(image)     except Exception as e:         raise HTTPException(status_code=400, detail=f\"Erreur lors du pr\u00e9traitement: {str(e)}\")          # Faire la pr\u00e9diction     try:         start_time = time.time()         predictions = model.predict(processed_image)         end_time = time.time()                  processing_time = end_time - start_time                  # Extraire la classe pr\u00e9dite et la confiance         predicted_class = np.argmax(predictions[0])         confidence = float(predictions[0][predicted_class])                  # Ajouter le nom de classe s'il est disponible         class_name = CLASS_NAMES[predicted_class] if predicted_class &lt; len(CLASS_NAMES) else None                  # Cr\u00e9er le r\u00e9sultat         result = PredictionResult(             prediction=int(predicted_class),             confidence=float(confidence * 100),  # En pourcentage             class_name=class_name,             processing_time=processing_time         )                  # Mettre en cache le r\u00e9sultat         prediction_cache[image_hash] = {             \"prediction\": result,             \"timestamp\": current_time         }                  # Nettoyer le cache si n\u00e9cessaire         if len(prediction_cache) &gt; 100:  # Limiter la taille du cache             oldest_keys = sorted(prediction_cache.keys(),                                 key=lambda k: prediction_cache[k][\"timestamp\"])[:10]             for key in oldest_keys:                 prediction_cache.pop(key, None)                  logger.info(f\"Pr\u00e9diction r\u00e9ussie pour {file.filename}: classe {predicted_class} avec {confidence*100:.2f}% de confiance\")         return result              except Exception as e:         logger.error(f\"Erreur lors de la pr\u00e9diction: {str(e)}\")         raise HTTPException(status_code=500, detail=f\"Erreur lors de la pr\u00e9diction: {str(e)}\") In\u00a0[\u00a0]: Copied! <pre>@app.get(\"/api/health\")\nasync def health_check():\n    \"\"\"\n    V\u00e9rifie l'\u00e9tat de sant\u00e9 de l'API\n    \n    Returns:\n        dict: Statut de l'API et du mod\u00e8le\n    \"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"model_loaded\": model is not None,\n        \"timestamp\": datetime.now().isoformat()\n    }\n</pre> @app.get(\"/api/health\") async def health_check():     \"\"\"     V\u00e9rifie l'\u00e9tat de sant\u00e9 de l'API          Returns:         dict: Statut de l'API et du mod\u00e8le     \"\"\"     return {         \"status\": \"healthy\",         \"model_loaded\": model is not None,         \"timestamp\": datetime.now().isoformat()     } In\u00a0[\u00a0]: Copied! <pre>@app.get(\"/api/info\")\nasync def api_info():\n    \"\"\"\n    Fournit des informations sur l'API et le mod\u00e8le\n    \n    Returns:\n        dict: Informations sur l'API et le mod\u00e8le\n    \"\"\"\n    model_info = {}\n    \n    if model is not None:\n        # Extraire les informations du mod\u00e8le\n        model_info = {\n            \"input_shape\": [dim if dim is not None else \"variable\" for dim in model.input_shape],\n            \"output_shape\": [dim if dim is not None else \"variable\" for dim in model.output_shape],\n            \"layers_count\": len(model.layers),\n            \"classes\": len(CLASS_NAMES) if CLASS_NAMES else \"unknown\"\n        }\n    \n    return {\n        \"api_version\": \"1.0.0\",\n        \"model_loaded\": model is not None,\n        \"model_info\": model_info,\n        \"class_names\": CLASS_NAMES,\n        \"max_requests_per_minute\": MAX_REQUESTS_PER_MINUTE\n    }\n</pre> @app.get(\"/api/info\") async def api_info():     \"\"\"     Fournit des informations sur l'API et le mod\u00e8le          Returns:         dict: Informations sur l'API et le mod\u00e8le     \"\"\"     model_info = {}          if model is not None:         # Extraire les informations du mod\u00e8le         model_info = {             \"input_shape\": [dim if dim is not None else \"variable\" for dim in model.input_shape],             \"output_shape\": [dim if dim is not None else \"variable\" for dim in model.output_shape],             \"layers_count\": len(model.layers),             \"classes\": len(CLASS_NAMES) if CLASS_NAMES else \"unknown\"         }          return {         \"api_version\": \"1.0.0\",         \"model_loaded\": model is not None,         \"model_info\": model_info,         \"class_names\": CLASS_NAMES,         \"max_requests_per_minute\": MAX_REQUESTS_PER_MINUTE     } In\u00a0[\u00a0]: Copied! <pre># Configuration des templates et des fichiers statiques (si n\u00e9cessaire)\nif os.path.exists(\"static\"):\n    app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n</pre> # Configuration des templates et des fichiers statiques (si n\u00e9cessaire) if os.path.exists(\"static\"):     app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\") In\u00a0[\u00a0]: Copied! <pre>if os.path.exists(\"templates\"):\n    templates = Jinja2Templates(directory=\"templates\")\n    \n    @app.get(\"/ui\", response_class=HTMLResponse)\n    async def ui(request: Request):\n        \"\"\"Interface utilisateur simple\"\"\"\n        return templates.TemplateResponse(\"index.html\", {\"request\": request})\n</pre> if os.path.exists(\"templates\"):     templates = Jinja2Templates(directory=\"templates\")          @app.get(\"/ui\", response_class=HTMLResponse)     async def ui(request: Request):         \"\"\"Interface utilisateur simple\"\"\"         return templates.TemplateResponse(\"index.html\", {\"request\": request}) In\u00a0[\u00a0]: Copied! <pre># --- Fonction principale ---\ndef main():\n    \"\"\"Point d'entr\u00e9e principal pour l'ex\u00e9cution directe\"\"\"\n    # V\u00e9rifier si les dossiers templates et static existent, sinon les cr\u00e9er\n    os.makedirs(\"static\", exist_ok=True)\n    os.makedirs(\"templates\", exist_ok=True)\n    \n    # Cr\u00e9er un fichier index.html minimal si n\u00e9cessaire\n    if not os.path.exists(\"templates/index.html\"):\n        with open(\"templates/index.html\", \"w\") as f:\n            f.write(\"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Interface de l'API Deep Learning&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: Arial, sans-serif;\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n        }\n        h1 {\n            color: #333;\n            text-align: center;\n        }\n        .container {\n            background-color: #f9f9f9;\n            border-radius: 5px;\n            padding: 20px;\n            margin-top: 20px;\n        }\n        .form-group {\n            margin-bottom: 15px;\n        }\n        label {\n            display: block;\n            margin-bottom: 5px;\n            font-weight: bold;\n        }\n        .btn {\n            background-color: #4CAF50;\n            color: white;\n            padding: 10px 15px;\n            border: none;\n            border-radius: 4px;\n            cursor: pointer;\n        }\n        .result {\n            margin-top: 20px;\n            padding: 15px;\n            border: 1px solid #ddd;\n            border-radius: 4px;\n            display: none;\n        }\n        .preview {\n            max-width: 300px;\n            max-height: 300px;\n            margin-bottom: 10px;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Interface de l'API Deep Learning&lt;/h1&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;h2&gt;Classification d'image&lt;/h2&gt;\n        &lt;div class=\"form-group\"&gt;\n            &lt;label for=\"image\"&gt;S\u00e9lectionnez une image :&lt;/label&gt;\n            &lt;input type=\"file\" id=\"image\" accept=\"image/*\"&gt;\n        &lt;/div&gt;\n        &lt;button class=\"btn\" id=\"predict\"&gt;Pr\u00e9dire&lt;/button&gt;\n        \n        &lt;div class=\"result\" id=\"result\"&gt;\n            &lt;h3&gt;R\u00e9sultat :&lt;/h3&gt;\n            &lt;img id=\"preview\" class=\"preview\"&gt;\n            &lt;p&gt;&lt;strong&gt;Classe pr\u00e9dite :&lt;/strong&gt; &lt;span id=\"prediction\"&gt;&lt;/span&gt;&lt;/p&gt;\n            &lt;p&gt;&lt;strong&gt;Confiance :&lt;/strong&gt; &lt;span id=\"confidence\"&gt;&lt;/span&gt;%&lt;/p&gt;\n            &lt;p&gt;&lt;strong&gt;Temps de traitement :&lt;/strong&gt; &lt;span id=\"time\"&gt;&lt;/span&gt; ms&lt;/p&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n        document.getElementById('predict').addEventListener('click', async () =&gt; {\n            const fileInput = document.getElementById('image');\n            const resultDiv = document.getElementById('result');\n            const predictionSpan = document.getElementById('prediction');\n            const confidenceSpan = document.getElementById('confidence');\n            const timeSpan = document.getElementById('time');\n            const preview = document.getElementById('preview');\n            \n            if (!fileInput.files || fileInput.files.length === 0) {\n                alert('Veuillez s\u00e9lectionner une image');\n                return;\n            }\n            \n            const file = fileInput.files[0];\n            const formData = new FormData();\n            formData.append('file', file);\n            \n            // Afficher l'aper\u00e7u\n            preview.src = URL.createObjectURL(file);\n            \n            try {\n                const response = await fetch('/api/predict', {\n                    method: 'POST',\n                    body: formData\n                });\n                \n                if (!response.ok) {\n                    const error = await response.json();\n                    throw new Error(error.detail || 'Erreur lors de la pr\u00e9diction');\n                }\n                \n                const result = await response.json();\n                \n                predictionSpan.textContent = result.class_name || result.prediction;\n                confidenceSpan.textContent = result.confidence.toFixed(2);\n                timeSpan.textContent = (result.processing_time * 1000).toFixed(2);\n                resultDiv.style.display = 'block';\n                \n            } catch (error) {\n                alert('Erreur: ' + error.message);\n            }\n        });\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n            \"\"\")\n    \n    # D\u00e9marrer le serveur\n    print(f\"D\u00e9marrage du serveur sur http://localhost:8000\")\n    print(\"Documentation API disponible sur http://localhost:8000/docs\")\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</pre> # --- Fonction principale --- def main():     \"\"\"Point d'entr\u00e9e principal pour l'ex\u00e9cution directe\"\"\"     # V\u00e9rifier si les dossiers templates et static existent, sinon les cr\u00e9er     os.makedirs(\"static\", exist_ok=True)     os.makedirs(\"templates\", exist_ok=True)          # Cr\u00e9er un fichier index.html minimal si n\u00e9cessaire     if not os.path.exists(\"templates/index.html\"):         with open(\"templates/index.html\", \"w\") as f:             f.write(\"\"\"  Interface de l'API Deep Learning Interface de l'API Deep Learning Classification d'image S\u00e9lectionnez une image : Pr\u00e9dire R\u00e9sultat : <p>Classe pr\u00e9dite : </p> <p>Confiance : %</p> <p>Temps de traitement :  ms</p>              \"\"\")          # D\u00e9marrer le serveur     print(f\"D\u00e9marrage du serveur sur http://localhost:8000\")     print(\"Documentation API disponible sur http://localhost:8000/docs\")     uvicorn.run(app, host=\"0.0.0.0\", port=8000) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"seance3/ressources/api-mistral-guide-simple/","title":"Guide simplifi\u00e9 d'utilisation de l'API Mistral AI","text":"<p>Ce guide vous explique comment int\u00e9grer l'API Mistral AI dans votre projet de chatbot p\u00e9dagogique de mani\u00e8re simple et efficace.</p>"},{"location":"seance3/ressources/api-mistral-guide-simple/#1-obtenir-une-cle-api","title":"1. Obtenir une cl\u00e9 API","text":""},{"location":"seance3/ressources/api-mistral-guide-simple/#creer-un-compte-et-obtenir-une-cle-gratuite","title":"Cr\u00e9er un compte et obtenir une cl\u00e9 gratuite","text":"<ol> <li>Rendez-vous sur https://console.mistral.ai/</li> <li>Inscrivez-vous avec votre adresse email</li> <li>Connectez-vous \u00e0 votre compte</li> <li>Dans le tableau de bord, cliquez sur \"API Keys\"</li> <li>Cliquez sur \"Create API Key\"</li> <li>Donnez un nom \u00e0 votre cl\u00e9 (ex: \"Projet-Chatbot-BTS\")</li> <li>IMPORTANT: Copiez imm\u00e9diatement la cl\u00e9 g\u00e9n\u00e9r\u00e9e et conservez-la en lieu s\u00fbr. Elle ne sera plus affich\u00e9e ensuite.</li> </ol>"},{"location":"seance3/ressources/api-mistral-guide-simple/#limites-du-compte-gratuit","title":"Limites du compte gratuit","text":"<ul> <li>Un nombre limit\u00e9 de requ\u00eates par minute</li> <li>Utilisation des mod\u00e8les de base (suffisant pour notre projet)</li> <li>Pas d'usage commercial</li> </ul>"},{"location":"seance3/ressources/api-mistral-guide-simple/#2-installation-des-bibliotheques-necessaires","title":"2. Installation des biblioth\u00e8ques n\u00e9cessaires","text":""},{"location":"seance3/ressources/api-mistral-guide-simple/#avec-pip","title":"Avec pip","text":"<pre><code>pip install mistralai requests python-dotenv\n</code></pre>"},{"location":"seance3/ressources/api-mistral-guide-simple/#dans-google-colab","title":"Dans Google Colab","text":"<pre><code>!pip install mistralai requests python-dotenv\n</code></pre>"},{"location":"seance3/ressources/api-mistral-guide-simple/#3-configuration-de-base","title":"3. Configuration de base","text":""},{"location":"seance3/ressources/api-mistral-guide-simple/#fichier-env-pour-stocker-votre-cle-api-bonne-pratique","title":"Fichier .env pour stocker votre cl\u00e9 API (bonne pratique)","text":"<p>Cr\u00e9ez un fichier nomm\u00e9 <code>.env</code> contenant: <pre><code>MISTRAL_API_KEY=votre_cl\u00e9_api_ici\n</code></pre></p>"},{"location":"seance3/ressources/api-mistral-guide-simple/#configuration-du-client-dans-votre-code-python","title":"Configuration du client dans votre code Python","text":"<pre><code>import os\nfrom dotenv import load_dotenv\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\n# Charger la cl\u00e9 API depuis le fichier .env\nload_dotenv()\napi_key = os.getenv(\"MISTRAL_API_KEY\")\n\n# Initialiser le client Mistral\nclient = MistralClient(api_key=api_key)\n\n# Choisir le mod\u00e8le\nmodel = \"mistral-tiny\"  # Mod\u00e8le de base, rapide et peu co\u00fbteux\n</code></pre>"},{"location":"seance3/ressources/api-mistral-guide-simple/#4-utilisation-de-base-pour-un-chatbot","title":"4. Utilisation de base pour un chatbot","text":""},{"location":"seance3/ressources/api-mistral-guide-simple/#creer-une-fonction-simple-dinteraction","title":"Cr\u00e9er une fonction simple d'interaction","text":"<pre><code>def get_ai_response(user_message, conversation_history=None):\n    \"\"\"\n    Obtient une r\u00e9ponse du mod\u00e8le Mistral AI.\n\n    Args:\n        user_message (str): Message de l'utilisateur\n        conversation_history (list, optional): Historique de la conversation\n\n    Returns:\n        str: R\u00e9ponse du mod\u00e8le\n    \"\"\"\n    # Initialiser l'historique si n\u00e9cessaire\n    if conversation_history is None:\n        conversation_history = []\n\n    # Ajouter le message de l'utilisateur \u00e0 l'historique\n    conversation_history.append(ChatMessage(role=\"user\", content=user_message))\n\n    # Obtenir la r\u00e9ponse du mod\u00e8le\n    chat_response = client.chat(\n        model=model,\n        messages=conversation_history\n    )\n\n    # Extraire le texte de la r\u00e9ponse\n    ai_response = chat_response.choices[0].message.content\n\n    # Ajouter la r\u00e9ponse \u00e0 l'historique\n    conversation_history.append(ChatMessage(role=\"assistant\", content=ai_response))\n\n    return ai_response, conversation_history\n</code></pre>"},{"location":"seance3/ressources/api-mistral-guide-simple/#exemple-dutilisation-simple","title":"Exemple d'utilisation simple","text":"<pre><code># Initialiser l'historique avec un message syst\u00e8me pour d\u00e9finir le r\u00f4le du chatbot\nconversation = [\n    ChatMessage(\n        role=\"system\", \n        content=\"Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans le Deep Learning. Tu donnes des explications claires, adapt\u00e9es \u00e0 des \u00e9tudiants de BTS SIO. Tu utilises des exemples concrets et tu expliques les concepts techniques de mani\u00e8re simple.\"\n    )\n]\n\n# Premier \u00e9change\nuser_input = \"Peux-tu m'expliquer ce qu'est un r\u00e9seau de neurones convolutif ?\"\nresponse, conversation = get_ai_response(user_input, conversation)\nprint(\"Assistant:\", response)\n\n# Deuxi\u00e8me \u00e9change (avec contexte de la conversation)\nuser_input = \"Peux-tu me donner un exemple d'application concr\u00e8te ?\"\nresponse, conversation = get_ai_response(user_input, conversation)\nprint(\"Assistant:\", response)\n</code></pre>"},{"location":"seance3/ressources/api-mistral-guide-simple/#5-ameliorations-pour-votre-chatbot-pedagogique","title":"5. Am\u00e9liorations pour votre chatbot p\u00e9dagogique","text":""},{"location":"seance3/ressources/api-mistral-guide-simple/#ajout-dune-base-de-connaissances-specifique","title":"Ajout d'une base de connaissances sp\u00e9cifique","text":"<p>Vous pouvez enrichir vos prompts syst\u00e8me pour guider le mod\u00e8le:</p> <pre><code>def create_education_prompt(topic):\n    \"\"\"Cr\u00e9e un prompt sp\u00e9cifique pour un sujet de Deep Learning\"\"\"\n\n    prompts = {\n        \"introduction\": \"Explique les concepts de base du Deep Learning comme si tu parlais \u00e0 un \u00e9tudiant de BTS SIO qui d\u00e9bute. Utilise des analogies simples et \u00e9vite les formules math\u00e9matiques complexes.\",\n\n        \"cnn\": \"Explique les r\u00e9seaux de neurones convolutifs (CNN) de mani\u00e8re simple. Mentionne leur utilisation dans la vision par ordinateur, la structure en couches de convolution et pooling, et donne des exemples d'applications comme la reconnaissance d'images.\",\n\n        \"rnn\": \"Explique les r\u00e9seaux de neurones r\u00e9currents (RNN) simplement. Pr\u00e9cise leur utilit\u00e9 pour les donn\u00e9es s\u00e9quentielles comme le texte, la notion de m\u00e9moire dans ces r\u00e9seaux, et des exemples comme la g\u00e9n\u00e9ration de texte ou la traduction.\"\n    }\n\n    return prompts.get(topic, \"Explique ce concept de Deep Learning de mani\u00e8re simple pour un \u00e9tudiant de BTS SIO.\")\n</code></pre>"},{"location":"seance3/ressources/api-mistral-guide-simple/#gestion-des-parametres-du-modele","title":"Gestion des param\u00e8tres du mod\u00e8le","text":"<p>Vous pouvez ajuster les param\u00e8tres pour contr\u00f4ler les r\u00e9ponses:</p> <pre><code>def get_ai_response_with_params(user_message, conversation_history=None, temperature=0.7, max_tokens=1000):\n    \"\"\"Version am\u00e9lior\u00e9e avec param\u00e8tres ajustables\"\"\"\n\n    # ... (m\u00eame code que pr\u00e9c\u00e9demment) ...\n\n    # Obtenir la r\u00e9ponse avec param\u00e8tres personnalis\u00e9s\n    chat_response = client.chat(\n        model=model,\n        messages=conversation_history,\n        temperature=temperature,  # Contr\u00f4le la cr\u00e9ativit\u00e9 (0.1-1.0)\n        max_tokens=max_tokens     # Limite la longueur de la r\u00e9ponse\n    )\n\n    # ... (suite identique) ...\n</code></pre>"},{"location":"seance3/ressources/api-mistral-guide-simple/#format-structure-pour-les-explications-pedagogiques","title":"Format structur\u00e9 pour les explications p\u00e9dagogiques","text":"<p>Pour obtenir des r\u00e9ponses plus structur\u00e9es, vous pouvez guider le format:</p> <pre><code>def get_structured_explanation(topic):\n    \"\"\"Obtient une explication structur\u00e9e sur un sujet de Deep Learning\"\"\"\n\n    prompt = f\"\"\"\n    Explique le concept de {topic} en Deep Learning avec la structure suivante:\n\n    1. D\u00e9finition simple (2-3 phrases)\n    2. Fonctionnement de base (sans formules math\u00e9matiques)\n    3. Cas d'usage concrets (2-3 exemples)\n    4. Avantages et limites\n    5. Analogie simple pour comprendre le concept\n\n    Utilise un langage accessible pour des \u00e9tudiants de niveau BTS.\n    \"\"\"\n\n    response, _ = get_ai_response(prompt)\n    return response\n</code></pre>"},{"location":"seance3/ressources/api-mistral-guide-simple/#6-gestion-des-erreurs-et-bonnes-pratiques","title":"6. Gestion des erreurs et bonnes pratiques","text":""},{"location":"seance3/ressources/api-mistral-guide-simple/#gestion-basique-des-erreurs","title":"Gestion basique des erreurs","text":"<pre><code>def safe_ai_request(user_message, conversation_history=None):\n    \"\"\"Fonction avec gestion des erreurs de base\"\"\"\n    try:\n        return get_ai_response(user_message, conversation_history)\n    except Exception as e:\n        error_message = f\"Erreur lors de la communication avec l'API: {str(e)}\"\n        print(error_message)\n        # R\u00e9ponse de secours\n        return \"D\u00e9sol\u00e9, je rencontre des difficult\u00e9s techniques actuellement. Pouvez-vous r\u00e9essayer?\", conversation_history\n</code></pre>"},{"location":"seance3/ressources/api-mistral-guide-simple/#mise-en-cache-des-reponses-frequentes","title":"Mise en cache des r\u00e9ponses fr\u00e9quentes","text":"<pre><code># Cache simple avec dictionnaire\nresponse_cache = {}\n\ndef get_cached_response(question, conversation_history=None):\n    \"\"\"Utilise un cache pour les questions fr\u00e9quentes\"\"\"\n    # Simplifie la question pour la mise en cache\n    simple_question = question.lower().strip()\n\n    if simple_question in response_cache:\n        print(\"R\u00e9ponse trouv\u00e9e dans le cache\")\n        return response_cache[simple_question], conversation_history\n\n    # Si pas dans le cache, obtient la r\u00e9ponse de l'API\n    response, updated_history = get_ai_response(question, conversation_history)\n\n    # Stocke dans le cache\n    response_cache[simple_question] = response\n\n    return response, updated_history\n</code></pre>"},{"location":"seance3/ressources/api-mistral-guide-simple/#7-exemple-complet-dintegration-flask","title":"7. Exemple complet d'int\u00e9gration Flask","text":"<pre><code>from flask import Flask, request, jsonify, render_template\nimport os\nfrom dotenv import load_dotenv\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\n# Configuration\nload_dotenv()\napp = Flask(__name__)\nclient = MistralClient(api_key=os.getenv(\"MISTRAL_API_KEY\"))\nmodel = \"mistral-tiny\"\n\n# Stockage des conversations (simple, pour d\u00e9monstration)\nconversations = {}\n\n@app.route('/')\ndef home():\n    return render_template('index.html')\n\n@app.route('/api/chat', methods=['POST'])\ndef chat():\n    data = request.json\n    user_message = data.get('message', '')\n    session_id = data.get('session_id', 'default')\n\n    # R\u00e9cup\u00e8re ou cr\u00e9e la conversation\n    if session_id not in conversations:\n        conversations[session_id] = [\n            ChatMessage(\n                role=\"system\", \n                content=\"Tu es un assistant p\u00e9dagogique expert en Deep Learning pour des \u00e9tudiants de BTS SIO.\"\n            )\n        ]\n\n    conversation = conversations[session_id]\n\n    # Ajoute le message utilisateur\n    conversation.append(ChatMessage(role=\"user\", content=user_message))\n\n    try:\n        # Obtient la r\u00e9ponse\n        response = client.chat(model=model, messages=conversation)\n        ai_message = response.choices[0].message.content\n\n        # Ajoute la r\u00e9ponse \u00e0 l'historique\n        conversation.append(ChatMessage(role=\"assistant\", content=ai_message))\n\n        return jsonify({\n            'reply': ai_message,\n            'session_id': session_id\n        })\n\n    except Exception as e:\n        return jsonify({\n            'error': str(e),\n            'reply': \"D\u00e9sol\u00e9, une erreur s'est produite.\"\n        }), 500\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"seance3/ressources/api-mistral-guide-simple/#8-ressources-complementaires","title":"8. Ressources compl\u00e9mentaires","text":""},{"location":"seance3/ressources/api-mistral-guide-simple/#documentation-officielle","title":"Documentation officielle","text":"<ul> <li>Documentation Mistral AI</li> <li>API Reference</li> </ul>"},{"location":"seance3/ressources/api-mistral-guide-simple/#templates-et-exemples","title":"Templates et exemples","text":"<ul> <li>GitHub avec exemples d'int\u00e9gration</li> <li>Documentation de la biblioth\u00e8que Python</li> </ul>"},{"location":"seance3/ressources/api-mistral-guide-simple/#tutoriels-recommandes","title":"Tutoriels recommand\u00e9s","text":"<ul> <li>Tutoriel d'int\u00e9gration Flask</li> <li>Am\u00e9liorer la qualit\u00e9 des prompts</li> </ul>"},{"location":"seance3/ressources/api-mistral-guide-simple/#9-conseils-pour-votre-projet-bts","title":"9. Conseils pour votre projet BTS","text":"<ol> <li>Commencez simple : Assurez-vous que l'int\u00e9gration de base fonctionne avant d'ajouter des fonctionnalit\u00e9s avanc\u00e9es</li> <li>Attention \u00e0 votre cl\u00e9 API : Ne la partagez pas et ne la publiez pas dans votre code source</li> <li>Testez vos prompts : La qualit\u00e9 des r\u00e9ponses d\u00e9pend beaucoup de la formulation de vos instructions</li> <li>G\u00e9rez le contexte : Utilisez intelligemment l'historique des conversations pour des r\u00e9ponses coh\u00e9rentes</li> <li>Pr\u00e9voyez les erreurs : Ajoutez une gestion robuste des erreurs d'API</li> <li>Optimisez les co\u00fbts : Limitez la taille des historiques de conversation pour r\u00e9duire les jetons utilis\u00e9s</li> <li>Interface r\u00e9active : Pr\u00e9voyez des indicateurs de chargement pendant les appels API</li> <li>Testez avec des utilisateurs r\u00e9els : Recueillez des feedbacks sur la qualit\u00e9 des r\u00e9ponses</li> </ol>"},{"location":"seance3/ressources/api-mistral-guide-simple/#10-faq-special-bts-sio","title":"10. FAQ sp\u00e9cial BTS SIO","text":""},{"location":"seance3/ressources/api-mistral-guide-simple/#q-ai-je-besoin-dune-carte-de-credit-pour-lapi-mistral-ai","title":"Q: Ai-je besoin d'une carte de cr\u00e9dit pour l'API Mistral AI ?","text":"<p>R: Non, vous pouvez cr\u00e9er un compte gratuit avec un quota limit\u00e9 suffisant pour le projet.</p>"},{"location":"seance3/ressources/api-mistral-guide-simple/#q-comment-stocker-lhistorique-des-conversations","title":"Q: Comment stocker l'historique des conversations ?","text":"<p>R: Pour un projet simple, une structure de donn\u00e9es en m\u00e9moire suffit. Pour un projet plus avanc\u00e9, utilisez une base de donn\u00e9es (SQLite est une option simple).</p>"},{"location":"seance3/ressources/api-mistral-guide-simple/#q-est-ce-que-le-modele-peut-generer-du-code-python","title":"Q: Est-ce que le mod\u00e8le peut g\u00e9n\u00e9rer du code Python ?","text":"<p>R: Oui, Mistral AI peut g\u00e9n\u00e9rer du code Python basique pour des exemples d'utilisation de TensorFlow/Keras.</p>"},{"location":"seance3/ressources/api-mistral-guide-simple/#q-comment-limiter-les-couts-dapi-si-je-veux-deployer-mon-application","title":"Q: Comment limiter les co\u00fbts d'API si je veux d\u00e9ployer mon application ?","text":"<p>R: Limitez la longueur des contextes, mettez en cache les r\u00e9ponses fr\u00e9quentes, et d\u00e9finissez des quotas d'utilisation par utilisateur.</p>"},{"location":"seance3/ressources/api-mistral-guide-simple/#q-comment-faire-comprendre-au-modele-le-contexte-specifique-du-deep-learning","title":"Q: Comment faire comprendre au mod\u00e8le le contexte sp\u00e9cifique du Deep Learning ?","text":"<p>R: Utilisez des messages syst\u00e8me pr\u00e9cis et incluez des exemples de questions/r\u00e9ponses dans le prompt initial.</p>"},{"location":"seance3/ressources/api-mistral-guide-simple/#11-exemple-de-template-htmlcssjs-minimal","title":"11. Exemple de template HTML/CSS/JS minimal","text":"<pre><code>&lt;!-- templates/index.html --&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Chatbot Deep Learning&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: Arial, sans-serif;\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n        }\n        #chat-container {\n            height: 400px;\n            border: 1px solid #ddd;\n            padding: 15px;\n            overflow-y: auto;\n            margin-bottom: 15px;\n            border-radius: 5px;\n        }\n        .message {\n            margin-bottom: 10px;\n            padding: 8px 15px;\n            border-radius: 20px;\n            max-width: 80%;\n            word-wrap: break-word;\n        }\n        .user-message {\n            background-color: #e6f7ff;\n            margin-left: auto;\n            text-align: right;\n            border-bottom-right-radius: 0;\n        }\n        .bot-message {\n            background-color: #f1f1f1;\n            margin-right: auto;\n            border-bottom-left-radius: 0;\n        }\n        #message-form {\n            display: flex;\n        }\n        #user-input {\n            flex: 1;\n            padding: 10px;\n            border: 1px solid #ddd;\n            border-radius: 5px;\n        }\n        #send-button {\n            padding: 10px 20px;\n            background-color: #4CAF50;\n            color: white;\n            border: none;\n            border-radius: 5px;\n            margin-left: 10px;\n            cursor: pointer;\n        }\n        #loading {\n            display: none;\n            text-align: center;\n            margin-top: 10px;\n            color: #888;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Assistant Deep Learning pour BTS SIO&lt;/h1&gt;\n    &lt;div id=\"chat-container\"&gt;&lt;/div&gt;\n    &lt;div id=\"loading\"&gt;L'assistant r\u00e9fl\u00e9chit...&lt;/div&gt;\n    &lt;form id=\"message-form\"&gt;\n        &lt;input type=\"text\" id=\"user-input\" placeholder=\"Posez votre question sur le Deep Learning...\" required&gt;\n        &lt;button type=\"submit\" id=\"send-button\"&gt;Envoyer&lt;/button&gt;\n    &lt;/form&gt;\n\n    &lt;script&gt;\n        let sessionId = 'session_' + Date.now();\n        const chatContainer = document.getElementById('chat-container');\n        const messageForm = document.getElementById('message-form');\n        const userInput = document.getElementById('user-input');\n        const loading = document.getElementById('loading');\n\n        // Ajouter un message d'accueil\n        addBotMessage(\"Bonjour ! Je suis votre assistant Deep Learning. Comment puis-je vous aider aujourd'hui ?\");\n\n        messageForm.addEventListener('submit', async function(e) {\n            e.preventDefault();\n            const message = userInput.value.trim();\n            if (!message) return;\n\n            // Afficher le message de l'utilisateur\n            addUserMessage(message);\n            userInput.value = '';\n\n            // Afficher l'indicateur de chargement\n            loading.style.display = 'block';\n\n            try {\n                // Envoyer la requ\u00eate au serveur\n                const response = await fetch('/api/chat', {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json'\n                    },\n                    body: JSON.stringify({\n                        message: message,\n                        session_id: sessionId\n                    })\n                });\n\n                const data = await response.json();\n\n                if (response.ok) {\n                    // Afficher la r\u00e9ponse du bot\n                    addBotMessage(data.reply);\n                } else {\n                    // Afficher l'erreur\n                    addBotMessage(\"D\u00e9sol\u00e9, une erreur s'est produite: \" + data.error);\n                }\n            } catch (error) {\n                addBotMessage(\"D\u00e9sol\u00e9, je n'ai pas pu communiquer avec le serveur. Veuillez r\u00e9essayer.\");\n                console.error('Error:', error);\n            } finally {\n                // Cacher l'indicateur de chargement\n                loading.style.display = 'none';\n                // Scroll to bottom\n                chatContainer.scrollTop = chatContainer.scrollHeight;\n            }\n        });\n\n        function addUserMessage(text) {\n            const messageDiv = document.createElement('div');\n            messageDiv.className = 'message user-message';\n            messageDiv.textContent = text;\n            chatContainer.appendChild(messageDiv);\n            chatContainer.scrollTop = chatContainer.scrollHeight;\n        }\n\n        function addBotMessage(text) {\n            const messageDiv = document.createElement('div');\n            messageDiv.className = 'message bot-message';\n\n            // Support for markdown-like formatting\n            text = text.replace(/\\*\\*(.*?)\\*\\*/g, '&lt;strong&gt;$1&lt;/strong&gt;');\n            text = text.replace(/\\*(.*?)\\*/g, '&lt;em&gt;$1&lt;/em&gt;');\n            text = text.replace(/```([\\s\\S]*?)```/g, '&lt;pre&gt;&lt;code&gt;$1&lt;/code&gt;&lt;/pre&gt;');\n            text = text.replace(/`(.*?)`/g, '&lt;code&gt;$1&lt;/code&gt;');\n\n            // Convert line breaks\n            text = text.replace(/\\n/g, '&lt;br&gt;');\n\n            messageDiv.innerHTML = text;\n            chatContainer.appendChild(messageDiv);\n            chatContainer.scrollTop = chatContainer.scrollHeight;\n        }\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Ce guide vous a fourni toutes les bases n\u00e9cessaires pour int\u00e9grer l'API Mistral AI dans votre projet de chatbot p\u00e9dagogique. N'h\u00e9sitez pas \u00e0 adapter les exemples \u00e0 vos besoins sp\u00e9cifiques et \u00e0 exp\u00e9rimenter avec diff\u00e9rentes approches pour am\u00e9liorer votre application.</p>"},{"location":"seance3/ressources/cas-utilisation-entreprise/","title":"Cas d'utilisation du Deep Learning en entreprise","text":"<p>Ce document pr\u00e9sente des exemples concrets d'utilisation du Deep Learning dans diff\u00e9rents secteurs d'activit\u00e9.</p>"},{"location":"seance3/ressources/cas-utilisation-entreprise/#retail-et-e-commerce","title":"Retail et e-commerce","text":""},{"location":"seance3/ressources/cas-utilisation-entreprise/#cas-1-moteur-de-recherche-visuelle-pour-site-e-commerce","title":"Cas 1: Moteur de recherche visuelle pour site e-commerce","text":"<p>Entreprise: PME de vente de v\u00eatements en ligne (50 employ\u00e9s) Probl\u00e9matique: Les clients ont du mal \u00e0 trouver des produits similaires \u00e0 ce qu'ils ont d\u00e9j\u00e0 vu Solution impl\u00e9ment\u00e9e: - Utilisation d'un mod\u00e8le ResNet50 pr\u00e9-entra\u00een\u00e9 pour extraire les caract\u00e9ristiques des produits - API Flask exposant une fonction de recherche par similarit\u00e9 visuelle - Interface web permettant de t\u00e9l\u00e9charger une image ou de prendre une photo - Base de donn\u00e9es vectorielle pour stocker les caract\u00e9ristiques des produits</p> <p>Technologies utilis\u00e9es: - TensorFlow/Keras pour le mod\u00e8le - Flask pour l'API - React pour l'interface utilisateur - PostgreSQL avec extension vectorielle</p> <p>R\u00e9sultat: Augmentation de 23% du temps pass\u00e9 sur le site et hausse de 8% du taux de conversion</p> <p>Niveau de complexit\u00e9: \u2b50\u2b50\u2606\u2606\u2606 (Accessible avec les connaissances acquises dans ce cours)</p>"},{"location":"seance3/ressources/cas-utilisation-entreprise/#cas-2-systeme-de-recommandation-de-produits","title":"Cas 2: Syst\u00e8me de recommandation de produits","text":"<p>Entreprise: Site e-commerce d'\u00e9lectronique (20 employ\u00e9s) Probl\u00e9matique: Recommandations basiques bas\u00e9es uniquement sur les cat\u00e9gories Solution impl\u00e9ment\u00e9e: - Syst\u00e8me de recommandation hybride utilisant:   - Filtrage collaboratif pour les suggestions \"Les clients ont aussi achet\u00e9\"   - Analyse des descriptions produits par NLP pour trouver des produits similaires - API REST pour servir les recommandations - Syst\u00e8me de feedback pour am\u00e9liorer les suggestions</p> <p>Technologies utilis\u00e9es: - TensorFlow pour le mod\u00e8le de recommandation - NLTK et TF-IDF pour l'analyse de texte - FastAPI pour l'interface de service - Redis pour le cache</p> <p>R\u00e9sultat: Augmentation de 15% du panier moyen</p> <p>Niveau de complexit\u00e9: \u2b50\u2b50\u2b50\u2606\u2606 (R\u00e9alisable avec un bon encadrement)</p>"},{"location":"seance3/ressources/cas-utilisation-entreprise/#services-et-assistance-client","title":"Services et assistance client","text":""},{"location":"seance3/ressources/cas-utilisation-entreprise/#cas-3-chatbot-dassistance-premiere-ligne","title":"Cas 3: Chatbot d'assistance premi\u00e8re ligne","text":"<p>Entreprise: Service client d'une compagnie d'assurance (100 employ\u00e9s) Probl\u00e9matique: Volume \u00e9lev\u00e9 de questions simples et r\u00e9p\u00e9titives Solution impl\u00e9ment\u00e9e: - Chatbot bas\u00e9 sur l'API Mistral AI pour r\u00e9pondre aux questions fr\u00e9quentes - Base de connaissances structur\u00e9e avec les proc\u00e9dures et informations de l'entreprise - Interface d'administration pour ajouter de nouvelles r\u00e9ponses - Syst\u00e8me de transfert vers un humain quand le chatbot ne peut pas r\u00e9pondre</p> <p>Technologies utilis\u00e9es: - API Mistral AI - Flask pour le backend - Vue.js pour l'interface utilisateur - MongoDB pour stocker les conversations et la base de connaissances</p> <p>R\u00e9sultat: R\u00e9duction de 35% du volume de tickets de support de niveau 1</p> <p>Niveau de complexit\u00e9: \u2b50\u2b50\u2606\u2606\u2606 (Accessible avec les connaissances de ce cours)</p>"},{"location":"seance3/ressources/cas-utilisation-entreprise/#cas-4-analyse-automatique-des-appels-service-client","title":"Cas 4: Analyse automatique des appels service client","text":"<p>Entreprise: Centre d'appels (75 employ\u00e9s) Probl\u00e9matique: Difficult\u00e9 \u00e0 identifier les motifs d'insatisfaction client Solution impl\u00e9ment\u00e9e: - Syst\u00e8me de transcription audio-texte des appels - Analyse de sentiment sur les transcriptions - Classification automatique des motifs d'appel - Dashboard pour les managers montrant les tendances et probl\u00e8mes r\u00e9currents</p> <p>Technologies utilis\u00e9es: - Whisper (OpenAI) pour la transcription (via API) - TensorFlow pour l'analyse de sentiment - scikit-learn pour la classification des motifs - Dash/Plotly pour le tableau de bord</p> <p>R\u00e9sultat: Identification plus rapide des probl\u00e8mes r\u00e9currents et am\u00e9lioration du NPS</p> <p>Niveau de complexit\u00e9: \u2b50\u2b50\u2b50\u2606\u2606 (Plusieurs composants \u00e0 int\u00e9grer)</p>"},{"location":"seance3/ressources/cas-utilisation-entreprise/#sante-et-bien-etre","title":"Sant\u00e9 et bien-\u00eatre","text":""},{"location":"seance3/ressources/cas-utilisation-entreprise/#cas-5-application-de-detection-de-posture-pour-cabinet-de-kinesitherapie","title":"Cas 5: Application de d\u00e9tection de posture pour cabinet de kin\u00e9sith\u00e9rapie","text":"<p>Entreprise: Cabinet de kin\u00e9sith\u00e9rapie (5 praticiens) Probl\u00e9matique: Difficult\u00e9 pour les patients \u00e0 maintenir une bonne posture entre les s\u00e9ances Solution impl\u00e9ment\u00e9e: - Application mobile utilisant la cam\u00e9ra pour d\u00e9tecter la posture - Mod\u00e8le de reconnaissance de posture bas\u00e9 sur MobileNet et PoseNet - Alertes et conseils en temps r\u00e9el - Suivi des progr\u00e8s et rapports pour le praticien</p> <p>Technologies utilis\u00e9es: - TensorFlow Lite pour le mod\u00e8le embarqu\u00e9 - React Native pour l'application mobile - Firebase pour le backend et le stockage - Flask pour l'API de synchronisation</p> <p>R\u00e9sultat: Am\u00e9lioration du suivi patient et r\u00e9duction du temps de r\u00e9cup\u00e9ration</p> <p>Niveau de complexit\u00e9: \u2b50\u2b50\u2b50\u2606\u2606 (Application mobile + IA)</p>"},{"location":"seance3/ressources/cas-utilisation-entreprise/#industrie-et-logistique","title":"Industrie et logistique","text":""},{"location":"seance3/ressources/cas-utilisation-entreprise/#cas-6-controle-qualite-automatise-par-vision","title":"Cas 6: Contr\u00f4le qualit\u00e9 automatis\u00e9 par vision","text":"<p>Entreprise: PME industrielle de fabrication de pi\u00e8ces m\u00e9talliques (30 employ\u00e9s) Probl\u00e9matique: Contr\u00f4le qualit\u00e9 manuel chronophage et peu fiable Solution impl\u00e9ment\u00e9e: - Syst\u00e8me de cam\u00e9ras sur la cha\u00eene de production - Mod\u00e8le de d\u00e9tection d'anomalies bas\u00e9 sur un r\u00e9seau convolutif - Interface technique pour configurer les crit\u00e8res de contr\u00f4le - Syst\u00e8me d'alerte en cas de d\u00e9tection de d\u00e9fauts</p> <p>Technologies utilis\u00e9es: - TensorFlow pour la d\u00e9tection d'anomalies - OpenCV pour le pr\u00e9traitement d'images - Flask pour l'API et l'interface technique - SQLite pour le stockage des configurations et r\u00e9sultats</p> <p>R\u00e9sultat: R\u00e9duction de 23% des retours clients pour d\u00e9fauts non d\u00e9tect\u00e9s</p> <p>Niveau de complexit\u00e9: \u2b50\u2b50\u2606\u2606\u2606 (Surtout de la vision par ordinateur basique)</p>"},{"location":"seance3/ressources/cas-utilisation-entreprise/#education-et-formation","title":"\u00c9ducation et formation","text":""},{"location":"seance3/ressources/cas-utilisation-entreprise/#cas-7-plateforme-dapprentissage-adaptatif","title":"Cas 7: Plateforme d'apprentissage adaptatif","text":"<p>Entreprise: Startup EdTech (15 employ\u00e9s) Probl\u00e9matique: Contenu de formation g\u00e9n\u00e9rique non adapt\u00e9 au niveau de chaque apprenant Solution impl\u00e9ment\u00e9e: - Syst\u00e8me de recommandation de contenu p\u00e9dagogique personnalis\u00e9 - Analyse du parcours et des r\u00e9sultats de l'apprenant - Pr\u00e9diction des domaines de difficult\u00e9 potentiels - Interface enseignant pour suivre la progression des \u00e9tudiants</p> <p>Technologies utilis\u00e9es: - Keras pour le syst\u00e8me de recommandation - pandas pour l'analyse des donn\u00e9es d'apprentissage - Django pour la plateforme web - PostgreSQL pour la base de donn\u00e9es</p> <p>R\u00e9sultat: Am\u00e9lioration de 27% des taux de compl\u00e9tion des formations</p> <p>Niveau de complexit\u00e9: \u2b50\u2b50\u2b50\u2606\u2606 (Mod\u00e8le de recommandation + interface web)</p>"},{"location":"seance3/ressources/cas-utilisation-entreprise/#secteur-public-et-services-a-la-personne","title":"Secteur public et services \u00e0 la personne","text":""},{"location":"seance3/ressources/cas-utilisation-entreprise/#cas-8-assistance-a-la-redaction-administrative","title":"Cas 8: Assistance \u00e0 la r\u00e9daction administrative","text":"<p>Entreprise: Service social municipal (40 employ\u00e9s) Probl\u00e9matique: Temps important pass\u00e9 \u00e0 r\u00e9diger des documents administratifs similaires Solution impl\u00e9ment\u00e9e: - Assistant de r\u00e9daction bas\u00e9 sur des mod\u00e8les g\u00e9n\u00e9ratifs - Templates pr\u00e9-configur\u00e9s pour diff\u00e9rents types de documents - Syst\u00e8me d'extraction automatique d'informations depuis les dossiers - Interface simple de g\u00e9n\u00e9ration et validation de documents</p> <p>Technologies utilis\u00e9es: - API Mistral AI pour la g\u00e9n\u00e9ration de texte - spaCy pour l'extraction d'informations - Flask pour l'interface web - PostgreSQL pour le stockage des templates et donn\u00e9es</p> <p>R\u00e9sultat: R\u00e9duction de 40% du temps de r\u00e9daction administrative</p> <p>Niveau de complexit\u00e9: \u2b50\u2b50\u2606\u2606\u2606 (Principalement int\u00e9gration d'API)</p>"},{"location":"seance3/ressources/cas-utilisation-entreprise/#comment-utiliser-ces-cas-detudes","title":"Comment utiliser ces cas d'\u00e9tudes","text":""},{"location":"seance3/ressources/cas-utilisation-entreprise/#pour-votre-recherche-de-stage","title":"Pour votre recherche de stage","text":"<ul> <li>Identifiez les cas qui correspondent le plus \u00e0 vos int\u00e9r\u00eats et comp\u00e9tences</li> <li>Recherchez des entreprises locales dans ces secteurs d'activit\u00e9</li> <li>Pr\u00e9parez une proposition simplifi\u00e9e bas\u00e9e sur ces exemples</li> <li>Montrez comment vous pourriez impl\u00e9menter une version MVP (produit minimum viable)</li> </ul>"},{"location":"seance3/ressources/cas-utilisation-entreprise/#pour-votre-projet-de-chatbot","title":"Pour votre projet de chatbot","text":"<ul> <li>Inspirez-vous des diff\u00e9rentes architectures pr\u00e9sent\u00e9es</li> <li>Identifiez les composants r\u00e9utilisables (API REST, base de connaissances, interface utilisateur)</li> <li>Adaptez l'approche \u00e0 votre cas d'usage \u00e9ducatif</li> <li>Pensez \u00e0 la mani\u00e8re dont vous pourriez l'\u00e9tendre \u00e0 d'autres domaines</li> </ul>"},{"location":"seance3/ressources/cas-utilisation-entreprise/#pour-vos-entretiens","title":"Pour vos entretiens","text":"<ul> <li>Utilisez ces cas concrets pour montrer votre compr\u00e9hension des applications r\u00e9elles du Deep Learning</li> <li>Expliquez comment vous aborderiez ces probl\u00e8mes avec vos comp\u00e9tences actuelles</li> <li>Soulignez votre capacit\u00e9 \u00e0 comprendre les besoins business au-del\u00e0 de la technologie</li> <li>Montrez que vous \u00eates conscient des contraintes pratiques (temps, budget, comp\u00e9tences)</li> </ul>"},{"location":"seance3/ressources/competences-stage-sio/","title":"Comp\u00e9tences Deep Learning recherch\u00e9es en stage BTS SIO","text":"<p>Ce document pr\u00e9sente les comp\u00e9tences en Deep Learning les plus demand\u00e9es, bas\u00e9es sur une analyse des offres de stage et des retours d'entreprises.</p>"},{"location":"seance3/ressources/competences-stage-sio/#competences-techniques-prioritaires","title":"Comp\u00e9tences techniques prioritaires","text":"Comp\u00e9tence Niveau attendu Application en entreprise Utilisation de frameworks Savoir utiliser TensorFlow/Keras pour des cas simples Int\u00e9gration de fonctionnalit\u00e9s IA dans des applications existantes API REST Cr\u00e9er et documenter une API exposant des mod\u00e8les ML Cr\u00e9ation de services accessibles par d'autres applications Mod\u00e8les pr\u00e9-entra\u00een\u00e9s Adapter des mod\u00e8les existants \u00e0 des besoins sp\u00e9cifiques Reconnaissance d'images, classification de textes, etc. Int\u00e9gration web Connecter des mod\u00e8les ML \u00e0 des interfaces web Applications web avec fonctionnalit\u00e9s intelligentes Documentation technique Documenter clairement le fonctionnement d'un syst\u00e8me IA Faciliter la maintenance et le transfert de connaissances"},{"location":"seance3/ressources/competences-stage-sio/#competences-differenciantes","title":"Comp\u00e9tences diff\u00e9renciantes","text":"<p>Ces comp\u00e9tences ne sont pas syst\u00e9matiquement demand\u00e9es mais constituent un avantage significatif :</p> <ul> <li>D\u00e9ploiement de mod\u00e8les : Mettre en production un mod\u00e8le (Docker, cloud)</li> <li>Optimisation de performances : Am\u00e9liorer la vitesse d'inf\u00e9rence d'un mod\u00e8le</li> <li>D\u00e9veloppement de chatbots : Cr\u00e9er des assistants conversationnels simples</li> <li>Visualisation de donn\u00e9es : Pr\u00e9senter efficacement les r\u00e9sultats d'un mod\u00e8le</li> <li>Tests et validation : Assurer la fiabilit\u00e9 d'un syst\u00e8me d'IA</li> </ul>"},{"location":"seance3/ressources/competences-stage-sio/#competences-non-techniques-valorisees","title":"Comp\u00e9tences non-techniques valoris\u00e9es","text":"Comp\u00e9tence Description Pourquoi c'est important Vulgarisation technique Expliquer simplement des concepts complexes Communication avec les \u00e9quipes non-techniques \u00c9valuation des limites Identifier ce qui est r\u00e9alisable ou non avec l'IA \u00c9viter les promesses irr\u00e9alistes Veille technologique Se tenir inform\u00e9 des nouvelles possibilit\u00e9s Proposer des solutions innovantes \u00c9thique de l'IA Conscience des implications \u00e9thiques D\u00e9velopper des solutions responsables Autonomie d'apprentissage Capacit\u00e9 \u00e0 s'autoformer sur de nouveaux outils S'adapter rapidement dans un domaine en \u00e9volution"},{"location":"seance3/ressources/competences-stage-sio/#exemples-de-missions-de-stage","title":"Exemples de missions de stage","text":""},{"location":"seance3/ressources/competences-stage-sio/#pme-startup","title":"PME / Startup","text":"<ul> <li>D\u00e9veloppement d'un syst\u00e8me de reconnaissance de documents (factures, BL)</li> <li>Int\u00e9gration d'un chatbot d'assistance client sur un site e-commerce</li> <li>Cr\u00e9ation d'un module d'analyse de sentiments pour les avis clients</li> </ul>"},{"location":"seance3/ressources/competences-stage-sio/#esn-agences-digitales","title":"ESN / Agences digitales","text":"<ul> <li>Cr\u00e9ation d'une API d'analyse d'images pour une application mobile</li> <li>D\u00e9veloppement d'une preuve de concept utilisant un LLM pour l'assistance utilisateur</li> <li>Int\u00e9gration d'un syst\u00e8me de recommandation dans une application existante</li> </ul>"},{"location":"seance3/ressources/competences-stage-sio/#grandes-entreprises","title":"Grandes entreprises","text":"<ul> <li>Am\u00e9lioration d'un syst\u00e8me existant de d\u00e9tection d'anomalies</li> <li>Automatisation de t\u00e2ches de classification de tickets support</li> <li>D\u00e9veloppement d'un dashboard de suivi de performances de mod\u00e8les ML</li> </ul>"},{"location":"seance3/ressources/competences-stage-sio/#technologies-couramment-utilisees","title":"Technologies couramment utilis\u00e9es","text":"Cat\u00e9gorie Technologies Niveau d'expertise attendu Frameworks ML TensorFlow/Keras, Hugging Face Interm\u00e9diaire Langages Python, JavaScript Interm\u00e9diaire D\u00e9ploiement Flask, FastAPI, Docker D\u00e9butant/Interm\u00e9diaire Front-end React, Vue.js D\u00e9butant Base de donn\u00e9es MongoDB, PostgreSQL D\u00e9butant Cloud Google Cloud, Azure Notions"},{"location":"seance3/ressources/competences-stage-sio/#conseils-pour-valoriser-ces-competences","title":"Conseils pour valoriser ces comp\u00e9tences","text":"<ol> <li>Cr\u00e9ez un portfolio avec des exemples concrets de mini-projets</li> <li>Documentez vos projets en expliquant clairement votre d\u00e9marche</li> <li>Pr\u00e9parez des d\u00e9mos fonctionnelles \u00e0 montrer lors des entretiens</li> <li>Participez \u00e0 des projets open source ou des hackathons IA</li> <li>Cr\u00e9ez un profil GitHub regroupant vos r\u00e9alisations en IA/ML</li> </ol>"},{"location":"seance3/ressources/diagnostic_problemes_performance/","title":"Guide simplifi\u00e9 de diagnostic des probl\u00e8mes de performance en Deep Learning","text":"<p>Ce guide vous aidera \u00e0 identifier et r\u00e9soudre les probl\u00e8mes de performance les plus courants dans vos mod\u00e8les de Deep Learning, sans n\u00e9cessiter de connaissances math\u00e9matiques avanc\u00e9es.</p>"},{"location":"seance3/ressources/diagnostic_problemes_performance/#1-reconnaitre-les-signes-de-surapprentissage-overfitting","title":"1. Reconna\u00eetre les signes de surapprentissage (overfitting)","text":""},{"location":"seance3/ressources/diagnostic_problemes_performance/#symptomes","title":"Sympt\u00f4mes","text":"<ul> <li>Performances excellentes sur les donn\u00e9es d'entra\u00eenement mais m\u00e9diocres sur les donn\u00e9es de test</li> <li>\u00c9cart grandissant entre les courbes d'apprentissage d'entra\u00eenement et de validation</li> <li>Courbe de perte de validation qui remonte apr\u00e8s avoir diminu\u00e9</li> </ul>"},{"location":"seance3/ressources/diagnostic_problemes_performance/#solutions","title":"Solutions","text":"<ul> <li>Ajouter du Dropout (ex: <code>layers.Dropout(0.2)</code> apr\u00e8s les couches denses)</li> <li>Utiliser la r\u00e9gularisation L2 (ex: <code>kernel_regularizer=tf.keras.regularizers.l2(0.001)</code>)</li> <li>Augmenter les donn\u00e9es (rotations, translations, zoom pour les images)</li> <li>Simplifier le mod\u00e8le (r\u00e9duire le nombre de couches ou de neurones)</li> <li>Utiliser l'Early Stopping pour arr\u00eater l'entra\u00eenement avant le surapprentissage</li> </ul> <pre><code># Exemple d'Early Stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    restore_best_weights=True\n)\n\nmodel.fit(X_train, y_train, \n          validation_data=(X_val, y_val),\n          callbacks=[early_stopping],\n          epochs=100)  # Nombre maximum d'\u00e9poques\n</code></pre>"},{"location":"seance3/ressources/diagnostic_problemes_performance/#2-identifier-le-sous-apprentissage-underfitting","title":"2. Identifier le sous-apprentissage (underfitting)","text":""},{"location":"seance3/ressources/diagnostic_problemes_performance/#symptomes_1","title":"Sympt\u00f4mes","text":"<ul> <li>Performances m\u00e9diocres sur les donn\u00e9es d'entra\u00eenement ET de validation</li> <li>Courbes d'apprentissage qui stagnent rapidement</li> <li>Erreurs syst\u00e9matiques sur certains types d'exemples</li> </ul>"},{"location":"seance3/ressources/diagnostic_problemes_performance/#solutions_1","title":"Solutions","text":"<ul> <li>Augmenter la complexit\u00e9 du mod\u00e8le (plus de couches ou de neurones)</li> <li>Entra\u00eener plus longtemps (augmenter le nombre d'\u00e9poques)</li> <li>R\u00e9duire la r\u00e9gularisation si elle est trop forte</li> <li>Optimiser l'architecture pour le probl\u00e8me sp\u00e9cifique</li> </ul>"},{"location":"seance3/ressources/diagnostic_problemes_performance/#3-problemes-lies-aux-donnees","title":"3. Probl\u00e8mes li\u00e9s aux donn\u00e9es","text":""},{"location":"seance3/ressources/diagnostic_problemes_performance/#symptomes_2","title":"Sympt\u00f4mes","text":"<ul> <li>Performances incoh\u00e9rentes ou erratiques</li> <li>Biais inexpliqu\u00e9s dans les pr\u00e9dictions</li> <li>Difficult\u00e9s d'apprentissage sur certaines classes</li> </ul>"},{"location":"seance3/ressources/diagnostic_problemes_performance/#solutions_2","title":"Solutions","text":"<ul> <li> <p>Normaliser correctement les donn\u00e9es <pre><code># Normalisation Z-score (moyenne=0, \u00e9cart-type=1)\nmean = X_train.mean(axis=0)\nstd = X_train.std(axis=0)\nX_train = (X_train - mean) / std\nX_test = (X_test - mean) / std  # Utiliser les statistiques de l'entra\u00eenement\n</code></pre></p> </li> <li> <p>\u00c9quilibrer les classes (r\u00e9\u00e9chantillonnage ou pond\u00e9ration) <pre><code># Pond\u00e9ration des classes\nclass_weights = {0: 1.0, 1: 5.0}  # Si classe 1 sous-repr\u00e9sent\u00e9e\nmodel.fit(X_train, y_train, class_weight=class_weights)\n</code></pre></p> </li> <li> <p>V\u00e9rifier la qualit\u00e9 des donn\u00e9es (valeurs aberrantes, erreurs d'\u00e9tiquetage)</p> </li> <li>Stratifier la division train/test pour maintenir les distributions</li> </ul>"},{"location":"seance3/ressources/diagnostic_problemes_performance/#4-problemes-doptimisation","title":"4. Probl\u00e8mes d'optimisation","text":""},{"location":"seance3/ressources/diagnostic_problemes_performance/#symptomes_3","title":"Sympt\u00f4mes","text":"<ul> <li>Apprentissage trop lent ou instable</li> <li>Courbe de perte qui oscille fortement</li> <li>Convergence vers un minimum local sous-optimal</li> </ul>"},{"location":"seance3/ressources/diagnostic_problemes_performance/#solutions_3","title":"Solutions","text":"<ul> <li> <p>Ajuster le taux d'apprentissage <pre><code># Taux d'apprentissage adaptatif\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=3\n)\n</code></pre></p> </li> <li> <p>Changer d'optimiseur (essayer Adam, RMSprop ou SGD selon le probl\u00e8me)</p> </li> <li> <p>Utiliser le Batch Normalization pour stabiliser l'entra\u00eenement <pre><code>model = tf.keras.Sequential([\n    layers.Dense(128),\n    layers.BatchNormalization(),\n    layers.Activation('relu')\n])\n</code></pre></p> </li> <li> <p>Ajuster la taille du batch (plus petit pour plus de stabilit\u00e9, plus grand pour plus de vitesse)</p> </li> </ul>"},{"location":"seance3/ressources/diagnostic_problemes_performance/#5-processus-de-diagnostic-pas-a-pas","title":"5. Processus de diagnostic pas \u00e0 pas","text":"<ol> <li>Observer les courbes d'apprentissage</li> <li>Comparer les courbes d'entra\u00eenement et de validation</li> <li> <p>Identifier les tendances (surapprentissage, sous-apprentissage, etc.)</p> </li> <li> <p>Analyser les erreurs</p> </li> <li>Examiner les cas les plus mal pr\u00e9dits</li> <li> <p>Rechercher des patterns dans les erreurs</p> </li> <li> <p>Tester des hypoth\u00e8ses</p> </li> <li>Modifier un seul param\u00e8tre \u00e0 la fois</li> <li> <p>Mesurer l'impact de chaque changement</p> </li> <li> <p>It\u00e9rer et documenter</p> </li> <li>Garder trace des exp\u00e9rimentations</li> <li>Progresser par petites am\u00e9liorations successives</li> </ol>"},{"location":"seance3/ressources/diagnostic_problemes_performance/#6-checklist-de-diagnostic-rapide","title":"6. Checklist de diagnostic rapide","text":"<ul> <li> Les donn\u00e9es sont-elles correctement normalis\u00e9es?</li> <li> Y a-t-il un \u00e9cart important entre performances d'entra\u00eenement et de validation?</li> <li> Le mod\u00e8le est-il adapt\u00e9 \u00e0 la complexit\u00e9 du probl\u00e8me?</li> <li> L'optimiseur et le taux d'apprentissage sont-ils appropri\u00e9s?</li> <li> Y a-t-il un d\u00e9s\u00e9quilibre dans les donn\u00e9es?</li> <li> Les hyperparam\u00e8tres ont-ils \u00e9t\u00e9 ajust\u00e9s?</li> </ul>"},{"location":"seance3/ressources/diagnostic_problemes_performance/#7-tableau-de-reference-des-problemes-et-solutions","title":"7. Tableau de r\u00e9f\u00e9rence des probl\u00e8mes et solutions","text":"Probl\u00e8me Sympt\u00f4mes Solutions principales Surapprentissage Bonne perf. entra\u00eenement, mauvaise perf. test Dropout, r\u00e9gularisation, plus de donn\u00e9es Sous-apprentissage Mauvaise perf. partout Mod\u00e8le plus complexe, plus d'entra\u00eenement Donn\u00e9es non normalis\u00e9es Apprentissage instable Normalisation adapt\u00e9e au probl\u00e8me Classes d\u00e9s\u00e9quilibr\u00e9es Biais vers classe majoritaire R\u00e9\u00e9chantillonnage, pond\u00e9ration Taux d'apprentissage inappropri\u00e9 Convergence lente ou oscillations Ajustement du taux, schedulers <p>N'oubliez pas que l'am\u00e9lioration des performances est souvent un processus it\u00e9ratif. Commencez par les probl\u00e8mes les plus \u00e9vidents et progressez vers des optimisations plus fines.</p>"},{"location":"seance3/ressources/grille-projet-simple/","title":"Grille d'\u00e9valuation pour projets de Deep Learning simples","text":"<p>Cette grille d'\u00e9valuation est sp\u00e9cifiquement con\u00e7ue pour des projets de Deep Learning adapt\u00e9s au niveau BTS SIO, avec une orientation vers des applications concr\u00e8tes et accessibles.</p>"},{"location":"seance3/ressources/grille-projet-simple/#criteres-devaluation-du-projet-chatbot","title":"Crit\u00e8res d'\u00e9valuation du projet chatbot","text":"Crit\u00e8re Points Description Fonctionnalit\u00e9 de base 30 Le chatbot fonctionne-t-il correctement ? Int\u00e9gration technique 20 Qualit\u00e9 de l'int\u00e9gration avec l'API Mistral AI Base de connaissances 15 Structure et qualit\u00e9 du contenu p\u00e9dagogique Exp\u00e9rience utilisateur 15 Facilit\u00e9 d'utilisation et clart\u00e9 des interactions Documentation 10 Qualit\u00e9 de la documentation technique et utilisateur Pr\u00e9sentation du projet 10 Clart\u00e9 et pertinence de la pr\u00e9sentation finale"},{"location":"seance3/ressources/grille-projet-simple/#bareme-detaille","title":"Bar\u00e8me d\u00e9taill\u00e9","text":""},{"location":"seance3/ressources/grille-projet-simple/#fonctionnalite-de-base-30-points","title":"Fonctionnalit\u00e9 de base (30 points)","text":"Niveau Points Description Excellent 25-30 Le chatbot r\u00e9pond correctement \u00e0 une large vari\u00e9t\u00e9 de questions sur le Deep Learning, g\u00e8re les conversations de mani\u00e8re fluide, et propose des ressources pertinentes. Tr\u00e8s bien 20-24 Le chatbot r\u00e9pond correctement \u00e0 la plupart des questions, maintient le contexte de la conversation, mais peut parfois donner des r\u00e9ponses impr\u00e9cises. Bien 15-19 Le chatbot r\u00e9pond correctement aux questions de base, mais a des difficult\u00e9s avec les questions complexes ou le maintien du contexte. Moyen 10-14 Le chatbot fonctionne mais avec des limitations significatives (r\u00e9ponses g\u00e9n\u00e9riques, perte fr\u00e9quente de contexte). Insuffisant 0-9 Le chatbot ne r\u00e9pond pas correctement ou de fa\u00e7on coh\u00e9rente aux questions de base."},{"location":"seance3/ressources/grille-projet-simple/#integration-technique-20-points","title":"Int\u00e9gration technique (20 points)","text":"Niveau Points Description Excellent 16-20 Int\u00e9gration fluide avec l'API Mistral AI, gestion efficace des erreurs, optimisation des requ\u00eates. Tr\u00e8s bien 12-15 Bonne int\u00e9gration avec l'API, gestion basique des erreurs, utilisation correcte des param\u00e8tres. Bien 8-11 Int\u00e9gration fonctionnelle mais sans optimisation, gestion minimale des erreurs. Moyen 4-7 Int\u00e9gration basique qui fonctionne mais avec des limitations ou bugs. Insuffisant 0-3 Int\u00e9gration d\u00e9faillante ou absente."},{"location":"seance3/ressources/grille-projet-simple/#base-de-connaissances-15-points","title":"Base de connaissances (15 points)","text":"Niveau Points Description Excellent 12-15 Contenu complet, bien structur\u00e9, pr\u00e9cis techniquement et adapt\u00e9 au niveau p\u00e9dagogique vis\u00e9. Tr\u00e8s bien 9-11 Contenu couvrant bien le sujet, g\u00e9n\u00e9ralement pr\u00e9cis et bien organis\u00e9. Bien 6-8 Contenu couvrant les bases du sujet, quelques impr\u00e9cisions. Moyen 3-5 Contenu minimal, plusieurs impr\u00e9cisions ou lacunes. Insuffisant 0-2 Contenu tr\u00e8s limit\u00e9, nombreuses erreurs ou contenu non pertinent."},{"location":"seance3/ressources/grille-projet-simple/#experience-utilisateur-15-points","title":"Exp\u00e9rience utilisateur (15 points)","text":"Niveau Points Description Excellent 12-15 Interface intuitive, feedback clair, temps de r\u00e9ponse optimis\u00e9, adapt\u00e9e aux besoins de l'utilisateur. Tr\u00e8s bien 9-11 Bonne exp\u00e9rience globale, navigation claire, temps de r\u00e9ponse acceptable. Bien 6-8 Interface fonctionnelle mais avec des am\u00e9liorations possibles, quelques frictions d'usage. Moyen 3-5 Interface basique avec plusieurs probl\u00e8mes d'utilisabilit\u00e9. Insuffisant 0-2 Interface confuse ou difficile \u00e0 utiliser."},{"location":"seance3/ressources/grille-projet-simple/#documentation-10-points","title":"Documentation (10 points)","text":"Niveau Points Description Excellent 8-10 Documentation compl\u00e8te, claire, bien structur\u00e9e, avec exemples et captures d'\u00e9cran. Tr\u00e8s bien 6-7 Documentation couvrant tous les aspects importants, bien organis\u00e9e. Bien 4-5 Documentation couvrant les fonctionnalit\u00e9s de base, structure acceptable. Moyen 2-3 Documentation minimale ou difficile \u00e0 suivre. Insuffisant 0-1 Documentation tr\u00e8s limit\u00e9e ou inexistante."},{"location":"seance3/ressources/grille-projet-simple/#presentation-du-projet-10-points","title":"Pr\u00e9sentation du projet (10 points)","text":"Niveau Points Description Excellent 8-10 Pr\u00e9sentation claire, concise, bien structur\u00e9e, d\u00e9monstration convaincante, bonne gestion du temps. Tr\u00e8s bien 6-7 Pr\u00e9sentation organis\u00e9e, d\u00e9monstration fonctionnelle, r\u00e9ponses pertinentes aux questions. Bien 4-5 Pr\u00e9sentation couvrant les points essentiels, d\u00e9monstration basique. Moyen 2-3 Pr\u00e9sentation d\u00e9sorganis\u00e9e ou incompl\u00e8te, d\u00e9monstration probl\u00e9matique. Insuffisant 0-1 Pr\u00e9sentation confuse ou inad\u00e9quate, absence de d\u00e9monstration fonctionnelle."},{"location":"seance3/ressources/grille-projet-simple/#bonus-et-malus","title":"Bonus et malus","text":""},{"location":"seance3/ressources/grille-projet-simple/#points-bonus-jusqua-10-points","title":"Points bonus (jusqu'\u00e0 +10 points)","text":"<ul> <li>Innovation (+1 \u00e0 +5) : Fonctionnalit\u00e9s originales et pertinentes</li> <li>D\u00e9ploiement (+1 \u00e0 +3) : Solution d\u00e9ploy\u00e9e et accessible en ligne</li> <li>Accessibilit\u00e9 (+1 \u00e0 +2) : Interface adapt\u00e9e aux standards d'accessibilit\u00e9</li> </ul>"},{"location":"seance3/ressources/grille-projet-simple/#points-malus-jusqua-10-points","title":"Points malus (jusqu'\u00e0 -10 points)","text":"<ul> <li>Retard (-2 par jour de retard)</li> <li>Plagiat (-5 \u00e0 -10 selon l'\u00e9tendue)</li> <li>Non-respect des consignes de base (-1 \u00e0 -5)</li> </ul>"},{"location":"seance3/ressources/grille-projet-simple/#niveaux-de-reussite","title":"Niveaux de r\u00e9ussite","text":"Niveau Pourcentage Description A 90-100% Projet exceptionnel, d\u00e9passant les attentes B 80-89% Tr\u00e8s bon projet, r\u00e9pondant \u00e0 tous les crit\u00e8res C 70-79% Bon projet avec quelques points \u00e0 am\u00e9liorer D 60-69% Projet acceptable avec plusieurs limitations E 50-59% Projet passable n\u00e9cessitant des am\u00e9liorations significatives F &lt;50% Projet ne r\u00e9pondant pas aux exigences minimales"},{"location":"seance3/ressources/grille-projet-simple/#conseils-pour-reussir","title":"Conseils pour r\u00e9ussir","text":"<ol> <li>Commencez simple : Assurez-vous d'avoir un prototype fonctionnel avant d'ajouter des fonctionnalit\u00e9s avanc\u00e9es</li> <li>Testez r\u00e9guli\u00e8rement avec des utilisateurs r\u00e9els pour identifier les probl\u00e8mes d'exp\u00e9rience utilisateur</li> <li>Documentez au fur et \u00e0 mesure pour \u00e9viter la surcharge de travail \u00e0 la fin</li> <li>Concentrez-vous sur l'int\u00e9gration technique plut\u00f4t que sur des fonctionnalit\u00e9s trop ambitieuses</li> <li>Pr\u00e9parezvotre pr\u00e9sentation en avance et entra\u00eenez-vous</li> </ol>"},{"location":"seance3/ressources/grille-projet-simple/#foire-aux-questions","title":"Foire aux questions","text":""},{"location":"seance3/ressources/grille-projet-simple/#q-est-il-necessaire-que-le-chatbot-fonctionne-en-temps-reel-pendant-la-presentation","title":"Q: Est-il n\u00e9cessaire que le chatbot fonctionne en temps r\u00e9el pendant la pr\u00e9sentation ?","text":"<p>R: Oui, une d\u00e9monstration en direct est requise. Pr\u00e9voyez un plan B (vid\u00e9o de d\u00e9monstration) en cas de probl\u00e8me technique.</p>"},{"location":"seance3/ressources/grille-projet-simple/#q-peut-on-utiliser-dautres-modeles-que-mistral-ai","title":"Q: Peut-on utiliser d'autres mod\u00e8les que Mistral AI ?","text":"<p>R: Oui, mais vous devez justifier votre choix et l'alternative doit offrir des fonctionnalit\u00e9s similaires ou sup\u00e9rieures.</p>"},{"location":"seance3/ressources/grille-projet-simple/#q-comment-sera-evaluee-la-base-de-connaissances","title":"Q: Comment sera \u00e9valu\u00e9e la base de connaissances ?","text":"<p>R: Sur sa compl\u00e9tude (couverture des sujets du programme), sa pr\u00e9cision technique, sa structure et son adaptation au niveau des utilisateurs.</p>"},{"location":"seance3/ressources/grille-projet-simple/#q-faut-il-developper-une-interface-graphique-elaboree","title":"Q: Faut-il d\u00e9velopper une interface graphique \u00e9labor\u00e9e ?","text":"<p>R: Une interface simple mais fonctionnelle et intuitive est suffisante. La qualit\u00e9 de l'interaction est plus importante que l'esth\u00e9tique.</p>"},{"location":"seance3/ressources/grille-projet-simple/#q-comment-documenter-efficacement-le-projet","title":"Q: Comment documenter efficacement le projet ?","text":"<p>R: Incluez un README clair, des commentaires dans le code, un guide d'installation, un manuel utilisateur simple et des exemples d'utilisation.</p>"},{"location":"seance3/ressources/grille-projet-simple/#processus-devaluation","title":"Processus d'\u00e9valuation","text":"<ol> <li>Soumission du code source : Via le d\u00e9p\u00f4t Git du projet (48h avant la pr\u00e9sentation)</li> <li>Pr\u00e9sentation orale : 10 minutes de pr\u00e9sentation + 5 minutes de d\u00e9monstration + 5 minutes de questions</li> <li>\u00c9valuation technique : Examen du code source et de la documentation par l'enseignant</li> <li>Retour d\u00e9taill\u00e9 : Fourni dans la semaine suivant la pr\u00e9sentation</li> </ol> <p>N'h\u00e9sitez pas \u00e0 consulter l'enseignant \u00e0 tout moment pendant le d\u00e9veloppement du projet pour obtenir des conseils ou clarifier des attentes.</p>"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/","title":"Guide d'utilisation des mod\u00e8les pr\u00e9-entra\u00een\u00e9s TensorFlow","text":""},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#introduction-aux-modeles-pre-entraines","title":"Introduction aux mod\u00e8les pr\u00e9-entra\u00een\u00e9s","text":"<p>Les mod\u00e8les pr\u00e9-entra\u00een\u00e9s sont un des moyens les plus efficaces d'obtenir rapidement d'excellentes performances en Deep Learning sans avoir besoin de grandes quantit\u00e9s de donn\u00e9es ni de ressources de calcul importantes. Cette approche est particuli\u00e8rement pertinente pour les projets  o\u00f9 le temps et les ressources sont souvent limit\u00e9s.</p>"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#quest-ce-quun-modele-pre-entraine","title":"Qu'est-ce qu'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 ?","text":"<p>Un mod\u00e8le pr\u00e9-entra\u00een\u00e9 est un r\u00e9seau de neurones qui a d\u00e9j\u00e0 \u00e9t\u00e9 entra\u00een\u00e9 sur un grand jeu de donn\u00e9es (souvent des millions d'images ou de textes). Ces mod\u00e8les ont appris des repr\u00e9sentations g\u00e9n\u00e9riques qui peuvent \u00eatre r\u00e9utilis\u00e9es pour d'autres t\u00e2ches similaires.</p>"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#avantages-pour-vos-projets","title":"Avantages pour vos projets","text":"<ul> <li>Gain de temps : \u00c9vitez des jours ou semaines d'entra\u00eenement</li> <li>\u00c9conomie de ressources : Pas besoin de GPU puissant</li> <li>Meilleures performances : B\u00e9n\u00e9ficiez de mod\u00e8les entra\u00een\u00e9s sur des datasets massifs</li> <li>Moins de donn\u00e9es requises : Parfait quand vous disposez de peu d'exemples</li> </ul>"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#modeles-pre-entraines-disponibles-dans-tensorflowkeras","title":"Mod\u00e8les pr\u00e9-entra\u00een\u00e9s disponibles dans TensorFlow/Keras","text":"<p>TensorFlow propose une large collection de mod\u00e8les pr\u00e9-entra\u00een\u00e9s via Keras et TensorFlow Hub. Voici les principaux mod\u00e8les disponibles class\u00e9s par domaine d'application :</p>"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#vision-par-ordinateur","title":"Vision par ordinateur","text":"Mod\u00e8le Taille Pr\u00e9cision Rapidit\u00e9 Cas d'usage MobileNetV2 14 MB Moyenne Tr\u00e8s rapide Applications mobiles, d\u00e9tection en temps r\u00e9el ResNet50 98 MB \u00c9lev\u00e9e Moyenne Classification d'images g\u00e9n\u00e9rale EfficientNetB0 29 MB \u00c9lev\u00e9e Rapide Bon compromis taille/performance EfficientNetB7 256 MB Tr\u00e8s \u00e9lev\u00e9e Lent Pr\u00e9cision maximale requise VGG16 528 MB Moyenne Lent Extraction de caract\u00e9ristiques InceptionV3 92 MB \u00c9lev\u00e9e Moyenne Classification avec pr\u00e9cision"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#traitement-du-langage-naturel","title":"Traitement du langage naturel","text":"Mod\u00e8le Taille Pr\u00e9cision Rapidit\u00e9 Cas d'usage BERT Small 55 MB Moyenne Rapide Classification de texte simple BERT Base 110 MB \u00c9lev\u00e9e Moyenne Analyse de sentiment, classification DistilBERT 66 MB Bonne Rapide Version l\u00e9g\u00e8re de BERT Universal Sentence Encoder 68 MB Moyenne Rapide Comparaison de textes, recherche"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#comment-utiliser-un-modele-pre-entraine","title":"Comment utiliser un mod\u00e8le pr\u00e9-entra\u00een\u00e9","text":""},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#1-chargement-dun-modele-pre-entraine","title":"1. Chargement d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2, ResNet50\n\n# Option 1: Mod\u00e8le pour la classification\nmodel = MobileNetV2(weights='imagenet', include_top=True)\n\n# Option 2: Mod\u00e8le pour l'extraction de caract\u00e9ristiques (sans les couches de classification)\nfeature_extractor = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n</code></pre>"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#parametres-importants","title":"Param\u00e8tres importants","text":"<ul> <li><code>weights='imagenet'</code> : Charge les poids pr\u00e9-entra\u00een\u00e9s sur ImageNet</li> <li><code>include_top=True/False</code> : Inclut ou non les couches de classification finale</li> <li><code>input_shape</code> : Dimensions requises des images d'entr\u00e9e</li> </ul>"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#2-utilisation-directe-prediction","title":"2. Utilisation directe (pr\u00e9diction)","text":"<pre><code>from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\n\n# Charger et pr\u00e9traiter l'image\nimg_path = 'mon_image.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)  # Pr\u00e9traitement sp\u00e9cifique au mod\u00e8le\n\n# Pr\u00e9diction\npreds = model.predict(x)\nresults = decode_predictions(preds, top=3)[0]\nprint(\"Pr\u00e9dictions:\")\nfor result in results:\n    print(f\"{result[1]}: {result[2]*100:.2f}%\")\n</code></pre>"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#3-transfer-learning-reutilisation-pour-votre-propre-tache","title":"3. Transfer Learning (r\u00e9utilisation pour votre propre t\u00e2che)","text":"<p>Le transfer learning est une technique qui vous permet d'adapter un mod\u00e8le pr\u00e9-entra\u00een\u00e9 \u00e0 votre propre jeu de donn\u00e9es. C'est extr\u00eamement utile quand vous avez peu de donn\u00e9es.</p> <pre><code># Cr\u00e9er un mod\u00e8le de base (sans les couches de classification)\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Geler les poids du mod\u00e8le de base\nbase_model.trainable = False\n\n# Ajouter vos propres couches de classification\nmodel = tf.keras.Sequential([\n    base_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(num_classes, activation='softmax')  # nombre de classes dans votre probl\u00e8me\n])\n\n# Compiler le mod\u00e8le\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Entra\u00eener uniquement les nouvelles couches\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=len(train_generator),\n    epochs=10,\n    validation_data=validation_generator,\n    validation_steps=len(validation_generator)\n)\n</code></pre>"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#4-fine-tuning-ajustement-fin","title":"4. Fine-tuning (ajustement fin)","text":"<p>Le fine-tuning est une \u00e9tape suppl\u00e9mentaire apr\u00e8s le transfer learning, o\u00f9 vous \"d\u00e9geler\" certaines couches du mod\u00e8le pr\u00e9-entra\u00een\u00e9 pour les ajuster \u00e0 vos donn\u00e9es.</p> <pre><code># Apr\u00e8s l'entra\u00eenement initial, d\u00e9geler une partie du mod\u00e8le de base\nbase_model.trainable = True\n\n# Geler les premi\u00e8res couches et n'entra\u00eener que les couches plus profondes\nfor layer in base_model.layers[:100]:\n    layer.trainable = False\nfor layer in base_model.layers[100:]:\n    layer.trainable = True\n\n# Recompiler avec un taux d'apprentissage plus faible\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Taux plus faible\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Fine-tuning\nhistory_fine = model.fit(\n    train_generator,\n    steps_per_epoch=len(train_generator),\n    epochs=5,\n    validation_data=validation_generator,\n    validation_steps=len(validation_generator)\n)\n</code></pre>"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#exemple-1-classification-dimages-personnalisee","title":"Exemple 1: Classification d'images personnalis\u00e9e","text":"<p>Voici un exemple complet pour cr\u00e9er un classificateur d'images personnalis\u00e9 \u00e0 partir de MobileNetV2 :</p> <p>```python import tensorflow as tf from tensorflow.keras.applications import MobileNetV2 from tensorflow.keras.preprocessing.image import ImageDataGenerator</p>"},{"location":"seance3/ressources/modeles-pretraines-tensorflow/#parametres","title":"Param\u00e8tres","text":"<p>IMAGE_SIZE = (224, 224)</p>"},{"location":"seance3/ressources/notebook-api-reconnaissance-images/","title":"Notebook api reconnaissance images","text":"In\u00a0[\u00a0]: Copied! <pre># API de reconnaissance d'images avec Flask et TensorFlow\n# Notebook \n\n# 1. Installation des biblioth\u00e8ques n\u00e9cessaires\n!pip install flask flask-cors pillow tensorflow\n\n# 2. Importation des biblioth\u00e8ques\nimport tensorflow as tf\nfrom tensorflow.keras import models\nimport numpy as np\nfrom PIL import Image\nimport io\nimport os\nimport base64\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nimport matplotlib.pyplot as plt\nfrom google.colab.output import eval_js\nfrom IPython.display import display, Javascript, HTML\n\n# 3. Chargement du mod\u00e8le (option 1 - utiliser le mod\u00e8le que vous avez d\u00e9j\u00e0 entra\u00een\u00e9)\n# D\u00e9commentez cette section si vous avez d\u00e9j\u00e0 entra\u00een\u00e9 et sauvegard\u00e9 un mod\u00e8le\n\"\"\"\ntry:\n    model = models.load_model('fashion_mnist_model.h5')\n    print(\"Mod\u00e8le charg\u00e9 avec succ\u00e8s!\")\nexcept:\n    print(\"Erreur: Impossible de charger le mod\u00e8le entra\u00een\u00e9.\")\n\"\"\"\n\n# 4. Chargement du mod\u00e8le (option 2 - utiliser un mod\u00e8le pr\u00e9-entra\u00een\u00e9)\n# Cette option est plus rapide si vous n'avez pas encore entra\u00een\u00e9 de mod\u00e8le\n\n# Charger le jeu de donn\u00e9es Fashion MNIST\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n\n# Pr\u00e9traiter les donn\u00e9es\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n\n# Cr\u00e9er et entra\u00eener rapidement un mod\u00e8le simple\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(train_images, train_labels, epochs=5, batch_size=32, verbose=1)\n\n# \u00c9valuer le mod\u00e8le\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\nprint(f\"Pr\u00e9cision sur donn\u00e9es de test: {test_acc:.4f}\")\n\n# D\u00e9finir les noms des classes\nclass_names = ['T-shirt/haut', 'Pantalon', 'Pull', 'Robe', 'Manteau',\n               'Sandale', 'Chemise', 'Baskets', 'Sac', 'Bottine']\n\n# 5. Cr\u00e9ation de l'application Flask\napp = Flask(__name__)\nCORS(app)  # Permet l'acc\u00e8s depuis d'autres domaines (important pour les applications web)\n\n# Fonction de pr\u00e9traitement des images\ndef preprocess_image(image):\n    \"\"\"\n    Pr\u00e9traite une image pour qu'elle soit compatible avec notre mod\u00e8le\n    \n    Args:\n        image: Image PIL\n    \n    Returns:\n        np.array: Image pr\u00e9trait\u00e9e et format\u00e9e pour le mod\u00e8le\n    \"\"\"\n    # Convertir en niveaux de gris si n\u00e9cessaire\n    if image.mode != 'L':\n        image = image.convert('L')\n    \n    # Redimensionner \u00e0 28x28 pixels\n    image = image.resize((28, 28))\n    \n    # Convertir en array numpy\n    img_array = np.array(image)\n    \n    # Normaliser les valeurs de pixels \u00e0 [0,1]\n    img_array = img_array / 255.0\n    \n    # Ajouter une dimension de batch\n    img_array = img_array.reshape(1, 28, 28)\n    \n    return img_array\n\n# Route principale\n@app.route('/')\ndef home():\n    return \"\"\"\n    &lt;h1&gt;API de reconnaissance d'images&lt;/h1&gt;\n    &lt;p&gt;Utilisez le point d'acc\u00e8s /predict pour classifier des images de v\u00eatements.&lt;/p&gt;\n    \"\"\"\n\n# Route pour les pr\u00e9dictions\n@app.route('/predict', methods=['POST'])\ndef predict():\n    # V\u00e9rifier si la requ\u00eate contient une image\n    if 'image' not in request.files:\n        return jsonify({'error': 'Aucune image trouv\u00e9e dans la requ\u00eate'}), 400\n    \n    # R\u00e9cup\u00e9rer l'image\n    file = request.files['image']\n    \n    try:\n        # Lire l'image\n        img = Image.open(file.stream)\n        \n        # Pr\u00e9traiter l'image\n        processed_image = preprocess_image(img)\n        \n        # Faire la pr\u00e9diction\n        predictions = model.predict(processed_image)\n        \n        # R\u00e9cup\u00e9rer la classe avec la plus haute probabilit\u00e9\n        predicted_class = np.argmax(predictions[0])\n        confidence = float(predictions[0][predicted_class])\n        \n        # Pr\u00e9parer la r\u00e9ponse\n        response = {\n            'prediction': {\n                'class_id': int(predicted_class),\n                'class_name': class_names[predicted_class],\n                'confidence': float(confidence)\n            },\n            'all_probabilities': {\n                class_names[i]: float(predictions[0][i]) for i in range(len(class_names))\n            }\n        }\n        \n        return jsonify(response)\n    \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n# 6. Route pour obtenir des informations sur le mod\u00e8le\n@app.route('/info', methods=['GET'])\ndef model_info():\n    model_summary = []\n    \n    # Capturer le r\u00e9sum\u00e9 du mod\u00e8le dans une liste\n    model.summary(print_fn=lambda x: model_summary.append(x))\n    \n    return jsonify({\n        'model_name': 'Fashion MNIST Classifier',\n        'input_shape': model.input_shape[1:],\n        'output_shape': model.output_shape[1:],\n        'number_of_classes': len(class_names),\n        'classes': class_names,\n        'model_summary': model_summary\n    })\n\n# 7. Configuration pour ex\u00e9cuter Flask dans Colab\nimport threading\nfrom google.colab.output import eval_js\nfrom IPython.display import display, Javascript\n\n# Fonction pour ex\u00e9cuter Flask dans Colab\ndef run_flask(app):\n    from google.colab.output import eval_js\n    from IPython.display import display, Javascript\n    import threading\n    \n    def b64_to_pil(b64_str):\n        import base64\n        import io\n        from PIL import Image\n        img_data = base64.b64decode(b64_str)\n        return Image.open(io.BytesIO(img_data))\n    \n    # D\u00e9marrer le serveur Flask dans un thread s\u00e9par\u00e9\n    threading.Thread(target=lambda: app.run(host='0.0.0.0', port=8000, debug=False, use_reloader=False)).start()\n    \n    # Afficher un message pour indiquer que le serveur est en cours d'ex\u00e9cution\n    display(HTML(\"\"\"\n    &lt;div style=\"background-color: #4CAF50; color: white; padding: 12px; margin: 10px 0; border-radius: 4px;\"&gt;\n        &lt;h3 style=\"margin: 0;\"&gt;API Flask en cours d'ex\u00e9cution!&lt;/h3&gt;\n        &lt;p style=\"margin: 5px 0 0 0;\"&gt;Utilisez l'interface ci-dessous pour tester votre API.&lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"))\n    \n    # Interface pour t\u00e9l\u00e9charger et tester l'API\n    display(HTML(\"\"\"\n    &lt;div style=\"width: 100%; max-width: 800px; margin: 0 auto; padding: 20px; border: 1px solid #ddd; border-radius: 8px;\"&gt;\n        &lt;h2&gt;Testez votre API de reconnaissance d'images&lt;/h2&gt;\n        &lt;div style=\"margin-bottom: 20px;\"&gt;\n            &lt;label for=\"file-upload\" style=\"display: block; margin-bottom: 10px;\"&gt;S\u00e9lectionnez une image:&lt;/label&gt;\n            &lt;input type=\"file\" id=\"file-upload\" accept=\"image/*\" style=\"display: block; margin-bottom: 10px;\"&gt;\n            &lt;button id=\"predict-button\" style=\"background-color: #4CAF50; color: white; padding: 10px 15px; border: none; border-radius: 4px; cursor: pointer;\"&gt;Pr\u00e9dire&lt;/button&gt;\n        &lt;/div&gt;\n        &lt;div id=\"spinner\" style=\"display: none; text-align: center; margin: 20px 0;\"&gt;\n            &lt;div style=\"border: 4px solid #f3f3f3; border-top: 4px solid #3498db; border-radius: 50%; width: 30px; height: 30px; animation: spin 2s linear infinite; margin: 0 auto;\"&gt;&lt;/div&gt;\n            &lt;p&gt;Traitement en cours...&lt;/p&gt;\n            &lt;style&gt;@keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }&lt;/style&gt;\n        &lt;/div&gt;\n        &lt;div id=\"result\" style=\"margin-top: 20px; display: none;\"&gt;\n            &lt;h3&gt;R\u00e9sultat:&lt;/h3&gt;\n            &lt;div id=\"prediction-result\" style=\"padding: 15px; background-color: #f9f9f9; border-radius: 4px;\"&gt;&lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div id=\"error\" style=\"margin-top: 20px; display: none; color: red;\"&gt;&lt;/div&gt;\n        &lt;div id=\"image-preview\" style=\"margin-top: 20px; text-align: center;\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n        document.getElementById('predict-button').addEventListener('click', function() {\n            const fileInput = document.getElementById('file-upload');\n            const file = fileInput.files[0];\n            \n            if (!file) {\n                document.getElementById('error').textContent = 'Veuillez s\u00e9lectionner une image';\n                document.getElementById('error').style.display = 'block';\n                document.getElementById('result').style.display = 'none';\n                return;\n            }\n            \n            const spinner = document.getElementById('spinner');\n            const result = document.getElementById('result');\n            const error = document.getElementById('error');\n            \n            spinner.style.display = 'block';\n            result.style.display = 'none';\n            error.style.display = 'none';\n            \n            // Afficher l'aper\u00e7u de l'image\n            const reader = new FileReader();\n            reader.onload = function(e) {\n                document.getElementById('image-preview').innerHTML = `\n                    &lt;img src=\"${e.target.result}\" style=\"max-width: 300px; max-height: 300px; border: 1px solid #ddd; border-radius: 4px;\"&gt;\n                `;\n            };\n            reader.readAsDataURL(file);\n            \n            // Envoyer l'image \u00e0 l'API\n            const formData = new FormData();\n            formData.append('image', file);\n            \n            fetch('http://localhost:8000/predict', {\n                method: 'POST',\n                body: formData\n            })\n            .then(response =&gt; {\n                if (!response.ok) {\n                    throw new Error('Erreur r\u00e9seau ou erreur serveur');\n                }\n                return response.json();\n            })\n            .then(data =&gt; {\n                spinner.style.display = 'none';\n                result.style.display = 'block';\n                \n                // Afficher les r\u00e9sultats\n                const predictionResult = document.getElementById('prediction-result');\n                \n                // Cr\u00e9er un tableau de classification avec barre de confiance\n                let resultsHTML = `\n                    &lt;div style=\"margin-bottom: 15px;\"&gt;\n                        &lt;h4 style=\"margin: 0 0 10px 0;\"&gt;Pr\u00e9diction: ${data.prediction.class_name} (${(data.prediction.confidence * 100).toFixed(2)}%)&lt;/h4&gt;\n                    &lt;/div&gt;\n                    &lt;table style=\"width: 100%; border-collapse: collapse;\"&gt;\n                        &lt;tr&gt;\n                            &lt;th style=\"text-align: left; padding: 8px; border-bottom: 1px solid #ddd;\"&gt;Classe&lt;/th&gt;\n                            &lt;th style=\"text-align: left; padding: 8px; border-bottom: 1px solid #ddd;\"&gt;Confiance&lt;/th&gt;\n                            &lt;th style=\"text-align: left; padding: 8px; border-bottom: 1px solid #ddd;\"&gt;&lt;/th&gt;\n                        &lt;/tr&gt;\n                `;\n                \n                // Trier les probabilit\u00e9s par ordre d\u00e9croissant\n                const sortedProbabilities = Object.entries(data.all_probabilities)\n                    .sort((a, b) =&gt; b[1] - a[1]);\n                \n                sortedProbabilities.forEach(([className, prob]) =&gt; {\n                    const percentage = (prob * 100).toFixed(2);\n                    const isHighest = className === data.prediction.class_name;\n                    \n                    resultsHTML += `\n                        &lt;tr&gt;\n                            &lt;td style=\"padding: 8px; border-bottom: 1px solid #ddd; ${isHighest ? 'font-weight: bold;' : ''}\"&gt;\n                                ${className}\n                            &lt;/td&gt;\n                            &lt;td style=\"padding: 8px; border-bottom: 1px solid #ddd; ${isHighest ? 'font-weight: bold;' : ''}\"&gt;\n                                ${percentage}%\n                            &lt;/td&gt;\n                            &lt;td style=\"padding: 8px; border-bottom: 1px solid #ddd;\"&gt;\n                                &lt;div style=\"background-color: #e0e0e0; border-radius: 4px; height: 20px; width: 100%;\"&gt;\n                                    &lt;div style=\"background-color: ${isHighest ? '#4CAF50' : '#3498db'}; height: 20px; border-radius: 4px; width: ${percentage}%\"&gt;&lt;/div&gt;\n                                &lt;/div&gt;\n                            &lt;/td&gt;\n                        &lt;/tr&gt;\n                    `;\n                });\n                \n                resultsHTML += `&lt;/table&gt;`;\n                predictionResult.innerHTML = resultsHTML;\n            })\n            .catch(error =&gt; {\n                spinner.style.display = 'none';\n                document.getElementById('error').textContent = 'Erreur: ' + error.message;\n                document.getElementById('error').style.display = 'block';\n                console.error('Erreur:', error);\n            });\n        });\n    &lt;/script&gt;\n    \"\"\"))\n\n# 8. Lancer l'application Flask\nrun_flask(app)\n\n# 9. Exercices d'am\u00e9lioration (\u00e0 faire par les \u00e9tudiants)\n\"\"\"\nExercice 1: Ajouter une fonction de cache pour les pr\u00e9dictions\n----------------------------------------------------------------\nPour \u00e9viter de faire des pr\u00e9dictions redondantes sur des images similaires,\najoutez un m\u00e9canisme de cache simple.\n\nConseil: Vous pouvez utiliser un dictionnaire o\u00f9 les cl\u00e9s sont des hash\ndes images et les valeurs sont les r\u00e9sultats de pr\u00e9diction.\n\nExercice 2: Ajouter une limite de requ\u00eates\n----------------------------------------------------------------\nImpl\u00e9mentez une limite de requ\u00eates par IP pour \u00e9viter une surcharge du service.\n\nConseil: Vous pouvez utiliser un dictionnaire pour stocker le nombre\nde requ\u00eates par IP et le timestamp de la derni\u00e8re requ\u00eate.\n\nExercice 3: Am\u00e9liorer le pr\u00e9traitement des images\n----------------------------------------------------------------\nModifiez la fonction de pr\u00e9traitement pour qu'elle puisse g\u00e9rer\ndiff\u00e9rents formats et tailles d'images d'entr\u00e9e.\n\nConseil: Ajoutez la d\u00e9tection automatique de fond et le recadrage\npour ne garder que l'objet principal.\n\"\"\"\n\n# 10. Pour aller plus loin: D\u00e9ploiement de l'API\n\"\"\"\nDans un environnement de production, vous voudriez d\u00e9ployer cette API\nsur un serveur web. Voici les \u00e9tapes que vous pourriez suivre:\n\n1. Sauvegarder le mod\u00e8le:\nmodel.save('mon_modele_fashion.h5')\n\n2. Cr\u00e9er un fichier app.py contenant le code Flask\n\n3. Cr\u00e9er un fichier requirements.txt avec les d\u00e9pendances:\nflask==2.0.1\nflask-cors==3.0.10\ntensorflow==2.8.0\npillow==8.3.1\nnumpy==1.21.2\ngunicorn==20.1.0\n\n4. D\u00e9ployer sur une plateforme comme Heroku, Google Cloud Run, ou AWS Elastic Beanstalk\n\n5. Pour un d\u00e9ploiement local ou sur votre propre serveur:\n   - Installer les d\u00e9pendances: pip install -r requirements.txt\n   - Lancer le serveur: gunicorn -w 4 -b 0.0.0.0:8000 app:app\n\"\"\"\n\n# Ce notebook a \u00e9t\u00e9 con\u00e7u pour \u00eatre utilis\u00e9 dans Google Colab.\n# Si vous souhaitez l'utiliser en local, vous devrez ajuster la partie d'ex\u00e9cution de Flask.\n</pre> # API de reconnaissance d'images avec Flask et TensorFlow # Notebook   # 1. Installation des biblioth\u00e8ques n\u00e9cessaires !pip install flask flask-cors pillow tensorflow  # 2. Importation des biblioth\u00e8ques import tensorflow as tf from tensorflow.keras import models import numpy as np from PIL import Image import io import os import base64 from flask import Flask, request, jsonify from flask_cors import CORS import matplotlib.pyplot as plt from google.colab.output import eval_js from IPython.display import display, Javascript, HTML  # 3. Chargement du mod\u00e8le (option 1 - utiliser le mod\u00e8le que vous avez d\u00e9j\u00e0 entra\u00een\u00e9) # D\u00e9commentez cette section si vous avez d\u00e9j\u00e0 entra\u00een\u00e9 et sauvegard\u00e9 un mod\u00e8le \"\"\" try:     model = models.load_model('fashion_mnist_model.h5')     print(\"Mod\u00e8le charg\u00e9 avec succ\u00e8s!\") except:     print(\"Erreur: Impossible de charger le mod\u00e8le entra\u00een\u00e9.\") \"\"\"  # 4. Chargement du mod\u00e8le (option 2 - utiliser un mod\u00e8le pr\u00e9-entra\u00een\u00e9) # Cette option est plus rapide si vous n'avez pas encore entra\u00een\u00e9 de mod\u00e8le  # Charger le jeu de donn\u00e9es Fashion MNIST (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()  # Pr\u00e9traiter les donn\u00e9es train_images = train_images / 255.0 test_images = test_images / 255.0  # Cr\u00e9er et entra\u00eener rapidement un mod\u00e8le simple model = tf.keras.Sequential([     tf.keras.layers.Flatten(input_shape=(28, 28)),     tf.keras.layers.Dense(128, activation='relu'),     tf.keras.layers.Dense(10, activation='softmax') ])  model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  model.fit(train_images, train_labels, epochs=5, batch_size=32, verbose=1)  # \u00c9valuer le mod\u00e8le test_loss, test_acc = model.evaluate(test_images, test_labels) print(f\"Pr\u00e9cision sur donn\u00e9es de test: {test_acc:.4f}\")  # D\u00e9finir les noms des classes class_names = ['T-shirt/haut', 'Pantalon', 'Pull', 'Robe', 'Manteau',                'Sandale', 'Chemise', 'Baskets', 'Sac', 'Bottine']  # 5. Cr\u00e9ation de l'application Flask app = Flask(__name__) CORS(app)  # Permet l'acc\u00e8s depuis d'autres domaines (important pour les applications web)  # Fonction de pr\u00e9traitement des images def preprocess_image(image):     \"\"\"     Pr\u00e9traite une image pour qu'elle soit compatible avec notre mod\u00e8le          Args:         image: Image PIL          Returns:         np.array: Image pr\u00e9trait\u00e9e et format\u00e9e pour le mod\u00e8le     \"\"\"     # Convertir en niveaux de gris si n\u00e9cessaire     if image.mode != 'L':         image = image.convert('L')          # Redimensionner \u00e0 28x28 pixels     image = image.resize((28, 28))          # Convertir en array numpy     img_array = np.array(image)          # Normaliser les valeurs de pixels \u00e0 [0,1]     img_array = img_array / 255.0          # Ajouter une dimension de batch     img_array = img_array.reshape(1, 28, 28)          return img_array  # Route principale @app.route('/') def home():     return \"\"\"     API de reconnaissance d'images <p>Utilisez le point d'acc\u00e8s /predict pour classifier des images de v\u00eatements.</p>     \"\"\"  # Route pour les pr\u00e9dictions @app.route('/predict', methods=['POST']) def predict():     # V\u00e9rifier si la requ\u00eate contient une image     if 'image' not in request.files:         return jsonify({'error': 'Aucune image trouv\u00e9e dans la requ\u00eate'}), 400          # R\u00e9cup\u00e9rer l'image     file = request.files['image']          try:         # Lire l'image         img = Image.open(file.stream)                  # Pr\u00e9traiter l'image         processed_image = preprocess_image(img)                  # Faire la pr\u00e9diction         predictions = model.predict(processed_image)                  # R\u00e9cup\u00e9rer la classe avec la plus haute probabilit\u00e9         predicted_class = np.argmax(predictions[0])         confidence = float(predictions[0][predicted_class])                  # Pr\u00e9parer la r\u00e9ponse         response = {             'prediction': {                 'class_id': int(predicted_class),                 'class_name': class_names[predicted_class],                 'confidence': float(confidence)             },             'all_probabilities': {                 class_names[i]: float(predictions[0][i]) for i in range(len(class_names))             }         }                  return jsonify(response)          except Exception as e:         return jsonify({'error': str(e)}), 500  # 6. Route pour obtenir des informations sur le mod\u00e8le @app.route('/info', methods=['GET']) def model_info():     model_summary = []          # Capturer le r\u00e9sum\u00e9 du mod\u00e8le dans une liste     model.summary(print_fn=lambda x: model_summary.append(x))          return jsonify({         'model_name': 'Fashion MNIST Classifier',         'input_shape': model.input_shape[1:],         'output_shape': model.output_shape[1:],         'number_of_classes': len(class_names),         'classes': class_names,         'model_summary': model_summary     })  # 7. Configuration pour ex\u00e9cuter Flask dans Colab import threading from google.colab.output import eval_js from IPython.display import display, Javascript  # Fonction pour ex\u00e9cuter Flask dans Colab def run_flask(app):     from google.colab.output import eval_js     from IPython.display import display, Javascript     import threading          def b64_to_pil(b64_str):         import base64         import io         from PIL import Image         img_data = base64.b64decode(b64_str)         return Image.open(io.BytesIO(img_data))          # D\u00e9marrer le serveur Flask dans un thread s\u00e9par\u00e9     threading.Thread(target=lambda: app.run(host='0.0.0.0', port=8000, debug=False, use_reloader=False)).start()          # Afficher un message pour indiquer que le serveur est en cours d'ex\u00e9cution     display(HTML(\"\"\"      API Flask en cours d'ex\u00e9cution! <p>Utilisez l'interface ci-dessous pour tester votre API.</p>      \"\"\"))          # Interface pour t\u00e9l\u00e9charger et tester l'API     display(HTML(\"\"\"      Testez votre API de reconnaissance d'images S\u00e9lectionnez une image: Pr\u00e9dire <p>Traitement en cours...</p> R\u00e9sultat:      \"\"\"))  # 8. Lancer l'application Flask run_flask(app)  # 9. Exercices d'am\u00e9lioration (\u00e0 faire par les \u00e9tudiants) \"\"\" Exercice 1: Ajouter une fonction de cache pour les pr\u00e9dictions ---------------------------------------------------------------- Pour \u00e9viter de faire des pr\u00e9dictions redondantes sur des images similaires, ajoutez un m\u00e9canisme de cache simple.  Conseil: Vous pouvez utiliser un dictionnaire o\u00f9 les cl\u00e9s sont des hash des images et les valeurs sont les r\u00e9sultats de pr\u00e9diction.  Exercice 2: Ajouter une limite de requ\u00eates ---------------------------------------------------------------- Impl\u00e9mentez une limite de requ\u00eates par IP pour \u00e9viter une surcharge du service.  Conseil: Vous pouvez utiliser un dictionnaire pour stocker le nombre de requ\u00eates par IP et le timestamp de la derni\u00e8re requ\u00eate.  Exercice 3: Am\u00e9liorer le pr\u00e9traitement des images ---------------------------------------------------------------- Modifiez la fonction de pr\u00e9traitement pour qu'elle puisse g\u00e9rer diff\u00e9rents formats et tailles d'images d'entr\u00e9e.  Conseil: Ajoutez la d\u00e9tection automatique de fond et le recadrage pour ne garder que l'objet principal. \"\"\"  # 10. Pour aller plus loin: D\u00e9ploiement de l'API \"\"\" Dans un environnement de production, vous voudriez d\u00e9ployer cette API sur un serveur web. Voici les \u00e9tapes que vous pourriez suivre:  1. Sauvegarder le mod\u00e8le: model.save('mon_modele_fashion.h5')  2. Cr\u00e9er un fichier app.py contenant le code Flask  3. Cr\u00e9er un fichier requirements.txt avec les d\u00e9pendances: flask==2.0.1 flask-cors==3.0.10 tensorflow==2.8.0 pillow==8.3.1 numpy==1.21.2 gunicorn==20.1.0  4. D\u00e9ployer sur une plateforme comme Heroku, Google Cloud Run, ou AWS Elastic Beanstalk  5. Pour un d\u00e9ploiement local ou sur votre propre serveur:    - Installer les d\u00e9pendances: pip install -r requirements.txt    - Lancer le serveur: gunicorn -w 4 -b 0.0.0.0:8000 app:app \"\"\"  # Ce notebook a \u00e9t\u00e9 con\u00e7u pour \u00eatre utilis\u00e9 dans Google Colab. # Si vous souhaitez l'utiliser en local, vous devrez ajuster la partie d'ex\u00e9cution de Flask."},{"location":"seance3/ressources/prototype-chatbot/","title":"Prototype chatbot","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nPrototype de chatbot p\u00e9dagogique sur le Deep Learning utilisant l'API Mistral AI\nPour les \u00e9tudiants BTS SIO \n\nCe script cr\u00e9e un chatbot simple avec interface web via Flask\nqui r\u00e9pond aux questions sur le Deep Learning.\n\"\"\"\n</pre> \"\"\" Prototype de chatbot p\u00e9dagogique sur le Deep Learning utilisant l'API Mistral AI Pour les \u00e9tudiants BTS SIO   Ce script cr\u00e9e un chatbot simple avec interface web via Flask qui r\u00e9pond aux questions sur le Deep Learning. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import os\nfrom fastapi import FastAPI, Request, HTTPException, Depends\nfrom fastapi.responses import JSONResponse, HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Optional, Any\nimport uuid\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\nfrom dotenv import load_dotenv\nimport json\nimport time\nimport uvicorn\nfrom starlette.middleware.sessions import SessionMiddleware\n</pre> import os from fastapi import FastAPI, Request, HTTPException, Depends from fastapi.responses import JSONResponse, HTMLResponse from fastapi.templating import Jinja2Templates from fastapi.staticfiles import StaticFiles from fastapi.middleware.cors import CORSMiddleware from pydantic import BaseModel from typing import List, Dict, Optional, Any import uuid from mistralai.client import MistralClient from mistralai.models.chat_completion import ChatMessage from dotenv import load_dotenv import json import time import uvicorn from starlette.middleware.sessions import SessionMiddleware In\u00a0[\u00a0]: Copied! <pre># Chargement des variables d'environnement\nload_dotenv()\n</pre> # Chargement des variables d'environnement load_dotenv() <p>Configuration de l'application</p> In\u00a0[\u00a0]: Copied! <pre>app = FastAPI(\n    title=\"Chatbot Deep Learning\",\n    description=\"Un chatbot p\u00e9dagogique sur le Deep Learning utilisant l'API Mistral AI\",\n    version=\"1.0.0\"\n)\n</pre> app = FastAPI(     title=\"Chatbot Deep Learning\",     description=\"Un chatbot p\u00e9dagogique sur le Deep Learning utilisant l'API Mistral AI\",     version=\"1.0.0\" ) In\u00a0[\u00a0]: Copied! <pre># Ajout du middleware pour les sessions\napp.add_middleware(SessionMiddleware, secret_key=os.getenv(\"SECRET_KEY\", \"deep_learning_chatbot_secret\"))\n</pre> # Ajout du middleware pour les sessions app.add_middleware(SessionMiddleware, secret_key=os.getenv(\"SECRET_KEY\", \"deep_learning_chatbot_secret\")) In\u00a0[\u00a0]: Copied! <pre># Ajout du middleware CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n</pre> # Ajout du middleware CORS app.add_middleware(     CORSMiddleware,     allow_origins=[\"*\"],     allow_credentials=True,     allow_methods=[\"*\"],     allow_headers=[\"*\"], ) In\u00a0[\u00a0]: Copied! <pre># Configuration des templates\ntemplates = Jinja2Templates(directory=\"templates\")\n</pre> # Configuration des templates templates = Jinja2Templates(directory=\"templates\") In\u00a0[\u00a0]: Copied! <pre># Configuration de l'API Mistral\nmistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\nclient = MistralClient(api_key=mistral_api_key)\nmodel = \"mistral-tiny\"  # Mod\u00e8le \u00e9conomique et rapide, suffisant pour notre prototype\n</pre> # Configuration de l'API Mistral mistral_api_key = os.getenv(\"MISTRAL_API_KEY\") client = MistralClient(api_key=mistral_api_key) model = \"mistral-tiny\"  # Mod\u00e8le \u00e9conomique et rapide, suffisant pour notre prototype In\u00a0[\u00a0]: Copied! <pre># Chargement de la base de connaissances simplifi\u00e9e\ndef load_knowledge_base():\n    try:\n        with open('knowledge_base.json', 'r', encoding='utf-8') as file:\n            return json.load(file)\n    except FileNotFoundError:\n        # Base de connaissances minimale si le fichier n'existe pas\n        return {\n            \"concepts\": [\n                {\n                    \"name\": \"r\u00e9seau de neurones\",\n                    \"definition\": \"Un mod\u00e8le informatique inspir\u00e9 du cerveau humain, compos\u00e9 de neurones artificiels connect\u00e9s qui traitent l'information.\"\n                },\n                {\n                    \"name\": \"deep learning\",\n                    \"definition\": \"Sous-domaine du machine learning utilisant des r\u00e9seaux de neurones profonds (avec plusieurs couches) pour apprendre \u00e0 partir de donn\u00e9es.\"\n                },\n                {\n                    \"name\": \"CNN\",\n                    \"definition\": \"R\u00e9seau de neurones convolutif, particuli\u00e8rement efficace pour traiter des images gr\u00e2ce \u00e0 des op\u00e9rations de convolution.\"\n                }\n            ]\n        }\n</pre> # Chargement de la base de connaissances simplifi\u00e9e def load_knowledge_base():     try:         with open('knowledge_base.json', 'r', encoding='utf-8') as file:             return json.load(file)     except FileNotFoundError:         # Base de connaissances minimale si le fichier n'existe pas         return {             \"concepts\": [                 {                     \"name\": \"r\u00e9seau de neurones\",                     \"definition\": \"Un mod\u00e8le informatique inspir\u00e9 du cerveau humain, compos\u00e9 de neurones artificiels connect\u00e9s qui traitent l'information.\"                 },                 {                     \"name\": \"deep learning\",                     \"definition\": \"Sous-domaine du machine learning utilisant des r\u00e9seaux de neurones profonds (avec plusieurs couches) pour apprendre \u00e0 partir de donn\u00e9es.\"                 },                 {                     \"name\": \"CNN\",                     \"definition\": \"R\u00e9seau de neurones convolutif, particuli\u00e8rement efficace pour traiter des images gr\u00e2ce \u00e0 des op\u00e9rations de convolution.\"                 }             ]         } In\u00a0[\u00a0]: Copied! <pre>knowledge_base = load_knowledge_base()\n</pre> knowledge_base = load_knowledge_base() In\u00a0[\u00a0]: Copied! <pre># Dictionnaire pour stocker les conversations (dans un vrai projet, utilisez une base de donn\u00e9es)\nconversations = {}\n</pre> # Dictionnaire pour stocker les conversations (dans un vrai projet, utilisez une base de donn\u00e9es) conversations = {} In\u00a0[\u00a0]: Copied! <pre># Cache simple pour les r\u00e9ponses fr\u00e9quentes\nresponse_cache = {}\n</pre> # Cache simple pour les r\u00e9ponses fr\u00e9quentes response_cache = {} In\u00a0[\u00a0]: Copied! <pre>def create_system_prompt():\n    \"\"\"Cr\u00e9e un prompt syst\u00e8me avec instructions et base de connaissances int\u00e9gr\u00e9e\"\"\"\n    \n    # Extraction des d\u00e9finitions pour les inclure dans le prompt\n    definitions = \"\"\n    for concept in knowledge_base[\"concepts\"]:\n        definitions += f\"- {concept['name']}: {concept['definition']}\\n\"\n    \n    # Construction du prompt syst\u00e8me complet\n    system_prompt = f\"\"\"\n    Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans le Deep Learning pour des \u00e9tudiants de BTS SIO SLAM.\n    Ton r\u00f4le est d'expliquer les concepts complexes de mani\u00e8re simple et accessible.\n    \n    Directives:\n    1. Utilise un langage clair et simple, \u00e9vite le jargon technique quand c'est possible\n    2. Propose des exemples concrets li\u00e9s au d\u00e9veloppement logiciel\n    3. N'utilise pas de formules math\u00e9matiques complexes\n    4. Adapte tes explications pour des \u00e9tudiants de niveau BTS\n    5. Si tu ne connais pas la r\u00e9ponse, dis-le honn\u00eatement\n    \n    Base de connaissances \u00e0 utiliser en priorit\u00e9:\n    {definitions}\n    \n    Format de r\u00e9ponse recommand\u00e9:\n    - Commencer par une d\u00e9finition simple et accessible\n    - Expliquer le concept en termes simples\n    - Donner un exemple concret d'application\n    - Si pertinent, mentionner les frameworks utilis\u00e9s (TensorFlow, Keras, etc.)\n    \"\"\"\n    \n    return system_prompt\n</pre> def create_system_prompt():     \"\"\"Cr\u00e9e un prompt syst\u00e8me avec instructions et base de connaissances int\u00e9gr\u00e9e\"\"\"          # Extraction des d\u00e9finitions pour les inclure dans le prompt     definitions = \"\"     for concept in knowledge_base[\"concepts\"]:         definitions += f\"- {concept['name']}: {concept['definition']}\\n\"          # Construction du prompt syst\u00e8me complet     system_prompt = f\"\"\"     Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans le Deep Learning pour des \u00e9tudiants de BTS SIO SLAM.     Ton r\u00f4le est d'expliquer les concepts complexes de mani\u00e8re simple et accessible.          Directives:     1. Utilise un langage clair et simple, \u00e9vite le jargon technique quand c'est possible     2. Propose des exemples concrets li\u00e9s au d\u00e9veloppement logiciel     3. N'utilise pas de formules math\u00e9matiques complexes     4. Adapte tes explications pour des \u00e9tudiants de niveau BTS     5. Si tu ne connais pas la r\u00e9ponse, dis-le honn\u00eatement          Base de connaissances \u00e0 utiliser en priorit\u00e9:     {definitions}          Format de r\u00e9ponse recommand\u00e9:     - Commencer par une d\u00e9finition simple et accessible     - Expliquer le concept en termes simples     - Donner un exemple concret d'application     - Si pertinent, mentionner les frameworks utilis\u00e9s (TensorFlow, Keras, etc.)     \"\"\"          return system_prompt In\u00a0[\u00a0]: Copied! <pre># Routes de l'application\n@app.route('/')\ndef home():\n    \"\"\"Page d'accueil avec l'interface du chatbot\"\"\"\n    \n    # Cr\u00e9ation d'un ID de session si inexistant\n    if 'session_id' not in session:\n        session['session_id'] = str(uuid.uuid4())\n    \n    return render_template('index.html')\n</pre> # Routes de l'application @app.route('/') def home():     \"\"\"Page d'accueil avec l'interface du chatbot\"\"\"          # Cr\u00e9ation d'un ID de session si inexistant     if 'session_id' not in session:         session['session_id'] = str(uuid.uuid4())          return render_template('index.html') In\u00a0[\u00a0]: Copied! <pre>@app.route('/api/chat', methods=['POST'])\ndef chat():\n    \"\"\"Endpoint API pour les interactions avec le chatbot\"\"\"\n    \n    # R\u00e9cup\u00e9ration des donn\u00e9es de la requ\u00eate\n    data = request.json\n    user_message = data.get('message', '')\n    session_id = session.get('session_id', str(uuid.uuid4()))\n    \n    # V\u00e9rification des param\u00e8tres\n    if not user_message:\n        return jsonify({\"error\": \"Le message ne peut pas \u00eatre vide\"}), 400\n    \n    # V\u00e9rification de la pr\u00e9sence dans le cache\n    cache_key = f\"{session_id}:{user_message.lower().strip()}\"\n    if cache_key in response_cache:\n        return jsonify({\n            \"reply\": response_cache[cache_key],\n            \"source\": \"cache\"\n        })\n    \n    # Initialisation ou r\u00e9cup\u00e9ration de l'historique de conversation\n    if session_id not in conversations:\n        conversations[session_id] = [\n            ChatMessage(role=\"system\", content=create_system_prompt())\n        ]\n    \n    # Ajout du message utilisateur \u00e0 l'historique\n    conversation_history = conversations[session_id]\n    conversation_history.append(ChatMessage(role=\"user\", content=user_message))\n    \n    # Limitation de la taille de l'historique pour \u00e9conomiser les tokens\n    if len(conversation_history) &gt; 10:  # Garder uniquement les 10 derniers messages\n        # Toujours conserver le message syst\u00e8me initial\n        conversation_history = [conversation_history[0]] + conversation_history[-9:]\n    \n    try:\n        # Appel \u00e0 l'API Mistral\n        start_time = time.time()\n        response = client.chat(\n            model=model,\n            messages=conversation_history,\n            temperature=0.7,  # Un peu de cr\u00e9ativit\u00e9 mais pas trop\n            max_tokens=800    # Limiter la longueur des r\u00e9ponses\n        )\n        api_time = time.time() - start_time\n        \n        # Extraction de la r\u00e9ponse\n        ai_message = response.choices[0].message.content\n        \n        # Ajout de la r\u00e9ponse \u00e0 l'historique\n        conversation_history.append(ChatMessage(role=\"assistant\", content=ai_message))\n        \n        # Mise en cache de la r\u00e9ponse\n        response_cache[cache_key] = ai_message\n        \n        # Si le cache devient trop grand, supprimer les entr\u00e9es les plus anciennes\n        if len(response_cache) &gt; 100:\n            keys = list(response_cache.keys())\n            for old_key in keys[:10]:  # Supprimer les 10 plus anciennes entr\u00e9es\n                response_cache.pop(old_key, None)\n        \n        # Mise \u00e0 jour de l'historique dans le dictionnaire\n        conversations[session_id] = conversation_history\n        \n        return jsonify({\n            \"reply\": ai_message,\n            \"api_time\": f\"{api_time:.2f}s\"\n        })\n    \n    except Exception as e:\n        # Gestion des erreurs d'API\n        error_message = str(e)\n        print(f\"Erreur API Mistral: {error_message}\")\n        \n        # Message d'erreur adapt\u00e9 \u00e0 l'utilisateur\n        user_friendly_message = \"D\u00e9sol\u00e9, je rencontre un probl\u00e8me technique. Veuillez r\u00e9essayer dans quelques instants.\"\n        \n        return jsonify({\n            \"error\": error_message,\n            \"reply\": user_friendly_message\n        }), 500\n</pre> @app.route('/api/chat', methods=['POST']) def chat():     \"\"\"Endpoint API pour les interactions avec le chatbot\"\"\"          # R\u00e9cup\u00e9ration des donn\u00e9es de la requ\u00eate     data = request.json     user_message = data.get('message', '')     session_id = session.get('session_id', str(uuid.uuid4()))          # V\u00e9rification des param\u00e8tres     if not user_message:         return jsonify({\"error\": \"Le message ne peut pas \u00eatre vide\"}), 400          # V\u00e9rification de la pr\u00e9sence dans le cache     cache_key = f\"{session_id}:{user_message.lower().strip()}\"     if cache_key in response_cache:         return jsonify({             \"reply\": response_cache[cache_key],             \"source\": \"cache\"         })          # Initialisation ou r\u00e9cup\u00e9ration de l'historique de conversation     if session_id not in conversations:         conversations[session_id] = [             ChatMessage(role=\"system\", content=create_system_prompt())         ]          # Ajout du message utilisateur \u00e0 l'historique     conversation_history = conversations[session_id]     conversation_history.append(ChatMessage(role=\"user\", content=user_message))          # Limitation de la taille de l'historique pour \u00e9conomiser les tokens     if len(conversation_history) &gt; 10:  # Garder uniquement les 10 derniers messages         # Toujours conserver le message syst\u00e8me initial         conversation_history = [conversation_history[0]] + conversation_history[-9:]          try:         # Appel \u00e0 l'API Mistral         start_time = time.time()         response = client.chat(             model=model,             messages=conversation_history,             temperature=0.7,  # Un peu de cr\u00e9ativit\u00e9 mais pas trop             max_tokens=800    # Limiter la longueur des r\u00e9ponses         )         api_time = time.time() - start_time                  # Extraction de la r\u00e9ponse         ai_message = response.choices[0].message.content                  # Ajout de la r\u00e9ponse \u00e0 l'historique         conversation_history.append(ChatMessage(role=\"assistant\", content=ai_message))                  # Mise en cache de la r\u00e9ponse         response_cache[cache_key] = ai_message                  # Si le cache devient trop grand, supprimer les entr\u00e9es les plus anciennes         if len(response_cache) &gt; 100:             keys = list(response_cache.keys())             for old_key in keys[:10]:  # Supprimer les 10 plus anciennes entr\u00e9es                 response_cache.pop(old_key, None)                  # Mise \u00e0 jour de l'historique dans le dictionnaire         conversations[session_id] = conversation_history                  return jsonify({             \"reply\": ai_message,             \"api_time\": f\"{api_time:.2f}s\"         })          except Exception as e:         # Gestion des erreurs d'API         error_message = str(e)         print(f\"Erreur API Mistral: {error_message}\")                  # Message d'erreur adapt\u00e9 \u00e0 l'utilisateur         user_friendly_message = \"D\u00e9sol\u00e9, je rencontre un probl\u00e8me technique. Veuillez r\u00e9essayer dans quelques instants.\"                  return jsonify({             \"error\": error_message,             \"reply\": user_friendly_message         }), 500 In\u00a0[\u00a0]: Copied! <pre>@app.route('/api/reset', methods=['POST'])\ndef reset_conversation():\n    \"\"\"R\u00e9initialise la conversation avec le chatbot\"\"\"\n    \n    session_id = session.get('session_id')\n    if session_id and session_id in conversations:\n        # Garde uniquement le message syst\u00e8me initial\n        system_message = conversations[session_id][0]\n        conversations[session_id] = [system_message]\n    \n    return jsonify({\"status\": \"success\", \"message\": \"Conversation r\u00e9initialis\u00e9e\"})\n</pre> @app.route('/api/reset', methods=['POST']) def reset_conversation():     \"\"\"R\u00e9initialise la conversation avec le chatbot\"\"\"          session_id = session.get('session_id')     if session_id and session_id in conversations:         # Garde uniquement le message syst\u00e8me initial         system_message = conversations[session_id][0]         conversations[session_id] = [system_message]          return jsonify({\"status\": \"success\", \"message\": \"Conversation r\u00e9initialis\u00e9e\"}) In\u00a0[\u00a0]: Copied! <pre>@app.route('/api/topics', methods=['GET'])\ndef get_topics():\n    \"\"\"Renvoie la liste des sujets disponibles dans la base de connaissances\"\"\"\n    \n    topics = [concept['name'] for concept in knowledge_base['concepts']]\n    return jsonify({\"topics\": topics})\n</pre> @app.route('/api/topics', methods=['GET']) def get_topics():     \"\"\"Renvoie la liste des sujets disponibles dans la base de connaissances\"\"\"          topics = [concept['name'] for concept in knowledge_base['concepts']]     return jsonify({\"topics\": topics}) In\u00a0[\u00a0]: Copied! <pre># Configuration des templates et des fichiers statiques\n@app.route('/templates/&lt;path:path&gt;')\ndef send_template(path):\n    return render_template(path)\n</pre> # Configuration des templates et des fichiers statiques @app.route('/templates/') def send_template(path):     return render_template(path) In\u00a0[\u00a0]: Copied! <pre>if __name__ == '__main__':\n    # Cr\u00e9ation du dossier templates s'il n'existe pas\n    os.makedirs('templates', exist_ok=True)\n    \n    # Cr\u00e9ation d'un template HTML minimal s'il n'existe pas\n    if not os.path.exists('templates/index.html'):\n        with open('templates/index.html', 'w', encoding='utf-8') as f:\n            f.write(\"\"\"&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;ChatBot Deep Learning&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #f5f5f5;\n        }\n        h1 {\n            color: #2c3e50;\n            text-align: center;\n            margin-bottom: 20px;\n        }\n        #chat-container {\n            height: 500px;\n            border: 1px solid #ddd;\n            padding: 15px;\n            overflow-y: auto;\n            margin-bottom: 15px;\n            border-radius: 8px;\n            background-color: white;\n            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n        }\n        .message {\n            margin-bottom: 15px;\n            padding: 10px 15px;\n            border-radius: 18px;\n            max-width: 80%;\n            line-height: 1.4;\n        }\n        .user-message {\n            background-color: #3498db;\n            color: white;\n            margin-left: auto;\n            border-bottom-right-radius: 0;\n            text-align: right;\n        }\n        .bot-message {\n            background-color: #f1f1f1;\n            margin-right: auto;\n            border-bottom-left-radius: 0;\n            color: #333;\n        }\n        #message-form {\n            display: flex;\n            margin-bottom: 15px;\n        }\n        #user-input {\n            flex: 1;\n            padding: 12px;\n            border: 1px solid #ddd;\n            border-radius: 4px 0 0 4px;\n            font-size: 16px;\n        }\n        #send-button {\n            padding: 12px 20px;\n            background-color: #3498db;\n            color: white;\n            border: none;\n            border-radius: 0 4px 4px 0;\n            cursor: pointer;\n            font-size: 16px;\n            transition: background-color 0.3s;\n        }\n        #send-button:hover {\n            background-color: #2980b9;\n        }\n        #loading {\n            display: none;\n            text-align: center;\n            margin: 10px 0;\n            color: #666;\n        }\n        .loader {\n            display: inline-block;\n            width: 20px;\n            height: 20px;\n            border: 3px solid rgba(0,0,0,.1);\n            border-radius: 50%;\n            border-top-color: #3498db;\n            animation: spin 1s ease-in-out infinite;\n            margin-right: 10px;\n        }\n        @keyframes spin {\n            to { transform: rotate(360deg); }\n        }\n        .controls {\n            display: flex;\n            justify-content: space-between;\n            margin-bottom: 10px;\n        }\n        #reset-button {\n            padding: 8px 15px;\n            background-color: #e74c3c;\n            color: white;\n            border: none;\n            border-radius: 4px;\n            cursor: pointer;\n            transition: background-color 0.3s;\n        }\n        #reset-button:hover {\n            background-color: #c0392b;\n        }\n        #topics-dropdown {\n            padding: 8px;\n            border-radius: 4px;\n            border: 1px solid #ddd;\n        }\n        .code-block {\n            background-color: #f8f8f8;\n            padding: 10px;\n            border-radius: 4px;\n            font-family: monospace;\n            overflow-x: auto;\n            margin: 10px 0;\n        }\n        .api-time {\n            font-size: 12px;\n            color: #999;\n            text-align: right;\n            margin-top: 5px;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Assistant Deep Learning BTS SIO&lt;/h1&gt;\n    \n    &lt;div class=\"controls\"&gt;\n        &lt;select id=\"topics-dropdown\"&gt;\n            &lt;option value=\"\"&gt;S\u00e9lectionnez un sujet...&lt;/option&gt;\n            &lt;!-- Topics will be loaded dynamically --&gt;\n        &lt;/select&gt;\n        &lt;button id=\"reset-button\"&gt;Nouvelle conversation&lt;/button&gt;\n    &lt;/div&gt;\n    \n    &lt;div id=\"chat-container\"&gt;&lt;/div&gt;\n    \n    &lt;div id=\"loading\"&gt;\n        &lt;div class=\"loader\"&gt;&lt;/div&gt;\n        &lt;span&gt;L'assistant r\u00e9fl\u00e9chit...&lt;/span&gt;\n    &lt;/div&gt;\n    \n    &lt;form id=\"message-form\"&gt;\n        &lt;input type=\"text\" id=\"user-input\" placeholder=\"Posez votre question sur le Deep Learning...\" required&gt;\n        &lt;button type=\"submit\" id=\"send-button\"&gt;Envoyer&lt;/button&gt;\n    &lt;/form&gt;\n    \n    &lt;script&gt;\n        // DOM Elements\n        const chatContainer = document.getElementById('chat-container');\n        const messageForm = document.getElementById('message-form');\n        const userInput = document.getElementById('user-input');\n        const loading = document.getElementById('loading');\n        const resetButton = document.getElementById('reset-button');\n        const topicsDropdown = document.getElementById('topics-dropdown');\n        \n        // Add welcome message\n        addBotMessage(\"\ud83d\udc4b Bonjour ! Je suis votre assistant sp\u00e9cialis\u00e9 en Deep Learning pour BTS SIO SLAM. Comment puis-je vous aider aujourd'hui ?\");\n        \n        // Load topics\n        fetch('/api/topics')\n            .then(response =&gt; response.json())\n            .then(data =&gt; {\n                data.topics.forEach(topic =&gt; {\n                    const option = document.createElement('option');\n                    option.value = topic;\n                    option.textContent = topic;\n                    topicsDropdown.appendChild(option);\n                });\n            });\n        \n        // Topic selection\n        topicsDropdown.addEventListener('change', function() {\n            const selectedTopic = this.value;\n            if (selectedTopic) {\n                userInput.value = `Qu'est-ce que ${selectedTopic} ?`;\n                messageForm.dispatchEvent(new Event('submit'));\n            }\n        });\n        \n        // Send message\n        messageForm.addEventListener('submit', async function(e) {\n            e.preventDefault();\n            const message = userInput.value.trim();\n            if (!message) return;\n            \n            // Display user message\n            addUserMessage(message);\n            userInput.value = '';\n            \n            // Show loading indicator\n            loading.style.display = 'flex';\n            \n            try {\n                // Send request to server\n                const response = await fetch('/api/chat', {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json'\n                    },\n                    body: JSON.stringify({\n                        message: message\n                    })\n                });\n                \n                const data = await response.json();\n                \n                if (response.ok) {\n                    // Display bot response\n                    addBotMessage(data.reply, data.api_time);\n                } else {\n                    // Display error\n                    addBotMessage(\"D\u00e9sol\u00e9, une erreur s'est produite. Veuillez r\u00e9essayer.\");\n                    console.error('Error:', data.error);\n                }\n            } catch (error) {\n                addBotMessage(\"D\u00e9sol\u00e9, je n'ai pas pu communiquer avec le serveur. Veuillez v\u00e9rifier votre connexion.\");\n                console.error('Error:', error);\n            } finally {\n                // Hide loading indicator\n                loading.style.display = 'none';\n            }\n        });\n        \n        // Reset conversation\n        resetButton.addEventListener('click', async function() {\n            try {\n                await fetch('/api/reset', {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json'\n                    }\n                });\n                \n                // Clear chat container\n                chatContainer.innerHTML = '';\n                \n                // Add welcome message\n                addBotMessage(\"\ud83d\udc4b Conversation r\u00e9initialis\u00e9e ! Comment puis-je vous aider ?\");\n            } catch (error) {\n                console.error('Error resetting conversation:', error);\n            }\n        });\n        \n        // Helper functions\n        function addUserMessage(text) {\n            const messageDiv = document.createElement('div');\n            messageDiv.className = 'message user-message';\n            messageDiv.textContent = text;\n            chatContainer.appendChild(messageDiv);\n            chatContainer.scrollTop = chatContainer.scrollHeight;\n        }\n        \n        function addBotMessage(text, apiTime = null) {\n            const messageDiv = document.createElement('div');\n            messageDiv.className = 'message bot-message';\n            \n            // Process markdown-like formatting\n            text = formatMessage(text);\n            \n            messageDiv.innerHTML = text;\n            \n            // Add API time if available\n            if (apiTime) {\n                const timeDiv = document.createElement('div');\n                timeDiv.className = 'api-time';\n                timeDiv.textContent = `Temps de r\u00e9ponse: ${apiTime}`;\n                messageDiv.appendChild(timeDiv);\n            }\n            \n            chatContainer.appendChild(messageDiv);\n            chatContainer.scrollTop = chatContainer.scrollHeight;\n        }\n        \n        function formatMessage(text) {\n            // Code blocks\n            text = text.replace(/```([\\s\\S]*?)```/g, '&lt;div class=\"code-block\"&gt;$1&lt;/div&gt;');\n            \n            // Inline code\n            text = text.replace(/`([^`]+)`/g, '&lt;code&gt;$1&lt;/code&gt;');\n            \n            // Bold\n            text = text.replace(/\\*\\*(.*?)\\*\\*/g, '&lt;strong&gt;$1&lt;/strong&gt;');\n            \n            // Italic\n            text = text.replace(/\\*(.*?)\\*/g, '&lt;em&gt;$1&lt;/em&gt;');\n            \n            // Lists\n            text = text.replace(/^\\s*[-*]\\s+(.*?)$/gm, '&lt;li&gt;$1&lt;/li&gt;');\n            text = text.replace(/&lt;li&gt;(.*?)&lt;\\/li&gt;(\\s*&lt;li&gt;)/g, '&lt;li&gt;$1&lt;/li&gt;&lt;ul&gt;$2');\n            text = text.replace(/(&lt;\\/li&gt;\\s*)(?![&lt;\\s])/g, '$1&lt;/ul&gt;');\n            \n            // Line breaks\n            text = text.replace(/\\n/g, '&lt;br&gt;');\n            \n            return text;\n        }\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\"\"\")\n</pre> if __name__ == '__main__':     # Cr\u00e9ation du dossier templates s'il n'existe pas     os.makedirs('templates', exist_ok=True)          # Cr\u00e9ation d'un template HTML minimal s'il n'existe pas     if not os.path.exists('templates/index.html'):         with open('templates/index.html', 'w', encoding='utf-8') as f:             f.write(\"\"\" ChatBot Deep Learning Assistant Deep Learning BTS SIO S\u00e9lectionnez un sujet... Nouvelle conversation L'assistant r\u00e9fl\u00e9chit... Envoyer \"\"\") In\u00a0[\u00a0]: Copied! <pre>print(\"Template index.html cr\u00e9\u00e9 avec succ\u00e8s!\")\n</pre> print(\"Template index.html cr\u00e9\u00e9 avec succ\u00e8s!\") In\u00a0[\u00a0]: Copied! <pre># Point d'entr\u00e9e principal\nif __name__ == \"__main__\":\n    # Cr\u00e9er les templates\n    create_templates()\n    \n    print(\"ChatBot d\u00e9marr\u00e9 ! Acc\u00e9dez \u00e0 http://127.0.0.1:8000 pour commencer.\")\n    print(\"Documentation API disponible \u00e0 http://127.0.0.1:8000/docs\")\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n</pre> # Point d'entr\u00e9e principal if __name__ == \"__main__\":     # Cr\u00e9er les templates     create_templates()          print(\"ChatBot d\u00e9marr\u00e9 ! Acc\u00e9dez \u00e0 http://127.0.0.1:8000 pour commencer.\")     print(\"Documentation API disponible \u00e0 http://127.0.0.1:8000/docs\")     uvicorn.run(app, host=\"127.0.0.1\", port=8000)"},{"location":"seance3/ressources/quiz-frameworks/","title":"Quiz : Frameworks de Deep Learning","text":"<p>Ce quiz vous permet d'\u00e9valuer vos connaissances sur les frameworks de Deep Learning vus durant la s\u00e9ance 3.  R\u00e9pondez aux questions suivantes en choisissant la ou les bonnes r\u00e9ponses.</p>"},{"location":"seance3/ressources/quiz-frameworks/#questions-a-choix-multiples","title":"Questions \u00e0 choix multiples","text":""},{"location":"seance3/ressources/quiz-frameworks/#1-parmi-ces-affirmations-sur-tensorflowkeras-lesquelles-sont-vraies","title":"1. Parmi ces affirmations sur TensorFlow/Keras, lesquelles sont vraies ?","text":"<ul> <li> a) TensorFlow et Keras sont deux frameworks compl\u00e8tement ind\u00e9pendants</li> <li> b) Keras est une API de haut niveau qui s'ex\u00e9cute au-dessus de TensorFlow</li> <li> c) TensorFlow ne peut pas \u00eatre utilis\u00e9 sans Keras</li> <li> d) Keras permet d'\u00e9crire du code plus concis que TensorFlow pur</li> <li> e) TensorFlow est principalement utilis\u00e9 pour le traitement d'images uniquement</li> </ul>"},{"location":"seance3/ressources/quiz-frameworks/#2-quels-avantages-presentent-les-modeles-pre-entraines","title":"2. Quels avantages pr\u00e9sentent les mod\u00e8les pr\u00e9-entra\u00een\u00e9s ?","text":"<ul> <li> a) Ils permettent de gagner du temps de d\u00e9veloppement</li> <li> b) Ils n\u00e9cessitent moins de donn\u00e9es pour \u00eatre adapt\u00e9s \u00e0 une nouvelle t\u00e2che</li> <li> c) Ils fonctionnent uniquement sur des images</li> <li> d) Ils sont g\u00e9n\u00e9ralement plus performants qu'un mod\u00e8le entra\u00een\u00e9 from scratch sur un petit dataset</li> <li> e) Ils ne peuvent pas \u00eatre modifi\u00e9s une fois t\u00e9l\u00e9charg\u00e9s</li> </ul>"},{"location":"seance3/ressources/quiz-frameworks/#3-dans-une-api-flask-pour-lintelligence-artificielle-quelles-fonctionnalites-sont-importantes-a-implementer","title":"3. Dans une API Flask pour l'intelligence artificielle, quelles fonctionnalit\u00e9s sont importantes \u00e0 impl\u00e9menter ?","text":"<ul> <li> a) Un endpoint pour les pr\u00e9dictions</li> <li> b) Une documentation des entr\u00e9es/sorties attendues</li> <li> c) Un pr\u00e9traitement des donn\u00e9es entrantes</li> <li> d) Un stockage permanent de toutes les pr\u00e9dictions</li> <li> e) Une gestion des erreurs appropri\u00e9e</li> </ul>"},{"location":"seance3/ressources/quiz-frameworks/#4-parmi-ces-techniques-lesquelles-permettent-dameliorer-les-performances-dun-modele-de-deep-learning","title":"4. Parmi ces techniques, lesquelles permettent d'am\u00e9liorer les performances d'un mod\u00e8le de Deep Learning ?","text":"<ul> <li> a) Augmentation de donn\u00e9es</li> <li> b) Transfer learning</li> <li> c) Diminution syst\u00e9matique du learning rate</li> <li> d) Normalisation des entr\u00e9es</li> <li> e) Utilisation exclusive de mod\u00e8les avec plus de couches</li> </ul>"},{"location":"seance3/ressources/quiz-frameworks/#5-pour-integrer-un-modele-de-deep-learning-dans-une-application-professionnelle-quelles-approches-sont-recommandees","title":"5. Pour int\u00e9grer un mod\u00e8le de Deep Learning dans une application professionnelle, quelles approches sont recommand\u00e9es ?","text":"<ul> <li> a) Entra\u00eener le mod\u00e8le directement sur le serveur de production</li> <li> b) Sauvegarder le mod\u00e8le entra\u00een\u00e9 dans un format standardis\u00e9 (h5, pb, onnx)</li> <li> c) Cr\u00e9er une API REST pour servir les pr\u00e9dictions</li> <li> d) Impl\u00e9menter une interface pour surveiller les performances du mod\u00e8le</li> <li> e) Toujours utiliser le GPU m\u00eame pour l'inf\u00e9rence simple</li> </ul>"},{"location":"seance3/ressources/quiz-frameworks/#questions-a-reponse-courte","title":"Questions \u00e0 r\u00e9ponse courte","text":""},{"location":"seance3/ressources/quiz-frameworks/#6-expliquez-en-2-3-phrases-comment-vous-pourriez-adapter-un-modele-pre-entraine-a-votre-propre-jeu-de-donnees","title":"6. Expliquez en 2-3 phrases comment vous pourriez adapter un mod\u00e8le pr\u00e9-entra\u00een\u00e9 \u00e0 votre propre jeu de donn\u00e9es.","text":"<p>[Votre r\u00e9ponse ici]</p>"},{"location":"seance3/ressources/quiz-frameworks/#7-quels-sont-les-avantages-dexposer-un-modele-de-ml-via-une-api-plutot-que-de-lintegrer-directement-dans-une-application","title":"7. Quels sont les avantages d'exposer un mod\u00e8le de ML via une API plut\u00f4t que de l'int\u00e9grer directement dans une application ?","text":"<p>[Votre r\u00e9ponse ici]</p>"},{"location":"seance3/ressources/quiz-frameworks/#8-citez-3-erreurs-courantes-a-eviter-lors-du-developpement-dune-application-basee-sur-le-deep-learning","title":"8. Citez 3 erreurs courantes \u00e0 \u00e9viter lors du d\u00e9veloppement d'une application bas\u00e9e sur le Deep Learning.","text":"<p>[Votre r\u00e9ponse ici]</p>"},{"location":"seance3/ressources/quiz-frameworks/#9-comment-expliqueriez-vous-a-un-client-non-technique-linteret-dutiliser-un-modele-pre-entraine-plutot-que-den-creer-un-de-zero","title":"9. Comment expliqueriez-vous \u00e0 un client non technique l'int\u00e9r\u00eat d'utiliser un mod\u00e8le pr\u00e9-entra\u00een\u00e9 plut\u00f4t que d'en cr\u00e9er un de z\u00e9ro ?","text":"<p>[Votre r\u00e9ponse ici]</p>"},{"location":"seance3/ressources/quiz-frameworks/#10-dans-le-contexte-dun-stage-en-bts-sio-comment-pourriez-vous-valoriser-vos-competences-en-deep-learning-aupres-dune-entreprise","title":"10. Dans le contexte d'un stage en BTS SIO, comment pourriez-vous valoriser vos comp\u00e9tences en Deep Learning aupr\u00e8s d'une entreprise ?","text":"<p>[Votre r\u00e9ponse ici]</p>"},{"location":"seance3/ressources/quiz-frameworks/#corrige-pour-lenseignant","title":"Corrig\u00e9 (pour l'enseignant)","text":""},{"location":"seance3/ressources/quiz-frameworks/#reponses-aux-qcm","title":"R\u00e9ponses aux QCM","text":"<ol> <li>b, d</li> <li>a, b, d</li> <li>a, b, c, e</li> <li>a, b, d</li> <li>b, c, d</li> </ol>"},{"location":"seance3/ressources/quiz-frameworks/#elements-de-reponse-aux-questions-ouvertes","title":"\u00c9l\u00e9ments de r\u00e9ponse aux questions ouvertes","text":"<ol> <li> <p>Pour adapter un mod\u00e8le pr\u00e9-entra\u00een\u00e9 : congeler les premi\u00e8res couches du mod\u00e8le qui contiennent les caract\u00e9ristiques g\u00e9n\u00e9rales, remplacer les derni\u00e8res couches par des couches adapt\u00e9es \u00e0 notre t\u00e2che sp\u00e9cifique, puis entra\u00eener uniquement ces nouvelles couches sur notre jeu de donn\u00e9es.</p> </li> <li> <p>Avantages d'une API : s\u00e9paration des pr\u00e9occupations (front/back), scalabilit\u00e9 ind\u00e9pendante, r\u00e9utilisation du mod\u00e8le par plusieurs applications, mise \u00e0 jour du mod\u00e8le sans toucher aux applications clientes, isolation des ressources intensives.</p> </li> <li> <p>Erreurs courantes : ne pas normaliser les donn\u00e9es d'entr\u00e9e, utiliser des architectures trop complexes pour le probl\u00e8me, ne pas diviser correctement les donn\u00e9es (train/validation/test), ne pas g\u00e9rer les cas d'erreur dans l'application, d\u00e9ployer sans surveillance des performances.</p> </li> <li> <p>Explication client : Utiliser un mod\u00e8le pr\u00e9-entra\u00een\u00e9, c'est comme acheter une voiture pr\u00eate \u00e0 rouler qu'on personnalise l\u00e9g\u00e8rement, plut\u00f4t que de construire une voiture enti\u00e8re \u00e0 partir de pi\u00e8ces d\u00e9tach\u00e9es. C'est plus rapide, moins co\u00fbteux, et souvent plus fiable car le mod\u00e8le a d\u00e9j\u00e0 \"appris\" sur des millions d'exemples.</p> </li> <li> <p>Valorisation en stage : montrer des exemples concrets d'applications d\u00e9velopp\u00e9es (API de reconnaissance d'images, chatbot), expliquer comment ces technologies peuvent r\u00e9soudre des probl\u00e8mes business sp\u00e9cifiques, pr\u00e9senter la capacit\u00e9 \u00e0 int\u00e9grer des solutions d'IA dans des applications existantes, mettre en avant la compr\u00e9hension des limites et possibilit\u00e9s r\u00e9elles de ces technologies.</p> </li> </ol>"},{"location":"seance3/ressources/tensorflow-keras-debutants/","title":"Tensorflow keras debutants","text":"In\u00a0[\u00a0]: Copied! <pre># TensorFlow/Keras pour d\u00e9butants\n# Notebook d'introduction \u00e0 TensorFlow pour applications simples\n\n# 1. Configuration et importation des biblioth\u00e8ques\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# V\u00e9rification de la version de TensorFlow\nprint(f\"TensorFlow version: {tf.__version__}\")\n\n# V\u00e9rification de la disponibilit\u00e9 du GPU (optionnel)\nprint(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))\n\n# 2. Jeu de donn\u00e9es Fashion MNIST (v\u00eatements)\n# Fashion MNIST est similaire \u00e0 MNIST mais avec des v\u00eatements au lieu de chiffres\n# C'est un bon jeu de donn\u00e9es pour commencer la vision par ordinateur\n\n# Charger les donn\u00e9es\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n\n# Noms des classes de v\u00eatements\nclass_names = ['T-shirt/haut', 'Pantalon', 'Pull', 'Robe', 'Manteau',\n               'Sandale', 'Chemise', 'Baskets', 'Sac', 'Bottine']\n\n# 3. Exploration des donn\u00e9es\nprint(f\"Forme des donn\u00e9es d'entra\u00eenement: {train_images.shape}\")\nprint(f\"Nombre d'images d'entra\u00eenement: {len(train_labels)}\")\nprint(f\"Forme des donn\u00e9es de test: {test_images.shape}\")\nprint(f\"Nombre d'images de test: {len(test_labels)}\")\n\n# Afficher quelques exemples d'images\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n    plt.xlabel(class_names[train_labels[i]])\nplt.show()\n\n# 4. Pr\u00e9traitement des donn\u00e9es\n# Normalisation des valeurs de pixels entre 0 et 1\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n\n# 5. Construction d'un mod\u00e8le simple\nmodel = models.Sequential([\n    layers.Flatten(input_shape=(28, 28)),      # Conversion de l'image 28x28 en un vecteur 1D de 784 valeurs\n    layers.Dense(128, activation='relu'),      # Couche cach\u00e9e avec 128 neurones et activation ReLU\n    layers.Dense(10, activation='softmax')     # Couche de sortie avec 10 neurones (un par classe) et softmax\n])\n\n# Afficher le r\u00e9sum\u00e9 du mod\u00e8le\nmodel.summary()\n\n# 6. Compilation du mod\u00e8le\nmodel.compile(\n    optimizer='adam',                          # Algorithme d'optimisation\n    loss='sparse_categorical_crossentropy',    # Fonction de perte pour la classification\n    metrics=['accuracy']                       # M\u00e9trique \u00e0 suivre pendant l'entra\u00eenement\n)\n\n# 7. Entra\u00eenement du mod\u00e8le\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=10,                 # Nombre de passages sur l'ensemble du jeu de donn\u00e9es\n    batch_size=32,             # Nombre d'\u00e9chantillons trait\u00e9s avant mise \u00e0 jour des poids\n    validation_split=0.2       # 20% des donn\u00e9es d'entra\u00eenement utilis\u00e9es pour la validation\n)\n\n# 8. Visualisation des courbes d'apprentissage\nplt.figure(figsize=(12, 4))\n\n# Courbe de pr\u00e9cision\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Pr\u00e9cision (entra\u00eenement)')\nplt.plot(history.history['val_accuracy'], label='Pr\u00e9cision (validation)')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Pr\u00e9cision')\nplt.legend()\nplt.title('\u00c9volution de la pr\u00e9cision')\n\n# Courbe de perte\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Perte (entra\u00eenement)')\nplt.plot(history.history['val_loss'], label='Perte (validation)')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Perte')\nplt.legend()\nplt.title('\u00c9volution de la fonction de perte')\n\nplt.tight_layout()\nplt.show()\n\n# 9. \u00c9valuation sur les donn\u00e9es de test\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\nprint(f\"\\nPr\u00e9cision sur les donn\u00e9es de test: {test_acc:.4f}\")\nprint(f\"Perte sur les donn\u00e9es de test: {test_loss:.4f}\")\n\n# 10. Faire des pr\u00e9dictions\n# Pr\u00e9dire la classe de quelques images de test\npredictions = model.predict(test_images[:5])\n\n# Afficher les 5 premi\u00e8res pr\u00e9dictions\nfor i in range(5):\n    plt.figure(figsize=(6, 3))\n    plt.subplot(1, 2, 1)\n    plt.imshow(test_images[i], cmap=plt.cm.binary)\n    plt.title(f\"Image: {class_names[test_labels[i]]}\")\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.barh(class_names, predictions[i])\n    plt.title('Probabilit\u00e9s pr\u00e9dites')\n    \n    predicted_class = np.argmax(predictions[i])\n    actual_class = test_labels[i]\n    \n    status = \"\u2713 Correct\" if predicted_class == actual_class else \"\u2717 Incorrect\"\n    plt.xlabel(f\"Pr\u00e9diction: {class_names[predicted_class]} ({status})\")\n    \n    plt.tight_layout()\n    plt.show()\n\n# 11. Sauvegarder le mod\u00e8le\nmodel.save('fashion_mnist_model.h5')\nprint(\"Mod\u00e8le sauvegard\u00e9 sous 'fashion_mnist_model.h5'\")\n\n# 12. Exercices pratiques\n#\n# \u00c0 FAIRE: Modifiez l'architecture du mod\u00e8le pour am\u00e9liorer ses performances\n# Par exemple, essayez d'ajouter des couches, changer le nombre de neurones,\n# ou ajouter une r\u00e9gularisation.\n#\n# Voici un exemple de d\u00e9part:\n#\n# model_improved = models.Sequential([\n#     layers.Flatten(input_shape=(28, 28)),\n#     layers.Dense(256, activation='relu'),\n#     layers.Dropout(0.2),                     # Ajout d'une couche Dropout pour r\u00e9duire le surapprentissage\n#     layers.Dense(128, activation='relu'),\n#     layers.Dense(10, activation='softmax')\n# ])\n#\n# model_improved.compile(optimizer='adam',\n#                       loss='sparse_categorical_crossentropy',\n#                       metrics=['accuracy'])\n#\n# # Entra\u00eener le mod\u00e8le am\u00e9lior\u00e9 et comparer les performances\n\n# 13. Passer \u00e0 des mod\u00e8les plus complexes\n#\n# Si vous avez le temps, essayez d'utiliser un mod\u00e8le CNN\n# qui est beaucoup plus adapt\u00e9 aux images:\n#\n# cnn_model = models.Sequential([\n#     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n#     layers.MaxPooling2D((2, 2)),\n#     layers.Conv2D(64, (3, 3), activation='relu'),\n#     layers.MaxPooling2D((2, 2)),\n#     layers.Flatten(),\n#     layers.Dense(128, activation='relu'),\n#     layers.Dense(10, activation='softmax')\n# ])\n#\n# # N'oubliez pas de redimensionner vos images pour le mod\u00e8le CNN\n# # train_images_reshaped = train_images.reshape(train_images.shape[0], 28, 28, 1)\n# # test_images_reshaped = test_images.reshape(test_images.shape[0], 28, 28, 1)\n</pre> # TensorFlow/Keras pour d\u00e9butants # Notebook d'introduction \u00e0 TensorFlow pour applications simples  # 1. Configuration et importation des biblioth\u00e8ques import tensorflow as tf from tensorflow.keras import layers, models import numpy as np import matplotlib.pyplot as plt import pandas as pd  # V\u00e9rification de la version de TensorFlow print(f\"TensorFlow version: {tf.__version__}\")  # V\u00e9rification de la disponibilit\u00e9 du GPU (optionnel) print(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))  # 2. Jeu de donn\u00e9es Fashion MNIST (v\u00eatements) # Fashion MNIST est similaire \u00e0 MNIST mais avec des v\u00eatements au lieu de chiffres # C'est un bon jeu de donn\u00e9es pour commencer la vision par ordinateur  # Charger les donn\u00e9es (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()  # Noms des classes de v\u00eatements class_names = ['T-shirt/haut', 'Pantalon', 'Pull', 'Robe', 'Manteau',                'Sandale', 'Chemise', 'Baskets', 'Sac', 'Bottine']  # 3. Exploration des donn\u00e9es print(f\"Forme des donn\u00e9es d'entra\u00eenement: {train_images.shape}\") print(f\"Nombre d'images d'entra\u00eenement: {len(train_labels)}\") print(f\"Forme des donn\u00e9es de test: {test_images.shape}\") print(f\"Nombre d'images de test: {len(test_labels)}\")  # Afficher quelques exemples d'images plt.figure(figsize=(10, 10)) for i in range(25):     plt.subplot(5, 5, i+1)     plt.xticks([])     plt.yticks([])     plt.grid(False)     plt.imshow(train_images[i], cmap=plt.cm.binary)     plt.xlabel(class_names[train_labels[i]]) plt.show()  # 4. Pr\u00e9traitement des donn\u00e9es # Normalisation des valeurs de pixels entre 0 et 1 train_images = train_images / 255.0 test_images = test_images / 255.0  # 5. Construction d'un mod\u00e8le simple model = models.Sequential([     layers.Flatten(input_shape=(28, 28)),      # Conversion de l'image 28x28 en un vecteur 1D de 784 valeurs     layers.Dense(128, activation='relu'),      # Couche cach\u00e9e avec 128 neurones et activation ReLU     layers.Dense(10, activation='softmax')     # Couche de sortie avec 10 neurones (un par classe) et softmax ])  # Afficher le r\u00e9sum\u00e9 du mod\u00e8le model.summary()  # 6. Compilation du mod\u00e8le model.compile(     optimizer='adam',                          # Algorithme d'optimisation     loss='sparse_categorical_crossentropy',    # Fonction de perte pour la classification     metrics=['accuracy']                       # M\u00e9trique \u00e0 suivre pendant l'entra\u00eenement )  # 7. Entra\u00eenement du mod\u00e8le history = model.fit(     train_images,      train_labels,      epochs=10,                 # Nombre de passages sur l'ensemble du jeu de donn\u00e9es     batch_size=32,             # Nombre d'\u00e9chantillons trait\u00e9s avant mise \u00e0 jour des poids     validation_split=0.2       # 20% des donn\u00e9es d'entra\u00eenement utilis\u00e9es pour la validation )  # 8. Visualisation des courbes d'apprentissage plt.figure(figsize=(12, 4))  # Courbe de pr\u00e9cision plt.subplot(1, 2, 1) plt.plot(history.history['accuracy'], label='Pr\u00e9cision (entra\u00eenement)') plt.plot(history.history['val_accuracy'], label='Pr\u00e9cision (validation)') plt.xlabel('\u00c9poque') plt.ylabel('Pr\u00e9cision') plt.legend() plt.title('\u00c9volution de la pr\u00e9cision')  # Courbe de perte plt.subplot(1, 2, 2) plt.plot(history.history['loss'], label='Perte (entra\u00eenement)') plt.plot(history.history['val_loss'], label='Perte (validation)') plt.xlabel('\u00c9poque') plt.ylabel('Perte') plt.legend() plt.title('\u00c9volution de la fonction de perte')  plt.tight_layout() plt.show()  # 9. \u00c9valuation sur les donn\u00e9es de test test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) print(f\"\\nPr\u00e9cision sur les donn\u00e9es de test: {test_acc:.4f}\") print(f\"Perte sur les donn\u00e9es de test: {test_loss:.4f}\")  # 10. Faire des pr\u00e9dictions # Pr\u00e9dire la classe de quelques images de test predictions = model.predict(test_images[:5])  # Afficher les 5 premi\u00e8res pr\u00e9dictions for i in range(5):     plt.figure(figsize=(6, 3))     plt.subplot(1, 2, 1)     plt.imshow(test_images[i], cmap=plt.cm.binary)     plt.title(f\"Image: {class_names[test_labels[i]]}\")     plt.axis('off')          plt.subplot(1, 2, 2)     plt.barh(class_names, predictions[i])     plt.title('Probabilit\u00e9s pr\u00e9dites')          predicted_class = np.argmax(predictions[i])     actual_class = test_labels[i]          status = \"\u2713 Correct\" if predicted_class == actual_class else \"\u2717 Incorrect\"     plt.xlabel(f\"Pr\u00e9diction: {class_names[predicted_class]} ({status})\")          plt.tight_layout()     plt.show()  # 11. Sauvegarder le mod\u00e8le model.save('fashion_mnist_model.h5') print(\"Mod\u00e8le sauvegard\u00e9 sous 'fashion_mnist_model.h5'\")  # 12. Exercices pratiques # # \u00c0 FAIRE: Modifiez l'architecture du mod\u00e8le pour am\u00e9liorer ses performances # Par exemple, essayez d'ajouter des couches, changer le nombre de neurones, # ou ajouter une r\u00e9gularisation. # # Voici un exemple de d\u00e9part: # # model_improved = models.Sequential([ #     layers.Flatten(input_shape=(28, 28)), #     layers.Dense(256, activation='relu'), #     layers.Dropout(0.2),                     # Ajout d'une couche Dropout pour r\u00e9duire le surapprentissage #     layers.Dense(128, activation='relu'), #     layers.Dense(10, activation='softmax') # ]) # # model_improved.compile(optimizer='adam', #                       loss='sparse_categorical_crossentropy', #                       metrics=['accuracy']) # # # Entra\u00eener le mod\u00e8le am\u00e9lior\u00e9 et comparer les performances  # 13. Passer \u00e0 des mod\u00e8les plus complexes # # Si vous avez le temps, essayez d'utiliser un mod\u00e8le CNN # qui est beaucoup plus adapt\u00e9 aux images: # # cnn_model = models.Sequential([ #     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)), #     layers.MaxPooling2D((2, 2)), #     layers.Conv2D(64, (3, 3), activation='relu'), #     layers.MaxPooling2D((2, 2)), #     layers.Flatten(), #     layers.Dense(128, activation='relu'), #     layers.Dense(10, activation='softmax') # ]) # # # N'oubliez pas de redimensionner vos images pour le mod\u00e8le CNN # # train_images_reshaped = train_images.reshape(train_images.shape[0], 28, 28, 1) # # test_images_reshaped = test_images.reshape(test_images.shape[0], 28, 28, 1) In\u00a0[\u00a0]: Copied! <pre># TensorFlow/Keras pour d\u00e9butants - BTS SIO \n# Notebook d'introduction \u00e0 TensorFlow pour applications simples\n\n# 1. Configuration et importation des biblioth\u00e8ques\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# V\u00e9rification de la version de TensorFlow\nprint(f\"TensorFlow version: {tf.__version__}\")\n\n# V\u00e9rification de la disponibilit\u00e9 du GPU (optionnel)\nprint(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))\n\n# 2. Jeu de donn\u00e9es Fashion MNIST (v\u00eatements)\n# Fashion MNIST est similaire \u00e0 MNIST mais avec des v\u00eatements au lieu de chiffres\n# C'est un bon jeu de donn\u00e9es pour commencer la vision par ordinateur\n\n# Charger les donn\u00e9es\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n\n# Noms des classes de v\u00eatements\nclass_names = ['T-shirt/haut', 'Pantalon', 'Pull', 'Robe', 'Manteau',\n               'Sandale', 'Chemise', 'Baskets', 'Sac', 'Bottine']\n\n# 3. Exploration des donn\u00e9es\nprint(f\"Forme des donn\u00e9es d'entra\u00eenement: {train_images.shape}\")\nprint(f\"Nombre d'images d'entra\u00eenement: {len(train_labels)}\")\nprint(f\"Forme des donn\u00e9es de test: {test_images.shape}\")\nprint(f\"Nombre d'images de test: {len(test_labels)}\")\n\n# Afficher quelques exemples d'images\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n    plt.xlabel(class_names[train_labels[i]])\nplt.show()\n\n# 4. Pr\u00e9traitement des donn\u00e9es\n# Normalisation des valeurs de pixels entre 0 et 1\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n\n# 5. Construction d'un mod\u00e8le simple\nmodel = models.Sequential([\n    layers.Flatten(input_shape=(28, 28)),      # Conversion de l'image 28x28 en un vecteur 1D de 784 valeurs\n    layers.Dense(128, activation='relu'),      # Couche cach\u00e9e avec 128 neurones et activation ReLU\n    layers.Dense(10, activation='softmax')     # Couche de sortie avec 10 neurones (un par classe) et softmax\n])\n\n# Afficher le r\u00e9sum\u00e9 du mod\u00e8le\nmodel.summary()\n\n# 6. Compilation du mod\u00e8le\nmodel.compile(\n    optimizer='adam',                          # Algorithme d'optimisation\n    loss='sparse_categorical_crossentropy',    # Fonction de perte pour la classification\n    metrics=['accuracy']                       # M\u00e9trique \u00e0 suivre pendant l'entra\u00eenement\n)\n\n# 7. Entra\u00eenement du mod\u00e8le\nhistory = model.fit(\n    train_images, \n    train_labels, \n    epochs=10,                 # Nombre de passages sur l'ensemble du jeu de donn\u00e9es\n    batch_size=32,             # Nombre d'\u00e9chantillons trait\u00e9s avant mise \u00e0 jour des poids\n    validation_split=0.2       # 20% des donn\u00e9es d'entra\u00eenement utilis\u00e9es pour la validation\n)\n\n# 8. Visualisation des courbes d'apprentissage\nplt.figure(figsize=(12, 4))\n\n# Courbe de pr\u00e9cision\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Pr\u00e9cision (entra\u00eenement)')\nplt.plot(history.history['val_accuracy'], label='Pr\u00e9cision (validation)')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Pr\u00e9cision')\nplt.legend()\nplt.title('\u00c9volution de la pr\u00e9cision')\n\n# Courbe de perte\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Perte (entra\u00eenement)')\nplt.plot(history.history['val_loss'], label='Perte (validation)')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Perte')\nplt.legend()\nplt.title('\u00c9volution de la fonction de perte')\n\nplt.tight_layout()\nplt.show()\n\n# 9. \u00c9valuation sur les donn\u00e9es de test\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\nprint(f\"\\nPr\u00e9cision sur les donn\u00e9es de test: {test_acc:.4f}\")\nprint(f\"Perte sur les donn\u00e9es de test: {test_loss:.4f}\")\n\n# 10. Faire des pr\u00e9dictions\n# Pr\u00e9dire la classe de quelques images de test\npredictions = model.predict(test_images[:5])\n\n# Afficher les 5 premi\u00e8res pr\u00e9dictions\nfor i in range(5):\n    plt.figure(figsize=(6, 3))\n    plt.subplot(1, 2, 1)\n    plt.imshow(test_images[i], cmap=plt.cm.binary)\n    plt.title(f\"Image: {class_names[test_labels[i]]}\")\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.barh(class_names, predictions[i])\n    plt.title('Probabilit\u00e9s pr\u00e9dites')\n    \n    predicted_class = np.argmax(predictions[i])\n    actual_class = test_labels[i]\n    \n    status = \"\u2713 Correct\" if predicted_class == actual_class else \"\u2717 Incorrect\"\n    plt.xlabel(f\"Pr\u00e9diction: {class_names[predicted_class]} ({status})\")\n    \n    plt.tight_layout()\n    plt.show()\n\n# 11. Sauvegarder le mod\u00e8le\nmodel.save('fashion_mnist_model.h5')\nprint(\"Mod\u00e8le sauvegard\u00e9 sous 'fashion_mnist_model.h5'\")\n\n# 12. Exercices pratiques\n#\n# \u00c0 FAIRE: Modifiez l'architecture du mod\u00e8le pour am\u00e9liorer ses performances\n# Par exemple, essayez d'ajouter des couches, changer le nombre de neurones,\n# ou ajouter une r\u00e9gularisation.\n#\n# Voici un exemple de d\u00e9part:\n#\n# model_improved = models.Sequential([\n#     layers.Flatten(input_shape=(28, 28)),\n#     layers.Dense(256, activation='relu'),\n#     layers.Dropout(0.2),                     # Ajout d'une couche Dropout pour r\u00e9duire le surapprentissage\n#     layers.Dense(128, activation='relu'),\n#     layers.Dense(10, activation='softmax')\n# ])\n#\n# model_improved.compile(optimizer='adam',\n#                       loss='sparse_categorical_crossentropy',\n#                       metrics=['accuracy'])\n#\n# # Entra\u00eener le mod\u00e8le am\u00e9lior\u00e9 et comparer les performances\n\n# 13. Passer \u00e0 des mod\u00e8les plus complexes\n#\n# Si vous avez le temps, essayez d'utiliser un mod\u00e8le CNN\n# qui est beaucoup plus adapt\u00e9 aux images:\n#\n# cnn_model = models.Sequential([\n#     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n#     layers.MaxPooling2D((2, 2)),\n#     layers.Conv2D(64, (3, 3), activation='relu'),\n#     layers.MaxPooling2D((2, 2)),\n#     layers.Flatten(),\n#     layers.Dense(128, activation='relu'),\n#     layers.Dense(10, activation='softmax')\n# ])\n#\n# # N'oubliez pas de redimensionner vos images pour le mod\u00e8le CNN\n# # train_images_reshaped = train_images.reshape(train_images.shape[0], 28, 28, 1)\n# # test_images_reshaped = test_images.reshape(test_images.shape[0], 28, 28, 1)\n</pre> # TensorFlow/Keras pour d\u00e9butants - BTS SIO  # Notebook d'introduction \u00e0 TensorFlow pour applications simples  # 1. Configuration et importation des biblioth\u00e8ques import tensorflow as tf from tensorflow.keras import layers, models import numpy as np import matplotlib.pyplot as plt import pandas as pd  # V\u00e9rification de la version de TensorFlow print(f\"TensorFlow version: {tf.__version__}\")  # V\u00e9rification de la disponibilit\u00e9 du GPU (optionnel) print(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))  # 2. Jeu de donn\u00e9es Fashion MNIST (v\u00eatements) # Fashion MNIST est similaire \u00e0 MNIST mais avec des v\u00eatements au lieu de chiffres # C'est un bon jeu de donn\u00e9es pour commencer la vision par ordinateur  # Charger les donn\u00e9es (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()  # Noms des classes de v\u00eatements class_names = ['T-shirt/haut', 'Pantalon', 'Pull', 'Robe', 'Manteau',                'Sandale', 'Chemise', 'Baskets', 'Sac', 'Bottine']  # 3. Exploration des donn\u00e9es print(f\"Forme des donn\u00e9es d'entra\u00eenement: {train_images.shape}\") print(f\"Nombre d'images d'entra\u00eenement: {len(train_labels)}\") print(f\"Forme des donn\u00e9es de test: {test_images.shape}\") print(f\"Nombre d'images de test: {len(test_labels)}\")  # Afficher quelques exemples d'images plt.figure(figsize=(10, 10)) for i in range(25):     plt.subplot(5, 5, i+1)     plt.xticks([])     plt.yticks([])     plt.grid(False)     plt.imshow(train_images[i], cmap=plt.cm.binary)     plt.xlabel(class_names[train_labels[i]]) plt.show()  # 4. Pr\u00e9traitement des donn\u00e9es # Normalisation des valeurs de pixels entre 0 et 1 train_images = train_images / 255.0 test_images = test_images / 255.0  # 5. Construction d'un mod\u00e8le simple model = models.Sequential([     layers.Flatten(input_shape=(28, 28)),      # Conversion de l'image 28x28 en un vecteur 1D de 784 valeurs     layers.Dense(128, activation='relu'),      # Couche cach\u00e9e avec 128 neurones et activation ReLU     layers.Dense(10, activation='softmax')     # Couche de sortie avec 10 neurones (un par classe) et softmax ])  # Afficher le r\u00e9sum\u00e9 du mod\u00e8le model.summary()  # 6. Compilation du mod\u00e8le model.compile(     optimizer='adam',                          # Algorithme d'optimisation     loss='sparse_categorical_crossentropy',    # Fonction de perte pour la classification     metrics=['accuracy']                       # M\u00e9trique \u00e0 suivre pendant l'entra\u00eenement )  # 7. Entra\u00eenement du mod\u00e8le history = model.fit(     train_images,      train_labels,      epochs=10,                 # Nombre de passages sur l'ensemble du jeu de donn\u00e9es     batch_size=32,             # Nombre d'\u00e9chantillons trait\u00e9s avant mise \u00e0 jour des poids     validation_split=0.2       # 20% des donn\u00e9es d'entra\u00eenement utilis\u00e9es pour la validation )  # 8. Visualisation des courbes d'apprentissage plt.figure(figsize=(12, 4))  # Courbe de pr\u00e9cision plt.subplot(1, 2, 1) plt.plot(history.history['accuracy'], label='Pr\u00e9cision (entra\u00eenement)') plt.plot(history.history['val_accuracy'], label='Pr\u00e9cision (validation)') plt.xlabel('\u00c9poque') plt.ylabel('Pr\u00e9cision') plt.legend() plt.title('\u00c9volution de la pr\u00e9cision')  # Courbe de perte plt.subplot(1, 2, 2) plt.plot(history.history['loss'], label='Perte (entra\u00eenement)') plt.plot(history.history['val_loss'], label='Perte (validation)') plt.xlabel('\u00c9poque') plt.ylabel('Perte') plt.legend() plt.title('\u00c9volution de la fonction de perte')  plt.tight_layout() plt.show()  # 9. \u00c9valuation sur les donn\u00e9es de test test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) print(f\"\\nPr\u00e9cision sur les donn\u00e9es de test: {test_acc:.4f}\") print(f\"Perte sur les donn\u00e9es de test: {test_loss:.4f}\")  # 10. Faire des pr\u00e9dictions # Pr\u00e9dire la classe de quelques images de test predictions = model.predict(test_images[:5])  # Afficher les 5 premi\u00e8res pr\u00e9dictions for i in range(5):     plt.figure(figsize=(6, 3))     plt.subplot(1, 2, 1)     plt.imshow(test_images[i], cmap=plt.cm.binary)     plt.title(f\"Image: {class_names[test_labels[i]]}\")     plt.axis('off')          plt.subplot(1, 2, 2)     plt.barh(class_names, predictions[i])     plt.title('Probabilit\u00e9s pr\u00e9dites')          predicted_class = np.argmax(predictions[i])     actual_class = test_labels[i]          status = \"\u2713 Correct\" if predicted_class == actual_class else \"\u2717 Incorrect\"     plt.xlabel(f\"Pr\u00e9diction: {class_names[predicted_class]} ({status})\")          plt.tight_layout()     plt.show()  # 11. Sauvegarder le mod\u00e8le model.save('fashion_mnist_model.h5') print(\"Mod\u00e8le sauvegard\u00e9 sous 'fashion_mnist_model.h5'\")  # 12. Exercices pratiques # # \u00c0 FAIRE: Modifiez l'architecture du mod\u00e8le pour am\u00e9liorer ses performances # Par exemple, essayez d'ajouter des couches, changer le nombre de neurones, # ou ajouter une r\u00e9gularisation. # # Voici un exemple de d\u00e9part: # # model_improved = models.Sequential([ #     layers.Flatten(input_shape=(28, 28)), #     layers.Dense(256, activation='relu'), #     layers.Dropout(0.2),                     # Ajout d'une couche Dropout pour r\u00e9duire le surapprentissage #     layers.Dense(128, activation='relu'), #     layers.Dense(10, activation='softmax') # ]) # # model_improved.compile(optimizer='adam', #                       loss='sparse_categorical_crossentropy', #                       metrics=['accuracy']) # # # Entra\u00eener le mod\u00e8le am\u00e9lior\u00e9 et comparer les performances  # 13. Passer \u00e0 des mod\u00e8les plus complexes # # Si vous avez le temps, essayez d'utiliser un mod\u00e8le CNN # qui est beaucoup plus adapt\u00e9 aux images: # # cnn_model = models.Sequential([ #     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)), #     layers.MaxPooling2D((2, 2)), #     layers.Conv2D(64, (3, 3), activation='relu'), #     layers.MaxPooling2D((2, 2)), #     layers.Flatten(), #     layers.Dense(128, activation='relu'), #     layers.Dense(10, activation='softmax') # ]) # # # N'oubliez pas de redimensionner vos images pour le mod\u00e8le CNN # # train_images_reshaped = train_images.reshape(train_images.shape[0], 28, 28, 1) # # test_images_reshaped = test_images.reshape(test_images.shape[0], 28, 28, 1)"},{"location":"seance4/","title":"S\u00e9ance 4 : Projet int\u00e9grateur - Chatbot p\u00e9dagogique","text":""},{"location":"seance4/#objectifs-de-la-seance","title":"Objectifs de la s\u00e9ance","text":"<p>Cette derni\u00e8re s\u00e9ance vous permettra de :</p> <ul> <li>Appliquer l'ensemble des connaissances acquises dans un projet concret et complet</li> <li>D\u00e9velopper un chatbot p\u00e9dagogique fonctionnel expliquant le Deep Learning</li> <li>Int\u00e9grer l'API Mistral AI dans une solution compl\u00e8te</li> <li>Pr\u00e9senter et d\u00e9fendre votre solution technique</li> </ul>"},{"location":"seance4/#vision-du-projet","title":"Vision du projet","text":"<p>Le projet consiste \u00e0 d\u00e9velopper un assistant virtuel conversationnel capable d'expliquer les concepts du Deep Learning, de r\u00e9pondre aux questions techniques et d'accompagner les apprenants dans leur d\u00e9couverte de ce domaine.</p> <p>\ud83c\udfaf Objectif : Concevoir un chatbot interactif qui aide les \u00e9tudiants de BTS SIO \u00e0 comprendre les concepts du Deep Learning \u00e0 travers des explications personnalis\u00e9es, des exemples concrets et des exercices adapt\u00e9s.</p>"},{"location":"seance4/#architecture-technique","title":"Architecture technique","text":"<p>Le chatbot s'appuiera sur une architecture moderne compos\u00e9e de trois \u00e9l\u00e9ments principaux :</p> <pre><code>flowchart LR\n    A[Interface Web] &lt;--&gt; B[Backend Python]\n    B &lt;--&gt; C[API Mistral AI]\n    D[Base de connaissances] &lt;--&gt; B</code></pre>"},{"location":"seance4/#1-interface-conversationnelle","title":"1. Interface conversationnelle","text":"<ul> <li>Interface web simple et intuitive</li> <li>Affichage des messages en format discussion</li> <li>Indicateur de chargement pendant le traitement</li> <li>Historique de conversation</li> </ul>"},{"location":"seance4/#2-backend-flaskfastapi","title":"2. Backend Flask/FastAPI","text":"<ul> <li>Gestion des requ\u00eates et des sessions</li> <li>Enrichissement des prompts avec la base de connaissances</li> <li>Communication avec l'API Mistral</li> <li>Logique de traitement des r\u00e9ponses</li> </ul>"},{"location":"seance4/#3-integration-api-mistral-ai","title":"3. Int\u00e9gration API Mistral AI","text":"<ul> <li>Configuration et param\u00e8trage des requ\u00eates</li> <li>Gestion du contexte de conversation</li> <li>Optimisation des prompts</li> <li>Gestion des erreurs et limitations</li> </ul>"},{"location":"seance4/#4-base-de-connaissances","title":"4. Base de connaissances","text":"<ul> <li>Structure JSON organis\u00e9e par concepts</li> <li>Exercices et quiz par th\u00e9matique</li> </ul>"},{"location":"seance4/#fonctionnalites-cles","title":"Fonctionnalit\u00e9s cl\u00e9s","text":"<p>Le chatbot p\u00e9dagogique offrira les fonctionnalit\u00e9s suivantes :</p> <ol> <li> <p>Explication des concepts</p> <ul> <li>D\u00e9finition adapt\u00e9e au niveau de l'utilisateur</li> <li>Exemples concrets pour illustrer chaque notion</li> <li>Analogies et comparaisons pour faciliter la compr\u00e9hension</li> </ul> </li> <li> <p>R\u00e9ponse aux questions</p> <ul> <li>Compr\u00e9hension des questions techniques</li> <li>R\u00e9ponses pr\u00e9cises bas\u00e9es sur la base de connaissances</li> <li>Capacit\u00e9 \u00e0 demander des clarifications si n\u00e9cessaire</li> </ul> </li> <li> <p>Progression adaptative</p> <ul> <li>D\u00e9tection du niveau de l'utilisateur</li> <li>Suggestions de concepts \u00e0 explorer ensuite</li> <li>Augmentation progressive de la complexit\u00e9</li> </ul> </li> <li> <p>Exercices interactifs</p> <ul> <li>G\u00e9n\u00e9ration de quiz sur les concepts vus</li> <li>Probl\u00e8mes simples \u00e0 r\u00e9soudre</li> <li>Feedback sur les r\u00e9ponses</li> </ul> </li> </ol>"},{"location":"seance4/#approche-pedagogique","title":"Approche p\u00e9dagogique","text":"<p>Cette s\u00e9ance est enti\u00e8rement bas\u00e9e sur la r\u00e9alisation d'un projet concret en \u00e9quipe. Vous devrez mobiliser toutes les comp\u00e9tences d\u00e9velopp\u00e9es lors des s\u00e9ances pr\u00e9c\u00e9dentes pour cr\u00e9er une application compl\u00e8te et fonctionnelle. L'accent est mis sur l'autonomie, la collaboration et la mise en pratique professionnelle.</p>"},{"location":"seance4/#structure-de-la-seance-4h","title":"Structure de la s\u00e9ance (4h)","text":"<pre><code>D\u00e9veloppement du chatbot       : 2h30m \nFinalisation et tests          : 1h    \nPr\u00e9sentation des projets       : 30m   \n</code></pre>"},{"location":"seance4/#trois-phases-de-realisation","title":"Trois phases de r\u00e9alisation","text":""},{"location":"seance4/#phase-1-developpement-du-chatbot-2h30","title":"Phase 1 : D\u00e9veloppement du chatbot (2h30)","text":"<p>Impl\u00e9mentez les fonctionnalit\u00e9s principales de votre chatbot p\u00e9dagogique :</p> <ul> <li>Mise en place de l'interface conversationnelle</li> <li>Int\u00e9gration avanc\u00e9e avec l'API Mistral AI</li> <li>Structuration et enrichissement de la base de connaissances</li> <li>D\u00e9veloppement des fonctionnalit\u00e9s d'aide \u00e0 l'apprentissage</li> </ul>"},{"location":"seance4/#phase-2-finalisation-et-tests-1h","title":"Phase 2 : Finalisation et tests (1h)","text":"<p>Peaufinez votre solution et assurez-vous de sa qualit\u00e9 :</p> <ul> <li>Tests fonctionnels et sc\u00e9narios d'utilisation</li> <li>Optimisation des performances</li> <li>Documentation technique et guide utilisateur</li> <li>Pr\u00e9paration de la d\u00e9monstration</li> </ul>"},{"location":"seance4/#phase-3-presentation-des-projets-30min","title":"Phase 3 : Pr\u00e9sentation des projets (30min)","text":"<p>Pr\u00e9sentez votre solution \u00e0 la classe :</p> <ul> <li>D\u00e9monstration en direct du chatbot</li> <li>Explication des choix techniques</li> <li>Retour sur les d\u00e9fis rencontr\u00e9s et les solutions adopt\u00e9es</li> <li>Questions-r\u00e9ponses</li> </ul>"},{"location":"seance4/#defis-techniques","title":"D\u00e9fis techniques","text":"<p>Les principaux d\u00e9fis \u00e0 relever seront :</p> <ol> <li>Prompt engineering efficace</li> <li>Formuler des instructions claires pour l'API Mistral</li> <li>Maintenir la coh\u00e9rence p\u00e9dagogique dans les r\u00e9ponses</li> <li> <p>\u00c9viter les hallucinations du mod\u00e8le</p> </li> <li> <p>Int\u00e9gration technique</p> </li> <li>Communication fluide entre frontend et backend</li> <li>Gestion asynchrone des requ\u00eates API</li> <li> <p>Optimisation des temps de r\u00e9ponse</p> </li> <li> <p>Qualit\u00e9 p\u00e9dagogique</p> </li> <li>Structure coh\u00e9rente de la base de connaissances</li> <li>Adaptation au niveau de l'utilisateur</li> <li>Progression logique entre les concepts</li> </ol>"},{"location":"seance4/#ressources-necessaires","title":"Ressources n\u00e9cessaires","text":"<p>Pour cette s\u00e9ance, vous aurez besoin de :</p> <ul> <li>Votre document de conception pr\u00e9par\u00e9 lors de la s\u00e9ance 3</li> <li>Compte et cl\u00e9 API Mistral AI</li> <li>Environnement de d\u00e9veloppement (Google Colab ou local)</li> <li>Templates fournis pour la documentation</li> </ul> <p>Ressources fournies : - Documentation compl\u00e8te de l'API Mistral - Structure JSON pour la base de connaissances - Templates de code pour l'interface et le backend - Exemples de prompts efficaces</p>"},{"location":"seance4/#livrables-attendus","title":"Livrables attendus","text":"<p>\u00c0 l'issue de cette s\u00e9ance, vous devrez remettre :</p> <ol> <li>Code source complet du chatbot p\u00e9dagogique</li> <li>Base de connaissances structur\u00e9e sur le Deep Learning</li> <li>Documentation technique expliquant l'architecture et les choix d'impl\u00e9mentation</li> <li>Guide utilisateur pour la prise en main</li> <li>Pr\u00e9sentation avec support \u00e0 fournir</li> </ol> <p>Ces livrables constituent l'aboutissement de votre parcours et seront \u00e9valu\u00e9s selon les crit\u00e8res d\u00e9taill\u00e9s dans la grille d'\u00e9valuation.</p>"},{"location":"seance4/#pret-a-relever-le-defi","title":"Pr\u00eat \u00e0 relever le d\u00e9fi ?","text":"<p>C'est l'heure de mettre en pratique tout ce que vous avez appris pour cr\u00e9er un outil r\u00e9ellement utile. Bonne chance !</p> <p>Commencer la Phase 1</p>"},{"location":"seance4/partie1-developpement/","title":"Phase 1 : D\u00e9veloppement du chatbot (2h30)","text":""},{"location":"seance4/partie1-developpement/#objectif","title":"Objectif","text":"<p>Impl\u00e9menter les fonctionnalit\u00e9s principales de votre chatbot p\u00e9dagogique en vous basant sur votre document de conception et en int\u00e9grant les connaissances acquises lors des s\u00e9ances pr\u00e9c\u00e9dentes.</p>"},{"location":"seance4/partie1-developpement/#fonctionnalites-a-implementer","title":"Fonctionnalit\u00e9s \u00e0 impl\u00e9menter","text":""},{"location":"seance4/partie1-developpement/#1-interface-conversationnelle-30-min","title":"1. Interface conversationnelle (30 min)","text":"<p>L'interface du chatbot doit \u00eatre simple mais fonctionnelle. Elle comprendra :</p> <ul> <li>Une zone d'affichage des messages</li> <li>Un champ de saisie pour les questions</li> <li>Un bouton d'envoi</li> <li>Une indication de chargement pendant le traitement</li> <li>Un syst\u00e8me d'historique de conversation</li> </ul> <p>Mod\u00e8le de code pour l'interface web <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Chatbot p\u00e9dagogique - Deep Learning&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"styles.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"chat-container\"&gt;\n        &lt;div class=\"chat-header\"&gt;\n            &lt;h1&gt;Chatbot Deep Learning&lt;/h1&gt;\n        &lt;/div&gt;\n        &lt;div class=\"chat-messages\" id=\"chat-messages\"&gt;\n            &lt;!-- Les messages s'afficheront ici --&gt;\n            &lt;div class=\"message bot\"&gt;\n                &lt;div class=\"message-content\"&gt;\n                    Bonjour ! Je suis un chatbot sp\u00e9cialis\u00e9 dans le Deep Learning. \n                    Comment puis-je vous aider aujourd'hui ?\n                &lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div class=\"chat-input\"&gt;\n            &lt;input type=\"text\" id=\"user-input\" placeholder=\"Posez votre question ici...\"&gt;\n            &lt;button id=\"send-button\"&gt;Envoyer&lt;/button&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script src=\"script.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Points cl\u00e9s \u00e0 respecter :  - Design responsive s'adaptant aux diff\u00e9rentes tailles d'\u00e9cran  - Indication claire des messages utilisateur vs assistant  - Gestion des erreurs (r\u00e9seau, API, etc.)</p>"},{"location":"seance4/partie1-developpement/#2-integration-avancee-avec-lapi-mistral-ai-45-min","title":"2. Int\u00e9gration avanc\u00e9e avec l'API Mistral AI (45 min)","text":"<p>L'objectif est d'exploiter efficacement l'API Mistral AI pour g\u00e9n\u00e9rer des r\u00e9ponses pertinentes et contextualis\u00e9es.</p> <p>Structure d'int\u00e9gration recommand\u00e9e :</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\n# Chargement des variables d'environnement\nload_dotenv()\napi_key = os.getenv(\"MISTRAL_API_KEY\")\n\n# Initialisation du client\nclient = MistralClient(api_key=api_key)\n\n# Syst\u00e8me de gestion de contexte\nclass ConversationManager:\n    def __init__(self, system_prompt):\n        self.history = [\n            ChatMessage(role=\"system\", content=system_prompt)\n        ]\n\n    def add_user_message(self, message):\n        self.history.append(ChatMessage(role=\"user\", content=message))\n\n    def add_assistant_message(self, message):\n        self.history.append(ChatMessage(role=\"assistant\", content=message))\n\n    def get_response(self, model=\"mistral-medium\", temperature=0.7):\n        response = client.chat(\n            model=model,\n            messages=self.history,\n            temperature=temperature\n        )\n        content = response.choices[0].message.content\n        self.add_assistant_message(content)\n        return content\n\n    def get_history(self):\n        # Exclure le message syst\u00e8me pour l'affichage\n        return self.history[1:]\n\n# Exemple d'utilisation\nsystem_prompt = \"\"\"\nTu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans le Deep Learning, con\u00e7u pour aider \nles \u00e9tudiants de BTS SIO. Tu expliques les concepts de mani\u00e8re claire et progressive, \nen adaptant ton niveau de technicit\u00e9 au niveau de l'\u00e9tudiant. Utilise des analogies \net des exemples concrets quand c'est pertinent.\n\"\"\"\n\nconversation = ConversationManager(system_prompt)\n</code></pre> <p>Aspects avanc\u00e9s \u00e0 impl\u00e9menter :</p> <ul> <li>Prompt engineering : Am\u00e9lioration des instructions syst\u00e8me pour obtenir des r\u00e9ponses optimales</li> <li>Gestion de contexte : Pr\u00e9servation de l'historique pour maintenir la coh\u00e9rence des conversations</li> <li>Param\u00e8tres ajustables : Contr\u00f4le de la temp\u00e9rature pour moduler la cr\u00e9ativit\u00e9 des r\u00e9ponses</li> <li>Contr\u00f4le de longueur : Limiter la longueur des r\u00e9ponses pour des explications concises</li> </ul>"},{"location":"seance4/partie1-developpement/#3-structuration-de-la-base-de-connaissances-45-min","title":"3. Structuration de la base de connaissances (45 min)","text":"<p>La base de connaissances est le c\u0153ur de votre chatbot. Elle doit \u00eatre structur\u00e9e de mani\u00e8re logique et couvrir les concepts essentiels du Deep Learning, correspondant au programme que vous avez suivi.</p> <p>Structure recommand\u00e9e (format JSON) :</p> <pre><code>{\n  \"topics\": [\n    {\n      \"id\": \"intro_dl\",\n      \"title\": \"Introduction au Deep Learning\",\n      \"subtopics\": [\n        {\n          \"id\": \"diff_ml_dl\",\n          \"title\": \"Diff\u00e9rence entre Machine Learning et Deep Learning\",\n          \"content\": \"Le Machine Learning classique n\u00e9cessite une extraction manuelle des caract\u00e9ristiques (feature engineering) tandis que le Deep Learning les extrait automatiquement gr\u00e2ce \u00e0 ses multiples couches...\",\n          \"examples\": [\n            \"Dans la reconnaissance d'images, le ML classique n\u00e9cessite d'extraire manuellement des caract\u00e9ristiques comme les contours, les textures, alors que le DL apprend directement ces caract\u00e9ristiques.\",\n            \"Pour la classification de texte, le ML classique utilise des approches comme TF-IDF ou Bag-of-Words, alors que le DL utilise des embeddings et des architectures comme LSTM.\"\n          ],\n          \"related\": [\"neural_networks\", \"applications_dl\"]\n        },\n        // Autres sous-topics...\n      ]\n    },\n    // Autres topics principaux...\n  ]\n}\n</code></pre> <p>\u00c9l\u00e9ments \u00e0 inclure :      - Concepts fondamentaux du Deep Learning      - Types de r\u00e9seaux (CNN, RNN, etc.)      - Techniques d'entra\u00eenement et d'optimisation      - Applications pratiques      - Exemples de code simplifi\u00e9s      - Analogies pour faciliter la compr\u00e9hension</p> <p>La base de connaissances peut \u00eatre utilis\u00e9e pour enrichir les prompts envoy\u00e9s \u00e0 l'API ou pour offrir des r\u00e9ponses pr\u00e9d\u00e9finies \u00e0 certaines questions.</p>"},{"location":"seance4/partie1-developpement/#4-fonctionnalites-daide-a-lapprentissage-30-min","title":"4. Fonctionnalit\u00e9s d'aide \u00e0 l'apprentissage (30 min)","text":"<p>Pour rendre votre chatbot v\u00e9ritablement p\u00e9dagogique, impl\u00e9mentez au moins deux des fonctionnalit\u00e9s suivantes :</p> <ol> <li> <p>G\u00e9n\u00e9ration de quiz : Cr\u00e9er des QCM pour tester les connaissances de l'utilisateur    <pre><code>def generate_quiz(topic):\n    # Exemple de structure\n    questions = {\n        \"intro_dl\": [\n            {\n                \"question\": \"Quelle est la principale diff\u00e9rence entre Machine Learning classique et Deep Learning ?\",\n                \"options\": [\n                    \"Le Deep Learning est toujours plus rapide\",\n                    \"Le Deep Learning extrait automatiquement les caract\u00e9ristiques pertinentes\",\n                    \"Le Deep Learning utilise exclusivement des GPUs\",\n                    \"Le Deep Learning n\u00e9cessite moins de donn\u00e9es\"\n                ],\n                \"correct\": 1,\n                \"explanation\": \"Le Deep Learning se distingue par sa capacit\u00e9 \u00e0 extraire automatiquement des caract\u00e9ristiques pertinentes gr\u00e2ce \u00e0 ses multiples couches, \u00e9liminant le besoin d'extraction manuelle (feature engineering).\"\n            },\n            # Autres questions...\n        ]\n    }\n    return questions.get(topic, [])\n</code></pre></p> </li> <li> <p>Syst\u00e8me de progression : Suivre le niveau de l'utilisateur et adapter le contenu    <pre><code>class LearnerProfile:\n    def __init__(self, user_id):\n        self.user_id = user_id\n        self.topics_seen = set()\n        self.quiz_scores = {}\n        self.current_level = \"beginner\"  # beginner, intermediate, advanced\n\n    def update_after_interaction(self, topic, subtopic):\n        self.topics_seen.add(f\"{topic}:{subtopic}\")\n        # Mise \u00e0 jour du niveau selon le nombre de topics vus\n        if len(self.topics_seen) &gt; 10:\n            self.current_level = \"intermediate\"\n        if len(self.topics_seen) &gt; 20:\n            self.current_level = \"advanced\"\n\n    def record_quiz_result(self, topic, score):\n        self.quiz_scores[topic] = score\n</code></pre></p> </li> <li> <p>Visualisations adaptatives : G\u00e9n\u00e9rer des sch\u00e9mas explicatifs selon le niveau    <pre><code>def get_visualization(concept, level):\n    visualizations = {\n        \"neural_network\": {\n            \"beginner\": \"simple_nn.svg\",\n            \"intermediate\": \"detailed_nn.svg\",\n            \"advanced\": \"complex_nn.svg\"\n        },\n        # Autres concepts...\n    }\n    return visualizations.get(concept, {}).get(level, \"default.svg\")\n</code></pre></p> </li> <li> <p>Suivi des mots-cl\u00e9s : Assistant remontant les d\u00e9finitions des termes techniques    <pre><code>def extract_technical_terms(message):\n    technical_terms = [\n        \"neurone\", \"couche\", \"poids\", \"biais\", \"fonction d'activation\",\n        \"r\u00e9tropropagation\", \"descente de gradient\", \"CNN\", \"RNN\", \"LSTM\"\n    ]\n    found_terms = []\n    for term in technical_terms:\n        if term.lower() in message.lower():\n            found_terms.append(term)\n    return found_terms\n</code></pre></p> </li> </ol>"},{"location":"seance4/partie1-developpement/#travail-dequipe-et-repartition-des-taches","title":"Travail d'\u00e9quipe et r\u00e9partition des t\u00e2ches","text":"<p>Si vous travaillez en bin\u00f4me, r\u00e9partissez-vous les t\u00e2ches efficacement :</p> <p>Suggestion de r\u00e9partition :        - Membre 1 : Interface + Int\u00e9gration API        - Membre 2 : Base de connaissances + Fonctionnalit\u00e9s p\u00e9dagogiques</p> <p>Ou alternativement :        - Membre 1 : Backend (API, logique, base de connaissances)        - Membre 2 : Frontend (interface, interactions, exp\u00e9rience utilisateur)</p>"},{"location":"seance4/partie1-developpement/#points-de-vigilance","title":"Points de vigilance","text":"<ul> <li>S\u00e9curit\u00e9 : Ne stockez jamais votre cl\u00e9 API directement dans le code</li> <li>R\u00e9activit\u00e9 : Optimisez les temps de r\u00e9ponse, ajoutez des indicateurs de chargement</li> <li>Robustesse : G\u00e9rez les erreurs (API indisponible, limite de requ\u00eates atteinte, etc.)</li> <li>Qualit\u00e9 des r\u00e9ponses : Testez r\u00e9guli\u00e8rement avec des questions vari\u00e9es pour v\u00e9rifier la pertinence</li> </ul>"},{"location":"seance4/partie1-developpement/#livrables-intermediaires","title":"Livrables interm\u00e9diaires","text":"<p>\u00c0 la fin de cette phase, vous devriez avoir :</p> <ul> <li>Une interface conversationnelle fonctionnelle</li> <li>Un syst\u00e8me d'int\u00e9gration avec l'API Mistral AI</li> <li>Une base de connaissances structur\u00e9e</li> <li>Au moins deux fonctionnalit\u00e9s p\u00e9dagogiques impl\u00e9ment\u00e9es</li> </ul> <p>Retour \u00e0 la vue d'ensemble Continuer vers la Phase 2: Finalisation et tests</p>"},{"location":"seance4/partie2-finalisation/","title":"Phase 2 : Finalisation et tests (1h)","text":""},{"location":"seance4/partie2-finalisation/#objectif","title":"Objectif","text":"<p>Cette phase est d\u00e9di\u00e9e \u00e0 la finalisation, aux tests et \u00e0 la pr\u00e9paration de la documentation de votre chatbot p\u00e9dagogique. C'est ici que vous vous assurez que votre solution est fiable, performante et bien document\u00e9e.</p>"},{"location":"seance4/partie2-finalisation/#1-tests-fonctionnels-20-min","title":"1. Tests fonctionnels (20 min)","text":""},{"location":"seance4/partie2-finalisation/#protocole-de-test","title":"Protocole de test","text":"<p>Mettez en place un protocole syst\u00e9matique pour tester votre chatbot avec des sc\u00e9narios r\u00e9els d'utilisation.</p> <p>Cat\u00e9gories de tests \u00e0 effectuer :</p> <ol> <li> <p>Tests de base</p> <ul> <li>Dialogue simple (question-r\u00e9ponse)</li> <li>Gestion de l'historique de conversation</li> <li>Comportement face \u00e0 des requ\u00eates vides ou incompl\u00e8tes</li> </ul> </li> <li> <p>Tests de connaissances</p> <ul> <li>Questions sur chaque concept majeur du Deep Learning</li> <li>V\u00e9rification de l'exactitude des informations fournies</li> <li>Coh\u00e9rence dans les explications</li> </ul> </li> <li> <p>Tests d'usage p\u00e9dagogique</p> <ul> <li>Adaptation au niveau de l'utilisateur</li> <li>Clart\u00e9 des explications techniques</li> <li>Utilit\u00e9 des exemples et analogies</li> </ul> </li> <li> <p>Tests de robustesse</p> <ul> <li>Gestion des erreurs API</li> <li>Questions hors sujet</li> <li>Questions mal formul\u00e9es ou avec des fautes</li> </ul> </li> </ol> <p>Grille d'\u00e9valuation des tests :</p> Test Crit\u00e8re R\u00e9sultat Commentaire T1: Dialogue simple L'assistant r\u00e9pond de fa\u00e7on coh\u00e9rente \u2b1c OK \u2b1c NOK T2: Gestion historique Les r\u00e9ponses tiennent compte du contexte pr\u00e9c\u00e9dent \u2b1c OK \u2b1c NOK T3: Concept CNN L'explication est exacte et p\u00e9dagogique \u2b1c OK \u2b1c NOK T4: Concept gradient Formulation adapt\u00e9e au niveau d\u00e9butant \u2b1c OK \u2b1c NOK T5: Erreur API Message d'erreur appropri\u00e9 \u2b1c OK \u2b1c NOK ... ... ... ... <p>Pour chaque test qui \u00e9choue, notez le probl\u00e8me et priorisez les corrections.</p>"},{"location":"seance4/partie2-finalisation/#profils-dutilisateurs-pour-les-tests","title":"Profils d'utilisateurs pour les tests","text":"<p>Testez votre chatbot avec diff\u00e9rents profils d'utilisateurs :      - D\u00e9butant complet : aucune connaissance pr\u00e9alable      - Niveau interm\u00e9diaire : connaissances de base en programmation      - Niveau avanc\u00e9 : familiarit\u00e9 avec l'IA et questions techniques d\u00e9taill\u00e9es</p>"},{"location":"seance4/partie2-finalisation/#2-optimisation-des-performances-20-min","title":"2. Optimisation des performances (20 min)","text":""},{"location":"seance4/partie2-finalisation/#optimisation-technique","title":"Optimisation technique","text":"<p>Am\u00e9liorez les performances techniques de votre chatbot :</p> <ol> <li>Temps de r\u00e9ponse</li> <li>R\u00e9duisez la taille des prompts envoy\u00e9s \u00e0 l'API</li> <li> <p>Ajoutez un syst\u00e8me de cache pour les questions fr\u00e9quentes    <pre><code># Exemple d'impl\u00e9mentation d'un cache simple\nresponse_cache = {}\n\ndef get_cached_response(question, user_level):\n    cache_key = f\"{question.lower().strip()}_{user_level}\"\n    return response_cache.get(cache_key)\n\ndef store_in_cache(question, user_level, response):\n    cache_key = f\"{question.lower().strip()}_{user_level}\"\n    response_cache[cache_key] = response\n</code></pre></p> </li> <li> <p>Efficacit\u00e9 de l'API</p> </li> <li>Utilisez des param\u00e8tres adapt\u00e9s pour chaque type de requ\u00eate</li> <li> <p>Optimisez la longueur des contextes transmis    <pre><code># Exemple de configuration par type de requ\u00eate\napi_configs = {\n    \"definition\": {\"temperature\": 0.3, \"max_tokens\": 100},  # D\u00e9finitions pr\u00e9cises\n    \"explanation\": {\"temperature\": 0.5, \"max_tokens\": 300}, # Explications d\u00e9taill\u00e9es\n    \"example\": {\"temperature\": 0.7, \"max_tokens\": 150}      # Exemples cr\u00e9atifs\n}\n</code></pre></p> </li> <li> <p>Gestion de la m\u00e9moire</p> </li> <li>Limitez la taille de l'historique de conversation</li> <li>Ajoutez un m\u00e9canisme de r\u00e9sum\u00e9 pour les longues conversations    <pre><code>class OptimizedConversationManager:\n    def __init__(self, max_history=10):\n        self.max_history = max_history\n        self.history = []\n\n    def add_message(self, role, content):\n        self.history.append({\"role\": role, \"content\": content})\n        # Si l'historique devient trop long, le r\u00e9sumer\n        if len(self.history) &gt; self.max_history + 5:  # +5 pour \u00e9viter de r\u00e9sumer trop souvent\n            self._summarize_history()\n\n    def _summarize_history(self):\n        # Demander \u00e0 l'API de r\u00e9sumer la conversation\n        # Puis remplacer l'historique par le r\u00e9sum\u00e9\n        # [Impl\u00e9mentation ici]\n</code></pre></li> </ol>"},{"location":"seance4/partie2-finalisation/#optimisation-pedagogique","title":"Optimisation p\u00e9dagogique","text":"<p>Am\u00e9liorez la qualit\u00e9 p\u00e9dagogique des r\u00e9ponses :</p> <ol> <li> <p>Am\u00e9lioration des prompts</p> <ul> <li>Refinez les instructions syst\u00e8me pour des r\u00e9ponses plus p\u00e9dagogiques</li> <li>Ajoutez des directives sp\u00e9cifiques pour les explications techniques</li> </ul> </li> <li> <p>Enrichissement des r\u00e9ponses</p> <ul> <li>Ajoutez automatiquement des liens vers des ressources compl\u00e9mentaires</li> <li>Incluez des suggestions de questions de suivi pertinentes</li> </ul> </li> <li> <p>Adaptation au niveau</p> <ul> <li>Affinez la d\u00e9tection du niveau de l'utilisateur</li> <li>Personnalisez la complexit\u00e9 des r\u00e9ponses en fonction du niveau d\u00e9tect\u00e9</li> </ul> </li> </ol>"},{"location":"seance4/partie2-finalisation/#3-documentation-20-min","title":"3. Documentation (20 min)","text":""},{"location":"seance4/partie2-finalisation/#documentation-technique","title":"Documentation technique","text":"<p>Cr\u00e9ez une documentation technique claire et compl\u00e8te :</p> <ol> <li> <p>Architecture du syst\u00e8me</p> <ul> <li>Diagramme des composants principaux</li> <li>Description des interactions entre les composants</li> <li>Technologies et biblioth\u00e8ques utilis\u00e9es</li> </ul> </li> <li> <p>Structure du code</p> <ul> <li>Organisation des fichiers et dossiers</li> <li>Description des classes et fonctions principales</li> <li>Points d'extension pour de futures am\u00e9liorations</li> </ul> </li> <li> <p>API et int\u00e9grations</p> <ul> <li>Configuration requise pour l'API Mistral</li> <li>Param\u00e8tres d'API et leur impact</li> <li>Limites et quotas \u00e0 consid\u00e9rer</li> </ul> </li> </ol> <p>Mod\u00e8le de documentation technique :</p> <pre><code># Documentation technique - Chatbot p\u00e9dagogique Deep Learning\n\n## 1. Vue d'ensemble du syst\u00e8me\n[Diagramme d'architecture]\n\nNotre chatbot est compos\u00e9 de trois composants principaux :\n- Interface utilisateur (HTML/CSS/JS)\n- Serveur backend (Python/Flask)\n- Int\u00e9gration API Mistral AI\n\n## 2. Composants principaux\n\n### 2.1 Interface utilisateur\nL'interface est d\u00e9velopp\u00e9e en HTML/CSS/JS et permet :\n- L'affichage des messages dans un format conversationnel\n- La saisie et l'envoi de questions\n- L'affichage d'indicateurs de chargement\n- [...]\n\n### 2.2 Serveur backend\nLe serveur est d\u00e9velopp\u00e9 en Python avec Flask et g\u00e8re :\n- Les requ\u00eates de l'interface utilisateur\n- L'enrichissement des prompts avec la base de connaissances\n- Les appels \u00e0 l'API Mistral AI\n- [...]\n\n### 2.3 Base de connaissances\nLa base de connaissances est structur\u00e9e en JSON et comprend :\n- X concepts principaux\n- Y sous-concepts\n- Z exemples pratiques\n- [...]\n\n## 3. Flux d'ex\u00e9cution\n1. L'utilisateur envoie une question via l'interface\n2. Le serveur re\u00e7oit la question et l'historique\n3. [...]\n\n## 4. Guide d'installation et de d\u00e9ploiement\n[Instructions d\u00e9taill\u00e9es]\n</code></pre>"},{"location":"seance4/partie2-finalisation/#guide-utilisateur","title":"Guide utilisateur","text":"<p>R\u00e9digez un guide utilisateur clair pour faciliter la prise en main :</p> <ol> <li> <p>Pr\u00e9sentation g\u00e9n\u00e9rale</p> <ul> <li>Objectif du chatbot</li> <li>Public cible</li> <li>Fonctionnalit\u00e9s principales</li> </ul> </li> <li> <p>Guide d'utilisation</p> <ul> <li>Comment poser des questions efficacement</li> <li>Exemples de questions pertinentes</li> <li>Commandes sp\u00e9ciales (si existantes)</li> </ul> </li> <li> <p>Conseils d'apprentissage</p> <ul> <li>Progression recommand\u00e9e dans les concepts</li> <li>Comment tester ses connaissances</li> <li>Ressources compl\u00e9mentaires</li> </ul> </li> </ol> <p>Mod\u00e8le de guide utilisateur :</p> <pre><code># Guide utilisateur - Chatbot p\u00e9dagogique Deep Learning\n\n## Bienvenue !\nCe chatbot a \u00e9t\u00e9 con\u00e7u pour vous aider \u00e0 comprendre les concepts du Deep Learning, \nde mani\u00e8re progressive et adapt\u00e9e \u00e0 votre niveau.\n\n## Comment utiliser le chatbot\n1. **Posez une question** dans la zone de texte en bas de l'\u00e9cran\n2. **Attendez la r\u00e9ponse** (g\u00e9n\u00e9ralement quelques secondes)\n3. **Poursuivez la conversation** en posant des questions compl\u00e9mentaires\n\n## Types de questions efficaces\n- \"Qu'est-ce qu'un r\u00e9seau de neurones convolutif ?\"\n- \"Explique-moi la descente de gradient comme si j'avais 12 ans\"\n- \"Quelles sont les diff\u00e9rences entre CNN et RNN ?\"\n- \"Montre-moi un exemple simple de code TensorFlow\"\n\n## Fonctionnalit\u00e9s sp\u00e9ciales\n- Tapez \"quiz\" pour g\u00e9n\u00e9rer un petit quiz sur le sujet de votre choix\n- Tapez \"progression\" pour voir votre avancement dans les concepts\n- [...]\n\n## Parcours d'apprentissage recommand\u00e9\nPour une progression optimale, nous vous sugg\u00e9rons d'explorer les concepts dans cet ordre :\n1. Introduction au Deep Learning\n2. R\u00e9seaux de neurones simples\n3. [...]\n</code></pre>"},{"location":"seance4/partie2-finalisation/#4-preparation-de-la-demonstration-10-min","title":"4. Pr\u00e9paration de la d\u00e9monstration (10 min)","text":"<p>Pr\u00e9parez une d\u00e9monstration efficace pour pr\u00e9senter votre travail :</p> <ol> <li> <p>Sc\u00e9nario de d\u00e9monstration</p> <ul> <li>Identifiez un parcours utilisateur repr\u00e9sentatif</li> <li>Pr\u00e9parez 3-5 questions qui mettent en valeur diff\u00e9rentes fonctionnalit\u00e9s</li> <li>Anticipez les points qui pourraient impressionner l'audience</li> </ul> </li> <li> <p>Support visuel</p> <ul> <li>Cr\u00e9ez 2-3 diapositives pr\u00e9sentant l'architecture et les fonctionnalit\u00e9s</li> <li>Pr\u00e9parez un tableau r\u00e9capitulatif des d\u00e9fis rencontr\u00e9s et solutions trouv\u00e9es</li> </ul> </li> <li> <p>R\u00e9partition des r\u00f4les</p> <ul> <li>D\u00e9cidez qui pr\u00e9sentera quelle partie (si en bin\u00f4me)</li> <li>Planifiez les transitions entre les d\u00e9monstrations</li> </ul> </li> </ol> <p>Exemple de sc\u00e9nario de d\u00e9monstration :  1. Introduction du projet et objectifs (1 min)  2. Pr\u00e9sentation de l'architecture (1 min)  3. D\u00e9monstration d'une conversation basique (1 min)  4. D\u00e9monstration d'une fonctionnalit\u00e9 p\u00e9dagogique sp\u00e9ciale (1 min)  5. Explication d'un d\u00e9fi technique rencontr\u00e9 et sa solution (1 min)  6. Questions-r\u00e9ponses (1 min)</p>"},{"location":"seance4/partie2-finalisation/#check-list-finale","title":"Check-list finale","text":"<p>Avant de terminer cette phase, v\u00e9rifiez les points suivants :</p> <ul> <li> Tous les tests fonctionnels critiques ont \u00e9t\u00e9 r\u00e9alis\u00e9s</li> <li> Les probl\u00e8mes prioritaires ont \u00e9t\u00e9 corrig\u00e9s</li> <li> La documentation technique est compl\u00e8te</li> <li> Le guide utilisateur est clair et informatif</li> <li> Le sc\u00e9nario de d\u00e9monstration est pr\u00eat</li> <li> Les livrables sont organis\u00e9s et accessibles</li> </ul> <p>Retour \u00e0 la vue d'ensemble Continuer vers la Phase 3: Pr\u00e9sentation des projets</p>"},{"location":"seance4/partie3-presentation/","title":"Phase 3 : Pr\u00e9sentation des projets (30 min)","text":""},{"location":"seance4/partie3-presentation/#objectif","title":"Objectif","text":"<p>Cette derni\u00e8re phase de la s\u00e9ance 4 est consacr\u00e9e \u00e0 la pr\u00e9sentation de votre chatbot p\u00e9dagogique. C'est l'aboutissement de votre travail et l'occasion de mettre en valeur votre solution devant la classe.</p>"},{"location":"seance4/partie3-presentation/#deroulement-des-presentations","title":"D\u00e9roulement des pr\u00e9sentations","text":"<p>Chaque \u00e9quipe dispose de 6 minutes au total :  - 5 minutes de pr\u00e9sentation  - 1 minute de questions-r\u00e9ponses</p>"},{"location":"seance4/partie3-presentation/#structure-recommandee-pour-votre-presentation","title":"Structure recommand\u00e9e pour votre pr\u00e9sentation","text":""},{"location":"seance4/partie3-presentation/#1-introduction-1-minute","title":"1. Introduction (1 minute)","text":"<ul> <li>Pr\u00e9sentez bri\u00e8vement votre \u00e9quipe</li> <li>Expliquez le concept g\u00e9n\u00e9ral de votre chatbot p\u00e9dagogique</li> <li>Pr\u00e9cisez le public cible et les objectifs p\u00e9dagogiques</li> </ul> <p>Exemple d'introduction :</p> <p>\"Bonjour, nous sommes [Pr\u00e9nom1] et [Pr\u00e9nom2]. Nous avons d\u00e9velopp\u00e9 'DeepLBot', un chatbot p\u00e9dagogique con\u00e7u pour accompagner les \u00e9tudiants de BTS SIO dans leur d\u00e9couverte du Deep Learning. Notre objectif est de rendre ces concepts complexes accessibles gr\u00e2ce \u00e0 des explications personnalis\u00e9es et interactives.\"</p>"},{"location":"seance4/partie3-presentation/#2-architecture-et-choix-techniques-1-minute","title":"2. Architecture et choix techniques (1 minute)","text":"<ul> <li>Pr\u00e9sentez un sch\u00e9ma simple de l'architecture</li> <li>Justifiez rapidement vos choix techniques</li> <li>Mettez en avant les technologies principales utilis\u00e9es</li> </ul> <p>Exemple :</p> <p>\"Notre solution repose sur une architecture \u00e0 trois composants : une interface web en HTML/CSS/JavaScript, un backend Flask en Python, et l'int\u00e9gration de l'API Mistral AI. Nous avons structur\u00e9 notre base de connaissances en JSON pour faciliter la maintenance et l'enrichissement du contenu.\"</p>"},{"location":"seance4/partie3-presentation/#3-demonstration-en-direct-2-minutes","title":"3. D\u00e9monstration en direct (2 minutes)","text":"<ul> <li>Montrez votre chatbot en action avec 2-3 sc\u00e9narios pr\u00e9d\u00e9finis</li> <li>Mettez en valeur les fonctionnalit\u00e9s distinctives</li> <li>Commentez les interactions pendant la d\u00e9monstration</li> </ul> <p>Conseils pour une d\u00e9monstration efficace : - Pr\u00e9parez un script pr\u00e9cis avec des questions pertinentes - Testez votre d\u00e9mo \u00e0 l'avance pour \u00e9viter les surprises - Ayez un \"plan B\" en cas de probl\u00e8me technique (captures d'\u00e9cran)</p>"},{"location":"seance4/partie3-presentation/#4-defis-et-solutions-1-minute","title":"4. D\u00e9fis et solutions (1 minute)","text":"<ul> <li>Pr\u00e9sentez 1-2 d\u00e9fis techniques majeurs rencontr\u00e9s</li> <li>Expliquez comment vous les avez surmont\u00e9s</li> <li>Partagez un apprentissage cl\u00e9 de ce processus</li> </ul> <p>Exemple :</p> <p>\"Notre principal d\u00e9fi a \u00e9t\u00e9 d'optimiser les prompts pour obtenir des r\u00e9ponses \u00e0 la fois p\u00e9dagogiquement pertinentes et concises. Nous avons r\u00e9solu ce probl\u00e8me en d\u00e9veloppant un syst\u00e8me de prompts dynamiques qui s'adaptent au niveau de l'utilisateur et au type de question pos\u00e9e.\"</p>"},{"location":"seance4/partie3-presentation/#5-conclusion-et-perspectives-30-secondes","title":"5. Conclusion et perspectives (30 secondes)","text":"<ul> <li>R\u00e9capitulez les points forts de votre solution</li> <li>Indiquez les am\u00e9liorations futures envisag\u00e9es</li> <li>Terminez sur une note positive</li> </ul>"},{"location":"seance4/partie3-presentation/#conseils-pour-une-presentation-reussie","title":"Conseils pour une pr\u00e9sentation r\u00e9ussie","text":""},{"location":"seance4/partie3-presentation/#preparation","title":"Pr\u00e9paration","text":"<ul> <li>R\u00e9p\u00e9tez votre pr\u00e9sentation plusieurs fois en chronom\u00e9trant</li> <li>Simplifiez votre discours, \u00e9vitez le jargon technique excessif</li> <li>Synchronisez les r\u00f4les si vous pr\u00e9sentez en bin\u00f4me</li> <li>Pr\u00e9parez vos transitions entre les diff\u00e9rentes parties</li> </ul>"},{"location":"seance4/partie3-presentation/#pendant-la-presentation","title":"Pendant la pr\u00e9sentation","text":"<ul> <li>Parlez clairement et \u00e0 un rythme mod\u00e9r\u00e9</li> <li>Faites face \u00e0 l'audience, pas \u00e0 l'\u00e9cran</li> <li>Mettez en valeur les fonctionnalit\u00e9s uniques de votre solution</li> <li>Respectez strictement le temps imparti</li> </ul>"},{"location":"seance4/partie3-presentation/#support-visuel","title":"Support visuel","text":"<p>Si vous utilisez des diapositives, limitez-les \u00e0 3-4 maximum :  1. Titre et pr\u00e9sentation de l'\u00e9quipe  2. Architecture du chatbot (sch\u00e9ma)  3. D\u00e9fis et solutions  4. Perspectives futures</p>"},{"location":"seance4/partie3-presentation/#grille-devaluation","title":"Grille d'\u00e9valuation","text":"<p>Votre pr\u00e9sentation sera \u00e9valu\u00e9e selon les crit\u00e8res suivants :</p> Crit\u00e8re Description Points Clart\u00e9 Explication claire du concept et de l'impl\u00e9mentation /4 D\u00e9monstration Qualit\u00e9 et pertinence de la d\u00e9monstration en direct /6 Technicit\u00e9 Ma\u00eetrise technique et pertinence des choix d'impl\u00e9mentation /5 Pr\u00e9sentation Organisation, respect du temps, qualit\u00e9 de l'\u00e9locution /3 R\u00e9ponses Qualit\u00e9 des r\u00e9ponses aux questions /2 Total /20"},{"location":"seance4/partie3-presentation/#feedback-et-evaluation-par-les-pairs","title":"Feedback et \u00e9valuation par les pairs","text":"<p>Pendant que vos camarades pr\u00e9sentent, vous \u00eates encourag\u00e9s \u00e0 :  - Prendre des notes sur les id\u00e9es int\u00e9ressantes  - R\u00e9fl\u00e9chir \u00e0 une question pertinente \u00e0 poser  - Compl\u00e9ter la grille d'\u00e9valuation par les pairs qui vous sera distribu\u00e9e</p> <p>Cette \u00e9valuation par les pairs sera prise en compte dans l'\u00e9valuation finale, mais de mani\u00e8re anonyme.</p>"},{"location":"seance4/partie3-presentation/#apres-les-presentations","title":"Apr\u00e8s les pr\u00e9sentations","text":"<p>Une fois toutes les pr\u00e9sentations termin\u00e9es :  - Un temps de d\u00e9briefing collectif sera organis\u00e9  - Les points forts de chaque projet seront mis en avant  - Des conseils d'am\u00e9lioration g\u00e9n\u00e9raux seront partag\u00e9s</p>"},{"location":"seance4/partie3-presentation/#conclusion-et-prochaines-etapes","title":"Conclusion et prochaines \u00e9tapes","text":"<p>Cette pr\u00e9sentation marque la fin du projet et du parcours de 4 s\u00e9ances sur le Deep Learning. Vous avez acquis des comp\u00e9tences pr\u00e9cieuses en :  - Compr\u00e9hension des concepts du Deep Learning  - D\u00e9veloppement d'applications d'IA pratiques  - Int\u00e9gration d'API de mod\u00e8les de langage  - Conception d'interfaces interactives</p> <p>Ces comp\u00e9tences sont directement transf\u00e9rables dans votre future vie professionnelle, que ce soit en stage ou en emploi.</p>"},{"location":"seance4/partie3-presentation/#remise-des-livrables-finaux","title":"Remise des livrables finaux","text":"<p>N'oubliez pas de d\u00e9poser tous vos livrables finaux sur la plateforme avant la date limite :  - Code source complet avec un lien GitHub  - Documentation technique sur Github en Markdown  - Guide utilisateur  - Support de pr\u00e9sentation</p> <p>Date limite de remise : [DATE_LIMITE]</p> <p>Retour \u00e0 la vue d'ensemble</p>"}]}