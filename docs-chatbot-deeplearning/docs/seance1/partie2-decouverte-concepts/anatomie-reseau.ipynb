{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anatomie d'un réseau de neurones\n",
    "# Exploration interactive du fonctionnement interne d'un réseau de neurones\n",
    "\n",
    "# Partie 1: Configuration initiale\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "print(\"Configuration terminée!\")\n",
    "\n",
    "# Partie 2: Exploration d'un neurone unique\n",
    "print(\"\\n--- Exploration d'un neurone unique ---\")\n",
    "print(\"Dans cette partie, nous allons observer le fonctionnement d'un neurone artificiel.\")\n",
    "\n",
    "# Fonction pour calculer la sortie d'un neurone\n",
    "def neuron_output(x1, x2, w1, w2, b, activation=\"relu\"):\n",
    "    # Calcul de la somme pondérée\n",
    "    z = x1 * w1 + x2 * w2 + b\n",
    "    \n",
    "    # Application de la fonction d'activation\n",
    "    if activation == \"relu\":\n",
    "        a = max(0, z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        a = 1 / (1 + np.exp(-z))\n",
    "    elif activation == \"tanh\":\n",
    "        a = np.tanh(z)\n",
    "    else:\n",
    "        a = z  # Linéaire\n",
    "    \n",
    "    return z, a\n",
    "\n",
    "# Fonction pour visualiser un neurone\n",
    "def visualize_neuron(x1, x2, w1, w2, b, activation=\"relu\"):\n",
    "    # Calculer la sortie\n",
    "    z, a = neuron_output(x1, x2, w1, w2, b, activation)\n",
    "    \n",
    "    # Créer la figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. Représentation du neurone\n",
    "    ax = axes[0]\n",
    "    ax.set_xlim(-0.5, 2.5)\n",
    "    ax.set_ylim(-0.5, 2.5)\n",
    "    \n",
    "    # Dessiner le neurone\n",
    "    circle = plt.Circle((1, 1), 0.4, fill=True, color='lightblue', alpha=0.7)\n",
    "    ax.add_artist(circle)\n",
    "    \n",
    "    # Dessiner les entrées\n",
    "    ax.plot(0, 0.7, 'ro', markersize=10)\n",
    "    ax.plot(0, 1.3, 'ro', markersize=10)\n",
    "    \n",
    "    # Dessiner la sortie\n",
    "    ax.plot(2, 1, 'go', markersize=10)\n",
    "    \n",
    "    # Ajouter les connexions\n",
    "    ax.arrow(0, 0.7, 0.6, 0.1, head_width=0.1, head_length=0.1, fc='black', ec='black', linewidth=2)\n",
    "    ax.arrow(0, 1.3, 0.6, -0.1, head_width=0.1, head_length=0.1, fc='black', ec='black', linewidth=2)\n",
    "    ax.arrow(1.4, 1, 0.6, 0, head_width=0.1, head_length=0.1, fc='black', ec='black', linewidth=2)\n",
    "    \n",
    "    # Ajouter les textes\n",
    "    ax.text(-0.1, 0.7, f\"x₁ = {x1:.2f}\", fontsize=12, ha='right')\n",
    "    ax.text(-0.1, 1.3, f\"x₂ = {x2:.2f}\", fontsize=12, ha='right')\n",
    "    ax.text(1, 1, f\"z = {z:.2f}\\na = {a:.2f}\", fontsize=12, ha='center')\n",
    "    ax.text(0.5, 0.95, f\"w₁ = {w1:.2f}\", fontsize=10, rotation=15)\n",
    "    ax.text(0.5, 1.15, f\"w₂ = {w2:.2f}\", fontsize=10, rotation=-15)\n",
    "    ax.text(2.1, 1, f\"Sortie = {a:.2f}\", fontsize=12, ha='left')\n",
    "    ax.text(1, 0.5, f\"Biais = {b:.2f}\", fontsize=10)\n",
    "    \n",
    "    ax.set_title(\"Neurone artificiel\", fontsize=14)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # 2. Représentation de la fonction d'activation\n",
    "    ax = axes[1]\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        y = np.maximum(0, x)\n",
    "        title = \"Fonction d'activation: ReLU\"\n",
    "    elif activation == \"sigmoid\":\n",
    "        y = 1 / (1 + np.exp(-x))\n",
    "        title = \"Fonction d'activation: Sigmoid\"\n",
    "    elif activation == \"tanh\":\n",
    "        y = np.tanh(x)\n",
    "        title = \"Fonction d'activation: Tanh\"\n",
    "    else:\n",
    "        y = x\n",
    "        title = \"Fonction d'activation: Linéaire\"\n",
    "    \n",
    "    ax.plot(x, y, 'b-', linewidth=2)\n",
    "    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Marquer le point correspondant à z\n",
    "    ax.plot(z, a, 'ro', markersize=8)\n",
    "    ax.plot([z, z], [0, a], 'r--', alpha=0.5)\n",
    "    ax.plot([0, z], [a, a], 'r--', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_xlabel(\"z (somme pondérée)\")\n",
    "    ax.set_ylabel(\"a (activation)\")\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Visualisation de la frontière de décision\n",
    "    ax = axes[2]\n",
    "    \n",
    "    # Créer des points pour former une grille\n",
    "    grid_size = 20\n",
    "    x1_values = np.linspace(0, 1, grid_size)\n",
    "    x2_values = np.linspace(0, 1, grid_size)\n",
    "    x1_grid, x2_grid = np.meshgrid(x1_values, x2_values)\n",
    "    \n",
    "    # Calculer la sortie pour chaque point de la grille\n",
    "    z_grid = x1_grid * w1 + x2_grid * w2 + b\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        a_grid = np.maximum(0, z_grid)\n",
    "    elif activation == \"sigmoid\":\n",
    "        a_grid = 1 / (1 + np.exp(-z_grid))\n",
    "    elif activation == \"tanh\":\n",
    "        a_grid = np.tanh(z_grid)\n",
    "    else:\n",
    "        a_grid = z_grid\n",
    "    \n",
    "    # Créer une carte de couleur\n",
    "    cmap = plt.get_cmap('coolwarm')\n",
    "    \n",
    "    # Tracer la heatmap\n",
    "    im = ax.imshow(a_grid, origin='lower', extent=[0, 1, 0, 1], \n",
    "                   cmap=cmap, vmin=0, vmax=1)\n",
    "    plt.colorbar(im, ax=ax, label=\"Activation\")\n",
    "    \n",
    "    # Ajouter le point actuel\n",
    "    ax.plot(x1, x2, 'ko', markersize=8)\n",
    "    \n",
    "    # Tracer la frontière de décision (a = 0.5)\n",
    "    if activation in [\"sigmoid\", \"tanh\"]:\n",
    "        threshold = 0.5\n",
    "        CS = ax.contour(x1_grid, x2_grid, a_grid, levels=[threshold], \n",
    "                         colors='k', linestyles='--')\n",
    "        ax.clabel(CS, inline=True, fontsize=10, fmt={threshold: \"a = 0.5\"})\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel(\"x₁\")\n",
    "    ax.set_ylabel(\"x₂\")\n",
    "    ax.set_title(\"Carte d'activation\", fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return a\n",
    "\n",
    "# Créer des widgets interactifs pour le neurone\n",
    "w1_slider = widgets.FloatSlider(value=1.0, min=-3.0, max=3.0, step=0.1, description='Poids w₁:')\n",
    "w2_slider = widgets.FloatSlider(value=1.0, min=-3.0, max=3.0, step=0.1, description='Poids w₂:')\n",
    "b_slider = widgets.FloatSlider(value=0.0, min=-3.0, max=3.0, step=0.1, description='Biais:')\n",
    "x1_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='Entrée x₁:')\n",
    "x2_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='Entrée x₂:')\n",
    "activation_dropdown = widgets.Dropdown(\n",
    "    options=['relu', 'sigmoid', 'tanh', 'linear'],\n",
    "    value='relu',\n",
    "    description='Activation:'\n",
    ")\n",
    "\n",
    "# Fonction pour mettre à jour la visualisation\n",
    "def update_neuron_visualization(w1, w2, b, x1, x2, activation):\n",
    "    clear_output(wait=True)\n",
    "    output = visualize_neuron(x1, x2, w1, w2, b, activation)\n",
    "    print(f\"Sortie du neurone: {output:.4f}\")\n",
    "    \n",
    "    # Expliquer le calcul\n",
    "    z = x1 * w1 + x2 * w2 + b\n",
    "    print(f\"\\nCalcul détaillé:\")\n",
    "    print(f\"z = (x₁ × w₁) + (x₂ × w₂) + b\")\n",
    "    print(f\"z = ({x1:.2f} × {w1:.2f}) + ({x2:.2f} × {w2:.2f}) + {b:.2f}\")\n",
    "    print(f\"z = {x1*w1:.2f} + {x2*w2:.2f} + {b:.2f}\")\n",
    "    print(f\"z = {z:.2f}\")\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        print(f\"a = ReLU(z) = max(0, z) = max(0, {z:.2f}) = {max(0, z):.2f}\")\n",
    "    elif activation == \"sigmoid\":\n",
    "        sig_z = 1 / (1 + np.exp(-z))\n",
    "        print(f\"a = Sigmoid(z) = 1 / (1 + e^(-z)) = 1 / (1 + e^(-{z:.2f})) = {sig_z:.2f}\")\n",
    "    elif activation == \"tanh\":\n",
    "        tanh_z = np.tanh(z)\n",
    "        print(f\"a = tanh(z) = tanh({z:.2f}) = {tanh_z:.2f}\")\n",
    "    else:\n",
    "        print(f\"a = z = {z:.2f}\")  # Linéaire\n",
    "\n",
    "# Interface interactive pour le neurone\n",
    "neuron_output = widgets.interactive_output(\n",
    "    update_neuron_visualization,\n",
    "    {'w1': w1_slider, 'w2': w2_slider, 'b': b_slider, \n",
    "     'x1': x1_slider, 'x2': x2_slider, 'activation': activation_dropdown}\n",
    ")\n",
    "\n",
    "# Afficher les widgets\n",
    "print(\"Utilisez les contrôles ci-dessous pour modifier les propriétés du neurone:\")\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([x1_slider, x2_slider]),\n",
    "    widgets.HBox([w1_slider, w2_slider]),\n",
    "    widgets.HBox([b_slider, activation_dropdown])\n",
    "]))\n",
    "display(neuron_output)\n",
    "\n",
    "# Partie 3: Exploration d'un réseau simple\n",
    "print(\"\\n--- De l'unique au réseau ---\")\n",
    "print(\"Dans cette partie, nous allons explorer un petit réseau de neurones.\")\n",
    "\n",
    "# Fonction pour créer et visualiser un réseau simple\n",
    "def create_simple_network(hidden_units=3, activation='relu'):\n",
    "    # Créer un modèle séquentiel\n",
    "    model = Sequential([\n",
    "        Dense(hidden_units, activation=activation, input_shape=(2,)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compiler le modèle (bien que nous ne l'entraînerons pas)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Fonction pour visualiser un réseau simple\n",
    "def visualize_network(inputs, weights1=None, biases1=None, weights2=None, biases2=None, hidden_units=3, activation='relu'):\n",
    "    # Créer le modèle si non fourni\n",
    "    model = create_simple_network(hidden_units, activation)\n",
    "    \n",
    "    # Si des poids sont fournis, les appliquer\n",
    "    if weights1 is not None and biases1 is not None and weights2 is not None and biases2 is not None:\n",
    "        model.layers[0].set_weights([weights1, biases1])\n",
    "        model.layers[1].set_weights([weights2, biases2])\n",
    "    \n",
    "    # Convertir les entrées pour prédiction\n",
    "    x = np.array([inputs])\n",
    "    \n",
    "    # Obtenir les activations intermédiaires\n",
    "    intermediate_layer_model = tf.keras.Model(inputs=model.input,\n",
    "                                             outputs=model.layers[0].output)\n",
    "    intermediate_activations = intermediate_layer_model.predict(x)[0]\n",
    "    \n",
    "    # Obtenir les activations de sortie\n",
    "    output_activation = model.predict(x)[0][0]\n",
    "    \n",
    "    # Extraire les poids et biais\n",
    "    weights1, biases1 = model.layers[0].get_weights()\n",
    "    weights2, biases2 = model.layers[1].get_weights()\n",
    "    \n",
    "    # Créer la figure pour visualiser le réseau\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Définir les positions des neurones\n",
    "    input_layer_y = np.array([0.2, 0.8])\n",
    "    hidden_layer_y = np.linspace(0.1, 0.9, hidden_units)\n",
    "    output_layer_y = np.array([0.5])\n",
    "    \n",
    "    input_layer_x = 0.1\n",
    "    hidden_layer_x = 0.5\n",
    "    output_layer_x = 0.9\n",
    "    \n",
    "    # Dessiner les neurones d'entrée\n",
    "    for i, y in enumerate(input_layer_y):\n",
    "        plt.scatter(input_layer_x, y, s=200, c='blue', alpha=0.7)\n",
    "        plt.text(input_layer_x, y, f\"x{i+1}={inputs[i]:.2f}\", fontsize=12, ha='center', va='center', color='white')\n",
    "    \n",
    "    # Dessiner les neurones cachés\n",
    "    for i, y in enumerate(hidden_layer_y):\n",
    "        # Calculer la somme pondérée\n",
    "        z = np.dot(inputs, weights1[:, i]) + biases1[i]\n",
    "        \n",
    "        # Appliquer l'activation\n",
    "        if activation == 'relu':\n",
    "            a = max(0, z)\n",
    "        elif activation == 'sigmoid':\n",
    "            a = 1 / (1 + np.exp(-z))\n",
    "        elif activation == 'tanh':\n",
    "            a = np.tanh(z)\n",
    "        else:\n",
    "            a = z\n",
    "        \n",
    "        # Couleur basée sur l'activation\n",
    "        color = plt.cm.viridis(a)\n",
    "        \n",
    "        plt.scatter(hidden_layer_x, y, s=200, c=[color], alpha=0.7)\n",
    "        plt.text(hidden_layer_x, y, f\"{a:.2f}\", fontsize=12, ha='center', va='center', color='white')\n",
    "    \n",
    "    # Dessiner le neurone de sortie\n",
    "    plt.scatter(output_layer_x, output_layer_y, s=200, c='red', alpha=0.7)\n",
    "    plt.text(output_layer_x, output_layer_y, f\"{output_activation:.2f}\", fontsize=12, ha='center', va='center', color='white')\n",
    "    \n",
    "    # Dessiner les connexions entre couches d'entrée et cachée\n",
    "    for i, y_in in enumerate(input_layer_y):\n",
    "        for j, y_hid in enumerate(hidden_layer_y):\n",
    "            # Couleur et épaisseur basées sur le poids\n",
    "            weight = weights1[i, j]\n",
    "            width = abs(weight) * 3\n",
    "            color = 'red' if weight < 0 else 'green'\n",
    "            alpha = min(abs(weight), 1.0)\n",
    "            \n",
    "            plt.plot([input_layer_x, hidden_layer_x], [y_in, y_hid], \n",
    "                    c=color, linewidth=width, alpha=alpha)\n",
    "    \n",
    "    # Dessiner les connexions entre couche cachée et sortie\n",
    "    for i, y_hid in enumerate(hidden_layer_y):\n",
    "        # Couleur et épaisseur basées sur le poids\n",
    "        weight = weights2[i, 0]\n",
    "        width = abs(weight) * 3\n",
    "        color = 'red' if weight < 0 else 'green'\n",
    "        alpha = min(abs(weight), 1.0)\n",
    "        \n",
    "        plt.plot([hidden_layer_x, output_layer_x], [y_hid, output_layer_y], \n",
    "                c=color, linewidth=width, alpha=alpha)\n",
    "    \n",
    "    # Étiquettes\n",
    "    plt.text(input_layer_x, 0.03, \"Couche d'entrée\", fontsize=14, ha='center')\n",
    "    plt.text(hidden_layer_x, 0.03, \"Couche cachée\", fontsize=14, ha='center')\n",
    "    plt.text(output_layer_x, 0.03, \"Couche de sortie\", fontsize=14, ha='center')\n",
    "    \n",
    "    # Enlever les axes\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Réseau de neurones - Activation cachée: {activation}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Afficher les calculs détaillés\n",
    "    print(\"\\nCalculs détaillés pour chaque neurone de la couche cachée:\")\n",
    "    for i in range(hidden_units):\n",
    "        z = np.dot(inputs, weights1[:, i]) + biases1[i]\n",
    "        print(f\"\\nNeurone caché {i+1}:\")\n",
    "        print(f\"z = (x₁ × w₁,{i+1}) + (x₂ × w₂,{i+1}) + b{i+1}\")\n",
    "        print(f\"z = ({inputs[0]:.2f} × {weights1[0, i]:.2f}) + ({inputs[1]:.2f} × {weights1[1, i]:.2f}) + {biases1[i]:.2f}\")\n",
    "        print(f\"z = {inputs[0] * weights1[0, i]:.2f} + {inputs[1] * weights1[1, i]:.2f} + {biases1[i]:.2f} = {z:.2f}\")\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            a = max(0, z)\n",
    "            print(f\"a = ReLU(z) = max(0, {z:.2f}) = {a:.2f}\")\n",
    "        elif activation == 'sigmoid':\n",
    "            a = 1 / (1 + np.exp(-z))\n",
    "            print(f\"a = Sigmoid(z) = 1 / (1 + e^(-{z:.2f})) = {a:.2f}\")\n",
    "        elif activation == 'tanh':\n",
    "            a = np.tanh(z)\n",
    "            print(f\"a = tanh(z) = tanh({z:.2f}) = {a:.2f}\")\n",
    "        else:\n",
    "            a = z\n",
    "            print(f\"a = z = {z:.2f}\")\n",
    "    \n",
    "    print(\"\\nCalcul pour le neurone de sortie:\")\n",
    "    z_out = np.dot(intermediate_activations, weights2[:, 0]) + biases2[0]\n",
    "    print(f\"z = Σ(a_caché × w_sortie) + b_sortie = {z_out:.2f}\")\n",
    "    print(f\"sortie = Sigmoid(z) = 1 / (1 + e^(-{z_out:.2f})) = {output_activation:.2f}\")\n",
    "    \n",
    "    return model, weights1, biases1, weights2, biases2\n",
    "\n",
    "# Fonction pour générer des poids aléatoires\n",
    "def generate_random_weights(hidden_units=3):\n",
    "    # Générer des poids aléatoires pour la première couche\n",
    "    weights1 = np.random.normal(0, 1, (2, hidden_units))\n",
    "    biases1 = np.random.normal(0, 1, hidden_units)\n",
    "    \n",
    "    # Générer des poids aléatoires pour la couche de sortie\n",
    "    weights2 = np.random.normal(0, 1, (hidden_units, 1))\n",
    "    biases2 = np.random.normal(0, 1, 1)\n",
    "    \n",
    "    return weights1, biases1, weights2, biases2\n",
    "\n",
    "# Créer des widgets interactifs pour le réseau\n",
    "x1_net_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='Entrée x₁:')\n",
    "x2_net_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='Entrée x₂:')\n",
    "hidden_units_slider = widgets.IntSlider(value=3, min=1, max=5, description='Neurones cachés:')\n",
    "activation_net_dropdown = widgets.Dropdown(\n",
    "    options=['relu', 'sigmoid', 'tanh', 'linear'],\n",
    "    value='relu',\n",
    "    description='Activation:'\n",
    ")\n",
    "random_button = widgets.Button(description=\"Poids aléatoires\")\n",
    "\n",
    "# Variables pour stocker les poids courants\n",
    "current_weights1, current_biases1, current_weights2, current_biases2 = generate_random_weights()\n",
    "\n",
    "# Fonction pour visualiser le réseau\n",
    "def update_network_visualization(x1, x2, hidden_units, activation):\n",
    "    global current_weights1, current_biases1, current_weights2, current_biases2\n",
    "    \n",
    "    # Ajuster les dimensions des poids si nécessaire\n",
    "    if current_weights1.shape[1] != hidden_units:\n",
    "        current_weights1, current_biases1, current_weights2, current_biases2 = generate_random_weights(hidden_units)\n",
    "    \n",
    "    # Visualiser le réseau\n",
    "    inputs = np.array([x1, x2])\n",
    "    _, w1, b1, w2, b2 = visualize_network(\n",
    "        inputs, current_weights1, current_biases1, current_weights2, current_biases2, \n",
    "        hidden_units, activation\n",
    "    )\n",
    "    \n",
    "    # Mettre à jour les poids courants\n",
    "    current_weights1, current_biases1 = w1, b1\n",
    "    current_weights2, current_biases2 = w2, b2\n",
    "\n",
    "# Fonction pour générer de nouveaux poids aléatoires\n",
    "def regenerate_weights(b):\n",
    "    global current_weights1, current_biases1, current_weights2, current_biases2\n",
    "    current_weights1, current_biases1, current_weights2, current_biases2 = generate_random_weights(\n",
    "        hidden_units_slider.value\n",
    "    )\n",
    "    # Mettre à jour la visualisation\n",
    "    update_network_visualization(\n",
    "        x1_net_slider.value, x2_net_slider.value,\n",
    "        hidden_units_slider.value, activation_net_dropdown.value\n",
    "    )\n",
    "\n",
    "# Associer la fonction au bouton\n",
    "random_button.on_click(regenerate_weights)\n",
    "\n",
    "# Interface interactive pour le réseau\n",
    "network_output = widgets.interactive_output(\n",
    "    update_network_visualization,\n",
    "    {'x1': x1_net_slider, 'x2': x2_net_slider, \n",
    "     'hidden_units': hidden_units_slider, 'activation': activation_net_dropdown}\n",
    ")\n",
    "\n",
    "# Afficher les widgets pour le réseau\n",
    "print(\"\\nExplorez le comportement d'un réseau simple:\")\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([x1_net_slider, x2_net_slider]),\n",
    "    widgets.HBox([hidden_units_slider, activation_net_dropdown]),\n",
    "    random_button\n",
    "]))\n",
    "display(network_output)\n",
    "\n",
    "# Partie 4: Visualisation de l'entraînement\n",
    "print(\"\\n--- Visualisation de l'entraînement ---\")\n",
    "print(\"Dans cette partie, nous allons observer l'évolution des poids pendant l'entraînement.\")\n",
    "\n",
    "# Générer des données XOR\n",
    "def generate_xor_data(n_samples=100):\n",
    "    X = np.random.rand(n_samples, 2)\n",
    "    y = np.logical_xor(X[:, 0] > 0.5, X[:, 1] > 0.5).astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "# Fonction pour visualiser l'évolution de l'apprentissage\n",
    "def visualize_training_animation(learning_rate=0.1, epochs=50, hidden_units=4):\n",
    "    # Générer des données\n",
    "    X_train, y_train = generate_xor_data(200)\n",
    "    \n",
    "    # Créer un modèle\n",
    "    model = Sequential([\n",
    "        Dense(hidden_units, activation='relu', input_shape=(2,)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compiler avec un optimiseur personnalisé pour suivre l'évolution des poids\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Extraire les poids initiaux\n",
    "    weights_history = [model.get_weights()]\n",
    "    \n",
    "    # Fonction pour collecter les poids après chaque époque\n",
    "    class WeightHistory(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            weights_history.append(self.model.get_weights())\n",
    "    \n",
    "    # Entraîner le modèle\n",
    "    history = model.fit(\n",
    "        X_train, y_train,"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
