# üõ†Ô∏è Mini-projet individuel : Am√©lioration d'un mod√®le de Deep Learning

![Mini-projet](../images/banner-mini-projet.svg)

## üéØ Objectifs

Ce mini-projet individuel vous permettra de :

- Appliquer les connaissances acquises sur les r√©seaux de neurones
- Exp√©rimenter avec diff√©rentes architectures et hyperparam√®tres
- D√©velopper une m√©thode d'analyse des performances
- Documenter vos r√©sultats de fa√ßon professionnelle

## Contexte

Vous √™tes stagiaire dans une entreprise qui souhaite impl√©menter un syst√®me de reconnaissance automatique de chiffres manuscrits pour traiter des formulaires papier. Un premier mod√®le a √©t√© d√©velopp√©, mais sa pr√©cision est encore insuffisante pour une utilisation en production.

Votre mission est d'am√©liorer ce mod√®le existant en explorant diff√©rentes configurations et en justifiant vos choix techniques.

## Instructions d√©taill√©es

### √âtape 1 : Pr√©paration de l'environnement (5 min)

1. Cr√©ez un nouveau notebook dans Google Colab
2. Importez les biblioth√®ques n√©cessaires :
   ```python
   import numpy as np
   import matplotlib.pyplot as plt
   import tensorflow as tf
   from tensorflow.keras.datasets import mnist
   from tensorflow.keras.models import Sequential
   from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
   from tensorflow.keras.utils import to_categorical
   ```

3. Chargez et pr√©parez les donn√©es MNIST :
   ```python
   # Chargement des donn√©es
   (X_train, y_train), (X_test, y_test) = mnist.load_data()
   
   # Normalisation et reshaping
   X_train = X_train.reshape(-1, 28, 28, 1) / 255.0
   X_test = X_test.reshape(-1, 28, 28, 1) / 255.0
   
   # Conversion des labels en cat√©gories
   y_train_cat = to_categorical(y_train, 10)
   y_test_cat = to_categorical(y_test, 10)
   ```

### √âtape 2 : Impl√©mentation du mod√®le de r√©f√©rence (10 min)

Le mod√®le de r√©f√©rence fourni par l'entreprise est un CNN simple :

```python
def create_baseline_model():
    model = Sequential([
        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
        MaxPooling2D(pool_size=(2, 2)),
        Conv2D(64, kernel_size=(3, 3), activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(10, activation='softmax')
    ])
    
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

# Cr√©er et entra√Æner le mod√®le de r√©f√©rence
baseline_model = create_baseline_model()
baseline_history = baseline_model.fit(
    X_train, y_train_cat,
    epochs=5,
    batch_size=128,
    validation_split=0.2,
    verbose=1
)

# √âvaluer le mod√®le de r√©f√©rence
baseline_score = baseline_model.evaluate(X_test, y_test_cat, verbose=0)
print(f"Mod√®le de r√©f√©rence - Pr√©cision: {baseline_score[1]*100:.2f}%")
```

### √âtape 3 : Analyse et modifications (30 min)

Votre t√¢che consiste √† am√©liorer ce mod√®le de r√©f√©rence. Exp√©rimentez avec au moins trois des modifications suivantes :

1. **Modification de l'architecture**
     - Ajouter/enlever des couches de convolution
     - Modifier le nombre de filtres
     - Changer la taille des noyaux de convolution

2. **Techniques de r√©gularisation**
     - Ajouter des couches de Dropout
     - Utiliser de la r√©gularisation L1/L2
     - Impl√©menter du Batch Normalization

3. **Optimisation des hyperparam√®tres**
     - Tester diff√©rents optimiseurs (SGD, RMSprop, Adam)
     - Modifier le taux d'apprentissage
     - Varier la taille du batch

4. **Augmentation de donn√©es**
     - Rotation des images
     - Zoom
     - D√©calage

5. **Strat√©gies d'entra√Ænement**
   - Modifier le nombre d'√©poques
   - Utiliser un learning rate scheduler
   - Impl√©menter early stopping

Pour chaque modification, cr√©ez une fonction qui retourne le mod√®le modifi√© :

```python
def create_improved_model_1():
    # Votre impl√©mentation avec la premi√®re modification
    model = Sequential([
        # Votre architecture modifi√©e
    ])
    
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

# Similairement pour les autres modifications
```

### √âtape 4 : √âvaluation comparative (10 min)

Cr√©ez une fonction d'√©valuation pour comparer syst√©matiquement vos mod√®les :

```python
def evaluate_model(model_creator, model_name, epochs=5):
    model = model_creator()
    
    # Mesurer le temps d'entra√Ænement
    import time
    start_time = time.time()
    
    # Entra√Æner le mod√®le
    history = model.fit(
        X_train, y_train_cat,
        epochs=epochs,
        batch_size=128,
        validation_split=0.2,
        verbose=1
    )
    
    training_time = time.time() - start_time
    
    # √âvaluer sur l'ensemble de test
    test_score = model.evaluate(X_test, y_test_cat, verbose=0)
    test_accuracy = test_score[1]
    
    # Visualiser l'√©volution de l'apprentissage
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train')
    plt.plot(history.history['val_accuracy'], label='Validation')
    plt.title(f'{model_name} - Pr√©cision')
    plt.xlabel('√âpoque')
    plt.ylabel('Pr√©cision')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train')
    plt.plot(history.history['val_loss'], label='Validation')
    plt.title(f'{model_name} - Perte')
    plt.xlabel('√âpoque')
    plt.ylabel('Perte')
    plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    # Retourner les m√©triques principales
    return {
        'accuracy': test_accuracy,
        'training_time': training_time,
        'history': history.history
    }

# √âvaluer tous les mod√®les
baseline_results = evaluate_model(create_baseline_model, "Mod√®le de r√©f√©rence")
model1_results = evaluate_model(create_improved_model_1, "Mod√®le am√©lior√© 1")
# ... √©valuez vos autres mod√®les
```

### √âtape 5 : Analyse des erreurs (5 min)

Pour mieux comprendre les forces et faiblesses de vos mod√®les, analysez les erreurs de classification :

```python
def analyze_errors(model, name):
    # Obtenir les pr√©dictions
    predictions = model.predict(X_test)
    pred_classes = np.argmax(predictions, axis=1)
    true_classes = np.argmax(y_test_cat, axis=1)
    
    # Identifier les erreurs
    misclassified = np.where(pred_classes != true_classes)[0]
    
    if len(misclassified) > 0:
        # Afficher quelques exemples mal class√©s
        plt.figure(figsize=(12, 4))
        for i, idx in enumerate(misclassified[:10]):
            plt.subplot(2, 5, i+1)
            plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')
            plt.title(f"R√©el: {true_classes[idx]}\nPr√©dit: {pred_classes[idx]}")
            plt.axis('off')
        plt.suptitle(f"Erreurs de classification - {name}")
        plt.tight_layout()
        plt.show()
    
    # Calculer le taux d'erreur par chiffre
    error_by_digit = {}
    for digit in range(10):
        digit_indices = np.where(true_classes == digit)[0]
        if len(digit_indices) > 0:
            digit_errors = np.sum(pred_classes[digit_indices] != digit)
            error_rate = digit_errors / len(digit_indices)
            error_by_digit[digit] = error_rate
    
    # Afficher un graphique des taux d'erreur par chiffre
    plt.figure(figsize=(10, 5))
    digits = list(error_by_digit.keys())
    error_rates = [error_by_digit[d] for d in digits]
    plt.bar(digits, error_rates)
    plt.title(f"Taux d'erreur par chiffre - {name}")
    plt.xlabel("Chiffre")
    plt.ylabel("Taux d'erreur")
    plt.xticks(range(10))
    plt.ylim(0, max(error_rates) * 1.2)
    plt.grid(axis='y', alpha=0.3)
    plt.show()

# Analyser les erreurs du meilleur mod√®le
analyze_errors(best_model, "Meilleur mod√®le")
```

### √âtape 6 : G√©n√©ralisations et robustesse (5 min)

Testez la robustesse de votre meilleur mod√®le face √† des donn√©es alt√©r√©es :

```python
def test_robustness(model, name):
    # Cr√©er des versions bruit√©es et rot√©es des images de test
    noisy_X_test = X_test + np.random.normal(0, 0.1, X_test.shape)
    noisy_X_test = np.clip(noisy_X_test, 0, 1)  # S'assurer que les valeurs restent entre 0 et 1
    
    from scipy.ndimage import rotate
    rotated_X_test = np.zeros_like(X_test)
    for i in range(len(X_test)):
        angle = np.random.uniform(-15, 15)
        rotated_X_test[i, :, :, 0] = rotate(X_test[i, :, :, 0], angle, reshape=False)
    
    # √âvaluer sur les donn√©es alt√©r√©es
    normal_score = model.evaluate(X_test, y_test_cat, verbose=0)[1]
    noisy_score = model.evaluate(noisy_X_test, y_test_cat, verbose=0)[1]
    rotated_score = model.evaluate(rotated_X_test, y_test_cat, verbose=0)[1]
    
    # Afficher les r√©sultats
    print(f"Performances du mod√®le {name}:")
    print(f"- Donn√©es normales: {normal_score*100:.2f}%")
    print(f"- Donn√©es bruit√©es: {noisy_score*100:.2f}%")
    print(f"- Donn√©es rot√©es: {rotated_score*100:.2f}%")
    
    # Visualiser quelques exemples
    plt.figure(figsize=(15, 5))
    for i in range(5):
        # Image originale
        plt.subplot(3, 5, i+1)
        plt.imshow(X_test[i].reshape(28, 28), cmap='gray')
        plt.axis('off')
        if i == 2:
            plt.title("Images originales")
        
        # Image bruit√©e
        plt.subplot(3, 5, i+6)
        plt.imshow(noisy_X_test[i].reshape(28, 28), cmap='gray')
        plt.axis('off')
        if i == 2:
            plt.title("Images bruit√©es")
        
        # Image rot√©e
        plt.subplot(3, 5, i+11)
        plt.imshow(rotated_X_test[i].reshape(28, 28), cmap='gray')
        plt.axis('off')
        if i == 2:
            plt.title("Images rot√©es")
    
    plt.tight_layout()
    plt.show()

# Tester la robustesse du meilleur mod√®le
test_robustness(best_model, "Meilleur mod√®le")
```

### √âtape 7 : Comparaison finale et documentation (5 min)

Cr√©ez un tableau r√©capitulatif de tous les mod√®les test√©s :

```python
def compare_models(results_dict):
    # Cr√©er un tableau comparatif
    models = list(results_dict.keys())
    metrics = ['accuracy', 'training_time']
    
    # Afficher le tableau
    print("-" * 60)
    print(f"{'Mod√®le':<25} {'Pr√©cision':<15} {'Temps (s)':<15}")
    print("-" * 60)
    
    for model_name in models:
        acc = results_dict[model_name]['accuracy'] * 100
        time = results_dict[model_name]['training_time']
        print(f"{model_name:<25} {acc:<15.2f} {time:<15.2f}")
    
    print("-" * 60)
    
    # Visualiser la comparaison
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    accuracies = [results_dict[m]['accuracy'] * 100 for m in models]
    plt.bar(models, accuracies)
    plt.title('Comparaison des pr√©cisions')
    plt.ylabel('Pr√©cision (%)')
    plt.xticks(rotation=45, ha='right')
    plt.ylim(min(accuracies) * 0.95, 100)
    
    plt.subplot(1, 2, 2)
    times = [results_dict[m]['training_time'] for m in models]
    plt.bar(models, times)
    plt.title('Comparaison des temps d\'entra√Ænement')
    plt.ylabel('Temps (secondes)')
    plt.xticks(rotation=45, ha='right')
    
    plt.tight_layout()
    plt.show()

# Comparer tous les mod√®les
all_results = {
    "Mod√®le de r√©f√©rence": baseline_results,
    "Mod√®le am√©lior√© 1": model1_results,
    # Ajoutez vos autres mod√®les
}

compare_models(all_results)
```

## Livrable attendu

Vous devez soumettre un notebook complet incluant :

1. **Introduction** : Pr√©sentation du contexte et des objectifs
2. **Mod√®le de r√©f√©rence** : Impl√©mentation et r√©sultats du mod√®le initial
3. **Am√©liorations propos√©es** : Description d√©taill√©e de chaque modification avec justification
4. **R√©sultats comparatifs** : Tableau et graphiques comparant tous les mod√®les test√©s
5. **Analyse des erreurs** : √âtude des cas o√π votre meilleur mod√®le √©choue encore
6. **Test de robustesse** : √âvaluation de la performance sur des donn√©es alt√©r√©es
7. **Conclusion** : Synth√®se des r√©sultats et recommandations pour l'entreprise

## Crit√®res d'√©valuation

Votre mini-projet sera √©valu√© selon les crit√®res suivants :

| Crit√®re | Points | Description |
|---------|--------|-------------|
| Qualit√© du code | 4 | Code bien structur√©, comment√© et fonctionnel |
| Pertinence des modifications | 6 | Choix judicieux et justifi√© des modifications apport√©es |
| Am√©lioration effective | 4 | Gain de performances par rapport au mod√®le de r√©f√©rence |
| Analyse critique | 3 | Capacit√© √† analyser les forces et faiblesses des mod√®les |
| Documentation | 3 | Clart√© et compl√©tude du rapport |

## Conseils pour r√©ussir

- **Commencez simple** : N'essayez pas d'impl√©menter toutes les modifications √† la fois
- **Exp√©rimentez m√©thodiquement** : Changez un param√®tre √† la fois pour bien comprendre son impact
- **Documentez vos observations** : Notez les effets de chaque modification sur les performances
- **Analysez les erreurs** : Comprendre pourquoi le mod√®le se trompe est aussi important que d'am√©liorer son score
- **Justifiez vos choix** : Expliquez pourquoi vous avez opt√© pour certaines modifications plut√¥t que d'autres

## Ressources compl√©mentaires

- [Guide Keras pour les CNN](https://keras.io/guides/working_with_images/)
- [TensorFlow Data Augmentation Tutorial](https://www.tensorflow.org/tutorials/images/data_augmentation)
- [Conseils pour am√©liorer les performances des mod√®les](https://machinelearningmastery.com/improve-deep-learning-performance/)

[Retour au Module 1](index.md){ .md-button }
[Continuer vers l'Auto-√©valuation](qcm-evaluation-module1.md){ .md-button .md-button--primary }