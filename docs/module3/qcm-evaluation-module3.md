# üìù Auto-√©valuation du Module 3:  D√©veloppement d'applications pratiques

![Auto-√©valuation](../images/banner-evaluation.svg)

Ce QCM vous permettra d'√©valuer votre compr√©hension des frameworks, de l'optimisation et de l'int√©gration des mod√®les de Deep Learning dans des applications concr√®tes.

## Instructions

- Cochez la ou les r√©ponses correctes pour chaque question
- Certaines questions peuvent avoir plusieurs r√©ponses correctes
- √Ä la fin du questionnaire, r√©pondez aux questions √† r√©ponse courte et compl√©tez l'exercice pratique
- Calculez votre score gr√¢ce au corrig√© fourni
- Dur√©e recommand√©e : 20 minutes

## Partie A: QCM : Frameworks et optimisation (15 points)

Pour chaque question, cochez la ou les r√©ponses correctes.

### 1. Parmi ces frameworks de Deep Learning, lequel est le plus adapt√© pour le d√©ploiement en production sur des appareils mobiles ?
- [ ] a) PyTorch
- [ ] b) TensorFlow/Keras
- [ ] c) Scikit-learn
- [ ] d) Theano

### 2. Quels sont les avantages des mod√®les pr√©-entra√Æn√©s ? (plusieurs r√©ponses possibles)
- [ ] a) R√©duction du temps de d√©veloppement
- [ ] b) Besoin de moins de donn√©es d'entra√Ænement
- [ ] c) Poids plus petits que les mod√®les entra√Æn√©s from scratch
- [ ] d) Meilleures performances sur des datasets limit√©s

### 3. La quantification d'un mod√®le consiste √† :
- [ ] a) R√©duire le nombre de couches du mod√®le
- [ ] b) R√©duire la pr√©cision des poids (ex: float32 ‚Üí int8)
- [ ] c) Supprimer les poids proches de z√©ro
- [ ] d) Combiner plusieurs mod√®les ensemble

### 4. Quelle technique d'optimisation consiste √† supprimer les connexions les moins importantes dans un r√©seau ?
- [ ] a) Quantification
- [ ] b) √âlagage (pruning)
- [ ] c) Distillation
- [ ] d) Factorisation matricielle

### 5. Quel format est g√©n√©ralement utilis√© pour d√©ployer des mod√®les sur des appareils mobiles ?
- [ ] a) HDF5
- [ ] b) SavedModel
- [ ] c) TensorFlow Lite
- [ ] d) ONNX

### 6. Dans une API de Deep Learning, quelle technique est recommand√©e pour am√©liorer les performances ?
- [ ] a) Recharger le mod√®le √† chaque requ√™te
- [ ] b) Convertir les images en CSV avant traitement
- [ ] c) Mettre en cache le mod√®le en m√©moire
- [ ] d) D√©sactiver la gestion d'erreurs

### 7. Quelle architecture de mod√®le est sp√©cifiquement con√ßue pour √™tre l√©g√®re et efficace ?
- [ ] a) VGG16
- [ ] b) MobileNetV2
- [ ] c) ResNet152
- [ ] d) InceptionV4

### 8. La distillation de connaissances consiste √† :
- [ ] a) Extraire les poids d'un mod√®le pour les analyser
- [ ] b) Entra√Æner un plus petit mod√®le (√©l√®ve) √† imiter un plus grand mod√®le (enseignant)
- [ ] c) D√©composer un gros mod√®le en plusieurs petits
- [ ] d) Fusionner plusieurs mod√®les sp√©cialis√©s

### 9. Lors de l'int√©gration d'un mod√®le dans une application web, quelle affirmation est correcte ?
- [ ] a) Le mod√®le doit toujours √™tre ex√©cut√© c√¥t√© client (JavaScript)
- [ ] b) Les pr√©dictions doivent √™tre trait√©es de mani√®re synchrone
- [ ] c) Il est recommand√© de pr√©traiter les images c√¥t√© client avant envoi
- [ ] d) L'API du mod√®le ne n√©cessite pas de documentation si elle est utilis√©e en interne

### 10. Parmi ces param√®tres, lequel peut √™tre ajust√© pour rendre les r√©ponses d'un mod√®le de langage plus d√©terministes (moins cr√©atives) ?
- [ ] a) Augmenter la temp√©rature
- [ ] b) Diminuer la temp√©rature
- [ ] c) Augmenter le nombre maximum de tokens
- [ ] d) Activer le mode streaming

## Partie B: Questions √† r√©ponse courte (10 points)

R√©pondez bri√®vement aux questions suivantes (2-3 phrases par question).

### 11. Expliquez pourquoi l'optimisation des mod√®les de Deep Learning est importante dans un contexte professionnel.

...................................................................

...................................................................

### 12. D√©crivez deux diff√©rences principales entre TensorFlow et PyTorch.

...................................................................

...................................................................

### 13. Quels sont les avantages et inconv√©nients de la quantification post-entra√Ænement ?

...................................................................

...................................................................

### 14. Comment l'API Mistral AI peut-elle √™tre utilis√©e pour cr√©er un chatbot p√©dagogique ?

...................................................................

...................................................................

### 15. Quelles sont les deux bonnes pratiques essentielles pour s√©curiser une API de reconnaissance d'images ?

...................................................................

...................................................................

## Partie C: Exercice pratique : Optimisation d'un mod√®le (15 points)

Compl√©tez le code suivant pour optimiser un mod√®le MobileNetV2 avec la quantification TensorFlow Lite.

```python
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
import numpy as np

# Chargement du mod√®le pr√©-entra√Æn√©
base_model = MobileNetV2(weights='imagenet', include_top=True)

# 1. Convertir le mod√®le en TensorFlow Lite
converter = tf.lite.TFLiteConverter.from_keras_model(base_model)
tflite_model = ................................. # √Ä compl√©ter

# 2. Appliquer la quantification post-entra√Ænement
converter = tf.lite.TFLiteConverter.from_keras_model(base_model)
converter.optimizations = ........................... # √Ä compl√©ter
quantized_model = converter.convert()

# 3. Comparer les tailles des mod√®les
original_size = .................. # √Ä compl√©ter : calculer la taille du mod√®le original
tflite_size = len(tflite_model) / (1024 * 1024)  # Taille en Mo
quantized_size = len(quantized_model) / (1024 * 1024)  # Taille en Mo

print(f"Taille du mod√®le original: {original_size:.2f} Mo")
print(f"Taille du mod√®le TFLite: {tflite_size:.2f} Mo")
print(f"Taille du mod√®le quantifi√©: {quantized_size:.2f} Mo")
print(f"R√©duction de taille: {(1 - quantized_size/original_size) * 100:.2f}%")

# 4. Fonction pour pr√©traiter une image pour l'inf√©rence
def preprocess_image(image_path):
    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))
    img_array = .................. # √Ä compl√©ter : convertir l'image en tableau
    img_array = np.expand_dims(img_array, axis=0)
    return tf.keras.applications.mobilenet_v2.preprocess_input(img_array)

# 5. Cr√©ation d'un interpr√©teur TFLite
interpreter = tf.lite.Interpreter(model_content=quantized_model)
....................... # √Ä compl√©ter : allouer les tenseurs

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# 6. Fonction d'inf√©rence avec le mod√®le quantifi√©
def predict_with_tflite(image_path):
    # Pr√©traitement de l'image
    input_data = preprocess_image(image_path)
    
    # D√©finir les donn√©es d'entr√©e
    interpreter.set_tensor(input_details[0]['index'], input_data)
    
    # Ex√©cuter l'inf√©rence
    ...................... # √Ä compl√©ter : invoquer l'interpr√©teur
    
    # Obtenir les r√©sultats
    output_data = interpreter.get_tensor(output_details[0]['index'])
    
    # Traiter les r√©sultats
    results = tf.keras.applications.mobilenet_v2.decode_predictions(output_data)
    return results[0]
```

## Partie D: Exercice de r√©flexion : Cas pratique d'int√©gration (10 points)

Vous √™tes d√©veloppeur dans une petite entreprise qui propose des solutions de reconnaissance d'objets pour le commerce de d√©tail. On vous demande de cr√©er une API qui permettra d'identifier les produits √† partir de photos prises par les employ√©s sur leurs smartphones.

1. Quelle architecture de mod√®le choisiriez-vous et pourquoi ? (2 points)

...................................................................

...................................................................

2. Quelles techniques d'optimisation mettriez-vous en place ? (2 points)

...................................................................

...................................................................

3. Comment structureriez-vous votre API REST ? D√©crivez les endpoints et leurs param√®tres. (3 points)

...................................................................

...................................................................

...................................................................

4. Quelles mesures de s√©curit√© impl√©menteriez-vous ? (3 points)

...................................................................

...................................................................

...................................................................

## Corrig√©

### QCM
1. b) TensorFlow/Keras
2. a) b) d)
3. b) R√©duire la pr√©cision des poids (ex: float32 ‚Üí int8)
4. b) √âlagage (pruning)
5. c) TensorFlow Lite
6. c) Mettre en cache le mod√®le en m√©moire
7. b) MobileNetV2
8. b) Entra√Æner un plus petit mod√®le (√©l√®ve) √† imiter un plus grand mod√®le (enseignant)
9. c) Il est recommand√© de pr√©traiter les images c√¥t√© client avant envoi
10. b) Diminuer la temp√©rature

### Questions √† r√©ponse courte (√©l√©ments attendus)
11. L'optimisation permet de r√©duire les co√ªts d'infrastructure, diminuer la latence pour une meilleure exp√©rience utilisateur, √©conomiser l'√©nergie (crucial pour les appareils mobiles) et rendre les mod√®les accessibles sur des appareils √† ressources limit√©es.

12. TensorFlow utilise des graphes statiques (plus efficaces en production) tandis que PyTorch utilise des graphes dynamiques (plus flexibles pour la recherche). TensorFlow a un √©cosyst√®me plus complet pour le d√©ploiement (TFLite, TF Serving) alors que PyTorch est g√©n√©ralement consid√©r√© comme plus intuitif pour le d√©veloppement.

13. Avantages : R√©duction significative de la taille du mod√®le (jusqu'√† 4x), acc√©l√©ration de l'inf√©rence, pas besoin de r√©entra√Ænement. Inconv√©nients : Perte potentielle de pr√©cision, surtout pour les t√¢ches complexes, incompatibilit√© avec certaines op√©rations avanc√©es.

14. L'API Mistral AI peut √™tre utilis√©e pour cr√©er un chatbot p√©dagogique en envoyant des requ√™tes avec un prompt syst√®me adapt√© √† l'enseignement, en enrichissant les prompts avec une base de connaissances sp√©cifique au domaine enseign√©, et en maintenant un contexte conversationnel pour assurer la coh√©rence des √©changes.

15. Validation des entr√©es (v√©rification du format et de la taille des images), limitation du taux de requ√™tes (rate limiting), authentification par cl√© API, sanitization des chemins de fichiers, restriction des types MIME accept√©s.

### Exercice pratique (√©l√©ments de correction)
```python
# 1. Convertir le mod√®le en TensorFlow Lite
tflite_model = converter.convert()

# 2. Appliquer la quantification post-entra√Ænement
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# 3. Comparer les tailles des mod√®les
original_size = sum(np.prod(w.shape) * w.dtype.size for w in base_model.weights) / (1024 * 1024)

# 4. Fonction pour pr√©traiter une image pour l'inf√©rence
img_array = tf.keras.preprocessing.image.img_to_array(img)

# 5. Cr√©ation d'un interpr√©teur TFLite
interpreter.allocate_tensors()

# 6. Fonction d'inf√©rence avec le mod√®le quantifi√©
interpreter.invoke()
```

### Exercice de r√©flexion
Les r√©ponses peuvent varier, mais devraient inclure des points comme:

1. Architecture: MobileNetV2 ou EfficientNet serait un bon choix car ils offrent un bon √©quilibre entre pr√©cision et performance, sont optimis√©s pour les appareils mobiles, et peuvent √™tre facilement affin√©s pour des t√¢ches sp√©cifiques.

2. Techniques d'optimisation: Quantification post-entra√Ænement pour r√©duire la taille du mod√®le, transfer learning sur un petit dataset de produits sp√©cifiques, et √©ventuellement pruning pour r√©duire davantage la taille.

3. Structure API REST:
   - POST /predict - Pour envoyer une image et recevoir des pr√©dictions
   - GET /categories - Pour r√©cup√©rer la liste des cat√©gories de produits
   - POST /feedback - Pour recueillir les retours sur les pr√©dictions incorrectes
   Param√®tres pour /predict: image (fichier ou base64), top_k (nombre de pr√©dictions), confidence_threshold.

4. S√©curit√©:
   - Authentification par cl√© API
   - Rate limiting pour pr√©venir les abus
   - Validation des entr√©es (taille et format d'image)
   - HTTPS pour le chiffrement des donn√©es
   - Logging s√©curis√© pour l'audit

## Bar√®me et auto-√©valuation

# Calcul de votre score

Partie A : 1 point par r√©ponse correcte = 10 points max
Partie B : 2 points par r√©ponse correcte = 10 points max
Partie C : 15 points pour l'exercice compl√©t√© correctement
Partie D : 10 points pour les r√©ponses pertinentes

# Total des points possibles : 45
Interpr√©tation

35-45 points : Excellente ma√Ætrise des concepts d'int√©gration et d'optimisation des mod√®les
24-35 points : Bonne compr√©hension, certains aspects √† approfondir
16-23 points : Compr√©hension de base, n√©cessite une r√©vision approfondie
0-15 points : R√©vision compl√®te recommand√©e avant de poursuivre

## Questions pour approfondir

Si vous avez obtenu un bon score, vous pouvez explorer ces questions pour aller plus loin :

1. Comment impl√©menteriez-vous un syst√®me de mise √† jour progressive des mod√®les en production ?
2. Quelles strat√©gies pourriez-vous utiliser pour g√©rer les biais potentiels dans un mod√®le de vision par ordinateur ?
3. Comment adapter l'architecture d'une API de Deep Learning pour g√©rer des millions de requ√™tes par jour ?
4. Quelles techniques permettraient d'optimiser les prompts pour un mod√®le de langage au-del√† des exemples vus dans ce module ?

[Retour au Module 3](index.md){ .md-button }
[Continuer vers le Module 4](../module4/index.md){ .md-button .md-button--primary }