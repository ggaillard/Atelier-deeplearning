{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"# CNN pour la classification d'images - MNIST\\n\",\n",
    "        \"\\n\",\n",
    "        \"## BTS SIO SLAM - S√©ance 2: Types de r√©seaux de neurones\\n\",\n",
    "        \"\\n\",\n",
    "        \"Ce notebook vous guidera √† travers l'impl√©mentation et l'utilisation d'un r√©seau de neurones convolutif (CNN) pour la classification d'images, en utilisant le c√©l√®bre dataset MNIST des chiffres manuscrits.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Objectifs d'apprentissage:\\n\",\n",
    "        \"- Comprendre l'architecture et le principe des r√©seaux convolutifs (CNN)\\n\",\n",
    "        \"- Impl√©menter un CNN avec TensorFlow/Keras pour la reconnaissance d'images\\n\",\n",
    "        \"- Visualiser et interpr√©ter les filtres et feature maps\\n\",\n",
    "        \"- Analyser les performances du mod√®le et les cas d'erreur\\n\",\n",
    "        \"- Explorer l'int√©gration d'un mod√®le CNN dans une application web\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Pr√©requis:\\n\",\n",
    "        \"- Connaissances de base en Python\\n\",\n",
    "        \"- Notions fondamentales de r√©seaux de neurones\\n\",\n",
    "        \"- Avoir suivi la s√©ance 1 d'introduction au Deep Learning\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 1. Configuration de l'environnement\\n\",\n",
    "        \"\\n\",\n",
    "        \"Commen√ßons par importer les biblioth√®ques n√©cessaires et configurer notre environnement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"import tensorflow as tf\\n\",\n",
    "        \"from tensorflow.keras.models import Sequential\\n\",\n",
    "        \"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\\n\",\n",
    "        \"from tensorflow.keras.utils import to_categorical\\n\",\n",
    "        \"from tensorflow.keras.datasets import mnist\\n\",\n",
    "        \"import time\\n\",\n",
    "        \"import seaborn as sns\\n\",\n",
    "        \"from sklearn.metrics import confusion_matrix\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Configuration pour reproductibilit√©\\n\",\n",
    "        \"np.random.seed(42)\\n\",\n",
    "        \"tf.random.set_seed(42)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# V√©rifier la version de TensorFlow\\n\",\n",
    "        \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 2. Chargement et pr√©paration du dataset MNIST\\n\",\n",
    "        \"\\n\",\n",
    "        \"Le dataset MNIST contient 70,000 images de chiffres manuscrits de taille 28x28 pixels. Il est divis√© en 60,000 images d'entra√Ænement et 10,000 images de test.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"print(\\\"Chargement des donn√©es MNIST...\\\")\\n\",\n",
    "        \"(X_train, y_train), (X_test, y_test) = mnist.load_data()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher les dimensions des donn√©es\\n\",\n",
    "        \"print(f\\\"Dimensions de X_train: {X_train.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Dimensions de y_train: {y_train.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Dimensions de X_test: {X_test.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Dimensions de y_test: {y_test.shape}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher les premi√®res valeurs de y_train\\n\",\n",
    "        \"print(f\\\"Classes dans y_train: {y_train[:20]}\\\")\\n\",\n",
    "        \"print(f\\\"Nombre de classes: {len(np.unique(y_train))}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Pr√©paration des donn√©es pour le CNN\\n\",\n",
    "        \"\\n\",\n",
    "        \"Pour utiliser nos images avec un CNN, nous devons :\\n\",\n",
    "        \"1. Ajouter une dimension pour le canal (les images sont en niveaux de gris, donc 1 seul canal)\\n\",\n",
    "        \"2. Normaliser les valeurs de pixels entre 0 et 1\\n\",\n",
    "        \"3. Convertir les √©tiquettes en format one-hot encoding (pas toujours n√©cessaire, mais souvent utile)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Redimensionnement et normalisation\\n\",\n",
    "        \"X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\\n\",\n",
    "        \"X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Conversion des √©tiquettes en cat√©gories one-hot\\n\",\n",
    "        \"y_train_onehot = to_categorical(y_train, 10)\\n\",\n",
    "        \"y_test_onehot = to_categorical(y_test, 10)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Nouvelle forme de X_train: {X_train.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Nouvelle forme de y_train_onehot: {y_train_onehot.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Exemple de label encod√© en one-hot: {y_train_onehot[0]} repr√©sente le chiffre {y_train[0]}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation de quelques exemples\\n\",\n",
    "        \"\\n\",\n",
    "        \"Regardons √† quoi ressemblent nos donn√©es.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"plt.figure(figsize=(10, 5))\\n\",\n",
    "        \"for i in range(10):\\n\",\n",
    "        \"    plt.subplot(2, 5, i+1)\\n\",\n",
    "        \"    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    plt.title(f\\\"Chiffre: {y_train[i]}\\\")\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 3. Cr√©ation d'un mod√®le CNN\\n\",\n",
    "        \"\\n\",\n",
    "        \"Nous allons maintenant cr√©er un r√©seau de neurones convolutif. Les CNN sont particuli√®rement adapt√©s au traitement d'images gr√¢ce √† leur capacit√© √† apprendre des caract√©ristiques spatiales hi√©rarchiques.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def create_cnn_model():\\n\",\n",
    "        \"    model = Sequential([\\n\",\n",
    "        \"        # Premi√®re couche de convolution\\n\",\n",
    "        \"        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\\n\",\n",
    "        \"        MaxPooling2D((2, 2), name='pool1'),\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Deuxi√®me couche de convolution\\n\",\n",
    "        \"        Conv2D(64, (3, 3), activation='relu', name='conv2'),\\n\",\n",
    "        \"        MaxPooling2D((2, 2), name='pool2'),\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Aplatissement pour passer aux couches denses\\n\",\n",
    "        \"        Flatten(name='flatten'),\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Couches denses (fully connected)\\n\",\n",
    "        \"        Dense(128, activation='relu', name='dense1'),\\n\",\n",
    "        \"        Dropout(0.5, name='dropout1'),  # R√©gularisation\\n\",\n",
    "        \"        Dense(10, activation='softmax', name='output')  # 10 classes (chiffres 0-9)\\n\",\n",
    "        \"    ])\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Compilation du mod√®le\\n\",\n",
    "        \"    model.compile(\\n\",\n",
    "        \"        optimizer='adam',\\n\",\n",
    "        \"        loss='categorical_crossentropy',  # Pour les √©tiquettes one-hot\\n\",\n",
    "        \"        metrics=['accuracy']\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    return model\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Cr√©er le mod√®le\\n\",\n",
    "        \"model = create_cnn_model()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher le r√©sum√© de l'architecture\\n\",\n",
    "        \"model.summary()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Explication de l'architecture du CNN\\n\",\n",
    "        \"\\n\",\n",
    "        \"Notre architecture CNN comprend :\\n\",\n",
    "        \"\\n\",\n",
    "        \"1. **Couches de convolution (Conv2D)** :\\n\",\n",
    "        \"   - Appliquent des filtres pour d√©tecter des caract√©ristiques (bords, textures, formes)\\n\",\n",
    "        \"   - Premi√®res couches : caract√©ristiques simples (lignes, bords)\\n\",\n",
    "        \"   - Couches profondes : caract√©ristiques complexes (formes, parties de chiffres)\\n\",\n",
    "        \"\\n\",\n",
    "        \"2. **Couches de pooling (MaxPooling2D)** :\\n\",\n",
    "        \"   - R√©duisent la dimension spatiale (downsampling)\\n\",\n",
    "        \"   - Rendent le mod√®le plus robuste aux variations de position\\n\",\n",
    "        \"   - Diminuent le nombre de param√®tres\\n\",\n",
    "        \"\\n\",\n",
    "        \"3. **Flatten** :\\n\",\n",
    "        \"   - Convertit les feature maps 2D en un vecteur 1D\\n\",\n",
    "        \"   - N√©cessaire pour passer aux couches denses\\n\",\n",
    "        \"\\n\",\n",
    "        \"4. **Couches denses (Dense)** :\\n\",\n",
    "        \"   - Combinent toutes les caract√©ristiques extraites\\n\",\n",
    "        \"   - Effectuent la classification finale\\n\",\n",
    "        \"\\n\",\n",
    "        \"5. **Dropout** :\\n\",\n",
    "        \"   - Technique de r√©gularisation\\n\",\n",
    "        \"   - D√©sactive al√©atoirement 50% des neurones pendant l'entra√Ænement\\n\",\n",
    "        \"   - Pr√©vient le surapprentissage\\n\",\n",
    "        \"\\n\",\n",
    "        \"Cette architecture est similaire au c√©l√®bre LeNet-5, mais avec plus de filtres et l'ajout de Dropout.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 4. Entra√Ænement du mod√®le\\n\",\n",
    "        \"\\n\",\n",
    "        \"Entra√Ænons maintenant notre mod√®le CNN sur les donn√©es MNIST.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Mesure du temps d'entra√Ænement\\n\",\n",
    "        \"start_time = time.time()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Entra√Ænement du mod√®le\\n\",\n",
    "        \"history = model.fit(\\n\",\n",
    "        \"    X_train, \\n\",\n",
    "        \"    y_train_onehot, \\n\",\n",
    "        \"    batch_size=128, \\n\",\n",
    "        \"    epochs=5,  # Nombre r√©duit d'√©poques pour la d√©monstration\\n\",\n",
    "        \"    validation_split=0.2,  # 20% des donn√©es d'entra√Ænement pour la validation\\n\",\n",
    "        \"    verbose=1\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"training_time = time.time() - start_time\\n\",\n",
    "        \"print(f\\\"\\\\nTemps d'entra√Ænement: {training_time:.2f} secondes\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation de l'√©volution de l'entra√Ænement\\n\",\n",
    "        \"\\n\",\n",
    "        \"Observons comment la pr√©cision et la perte ont √©volu√© au cours de l'entra√Ænement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"plt.figure(figsize=(12, 4))\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Graphique de pr√©cision\\n\",\n",
    "        \"plt.subplot(1, 2, 1)\\n\",\n",
    "        \"plt.plot(history.history['accuracy'], label='Entra√Ænement')\\n\",\n",
    "        \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",\n",
    "        \"plt.title('√âvolution de la pr√©cision')\\n\",\n",
    "        \"plt.xlabel('√âpoque')\\n\",\n",
    "        \"plt.ylabel('Pr√©cision')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Graphique de perte\\n\",\n",
    "        \"plt.subplot(1, 2, 2)\\n\",\n",
    "        \"plt.plot(history.history['loss'], label='Entra√Ænement')\\n\",\n",
    "        \"plt.plot(history.history['val_loss'], label='Validation')\\n\",\n",
    "        \"plt.title('√âvolution de la perte')\\n\",\n",
    "        \"plt.xlabel('√âpoque')\\n\",\n",
    "        \"plt.ylabel('Perte')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 5. √âvaluation du mod√®le\\n\",\n",
    "        \"\\n\",\n",
    "        \"√âvaluons maintenant notre mod√®le sur l'ensemble de test.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# √âvaluation sur l'ensemble de test\\n\",\n",
    "        \"test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)\\n\",\n",
    "        \"print(f\\\"Pr√©cision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Pr√©dictions\\n\",\n",
    "        \"y_pred = model.predict(X_test)\\n\",\n",
    "        \"y_pred_classes = np.argmax(y_pred, axis=1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Matrice de confusion\\n\",\n",
    "        \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",\n",
    "        \"plt.figure(figsize=(10, 8))\\n\",\n",
    "        \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\\n\",\n",
    "        \"plt.xlabel('Pr√©dit')\\n\",\n",
    "        \"plt.ylabel('R√©el')\\n\",\n",
    "        \"plt.title('Matrice de confusion')\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation des exemples mal classifi√©s\\n\",\n",
    "        \"\\n\",\n",
    "        \"Explorons quelques exemples que notre mod√®le a mal classifi√©s pour comprendre ses faiblesses.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Identifier les erreurs\\n\",\n",
    "        \"misclassified_indices = np.where(y_pred_classes != y_test)[0]\\n\",\n",
    "        \"misclassified_count = len(misclassified_indices)\\n\",\n",
    "        \"print(f\\\"Nombre total d'erreurs: {misclassified_count} sur {len(y_test)} images de test\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher quelques exemples mal classifi√©s\\n\",\n",
    "        \"num_examples = min(10, misclassified_count)\\n\",\n",
    "        \"plt.figure(figsize=(15, 6))\\n\",\n",
    "        \"\\n\",\n",
    "        \"for i, idx in enumerate(misclassified_indices[:num_examples]):\\n\",\n",
    "        \"    plt.subplot(2, 5, i+1)\\n\",\n",
    "        \"    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    plt.title(f\\\"R√©el: {y_test[idx]}\\\\nPr√©dit: {y_pred_classes[idx]}\\\")\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### üß† R√©flexion sur les erreurs\\n\",\n",
    "        \"\\n\",\n",
    "        \"**Question**: En observant les exemples mal classifi√©s, quelles pourraient √™tre les raisons de ces erreurs? Notez vos observations et hypoth√®ses ci-dessous.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"*√âcrivez vos observations ici...*\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 6. Visualisation des filtres et feature maps\\n\",\n",
    "        \"\\n\",\n",
    "        \"Une des grandes forces des CNNs est leur interpr√©tabilit√© visuelle. Explorons ce que le r√©seau \\\"voit\\\" r√©ellement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Fonction pour visualiser les filtres de convolution\\n\",\n",
    "        \"def visualize_filters(model, layer_name, num_filters=8):\\n\",\n",
    "        \"    \\\"\\\"\\\"Visualise les filtres d'une couche de convolution\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # R√©cup√©rer les poids du filtre de la couche sp√©cifi√©e\\n\",\n",
    "        \"    filters, biases = model.get_layer(layer_name).get_weights()\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Normaliser les filtres pour une meilleure visualisation\\n\",\n",
    "        \"    f_min, f_max = filters.min(), filters.max()\\n\",\n",
    "        \"    filters = (filters - f_min) / (f_max - f_min)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Afficher les premiers filtres\\n\",\n",
    "        \"    plt.figure(figsize=(12, 4))\\n\",\n",
    "        \"    for i in range(num_filters):\\n\",\n",
    "        \"        plt.subplot(2, 4, i+1)\\n\",\n",
    "        \"        # Pour la premi√®re couche de convolution, les filtres sont 3D (hauteur, largeur, canaux)\\n\",\n",
    "        \"        # Nous affichons le filtre pour le premier canal (0)\\n\",\n",
    "        \"        plt.imshow(filters[:, :, 0, i], cmap='viridis')\\n\",\n",
    "        \"        plt.title(f'Filtre {i+1}')\\n\",\n",
    "        \"        plt.axis('off')\\n\",\n",
    "        \"    plt.suptitle(f'Filtres de la couche {layer_name}')\\n\",\n",
    "        \"    plt.tight_layout()\\n\",\n",
    "        \"    plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Visualiser les filtres de la premi√®re couche de convolution\\n\",\n",
    "        \"visualize_filters(model, 'conv1')\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation des feature maps (cartes d'activation)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def visualize_feature_maps(model, image, layer_name, num_features=8):\\n\",\n",
    "        \"    \\\"\\\"\\\"Visualise les feature maps (activations) d'une couche pour une image donn√©e\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Cr√©er un mod√®le qui renvoie les activations de la couche sp√©cifi√©e\\n\",\n",
    "        \"    layer_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Obtenir les activations pour une image\\n\",\n",
    "        \"    feature_maps = layer_model.predict(image.reshape(1, 28, 28, 1))\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Afficher les premi√®res cartes d'activation\\n\",\n",
    "        \"    plt.figure(figsize=(12, 4))\\n\",\n",
    "        \"    for i in range(min(num_features, feature_maps.shape[3])):\\n\",\n",
    "        \"        plt.subplot(2, 4, i+1)\\n\",\n",
    "        \"        plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\\n\",\n",
    "        \"        plt.title(f'Feature {i+1}')\\n\",\n",
    "        \"        plt.axis('off')\\n\",\n",
    "        \"    plt.suptitle(f'Feature Maps de la couche {layer_name}')\\n\",\n",
    "        \"    plt.tight_layout()\\n\",\n",
    "        \"    plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Choisir une image de test\\n\",\n",
    "        \"sample_idx = 12  # Vous pouvez essayer avec diff√©rents indices\\n\",\n",
    "        \"sample_image = X_test[sample_idx]\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher l'image originale\\n\",\n",
    "        \"plt.figure(figsize=(3, 3))\\n\",\n",
    "        \"plt.imshow(sample_image.reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"plt.title(f\\\"Chiffre: {y_test[sample_idx]}\\\")\\n\",\n",
    "        \"plt.axis('off')\\n\",\n",
    "        \"plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Visualiser les feature maps pour chaque couche de convolution\\n\",\n",
    "        \"print(\\\"Feature maps de la premi√®re couche de convolution:\\\")\\n\",\n",
    "        \"visualize_feature_maps(model, sample_image, 'conv1')\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Feature maps de la deuxi√®me couche de convolution:\\\")\\n\",\n",
    "        \"visualize_feature_maps(model, sample_image, 'conv2')\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### üí° Interpr√©tation des feature maps\\n\",\n",
    "        \"\\n\",\n",
    "        \"Les feature maps nous montrent ce que \\\"voit\\\" chaque filtre de convolution :\\n\",\n",
    "        \"\\n\",\n",
    "        \"- **Premi√®re couche** : D√©tecte principalement des caract√©ristiques de base comme les bords et les contours\\n\",\n",
    "        \"- **Deuxi√®me couche** : Combine ces caract√©ristiques de base pour d√©tecter des formes plus complexes\\n\",\n",
    "        \"\\n\",\n",
    "        \"Cette hi√©rarchie de repr√©sentations est ce qui rend les CNNs si puissants pour la vision par ordinateur.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 7. Test du mod√®le avec des images bruit√©es\\n\",\n",
    "        \"\\n\",\n",
    "        \"Testons la robustesse de notre mod√®le face √† des perturbations.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Fonction pour ajouter du bruit aux images\\n\",\n",
    "        \"def add_noise(images, noise_level=0.2):\\n\",\n",
    "        \"    \\\"\\\"\\\"Ajoute du bruit gaussien aux images\\\"\\\"\\\"\\n\",\n",
    "        \"    noisy_images = images.copy()\\n\",\n",
    "        \"    noise = np.random.normal(0, noise_level, images.shape)\\n\",\n",
    "        \"    noisy_images = noisy_images + noise\\n\",\n",
    "        \"    # Assurer que les valeurs restent entre 0 et 1\\n\",\n",
    "        \"    return np.clip(noisy_images, 0, 1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Cr√©er des versions bruit√©es de quelques images de test\\n\",\n",
    "        \"num_test_images = 10\\n\",\n",
    "        \"test_samples = X_test[:num_test_images]\\n\",\n",
    "        \"noisy_samples = add_noise(test_samples, noise_level=0.3)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Visualiser les images originales et bruit√©es\\n\",\n",
    "        \"plt.figure(figsize=(15, 6))\\n\",\n",
    "        \"for i in range(num_test_images):\\n\",\n",
    "        \"    # Image originale\\n\",\n",
    "        \"    plt.subplot(2, num_test_images, i+1)\\n\",\n",
    "        \"    plt.imshow(test_samples[i].reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    plt.title(f\\\"Original: {y_test[i]}\\\")\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Image bruit√©e\\n\",\n",
    "        \"    plt.subplot(2, num_test_images, i+num_test_images+1)\\n\",\n",
    "        \"    plt.imshow(noisy_samples[i].reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Pr√©dire sur les images bruit√©es\\n\",\n",
    "        \"noisy_predictions = model.predict(noisy_samples)\\n\",\n",
    "        \"noisy_pred_classes = np.argmax(noisy_predictions, axis=1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher les r√©sultats\\n\",\n",
    "        \"print(\\\"R√©sultats des pr√©dictions sur les images bruit√©es:\\\")\\n\",\n",
    "        \"for i in range(num_test_images):\\n\",\n",
    "        \"    confidence = noisy_predictions[i][noisy_pred_classes[i]] * 100\\n\",\n",
    "        \"    status = \\\"‚úì\\\" if noisy_pred_classes[i] == y_test[i] else \\\"‚úó\\\"\\n\",\n",
    "        \"    print(f\\\"Image {i+1} - R√©el: {y_test[i]}, Pr√©dit: {noisy_pred_classes[i]}, \\\"  \\n\",\n",
    "        \"          f\\\"Confiance: {confidence:.1f}% {status}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Calculer la pr√©cision sur les images bruit√©es\\n\",\n",
    "        \"accuracy_on_noisy = np.mean(noisy_pred_classes == y_test[:num_test_images]) * 100\\n\",\n",
    "        \"print(f\\\"\\\\nPr√©cision sur les images bruit√©es: {accuracy_on_noisy:.1f}%\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 8. Pr√©dictions d√©taill√©es sur de nouvelles images\\n\",\n",
    "        \"\\n\",\n",
    "        \"Voyons comment notre mod√®le pr√©dit des images sp√©cifiques et quelles sont les probabilit√©s pour chaque classe.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def predict_and_visualize_probabilities(model, image, true_label=None):\\n\",\n",
    "        \"    \\\"\\\"\\\"Pr√©dit et visualise les probabilit√©s pour chaque classe\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Pr√©diction\\n\",\n",
    "        \"    prediction = model.predict(image.reshape(1, 28, 28, 1))[0]\\n\",\n",
    "        \"    predicted_class = np.argmax(prediction)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Visualisation\\n\",\n",
    "        \"    plt.figure(figsize=(12, 4))\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Afficher l'image\\n\",\n",
    "        \"    plt.subplot(1, 2, 1)\\n\",\n",
    "        \"    plt.imshow(image.reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    if true_label is not None:\\n\",\n",
    "        \"        title = f\\\"Image (Vrai: {true_label})\\\"\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        title = \\\"Image\\\"\\n\",\n",
    "        \"    plt.title(title)\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Afficher les probabilit√©s\\n\",\n",
    "        \"    plt.subplot(1, 2, 2)\\n\",\n",
    "        \"    bars = plt.bar(range(10), prediction, color='skyblue')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Mettre en √©vidence la classe pr√©dite\\n\",\n",
    "        \"    bars[predicted_class].set_color('red')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    plt.xticks(range(10))\\n\",\n",
    "        \"    plt.xlabel('Chiffre')\\n\",\n",
    "        \"    plt.ylabel('"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
