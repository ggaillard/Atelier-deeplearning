{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"# CNN pour la classification d'images - MNIST\\n\",\n",
    "        \"\\n\",\n",
    "        \"## BTS SIO SLAM - Séance 2: Types de réseaux de neurones\\n\",\n",
    "        \"\\n\",\n",
    "        \"Ce notebook vous guidera à travers l'implémentation et l'utilisation d'un réseau de neurones convolutif (CNN) pour la classification d'images, en utilisant le célèbre dataset MNIST des chiffres manuscrits.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Objectifs d'apprentissage:\\n\",\n",
    "        \"- Comprendre l'architecture et le principe des réseaux convolutifs (CNN)\\n\",\n",
    "        \"- Implémenter un CNN avec TensorFlow/Keras pour la reconnaissance d'images\\n\",\n",
    "        \"- Visualiser et interpréter les filtres et feature maps\\n\",\n",
    "        \"- Analyser les performances du modèle et les cas d'erreur\\n\",\n",
    "        \"- Explorer l'intégration d'un modèle CNN dans une application web\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Prérequis:\\n\",\n",
    "        \"- Connaissances de base en Python\\n\",\n",
    "        \"- Notions fondamentales de réseaux de neurones\\n\",\n",
    "        \"- Avoir suivi la séance 1 d'introduction au Deep Learning\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 1. Configuration de l'environnement\\n\",\n",
    "        \"\\n\",\n",
    "        \"Commençons par importer les bibliothèques nécessaires et configurer notre environnement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"import tensorflow as tf\\n\",\n",
    "        \"from tensorflow.keras.models import Sequential\\n\",\n",
    "        \"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\\n\",\n",
    "        \"from tensorflow.keras.utils import to_categorical\\n\",\n",
    "        \"from tensorflow.keras.datasets import mnist\\n\",\n",
    "        \"import time\\n\",\n",
    "        \"import seaborn as sns\\n\",\n",
    "        \"from sklearn.metrics import confusion_matrix\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Configuration pour reproductibilité\\n\",\n",
    "        \"np.random.seed(42)\\n\",\n",
    "        \"tf.random.set_seed(42)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Vérifier la version de TensorFlow\\n\",\n",
    "        \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 2. Chargement et préparation du dataset MNIST\\n\",\n",
    "        \"\\n\",\n",
    "        \"Le dataset MNIST contient 70,000 images de chiffres manuscrits de taille 28x28 pixels. Il est divisé en 60,000 images d'entraînement et 10,000 images de test.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"print(\\\"Chargement des données MNIST...\\\")\\n\",\n",
    "        \"(X_train, y_train), (X_test, y_test) = mnist.load_data()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher les dimensions des données\\n\",\n",
    "        \"print(f\\\"Dimensions de X_train: {X_train.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Dimensions de y_train: {y_train.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Dimensions de X_test: {X_test.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Dimensions de y_test: {y_test.shape}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher les premières valeurs de y_train\\n\",\n",
    "        \"print(f\\\"Classes dans y_train: {y_train[:20]}\\\")\\n\",\n",
    "        \"print(f\\\"Nombre de classes: {len(np.unique(y_train))}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Préparation des données pour le CNN\\n\",\n",
    "        \"\\n\",\n",
    "        \"Pour utiliser nos images avec un CNN, nous devons :\\n\",\n",
    "        \"1. Ajouter une dimension pour le canal (les images sont en niveaux de gris, donc 1 seul canal)\\n\",\n",
    "        \"2. Normaliser les valeurs de pixels entre 0 et 1\\n\",\n",
    "        \"3. Convertir les étiquettes en format one-hot encoding (pas toujours nécessaire, mais souvent utile)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Redimensionnement et normalisation\\n\",\n",
    "        \"X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\\n\",\n",
    "        \"X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Conversion des étiquettes en catégories one-hot\\n\",\n",
    "        \"y_train_onehot = to_categorical(y_train, 10)\\n\",\n",
    "        \"y_test_onehot = to_categorical(y_test, 10)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Nouvelle forme de X_train: {X_train.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Nouvelle forme de y_train_onehot: {y_train_onehot.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Exemple de label encodé en one-hot: {y_train_onehot[0]} représente le chiffre {y_train[0]}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation de quelques exemples\\n\",\n",
    "        \"\\n\",\n",
    "        \"Regardons à quoi ressemblent nos données.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"plt.figure(figsize=(10, 5))\\n\",\n",
    "        \"for i in range(10):\\n\",\n",
    "        \"    plt.subplot(2, 5, i+1)\\n\",\n",
    "        \"    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    plt.title(f\\\"Chiffre: {y_train[i]}\\\")\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 3. Création d'un modèle CNN\\n\",\n",
    "        \"\\n\",\n",
    "        \"Nous allons maintenant créer un réseau de neurones convolutif. Les CNN sont particulièrement adaptés au traitement d'images grâce à leur capacité à apprendre des caractéristiques spatiales hiérarchiques.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def create_cnn_model():\\n\",\n",
    "        \"    model = Sequential([\\n\",\n",
    "        \"        # Première couche de convolution\\n\",\n",
    "        \"        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\\n\",\n",
    "        \"        MaxPooling2D((2, 2), name='pool1'),\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Deuxième couche de convolution\\n\",\n",
    "        \"        Conv2D(64, (3, 3), activation='relu', name='conv2'),\\n\",\n",
    "        \"        MaxPooling2D((2, 2), name='pool2'),\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Aplatissement pour passer aux couches denses\\n\",\n",
    "        \"        Flatten(name='flatten'),\\n\",\n",
    "        \"        \\n\",\n",
    "        \"        # Couches denses (fully connected)\\n\",\n",
    "        \"        Dense(128, activation='relu', name='dense1'),\\n\",\n",
    "        \"        Dropout(0.5, name='dropout1'),  # Régularisation\\n\",\n",
    "        \"        Dense(10, activation='softmax', name='output')  # 10 classes (chiffres 0-9)\\n\",\n",
    "        \"    ])\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Compilation du modèle\\n\",\n",
    "        \"    model.compile(\\n\",\n",
    "        \"        optimizer='adam',\\n\",\n",
    "        \"        loss='categorical_crossentropy',  # Pour les étiquettes one-hot\\n\",\n",
    "        \"        metrics=['accuracy']\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    return model\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Créer le modèle\\n\",\n",
    "        \"model = create_cnn_model()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher le résumé de l'architecture\\n\",\n",
    "        \"model.summary()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Explication de l'architecture du CNN\\n\",\n",
    "        \"\\n\",\n",
    "        \"Notre architecture CNN comprend :\\n\",\n",
    "        \"\\n\",\n",
    "        \"1. **Couches de convolution (Conv2D)** :\\n\",\n",
    "        \"   - Appliquent des filtres qui glissent sur l'image pour détecter des caractéristiques (bords, textures, formes)\\n\",\n",
    "        \"   - Premières couches : caractéristiques simples (lignes, bords)\\n\",\n",
    "        \"   - Couches profondes : caractéristiques complexes (formes, parties de chiffres)\\n\",\n",
    "        \"\\n\",\n",
    "        \"2. **Couches de pooling (MaxPooling2D)** :\\n\",\n",
    "        \"   - Réduisent la dimension spatiale (downsampling)\\n\",\n",
    "        \"   - Rendent le modèle plus robuste aux variations de position\\n\",\n",
    "        \"   - Diminuent le nombre de paramètres\\n\",\n",
    "        \"\\n\",\n",
    "        \"3. **Flatten** :\\n\",\n",
    "        \"   - Convertit les feature maps 2D en un vecteur 1D\\n\",\n",
    "        \"   - Nécessaire pour passer aux couches denses\\n\",\n",
    "        \"\\n\",\n",
    "        \"4. **Couches denses (Dense)** :\\n\",\n",
    "        \"   - Combinent toutes les caractéristiques extraites\\n\",\n",
    "        \"   - Effectuent la classification finale\\n\",\n",
    "        \"\\n\",\n",
    "        \"5. **Dropout** :\\n\",\n",
    "        \"   - Technique de régularisation\\n\",\n",
    "        \"   - Désactive aléatoirement 50% des neurones pendant l'entraînement\\n\",\n",
    "        \"   - Prévient le surapprentissage\\n\",\n",
    "        \"\\n\",\n",
    "        \"Cette architecture est similaire au célèbre LeNet-5, mais avec plus de filtres et l'ajout de Dropout.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 4. Entraînement du modèle\\n\",\n",
    "        \"\\n\",\n",
    "        \"Entraînons maintenant notre modèle CNN sur les données MNIST.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Mesure du temps d'entraînement\\n\",\n",
    "        \"start_time = time.time()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Entraînement du modèle\\n\",\n",
    "        \"history = model.fit(\\n\",\n",
    "        \"    X_train, \\n\",\n",
    "        \"    y_train_onehot, \\n\",\n",
    "        \"    batch_size=128, \\n\",\n",
    "        \"    epochs=5,  # Nombre réduit d'époques pour la démonstration\\n\",\n",
    "        \"    validation_split=0.2,  # 20% des données d'entraînement pour la validation\\n\",\n",
    "        \"    verbose=1\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"training_time = time.time() - start_time\\n\",\n",
    "        \"print(f\\\"\\\\nTemps d'entraînement: {training_time:.2f} secondes\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation de l'évolution de l'entraînement\\n\",\n",
    "        \"\\n\",\n",
    "        \"Observons comment la précision et la perte ont évolué au cours de l'entraînement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"plt.figure(figsize=(12, 4))\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Graphique de précision\\n\",\n",
    "        \"plt.subplot(1, 2, 1)\\n\",\n",
    "        \"plt.plot(history.history['accuracy'], label='Entraînement')\\n\",\n",
    "        \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",\n",
    "        \"plt.title('Évolution de la précision')\\n\",\n",
    "        \"plt.xlabel('Époque')\\n\",\n",
    "        \"plt.ylabel('Précision')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Graphique de perte\\n\",\n",
    "        \"plt.subplot(1, 2, 2)\\n\",\n",
    "        \"plt.plot(history.history['loss'], label='Entraînement')\\n\",\n",
    "        \"plt.plot(history.history['val_loss'], label='Validation')\\n\",\n",
    "        \"plt.title('Évolution de la perte')\\n\",\n",
    "        \"plt.xlabel('Époque')\\n\",\n",
    "        \"plt.ylabel('Perte')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 5. Évaluation du modèle\\n\",\n",
    "        \"\\n\",\n",
    "        \"Évaluons maintenant notre modèle sur l'ensemble de test.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Évaluation sur l'ensemble de test\\n\",\n",
    "        \"test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)\\n\",\n",
    "        \"print(f\\\"Précision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Prédictions\\n\",\n",
    "        \"y_pred = model.predict(X_test)\\n\",\n",
    "        \"y_pred_classes = np.argmax(y_pred, axis=1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Matrice de confusion\\n\",\n",
    "        \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",\n",
    "        \"plt.figure(figsize=(10, 8))\\n\",\n",
    "        \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\\n\",\n",
    "        \"plt.xlabel('Prédit')\\n\",\n",
    "        \"plt.ylabel('Réel')\\n\",\n",
    "        \"plt.title('Matrice de confusion')\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation des exemples mal classifiés\\n\",\n",
    "        \"\\n\",\n",
    "        \"Explorons quelques exemples que notre modèle a mal classifiés pour comprendre ses faiblesses.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Identifier les erreurs\\n\",\n",
    "        \"misclassified_indices = np.where(y_pred_classes != y_test)[0]\\n\",\n",
    "        \"misclassified_count = len(misclassified_indices)\\n\",\n",
    "        \"print(f\\\"Nombre total d'erreurs: {misclassified_count} sur {len(y_test)} images de test\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher quelques exemples mal classifiés\\n\",\n",
    "        \"num_examples = min(10, misclassified_count)\\n\",\n",
    "        \"plt.figure(figsize=(15, 6))\\n\",\n",
    "        \"\\n\",\n",
    "        \"for i, idx in enumerate(misclassified_indices[:num_examples]):\\n\",\n",
    "        \"    plt.subplot(2, 5, i+1)\\n\",\n",
    "        \"    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    plt.title(f\\\"Réel: {y_test[idx]}\\\\nPrédit: {y_pred_classes[idx]}\\\")\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 🧠 Réflexion sur les erreurs\\n\",\n",
    "        \"\\n\",\n",
    "        \"**Question**: En observant les exemples mal classifiés, quelles pourraient être les raisons de ces erreurs? Notez vos observations et hypothèses ci-dessous.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"*Écrivez vos observations ici...*\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 6. Visualisation des filtres et feature maps\\n\",\n",
    "        \"\\n\",\n",
    "        \"Une des grandes forces des CNNs est leur interprétabilité visuelle. Explorons ce que le réseau \\\"voit\\\" réellement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Fonction pour visualiser les filtres de convolution\\n\",\n",
    "        \"def visualize_filters(model, layer_name, num_filters=8):\\n\",\n",
    "        \"    \\\"\\\"\\\"Visualise les filtres d'une couche de convolution\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Récupérer les poids du filtre de la couche spécifiée\\n\",\n",
    "        \"    filters, biases = model.get_layer(layer_name).get_weights()\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Normaliser les filtres pour une meilleure visualisation\\n\",\n",
    "        \"    f_min, f_max = filters.min(), filters.max()\\n\",\n",
    "        \"    filters = (filters - f_min) / (f_max - f_min)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Afficher les premiers filtres\\n\",\n",
    "        \"    plt.figure(figsize=(12, 4))\\n\",\n",
    "        \"    for i in range(num_filters):\\n\",\n",
    "        \"        plt.subplot(2, 4, i+1)\\n\",\n",
    "        \"        # Pour la première couche de convolution, les filtres sont 3D (hauteur, largeur, canaux)\\n\",\n",
    "        \"        # Nous affichons le filtre pour le premier canal (0)\\n\",\n",
    "        \"        plt.imshow(filters[:, :, 0, i], cmap='viridis')\\n\",\n",
    "        \"        plt.title(f'Filtre {i+1}')\\n\",\n",
    "        \"        plt.axis('off')\\n\",\n",
    "        \"    plt.suptitle(f'Filtres de la couche {layer_name}')\\n\",\n",
    "        \"    plt.tight_layout()\\n\",\n",
    "        \"    plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Visualiser les filtres de la première couche de convolution\\n\",\n",
    "        \"visualize_filters(model, 'conv1')\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation des feature maps (cartes d'activation)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def visualize_feature_maps(model, image, layer_name, num_features=8):\\n\",\n",
    "        \"    \\\"\\\"\\\"Visualise les feature maps (activations) d'une couche pour une image donnée\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Créer un modèle qui renvoie les activations de la couche spécifiée\\n\",\n",
    "        \"    layer_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Obtenir les activations pour une image\\n\",\n",
    "        \"    feature_maps = layer_model.predict(image.reshape(1, 28, 28, 1))\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Afficher les premières cartes d'activation\\n\",\n",
    "        \"    plt.figure(figsize=(12, 4))\\n\",\n",
    "        \"    for i in range(min(num_features, feature_maps.shape[3])):\\n\",\n",
    "        \"        plt.subplot(2, 4, i+1)\\n\",\n",
    "        \"        plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\\n\",\n",
    "        \"        plt.title(f'Feature {i+1}')\\n\",\n",
    "        \"        plt.axis('off')\\n\",\n",
    "        \"    plt.suptitle(f'Feature Maps de la couche {layer_name}')\\n\",\n",
    "        \"    plt.tight_layout()\\n\",\n",
    "        \"    plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Choisir une image de test\\n\",\n",
    "        \"sample_idx = 12  # Vous pouvez essayer avec différents indices\\n\",\n",
    "        \"sample_image = X_test[sample_idx]\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher l'image originale\\n\",\n",
    "        \"plt.figure(figsize=(3, 3))\\n\",\n",
    "        \"plt.imshow(sample_image.reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"plt.title(f\\\"Chiffre: {y_test[sample_idx]}\\\")\\n\",\n",
    "        \"plt.axis('off')\\n\",\n",
    "        \"plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Visualiser les feature maps pour chaque couche de convolution\\n\",\n",
    "        \"print(\\\"Feature maps de la première couche de convolution:\\\")\\n\",\n",
    "        \"visualize_feature_maps(model, sample_image, 'conv1')\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Feature maps de la deuxième couche de convolution:\\\")\\n\",\n",
    "        \"visualize_feature_maps(model, sample_image, 'conv2')\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 💡 Interprétation des feature maps\\n\",\n",
    "        \"\\n\",\n",
    "        \"Les feature maps nous montrent ce que \\\"voit\\\" chaque filtre de convolution :\\n\",\n",
    "        \"\\n\",\n",
    "        \"- **Première couche** : Détecte principalement des caractéristiques de base comme les bords et les contours\\n\",\n",
    "        \"- **Deuxième couche** : Combine ces caractéristiques de base pour détecter des formes plus complexes\\n\",\n",
    "        \"\\n\",\n",
    "        \"Cette hiérarchie de représentations est ce qui rend les CNNs si puissants pour la vision par ordinateur.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 7. Test du modèle avec des images bruitées\\n\",\n",
    "        \"\\n\",\n",
    "        \"Testons la robustesse de notre modèle face à des perturbations.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Fonction pour ajouter du bruit aux images\\n\",\n",
    "        \"def add_noise(images, noise_level=0.2):\\n\",\n",
    "        \"    \\\"\\\"\\\"Ajoute du bruit gaussien aux images\\\"\\\"\\\"\\n\",\n",
    "        \"    noisy_images = images.copy()\\n\",\n",
    "        \"    noise = np.random.normal(0, noise_level, images.shape)\\n\",\n",
    "        \"    noisy_images = noisy_images + noise\\n\",\n",
    "        \"    # Assurer que les valeurs restent entre 0 et 1\\n\",\n",
    "        \"    return np.clip(noisy_images, 0, 1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Créer des versions bruitées de quelques images de test\\n\",\n",
    "        \"num_test_images = 10\\n\",\n",
    "        \"test_samples = X_test[:num_test_images]\\n\",\n",
    "        \"noisy_samples = add_noise(test_samples, noise_level=0.3)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Visualiser les images originales et bruitées\\n\",\n",
    "        \"plt.figure(figsize=(15, 6))\\n\",\n",
    "        \"for i in range(num_test_images):\\n\",\n",
    "        \"    # Image originale\\n\",\n",
    "        \"    plt.subplot(2, num_test_images, i+1)\\n\",\n",
    "        \"    plt.imshow(test_samples[i].reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    plt.title(f\\\"Original: {y_test[i]}\\\")\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Image bruitée\\n\",\n",
    "        \"    plt.subplot(2, num_test_images, i+num_test_images+1)\\n\",\n",
    "        \"    plt.imshow(noisy_samples[i].reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Prédire sur les images bruitées\\n\",\n",
    "        \"noisy_predictions = model.predict(noisy_samples)\\n\",\n",
    "        \"noisy_pred_classes = np.argmax(noisy_predictions, axis=1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher les résultats\\n\",\n",
    "        \"print(\\\"Résultats des prédictions sur les images bruitées:\\\")\\n\",\n",
    "        \"for i in range(num_test_images):\\n\",\n",
    "        \"    confidence = noisy_predictions[i][noisy_pred_classes[i]] * 100\\n\",\n",
    "        \"    status = \\\"✓\\\" if noisy_pred_classes[i] == y_test[i] else \\\"✗\\\"\\n\",\n",
    "        \"    print(f\\\"Image {i+1} - Réel: {y_test[i]}, Prédit: {noisy_pred_classes[i]}, \\\"  \\n\",\n",
    "        \"          f\\\"Confiance: {confidence:.1f}% {status}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Calculer la précision sur les images bruitées\\n\",\n",
    "        \"accuracy_on_noisy = np.mean(noisy_pred_classes == y_test[:num_test_images]) * 100\\n\",\n",
    "        \"print(f\\\"\\\\nPrécision sur les images bruitées: {accuracy_on_noisy:.1f}%\\\")\n",
    "\n",
    "# Comparaison avec la précision sur les données originales\n",
    "original_accuracy = np.mean(y_pred_classes[:num_test_images] == y_test[:num_test_images]) * 100\n",
    "print(f\\\"Précision sur les images originales (pour le même sous-ensemble): {original_accuracy:.1f}%\\\")\n",
    "print(f\\\"Baisse de précision due au bruit: {original_accuracy - accuracy_on_noisy:.1f} points de pourcentage\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 8. Prédictions détaillées sur de nouvelles images\\n\",\n",
    "        \"\\n\",\n",
    "        \"Voyons comment notre modèle prédit des images spécifiques et quelles sont les probabilités pour chaque classe.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def predict_and_visualize_probabilities(model, image, true_label=None):\\n\",\n",
    "        \"    \\\"\\\"\\\"Prédit et visualise les probabilités pour chaque classe\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Prédiction\\n\",\n",
    "        \"    prediction = model.predict(image.reshape(1, 28, 28, 1))[0]\\n\",\n",
    "        \"    predicted_class = np.argmax(prediction)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Visualisation\\n\",\n",
    "        \"    plt.figure(figsize=(12, 4))\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Afficher l'image\\n\",\n",
    "        \"    plt.subplot(1, 2, 1)\\n\",\n",
    "        \"    plt.imshow(image.reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    if true_label is not None:\\n\",\n",
    "        \"        title = f\\\"Image (Vrai: {true_label})\\\"\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        title = \\\"Image\\\"\\n\",\n",
    "        \"    plt.title(title)\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Afficher les probabilités\\n\",\n",
    "        \"    plt.subplot(1, 2, 2)\\n\",\n",
    "        \"    bars = plt.bar(range(10), prediction, color='skyblue')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"\n",
    "\n",
    "      ## 9. Exportation et intégration du modèle dans une application web\n",
    "\n",
    "Dans cette section, nous allons voir comment exporter notre modèle CNN entraîné et l'intégrer dans une application web simple.\n",
    "\n",
    "### Sauvegarde du modèle\n",
    "\n",
    "Commençons par sauvegarder notre modèle entraîné pour pouvoir le réutiliser ultérieurement.\n",
    "\n",
    "```python\n",
    "# Sauvegarder le modèle complet (architecture + poids)\n",
    "model.save('mnist_cnn_model.h5')\n",
    "print(\"Modèle sauvegardé avec succès!\")\n",
    "\n",
    "# Sauvegarder uniquement les poids (si nécessaire)\n",
    "model.save_weights('mnist_cnn_weights.h5')\n",
    "print(\"Poids du modèle sauvegardés avec succès!\")\n",
    "```\n",
    "\n",
    "### Conversion du modèle pour le web (TensorFlow.js)\n",
    "\n",
    "Pour intégrer notre modèle dans une application web, nous pouvons utiliser TensorFlow.js. Il faut d'abord convertir notre modèle Keras en format TensorFlow.js.\n",
    "\n",
    "```python\n",
    "# Installer tensorflowjs (si pas déjà installé)\n",
    "!pip install tensorflowjs\n",
    "\n",
    "# Convertir le modèle pour TensorFlow.js\n",
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(model, 'tfjs_mnist_model')\n",
    "print(\"Modèle converti pour TensorFlow.js!\")\n",
    "```\n",
    "\n",
    "### Création d'une application web simple\n",
    "\n",
    "Voici un exemple de code HTML et JavaScript pour créer une application web simple permettant aux utilisateurs de dessiner un chiffre et d'obtenir une prédiction de notre modèle CNN.\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"fr\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Reconnaissance de chiffres manuscrits</title>\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0/dist/tf.min.js\"></script>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            text-align: center;\n",
    "            margin: 20px;\n",
    "        }\n",
    "        #canvas-container {\n",
    "            display: flex;\n",
    "            justify-content: center;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        canvas {\n",
    "            border: 2px solid #333;\n",
    "            cursor: crosshair;\n",
    "        }\n",
    "        button {\n",
    "            margin: 10px;\n",
    "            padding: 10px 20px;\n",
    "            font-size: 16px;\n",
    "            cursor: pointer;\n",
    "        }\n",
    "        #result {\n",
    "            font-size: 24px;\n",
    "            margin: 20px;\n",
    "            font-weight: bold;\n",
    "        }\n",
    "        .confidence-bar {\n",
    "            height: 20px;\n",
    "            background-color: #4CAF50;\n",
    "            text-align: left;\n",
    "            margin: 5px 0;\n",
    "            color: white;\n",
    "            font-weight: bold;\n",
    "            padding-left: 5px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Reconnaissance de chiffres manuscrits avec CNN</h1>\n",
    "    <p>Dessinez un chiffre (0-9) dans le cadre ci-dessous:</p>\n",
    "    \n",
    "    <div id=\"canvas-container\">\n",
    "        <canvas id=\"drawingCanvas\" width=\"280\" height=\"280\"></canvas>\n",
    "    </div>\n",
    "    \n",
    "    <div>\n",
    "        <button id=\"predictBtn\">Prédire</button>\n",
    "        <button id=\"clearBtn\">Effacer</button>\n",
    "    </div>\n",
    "    \n",
    "    <div id=\"result\">Résultat: -</div>\n",
    "    \n",
    "    <div id=\"confidences\" style=\"width: 300px; margin: 0 auto;\"></div>\n",
    "    \n",
    "    <script>\n",
    "        // Configuration du canvas\n",
    "        const canvas = document.getElementById('drawingCanvas');\n",
    "        const ctx = canvas.getContext('2d');\n",
    "        ctx.lineWidth = 15;\n",
    "        ctx.lineCap = 'round';\n",
    "        ctx.strokeStyle = 'black';\n",
    "        ctx.fillStyle = 'white';\n",
    "        ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
    "        \n",
    "        // Variables pour le dessin\n",
    "        let isDrawing = false;\n",
    "        let lastX = 0;\n",
    "        let lastY = 0;\n",
    "        \n",
    "        // Chargement du modèle\n",
    "        let model;\n",
    "        async function loadModel() {\n",
    "            model = await tf.loadLayersModel('tfjs_mnist_model/model.json');\n",
    "            console.log('Modèle chargé!');\n",
    "        }\n",
    "        loadModel();\n",
    "        \n",
    "        // Fonctions de dessin\n",
    "        function startDrawing(e) {\n",
    "            isDrawing = true;\n",
    "            [lastX, lastY] = [e.offsetX, e.offsetY];\n",
    "        }\n",
    "        \n",
    "        function draw(e) {\n",
    "            if (!isDrawing) return;\n",
    "            ctx.beginPath();\n",
    "            ctx.moveTo(lastX, lastY);\n",
    "            ctx.lineTo(e.offsetX, e.offsetY);\n",
    "            ctx.stroke();\n",
    "            [lastX, lastY] = [e.offsetX, e.offsetY];\n",
    "        }\n",
    "        \n",
    "        function stopDrawing() {\n",
    "            isDrawing = false;\n",
    "        }\n",
    "        \n",
    "        // Event listeners pour le dessin\n",
    "        canvas.addEventListener('mousedown', startDrawing);\n",
    "        canvas.addEventListener('mousemove', draw);\n",
    "        canvas.addEventListener('mouseup', stopDrawing);\n",
    "        canvas.addEventListener('mouseout', stopDrawing);\n",
    "        \n",
    "        // Prétraitement de l'image\n",
    "        function preprocessCanvas() {\n",
    "            // Redimensionner à 28x28 (taille attendue par le modèle)\n",
    "            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);\n",
    "            const tempCanvas = document.createElement('canvas');\n",
    "            tempCanvas.width = 28;\n",
    "            tempCanvas.height = 28;\n",
    "            const tempCtx = tempCanvas.getContext('2d');\n",
    "            tempCtx.drawImage(canvas, 0, 0, canvas.width, canvas.height, 0, 0, 28, 28);\n",
    "            \n",
    "            // Convertir en niveaux de gris et normaliser (0-1)\n",
    "            const tempImageData = tempCtx.getImageData(0, 0, 28, 28);\n",
    "            const data = tempImageData.data;\n",
    "            const grayscale = new Float32Array(28 * 28);\n",
    "            \n",
    "            for (let i = 0; i < 28 * 28; i++) {\n",
    "                // Inverser les couleurs (fond blanc -> 0, trait noir -> 1)\n",
    "                grayscale[i] = (255 - data[i * 4]) / 255.0;\n",
    "            }\n",
    "            \n",
    "            // Créer un tenseur au format attendu par le modèle\n",
    "            return tf.tensor(grayscale).reshape([1, 28, 28, 1]);\n",
    "        }\n",
    "        \n",
    "        // Prédiction\n",
    "        async function predict() {\n",
    "            if (!model) {\n",
    "                alert('Le modèle n\\'est pas encore chargé. Veuillez patienter.');\n",
    "                return;\n",
    "            }\n",
    "            \n",
    "            // Prétraiter l'image\n",
    "            const input = preprocessCanvas();\n",
    "            \n",
    "            // Faire la prédiction\n",
    "            const predictions = await model.predict(input).data();\n",
    "            \n",
    "            // Trouver la classe avec la plus haute probabilité\n",
    "            let maxProb = 0;\n",
    "            let predictedClass = -1;\n",
    "            \n",
    "            for (let i = 0; i < predictions.length; i++) {\n",
    "                if (predictions[i] > maxProb) {\n",
    "                    maxProb = predictions[i];\n",
    "                    predictedClass = i;\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            // Afficher le résultat\n",
    "            document.getElementById('result').textContent = `Résultat: ${predictedClass} (${(maxProb * 100).toFixed(2)}%)`;\n",
    "            \n",
    "            // Afficher les probabilités pour chaque classe\n",
    "            const confidencesDiv = document.getElementById('confidences');\n",
    "            confidencesDiv.innerHTML = '';\n",
    "            \n",
    "            for (let i = 0; i < predictions.length; i++) {\n",
    "                const prob = predictions[i] * 100;\n",
    "                const barDiv = document.createElement('div');\n",
    "                barDiv.className = 'confidence-bar';\n",
    "                barDiv.style.width = `${prob}%`;\n",
    "                barDiv.textContent = i + ': ' + prob.toFixed(1) + '%';\n",
    "                confidencesDiv.appendChild(barDiv);\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Effacer le canvas\n",
    "        function clearCanvas() {\n",
    "            ctx.fillStyle = 'white';\n",
    "            ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
    "            document.getElementById('result').textContent = 'Résultat: -';\n",
    "            document.getElementById('confidences').innerHTML = '';\n",
    "        }\n",
    "        \n",
    "        // Event listeners pour les boutons\n",
    "        document.getElementById('predictBtn').addEventListener('click', predict);\n",
    "        document.getElementById('clearBtn').addEventListener('click', clearCanvas);\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "### Déploiement de l'application\n",
    "\n",
    "Pour déployer cette application, suivez ces étapes:\n",
    "\n",
    "1. Créez un dossier pour votre application web\n",
    "2. Placez le code HTML ci-dessus dans un fichier `index.html`\n",
    "3. Copiez le dossier `tfjs_mnist_model` (créé lors de la conversion) dans le même répertoire\n",
    "4. Utilisez un serveur web pour servir ces fichiers (pour éviter les problèmes CORS)\n",
    "   \n",
    "Exemple avec Python:\n",
    "\n",
    "```python\n",
    "# Lancer un serveur web simple avec Python\n",
    "!python -m http.server 8000\n",
    "```\n",
    "\n",
    "Vous pouvez alors accéder à votre application à l'adresse `http://localhost:8000`.\n",
    "\n",
    "### Améliorations possibles\n",
    "\n",
    "Voici quelques idées pour améliorer cette application:\n",
    "\n",
    "1. **Augmenter la robustesse**:\n",
    "   - Ajouter un prétraitement plus avancé pour centrer et normaliser les dessins\n",
    "   - Implémenter l'augmentation de données côté client\n",
    "\n",
    "2. **Améliorer l'expérience utilisateur**:\n",
    "   - Ajouter une animation pendant le chargement du modèle\n",
    "   - Permettre le dessin sur mobile (événements tactiles)\n",
    "   - Ajouter un historique des prédictions\n",
    "\n",
    "3. **Optimisations**:\n",
    "   - Quantifier le modèle pour réduire sa taille\n",
    "   - Utiliser la détection de changements pour prédire automatiquement après le dessin\n",
    "\n",
    "## 10. Conclusion et perspectives\n",
    "\n",
    "Dans ce notebook, nous avons:\n",
    "\n",
    "- Compris l'architecture et le principe des réseaux convolutifs (CNN)\n",
    "- Implémenté un CNN avec TensorFlow/Keras pour la reconnaissance d'images MNIST\n",
    "- Visualisé et interprété les filtres et feature maps\n",
    "- Analysé les performances du modèle et les cas d'erreur\n",
    "- Exploré l'intégration d'un modèle CNN dans une application web\n",
    "\n",
    "### Perspectives et extensions possibles\n",
    "\n",
    "- **Améliorer le modèle**:\n",
    "  - Expérimenter avec des architectures plus complexes (VGG, ResNet, etc.)\n",
    "  - Utiliser des techniques d'augmentation de données pour améliorer la robustesse\n",
    "  - Appliquer l'apprentissage par transfert\n",
    "\n",
    "- **Applications à d'autres jeux de données**:\n",
    "  - Fashion MNIST (vêtements)\n",
    "  - CIFAR-10 (objets colorés)\n",
    "  - ImageNet (classification à grande échelle)\n",
    "\n",
    "- **Techniques avancées**:\n",
    "  - Détection d'objets (YOLO, SSD)\n",
    "  - Segmentation sémantique (U-Net)\n",
    "  - Style transfer\n",
    "\n",
    "Les réseaux de neurones convolutifs sont aujourd'hui la base de nombreuses applications de vision par ordinateur. La compréhension de leur fonctionnement est essentielle pour tout développeur souhaitant exploiter le potentiel de l'IA dans ce domaine.\n",
    "\n",
    "## Exercices supplémentaires\n",
    "\n",
    "1. Modifiez l'architecture du CNN pour améliorer sa précision (ajoutez des couches, modifiez les hyperparamètres)\n",
    "2. Implémentez l'augmentation de données pour améliorer la robustesse du modèle\n",
    "3. Testez le modèle sur vos propres dessins de chiffres\n",
    "4. Adaptez ce notebook pour classifier le dataset Fashion MNIST\n",
    "5. Créez une version web plus élaborée avec une interface utilisateur améliorée"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
