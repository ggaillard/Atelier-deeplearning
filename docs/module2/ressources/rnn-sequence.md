# RNN/LSTM pour l'analyse de sentiment - Notebook complet

Ce notebook vous guide dans la crÃ©ation d'un modÃ¨le LSTM pour analyser le sentiment de textes (positif/nÃ©gatif).

## Cellule 1 (Markdown) - Introduction

```markdown
# ğŸ§  RNN/LSTM pour l'analyse de sentiment

## DÃ©couverte des rÃ©seaux rÃ©currents avec un cas concret

Dans ce notebook, vous allez :
- âœ… Comprendre comment les RNN traitent les sÃ©quences de texte
- âœ… CrÃ©er un modÃ¨le LSTM pour analyser le sentiment
- âœ… Visualiser les embeddings de mots
- âœ… Tester le modÃ¨le sur vos propres phrases

**DurÃ©e estimÃ©e** : 50 minutes

**Cas d'usage** : Analyser automatiquement les avis clients, commentaires, etc.
```

## Cellule 2 (Code) - Configuration et imports

```python
# Imports nÃ©cessaires
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing import sequence
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
import pandas as pd

# Configuration
print(f"TensorFlow version: {tf.__version__}")
print("GPU disponible:", "Oui" if tf.config.list_physical_devices('GPU') else "Non")

# ParamÃ¨tres du modÃ¨le
MAX_FEATURES = 5000  # Nombre de mots dans le vocabulaire
MAX_LEN = 200       # Longueur maximale des sÃ©quences
EMBEDDING_SIZE = 128 # Taille des embeddings

print("\nâœ… Configuration terminÃ©e !")
```

## Cellule 3 (Code) - Chargement et exploration des donnÃ©es

```python
# Chargement du dataset IMDB (avis de films)
print("ğŸ“¥ Chargement du dataset IMDB...")
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=MAX_FEATURES)

print(f"ğŸ“Š DonnÃ©es d'entraÃ®nement : {len(X_train)} avis")
print(f"ğŸ“Š DonnÃ©es de test : {len(X_test)} avis")
print(f"ğŸ“Š Vocabulaire : {MAX_FEATURES} mots les plus frÃ©quents")

# RÃ©cupÃ©ration du dictionnaire de mots
word_index = imdb.get_word_index()
reverse_word_index = {v: k for k, v in word_index.items()}
reverse_word_index[0] = '<PAD>'
reverse_word_index[1] = '<START>'
reverse_word_index[2] = '<UNKNOWN>'

def decode_review(encoded_review):
    """Convertit une sÃ©quence d'indices en texte lisible"""
    return ' '.join([reverse_word_index.get(i, '<UNKNOWN>') for i in encoded_review])

# Affichage de quelques exemples
print("\nğŸ“ Exemples d'avis (avant preprocessing) :")
for i in range(3):
    sentiment = "ğŸ˜Š POSITIF" if y_train[i] == 1 else "ğŸ˜ NÃ‰GATIF"
    print(f"\n{sentiment} - Longueur: {len(X_train[i])} mots")
    decoded = decode_review(X_train[i])
    # Afficher seulement les premiers mots pour la lisibilitÃ©
    print(f"Texte: {decoded[:200]}...")

# Distribution des longueurs
lengths = [len(x) for x in X_train]
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.hist(lengths, bins=50, alpha=0.7)
plt.title('Distribution des longueurs d\'avis')
plt.xlabel('Nombre de mots')
plt.ylabel('FrÃ©quence')
plt.axvline(MAX_LEN, color='red', linestyle='--', label=f'Limite fixÃ©e: {MAX_LEN}')
plt.legend()

plt.subplot(1, 2, 2)
plt.hist(y_train, bins=2, alpha=0.7, color=['red', 'green'])
plt.title('Distribution des sentiments')
plt.xlabel('Sentiment (0=NÃ©gatif, 1=Positif)')
plt.ylabel('Nombre d\'avis')
plt.xticks([0, 1], ['NÃ©gatif', 'Positif'])

plt.tight_layout()
plt.show()

print(f"\nğŸ“Š Statistiques des longueurs :")
print(f"   - Longueur moyenne : {np.mean(lengths):.1f} mots")
print(f"   - Longueur mÃ©diane : {np.median(lengths):.1f} mots")
print(f"   - Avis > {MAX_LEN} mots : {np.sum(np.array(lengths) > MAX_LEN)}")
```

## Cellule 4 (Code) - PrÃ©paration des donnÃ©es

```python
# Padding des sÃ©quences (toutes Ã  la mÃªme longueur)
print("ğŸ”§ PrÃ©paration des donnÃ©es...")
print(f"   - Troncature/padding Ã  {MAX_LEN} mots")

X_train = sequence.pad_sequences(X_train, maxlen=MAX_LEN, padding='post')
X_test = sequence.pad_sequences(X_test, maxlen=MAX_LEN, padding='post')

print(f"   - Forme finale X_train : {X_train.shape}")
print(f"   - Forme finale X_test : {X_test.shape}")

# Visualisation de l'effet du padding
exemple_idx = 0
print(f"\nğŸ“ Exemple de preprocessing :")
print(f"   - Avis original : {len([x for x in X_train[exemple_idx] if x != 0])} mots utiles")
print(f"   - AprÃ¨s padding : {X_train.shape[1]} positions")
print(f"   - PremiÃ¨res valeurs : {X_train[exemple_idx][:20]}")
print(f"   - (0 = padding, autres = indices de mots)")

# Conversion en format appropriÃ© pour TensorFlow
X_train = X_train.astype('int32')
X_test = X_test.astype('int32')
y_train = y_train.astype('int32') 
y_test = y_test.astype('int32')

print("\nâœ… DonnÃ©es prÃ©parÃ©es pour l'entraÃ®nement !")
```

## Cellule 5 (Code) - Construction du modÃ¨le LSTM

```python
# Construction du modÃ¨le LSTM
print("ğŸ—ï¸ Construction du modÃ¨le LSTM...")

model = Sequential([
    # Couche d'embedding : convertit les indices en vecteurs denses
    Embedding(
        input_dim=MAX_FEATURES,    # Taille du vocabulaire
        output_dim=EMBEDDING_SIZE, # Dimension des embeddings
        input_length=MAX_LEN,      # Longueur des sÃ©quences
        name='embedding'
    ),
    
    # Couche LSTM : traite les sÃ©quences
    LSTM(
        units=64,           # Nombre d'unitÃ©s LSTM
        dropout=0.2,        # Dropout sur les entrÃ©es
        recurrent_dropout=0.2,  # Dropout sur les connexions rÃ©currentes
        name='lstm'
    ),
    
    # Couche de rÃ©gularisation
    Dropout(0.5, name='dropout'),
    
    # Couche de sortie : classification binaire
    Dense(1, activation='sigmoid', name='output')
])

# Compilation du modÃ¨le
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Affichage de l'architecture
print("\nğŸ“‹ Architecture du modÃ¨le :")
model.summary()

print(f"\nğŸ”¢ DÃ©tails des couches :")
print(f"   - Embedding : {MAX_FEATURES} mots â†’ {EMBEDDING_SIZE} dimensions")
print(f"   - LSTM : 64 unitÃ©s avec mÃ©moire sÃ©quentielle")
print(f"   - Dense : 1 neurone pour classification binaire (0-1)")
print(f"   - Total paramÃ¨tres : {model.count_params():,}")
```

## Cellule 6 (Code) - EntraÃ®nement du modÃ¨le

```python
# EntraÃ®nement du modÃ¨le
print("ğŸš€ DÃ©but de l'entraÃ®nement...")
print("â±ï¸ Les LSTM sont plus lents que les CNN, patience !")

# EntraÃ®nement avec validation
history = model.fit(
    X_train, y_train,
    batch_size=128,
    epochs=3,  # Peu d'Ã©poques pour la dÃ©monstration
    validation_split=0.2,
    verbose=1
)

# Ã‰valuation finale
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nğŸ“Š RÃ©sultats finaux :")
print(f"   - PrÃ©cision sur test : {test_accuracy*100:.2f}%")
print(f"   - Perte sur test : {test_loss:.4f}")

# Visualisation des courbes d'apprentissage
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], 'b-', label='EntraÃ®nement', linewidth=2)
plt.plot(history.history['val_accuracy'], 'r-', label='Validation', linewidth=2)
plt.title('Ã‰volution de la prÃ©cision')
plt.xlabel('Ã‰poque')
plt.ylabel('PrÃ©cision')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], 'b-', label='EntraÃ®nement', linewidth=2)
plt.plot(history.history['val_loss'], 'r-', label='Validation', linewidth=2)
plt.title('Ã‰volution de la perte')
plt.xlabel('Ã‰poque')
plt.ylabel('Perte')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nâœ… EntraÃ®nement terminÃ© !")
```

## Cellule 7 (Code) - Test et prÃ©dictions

```python
# Test du modÃ¨le sur quelques exemples
print("ğŸ” Test du modÃ¨le sur des exemples...")

# SÃ©lection d'exemples
indices = np.random.choice(len(X_test), 8, replace=False)
test_examples = X_test[indices]
true_labels = y_test[indices]

# PrÃ©dictions
predictions = model.predict(test_examples, verbose=0)
predicted_probs = predictions.flatten()
predicted_labels = (predicted_probs > 0.5).astype(int)

# Affichage des rÃ©sultats
print("\nğŸ“ Exemples de prÃ©dictions :")
for i in range(8):
    true_sentiment = "ğŸ˜Š POSITIF" if true_labels[i] == 1 else "ğŸ˜ NÃ‰GATIF"
    pred_sentiment = "ğŸ˜Š POSITIF" if predicte