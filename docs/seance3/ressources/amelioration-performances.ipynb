{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2.6 Solution 2: Rééchantillonnage\n",
    "print(\"\\n--- SOLUTION 2: RÉÉCHANTILLONNAGE ---\")\n",
    "\n",
    "# Sous-échantillonnage des classes majoritaires et sur-échantillonnage des classes minoritaires\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Séparer les exemples par classe\n",
    "class_samples = []\n",
    "for i in range(n_classes):\n",
    "    class_samples.append(imbalanced_images[imbalanced_labels == i])\n",
    "\n",
    "# Définir la taille cible (moyenne)\n",
    "target_size = int(total_samples / n_classes)\n",
    "print(f\"Taille cible par classe après rééchantillonnage: {target_size}\")\n",
    "\n",
    "# Rééchantillonner chaque classe\n",
    "resampled_images = []\n",
    "resampled_labels = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    if len(class_samples[i]) == 0:\n",
    "        continue\n",
    "        \n",
    "    # Sur-échantillonnage pour les classes minoritaires\n",
    "    if len(class_samples[i]) < target_size:\n",
    "        resampled = resample(class_samples[i], \n",
    "                            replace=True,        # Avec remplacement\n",
    "                            n_samples=target_size,\n",
    "                            random_state=42)\n",
    "    # Sous-échantillonnage pour les classes majoritaires\n",
    "    elif len(class_samples[i]) > target_size:\n",
    "        resampled = resample(class_samples[i],\n",
    "                            replace=False,       # Sans remplacement\n",
    "                            n_samples=target_size,\n",
    "                            random_state=42)\n",
    "    else:\n",
    "        resampled = class_samples[i]\n",
    "    \n",
    "    resampled_images.append(resampled)\n",
    "    resampled_labels.extend([i] * len(resampled))\n",
    "\n",
    "# Convertir en arrays numpy\n",
    "resampled_images = np.vstack([img for img in resampled_images if len(img) > 0])\n",
    "resampled_labels = np.array(resampled_labels)\n",
    "\n",
    "# Vérifier la nouvelle distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "resampled_class_counts = np.bincount(resampled_labels, minlength=n_classes)\n",
    "plt.bar(range(len(class_names)), resampled_class_counts)\n",
    "plt.xticks(range(len(class_names)), class_names, rotation=90)\n",
    "plt.title(\"Distribution après rééchantillonnage\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Nombre d'échantillons\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Diviser en ensembles d'entraînement et de validation\n",
    "X_train_res, X_val_res, y_train_res, y_val_res = train_test_split(\n",
    "    resampled_images, resampled_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Entraîner sur les données rééchantillonnées\n",
    "resampled_model = create_base_model()\n",
    "history_resampled = resampled_model.fit(\n",
    "    X_train_res, y_train_res,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_res, y_val_res),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Évaluation du modèle rééchantillonné\n",
    "resampled_predictions = resampled_model.predict(test_images)\n",
    "resampled_pred_classes = np.argmax(resampled_predictions, axis=1)\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de classification - Modèle avec rééchantillonnage:\")\n",
    "print(classification_report(test_labels, resampled_pred_classes, target_names=class_names))\n",
    "\n",
    "# 2.7 Comparaison des solutions pour les problèmes de données\n",
    "print(\"\\n--- COMPARAISON DES SOLUTIONS POUR LES PROBLÈMES DE DONNÉES ---\")\n",
    "\n",
    "# Comparaison des performances par classe\n",
    "models_data = {\n",
    "    \"Déséquilibré\": imbalanced_pred_classes,\n",
    "    \"Pondération\": weighted_pred_classes,\n",
    "    \"Rééchantillonnage\": resampled_pred_classes\n",
    "}\n",
    "\n",
    "# Calculer la précision par classe pour chaque modèle\n",
    "class_accuracies = {}\n",
    "\n",
    "for model_name, predictions in models_data.items():\n",
    "    accuracies = []\n",
    "    for class_idx in range(n_classes):\n",
    "        # Indices des exemples de cette classe\n",
    "        class_indices = np.where(test_labels == class_idx)[0]\n",
    "        # Précision sur cette classe\n",
    "        class_acc = np.mean(predictions[class_indices] == test_labels[class_indices])\n",
    "        accuracies.append(class_acc)\n",
    "    class_accuracies[model_name] = accuracies\n",
    "\n",
    "# Visualisation des précisions par classe\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "\n",
    "for model_name, accuracies in class_accuracies.items():\n",
    "    offset = width * multiplier\n",
    "    plt.bar(x + offset, accuracies, width, label=model_name)\n",
    "    multiplier += 1\n",
    "\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Précision')\n",
    "plt.title('Précision par classe et par modèle')\n",
    "plt.xticks(x + width, class_names, rotation=90)\n",
    "plt.legend(loc='lower center')\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. AMÉLIORATION POUR APPLICATION WEB\n",
    "# ===================================\n",
    "\n",
    "print(\"\\n\\n--- AMÉLIORATION D'UN MODÈLE POUR APPLICATION WEB ---\")\n",
    "\n",
    "# 3.1 Définition des contraintes d'une application web\n",
    "print(\"Dans un contexte d'application web, un modèle doit être:\")\n",
    "print(\"1. Rapide (temps d'inférence court)\")\n",
    "print(\"2. Léger (taille réduite)\")\n",
    "print(\"3. Précis sur les cas d'utilisation réels\")\n",
    "print(\"4. Robuste aux variations (rotations, luminosité, etc.)\")\n",
    "\n",
    "# 3.2 Modèle de base pour une application de reconnaissance de vêtements\n",
    "print(\"\\nCréation d'un modèle de base...\")\n",
    "\n",
    "def create_web_model():\n",
    "    \"\"\"Crée un modèle CNN simple pour la classification de vêtements\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Reshaping pour CNN\n",
    "        layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "        \n",
    "        # Première couche de convolution\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Deuxième couche de convolution\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Aplatissement et couches denses\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Entraînement du modèle de base\n",
    "base_web_model = create_web_model()\n",
    "\n",
    "# Reshape pour CNN\n",
    "train_images_reshaped = train_images.reshape(-1, 28, 28, 1)\n",
    "test_images_reshaped = test_images.reshape(-1, 28, 28, 1)\n",
    "\n",
    "history_base_web = base_web_model.fit(\n",
    "    train_images_reshaped, train_labels,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3.3 Mesure des performances initiales\n",
    "print(\"\\nÉvaluation des performances initiales...\")\n",
    "\n",
    "# Précision\n",
    "test_loss, test_acc = base_web_model.evaluate(test_images_reshaped, test_labels, verbose=0)\n",
    "print(f\"Précision sur test: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Temps d'inférence\n",
    "start_time = time.time()\n",
    "base_web_model.predict(test_images_reshaped[:100])\n",
    "inference_time = (time.time() - start_time) / 100  # Temps moyen par image\n",
    "print(f\"Temps d'inférence moyen: {inference_time*1000:.2f} ms par image\")\n",
    "\n",
    "# Taille du modèle\n",
    "base_web_model.save(\"base_web_model.h5\")\n",
    "import os\n",
    "model_size_mb = os.path.getsize(\"base_web_model.h5\") / (1024 * 1024)\n",
    "print(f\"Taille du modèle: {model_size_mb:.2f} MB\")\n",
    "\n",
    "# 3.4 Amélioration 1: Data Augmentation\n",
    "print(\"\\n--- AMÉLIORATION 1: DATA AUGMENTATION ---\")\n",
    "\n",
    "# Créer un générateur de données avec augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomTranslation(0.1, 0.1)\n",
    "])\n",
    "\n",
    "def create_augmented_model():\n",
    "    \"\"\"Crée un modèle avec data augmentation intégrée\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Reshaping pour CNN\n",
    "        layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "        \n",
    "        # Couche d'augmentation de données (active uniquement pendant l'entraînement)\n",
    "        data_augmentation,\n",
    "        \n",
    "        # Première couche de convolution\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Deuxième couche de convolution\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Aplatissement et couches denses\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "augmented_model = create_augmented_model()\n",
    "\n",
    "# Visualiser des exemples d'augmentation\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    augmented_image = data_augmentation(train_images_reshaped[i:i+1])\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_image[0, :, :, 0], cmap='gray')\n",
    "    plt.title(class_names[train_labels[i]])\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Entraînement avec augmentation\n",
    "history_augmented = augmented_model.fit(\n",
    "    train_images_reshaped, train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3.5 Amélioration 2: Architecture plus légère\n",
    "print(\"\\n--- AMÉLIORATION 2: ARCHITECTURE PLUS LÉGÈRE ---\")\n",
    "\n",
    "def create_lightweight_model():\n",
    "    \"\"\"Crée un modèle plus léger avec séparable convolutions\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Reshaping pour CNN\n",
    "        layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "        \n",
    "        # Première couche de convolution séparable (moins de paramètres)\n",
    "        layers.SeparableConv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Deuxième couche de convolution séparable\n",
    "        layers.SeparableConv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Aplatissement et couches denses\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),  # Plus petite que l'original\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "lightweight_model = create_lightweight_model()\n",
    "lightweight_model.summary()  # Afficher le résumé pour voir la réduction de paramètres\n",
    "\n",
    "# Entraînement du modèle léger\n",
    "history_lightweight = lightweight_model.fit(\n",
    "    train_images_reshaped, train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3.6 Amélioration 3: Optimisation pour le déploiement\n",
    "print(\"\\n--- AMÉLIORATION 3: OPTIMISATION POUR LE DÉPLOIEMENT ---\")\n",
    "\n",
    "# Convertir en TensorFlow Lite pour le déploiement web/mobile\n",
    "# (Simulé ici pour démonstration)\n",
    "\n",
    "print(\"Conversion TensorFlow → TensorFlow Lite...\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(lightweight_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Écrire le modèle TFLite dans un fichier\n",
    "with open('model_lite.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Taille du modèle TFLite\n",
    "tflite_model_size_mb = os.path.getsize(\"model_lite.tflite\") / (1024 * 1024)\n",
    "print(f\"Taille du modèle TFLite: {tflite_model_size_mb:.2f} MB\")\n",
    "\n",
    "# Quantification (simulation)\n",
    "print(\"\\nQuantification pour réduire davantage la taille...\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(lightweight_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# Écrire le modèle quantifié dans un fichier\n",
    "with open('model_quantized.tflite', 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "# Taille du modèle quantifié\n",
    "quantized_model_size_mb = os.path.getsize(\"model_quantized.tflite\") / (1024 * 1024)\n",
    "print(f\"Taille du modèle quantifié: {quantized_model_size_mb:.2f} MB\")\n",
    "\n",
    "# 3.7 Comparaison finale des améliorations\n",
    "print(\"\\n--- COMPARAISON FINALE DES AMÉLIORATIONS ---\")\n",
    "\n",
    "# Évaluation des modèles améliorés\n",
    "models_web = {\n",
    "    \"Base\": base_web_model,\n",
    "    \"Augmenté\": augmented_model,\n",
    "    \"Léger\": lightweight_model\n",
    "}\n",
    "\n",
    "# Tableau comparatif\n",
    "web_results = []\n",
    "for name, model in models_web.items():\n",
    "    # Mesurer la précision\n",
    "    test_loss, test_acc = model.evaluate(test_images_reshaped, test_labels, verbose=0)\n",
    "    \n",
    "    # Mesurer le temps d'inférence\n",
    "    start_time = time.time()\n",
    "    model.predict(test_images_reshaped[:100])\n",
    "    inference_time = (time.time() - start_time) / 100\n",
    "    \n",
    "    # Sauvegarder et mesurer la taille\n",
    "    model_filename = f\"{name.lower()}_model.h5\"\n",
    "    model.save(model_filename)\n",
    "    model_size_mb = os.path.getsize(model_filename) / (1024 * 1024)\n",
    "    \n",
    "    web_results.append({\n",
    "        \"Modèle\": name,\n",
    "        \"Précision\": f\"{test_acc*100:.2f}%\",\n",
    "        \"Temps d'inférence\": f\"{inference_time*1000:.2f} ms\",\n",
    "        \"Taille\": f\"{model_size_mb:.2f} MB\"\n",
    "    })\n",
    "\n",
    "df_web_results = pd.DataFrame(web_results)\n",
    "print(df_web_results)\n",
    "\n",
    "# 3.8 Conseils pour l'intégration dans une application web\n",
    "print(\"\\n--- CONSEILS POUR L'INTÉGRATION WEB ---\")\n",
    "print(\"\"\"\n",
    "Pour intégrer efficacement votre modèle dans une application web:\n",
    "\n",
    "1. Format de déploiement:\n",
    "   - TensorFlow.js pour exécution côté client (navigateur)\n",
    "   - TensorFlow Serving pour API REST côté serveur\n",
    "   - TensorFlow Lite pour applications mobiles\n",
    "\n",
    "2. Optimisations importantes:\n",
    "   - Quantification pour réduire la taille et accélérer l'inférence\n",
    "   - Batch processing pour les requêtes multiples\n",
    "   - Mise en cache des résultats fréquents\n",
    "\n",
    "3. Architecture recommandée:\n",
    "   - API Python (Flask/FastAPI) exposant le modèle\n",
    "   - Frontend JavaScript/React pour l'interface utilisateur\n",
    "   - WebSockets pour les prédictions en temps réel\n",
    "\n",
    "4. Considérations de performances:\n",
    "   - Limiter la taille des images uploadées\n",
    "   - Prétraitement côté client quand c'est possible\n",
    "   - Monitoring des temps de réponse\n",
    "\"\"\")\n",
    "\n",
    "# 4. EXERCICE FINAL\n",
    "# ================\n",
    "\n",
    "print(\"\\n\\n--- EXERCICE FINAL: AMÉLIORATION DE MODÈLE ---\")\n",
    "print(\"\"\"\n",
    "Votre mission: Améliorer un modèle de reconnaissance de chiffres manuscrits pour une application web.\n",
    "\n",
    "Objectifs:\n",
    "1. Atteindre une précision d'au moins 98% sur MNIST\n",
    "2. Réduire le temps d'inférence à moins de 10ms par image\n",
    "3. Maintenir la taille du modèle sous 1 MB\n",
    "\n",
    "Approche suggérée:\n",
    "1. Commencez par analyser les performances du modèle de base\n",
    "2. Identifiez les points faibles (précision, vitesse, taille)\n",
    "3. Testez différentes améliorations en isolant leur impact\n",
    "4. Documentez vos modifications et leurs effets\n",
    "5. Préparez le modèle final pour le déploiement\n",
    "\n",
    "Ressources:\n",
    "- Dataset MNIST intégré dans TensorFlow\n",
    "- Modèle de base fourni\n",
    "- Documentation TensorFlow et TensorFlow Lite\n",
    "\n",
    "Bonus:\n",
    "- Créez une interface web simple pour tester le modèle\n",
    "- Implémentez la reconnaissance en temps réel via la webcam\n",
    "\"\"\")\n",
    "\n",
    "# Modèle de base pour l'exercice\n",
    "def create_exercise_base_model():\n",
    "    \"\"\"Crée un modèle de base pour l'exercice final\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Charger MNIST\n",
    "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "mnist_train_images = mnist_train_images / 255.0\n",
    "mnist_test_images = mnist_test_images / 255.0\n",
    "\n",
    "# Entraîner le modèle de base\n",
    "exercise_model = create_exercise_base_model()\n",
    "exercise_model.fit(\n",
    "    mnist_train_images, mnist_train_labels,\n",
    "    epochs=3,  # Intentionnellement peu d'époques\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Évaluer le modèle de base\n",
    "test_loss, test_acc = exercise_model.evaluate(mnist_test_images, mnist_test_labels, verbose=0)\n",
    "print(f\"\\nModèle de base pour l'exercice:\")\n",
    "print(f\"- Précision: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Temps d'inférence\n",
    "start_time = time.time()\n",
    "exercise_model.predict(mnist_test_images[:100])\n",
    "inference_time = (time.time() - start_time) / 100\n",
    "print(f\"- Temps d'inférence: {inference_time*1000:.2f} ms par image\")\n",
    "\n",
    "# Taille du modèle\n",
    "exercise_model.save(\"exercise_base.h5\")\n",
    "model_size_mb = os.path.getsize(\"exercise_base.h5\") / (1024 * 1024)\n",
    "print(f\"- Taille du modèle: {model_size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\nÀ vous de jouer! Améliorez ce modèle pour atteindre les objectifs.\")\n",
    "\n",
    "# 5. CONCLUSION\n",
    "# ============\n",
    "\n",
    "print(\"\\n\\n--- CONCLUSION ---\")\n",
    "print(\"\"\"\n",
    "Points clés à retenir:\n",
    "\n",
    "1. Diagnostic:\n",
    "   - Observer les courbes d'apprentissage pour détecter le surapprentissage\n",
    "   - Analyser la distribution des données et les performances par classe\n",
    "   - Mesurer le temps d'inférence et la taille du modèle pour applications web\n",
    "\n",
    "2. Solutions au surapprentissage:\n",
    "   - Régularisation (Dropout, L2)\n",
    "   - Early Stopping\n",
    "   - Augmentation de données\n",
    "   - Modèles plus simples\n",
    "\n",
    "3. Solutions aux problèmes de données:\n",
    "   - Pondération des classes\n",
    "   - Rééchantillonnage (sur/sous-échantillonnage)\n",
    "   - Stratification des ensembles d'entraînement/validation\n",
    "\n",
    "4. Optimisation pour le web:\n",
    "   - Architectures légères (SeparableConv2D)\n",
    "   - Quantification\n",
    "   - TensorFlow Lite ou TensorFlow.js\n",
    "   - Prétraitement efficace\n",
    "\n",
    "L'amélioration d'un modèle est un processus itératif qui demande de:\n",
    "1. Identifier précisément le problème\n",
    "2. Tester une solution à la fois\n",
    "3. Mesurer objectivement l'impact\n",
    "4. Documenter les résultats\n",
    "\n",
    "Ces compétences sont directement valorisables en stage et en entreprise!\n",
    "\"\"\")\n",
    "# Amélioration des performances de modèles de Deep Learning\n",
    "# Notebook pour BTS SIO - Séance 3\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# 1. DIAGNOSTIC: SURAPPRENTISSAGE\n",
    "# ===============================\n",
    "\n",
    "# 1.1 Chargement du jeu de données Fashion MNIST\n",
    "print(\"\\n--- CAS 1: DIAGNOSTIC DU SURAPPRENTISSAGE ---\")\n",
    "print(\"Chargement des données...\")\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Normalisation\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Noms des classes\n",
    "class_names = ['T-shirt/top', 'Pantalon', 'Pull', 'Robe', 'Manteau',\n",
    "               'Sandale', 'Chemise', 'Baskets', 'Sac', 'Bottine']\n",
    "\n",
    "print(f\"Dimensions des données: {train_images.shape[0]} images d'entraînement, {test_images.shape[0]} images de test\")\n",
    "\n",
    "# 1.2 Créer un modèle volontairement sujet au surapprentissage\n",
    "def create_overfitting_model():\n",
    "    \"\"\"Crée un modèle intentionnellement sujet au surapprentissage\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(512, activation='relu'),  # Couche très large\n",
    "        layers.Dense(512, activation='relu'),  # Seconde couche large\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 1.3 Entraînement du modèle sujet au surapprentissage\n",
    "print(\"Entraînement du modèle sujet au surapprentissage...\")\n",
    "model_overfit = create_overfitting_model()\n",
    "\n",
    "# Utiliser peu de données pour accélérer le surapprentissage\n",
    "history_overfit = model_overfit.fit(\n",
    "    train_images[:6000], train_labels[:6000],\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 1.4 Visualisation du surapprentissage\n",
    "def plot_learning_curves(history, title=\"Courbes d'apprentissage\"):\n",
    "    \"\"\"Trace les courbes d'apprentissage à partir de l'historique d'entraînement\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Courbe de précision\n",
    "    ax1.plot(history.history['accuracy'], label='Entraînement')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation')\n",
    "    ax1.set_xlabel('Époque')\n",
    "    ax1.set_ylabel('Précision')\n",
    "    ax1.set_title('Évolution de la précision')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Courbe de perte\n",
    "    ax2.plot(history.history['loss'], label='Entraînement')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation')\n",
    "    ax2.set_xlabel('Époque')\n",
    "    ax2.set_ylabel('Perte')\n",
    "    ax2.set_title('Évolution de la fonction de perte')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history_overfit, \"Surapprentissage: Écart croissant entre entraînement et validation\")\n",
    "\n",
    "# 1.5 Évaluation sur les données de test\n",
    "test_loss, test_acc = model_overfit.evaluate(test_images, test_labels, verbose=0)\n",
    "print(f\"Précision sur les données d'entraînement: {history_overfit.history['accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"Précision sur les données de validation: {history_overfit.history['val_accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"Précision sur les données de test: {test_acc*100:.2f}%\")\n",
    "print(\"\\nNOTE: Un grand écart entre entraînement et validation/test indique un surapprentissage.\")\n",
    "\n",
    "# 1.6 Solution 1: Régularisation L2 (Weight Decay)\n",
    "print(\"\\n--- SOLUTION 1: RÉGULARISATION L2 ---\")\n",
    "\n",
    "def create_l2_model():\n",
    "    \"\"\"Crée un modèle avec régularisation L2\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(512, activation='relu', \n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Ajout de L2\n",
    "        layers.Dense(512, activation='relu',\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Ajout de L2\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_l2 = create_l2_model()\n",
    "\n",
    "history_l2 = model_l2.fit(\n",
    "    train_images[:6000], train_labels[:6000],\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_learning_curves(history_l2, \"Avec régularisation L2: Réduction du surapprentissage\")\n",
    "\n",
    "# 1.7 Solution 2: Dropout\n",
    "print(\"\\n--- SOLUTION 2: DROPOUT ---\")\n",
    "\n",
    "def create_dropout_model():\n",
    "    \"\"\"Crée un modèle avec Dropout\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),  # Désactive aléatoirement 50% des neurones\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),  # Désactive aléatoirement 50% des neurones\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_dropout = create_dropout_model()\n",
    "\n",
    "history_dropout = model_dropout.fit(\n",
    "    train_images[:6000], train_labels[:6000],\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_learning_curves(history_dropout, \"Avec Dropout: Régularisation efficace\")\n",
    "\n",
    "# 1.8 Solution 3: Early Stopping\n",
    "print(\"\\n--- SOLUTION 3: EARLY STOPPING ---\")\n",
    "\n",
    "def create_base_model():\n",
    "    \"\"\"Crée un modèle de base identique au modèle de surapprentissage\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_early = create_base_model()\n",
    "\n",
    "# Ajout du callback Early Stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',    # Surveille la perte sur validation\n",
    "    patience=5,            # Nombre d'époques sans amélioration avant l'arrêt\n",
    "    restore_best_weights=True  # Restaure les meilleurs poids\n",
    ")\n",
    "\n",
    "history_early = model_early.fit(\n",
    "    train_images[:6000], train_labels[:6000],\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_learning_curves(history_early, \"Avec Early Stopping: Arrêt avant la dégradation\")\n",
    "\n",
    "# 1.9 Comparaison des solutions\n",
    "print(\"\\n--- COMPARAISON DES SOLUTIONS CONTRE LE SURAPPRENTISSAGE ---\")\n",
    "\n",
    "# Évaluation sur les données de test\n",
    "models = {\n",
    "    \"Base (surapprentissage)\": model_overfit,\n",
    "    \"Régularisation L2\": model_l2,\n",
    "    \"Dropout\": model_dropout,\n",
    "    \"Early Stopping\": model_early\n",
    "}\n",
    "\n",
    "# Tableau comparatif\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
    "    results.append({\n",
    "        \"Modèle\": name,\n",
    "        \"Précision Test\": f\"{test_acc*100:.2f}%\"\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n",
    "\n",
    "# 2. DIAGNOSTIC: PROBLÈMES DE DONNÉES\n",
    "# ==================================\n",
    "\n",
    "print(\"\\n\\n--- CAS 2: DIAGNOSTIC DES PROBLÈMES DE DONNÉES ---\")\n",
    "\n",
    "# 2.1 Simulation de problèmes de données\n",
    "print(\"Simulation de problèmes de données...\")\n",
    "\n",
    "# Création d'un jeu de données déséquilibré\n",
    "np.random.seed(42)\n",
    "n_samples = 3000\n",
    "\n",
    "# Sélectionner principalement des T-shirts et pantalons\n",
    "selected_classes = [0, 1]  # T-shirt et pantalon\n",
    "mask_majority = np.isin(train_labels[:n_samples], selected_classes)\n",
    "# Ajouter quelques exemples des autres classes\n",
    "mask_minority = ~mask_majority\n",
    "minority_indices = np.where(mask_minority)[0][:500]  # Seulement 500 exemples des autres classes\n",
    "\n",
    "# Combiner pour créer un dataset déséquilibré\n",
    "combined_indices = np.concatenate([np.where(mask_majority)[0], minority_indices])\n",
    "imbalanced_images = train_images[combined_indices]\n",
    "imbalanced_labels = train_labels[combined_indices]\n",
    "\n",
    "# 2.2 Visualisation du déséquilibre\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts = np.bincount(imbalanced_labels)\n",
    "plt.bar(range(len(class_names)), class_counts)\n",
    "plt.xticks(range(len(class_names)), class_names, rotation=90)\n",
    "plt.title(\"Distribution déséquilibrée des classes\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Nombre d'échantillons\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistribution des classes:\")\n",
    "for i, count in enumerate(class_counts):\n",
    "    print(f\"{class_names[i]}: {count} échantillons ({count/len(imbalanced_labels)*100:.1f}%)\")\n",
    "\n",
    "# 2.3 Entraînement sur données déséquilibrées\n",
    "print(\"\\nEntraînement sur données déséquilibrées...\")\n",
    "\n",
    "# Diviser en ensembles d'entraînement et de validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_imb, X_val_imb, y_train_imb, y_val_imb = train_test_split(\n",
    "    imbalanced_images, imbalanced_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "imbalanced_model = create_base_model()\n",
    "history_imbalanced = imbalanced_model.fit(\n",
    "    X_train_imb, y_train_imb,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_imb, y_val_imb),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 2.4 Évaluation du modèle déséquilibré\n",
    "imbalanced_predictions = imbalanced_model.predict(test_images)\n",
    "imbalanced_pred_classes = np.argmax(imbalanced_predictions, axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(test_labels, imbalanced_pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(\"Matrice de confusion - Modèle entraîné sur données déséquilibrées\")\n",
    "plt.ylabel('Valeur réelle')\n",
    "plt.xlabel('Valeur prédite')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de classification - Modèle déséquilibré:\")\n",
    "print(classification_report(test_labels, imbalanced_pred_classes, target_names=class_names))\n",
    "\n",
    "# 2.5 Solution: Pondération des classes\n",
    "print(\"\\n--- SOLUTION 1: PONDÉRATION DES CLASSES ---\")\n",
    "\n",
    "# Calculer les poids des classes\n",
    "total_samples = len(imbalanced_labels)\n",
    "n_classes = len(np.unique(imbalanced_labels))\n",
    "class_weights = {}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    class_weights[i] = total_samples / (n_classes * class_counts[i]) if class_counts[i] > 0 else 0\n",
    "\n",
    "print(\"Poids des classes:\")\n",
    "for i, weight in class_weights.items():\n",
    "    if class_counts[i] > 0:\n",
    "        print(f\"{class_names[i]}: {weight:.2f}\")\n",
    "\n",
    "# Entraîner avec des poids de classe\n",
    "weighted_model = create_base_model()\n",
    "history_weighted = weighted_model.fit(\n",
    "    X_train_imb, y_train_imb,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_imb, y_val_imb),\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Évaluation du modèle pondéré\n",
    "weighted_predictions = weighted_model.predict(test_images)\n",
    "weighted_pred_classes = np.argmax(weighted_predictions, axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "cm_weighted = confusion_matrix(test_labels, weighted_pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_weighted, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(\"Matrice de confusion - Modèle avec pondération des classes\")\n",
    "plt.ylabel('Valeur réelle')\n",
    "plt.xlabel('Valeur prédite')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de classification - Modèle avec pondération des classes:\")\n",
    "print(classification_report(test_labels, weighted_pred_classes, target_names=class_names))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
