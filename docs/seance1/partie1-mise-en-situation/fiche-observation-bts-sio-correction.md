# üìã Fiche d'observations - Hello World du Deep Learning (CORRIG√â)

*Cette fiche d'observations vous accompagne √©tape par √©tape dans l'exploration du notebook. Pour chaque section, notez les r√©f√©rences aux cellules correspondantes du notebook.*

## Informations g√©n√©rales

**Nom et pr√©nom :** ____________________________

**Date :** ____________________________

**Groupe :** ____________________________

## Partie 1 : Configuration et v√©rification de l'environnement *(Cellule 2)*

| Question | Observation |
|----------|-------------|
| Version de TensorFlow d√©tect√©e | 2.x.x (la version exacte d√©pendra de l'environnement Colab) |
| GPU disponible ? (Oui/Non) | Oui (g√©n√©ralement disponible dans Colab) |
| Quelle est l'importance d'avoir un GPU pour le Deep Learning ? | Les GPUs sont essentiels pour acc√©l√©rer l'entra√Ænement des mod√®les de Deep Learning car ils peuvent effectuer des calculs matriciels en parall√®le, r√©duisant consid√©rablement le temps d'entra√Ænement par rapport aux CPUs. Sans GPU, l'entra√Ænement de mod√®les complexes peut prendre des heures, voire des jours au lieu de minutes. |

## Partie 2 : Chargement et pr√©paration des donn√©es *(Cellule 3)*

| Question | Observation |
|----------|-------------|
| Combien d'exemples d'entra√Ænement sont disponibles ? | 60 000 exemples |
| Combien d'exemples de test sont disponibles ? | 10 000 exemples |
| Quelle est la dimension des images ? | 28 x 28 pixels |
| Pourquoi normalise-t-on les valeurs des pixels entre 0 et 1 ? | La normalisation permet d'avoir des valeurs dans une plage uniforme, ce qui am√©liore la stabilit√© et la vitesse de convergence lors de l'entra√Ænement. Sans normalisation, les gradients peuvent exploser ou dispara√Ætre pendant la r√©tropropagation. |
| D'apr√®s les exemples affich√©s, quelles difficult√©s pourrait rencontrer le mod√®le ? | Variations dans l'√©criture (forme, √©paisseur, inclinaison), certains chiffres qui se ressemblent (comme 1 et 7, ou 3 et 8), qualit√© variable des images, positions l√©g√®rement diff√©rentes des chiffres dans l'image. |

## Partie 3 : Architecture du mod√®le *(Cellule 4)*

Dessinez le sch√©ma simplifi√© de l'architecture du r√©seau de neurones utilis√© :

```
Input (28x28x1) ‚Üí Conv2D (32 filtres) ‚Üí MaxPooling ‚Üí Conv2D (64 filtres) ‚Üí MaxPooling ‚Üí Flatten ‚Üí Dense (64) ‚Üí Dense (10, softmax)
```

| Question | Observation |
|----------|-------------|
| Combien de couches comporte le mod√®le ? | 7 couches (Input, 2 couches Conv2D, 2 couches MaxPooling, Flatten, 2 couches Dense) |
| Combien de param√®tres entra√Ænables au total ? | Environ 93,322 param√®tres (le nombre exact d√©pend de l'impl√©mentation) |
| Quel est le r√¥le des couches de convolution ? | Les couches de convolution appliquent des filtres sur l'image pour d√©tecter des caract√©ristiques (bords, textures, formes). Elles permettent d'extraire des informations spatiales en analysant des r√©gions locales de l'image d'entr√©e. |
| Quel est le r√¥le des couches de pooling ? | Les couches de pooling r√©duisent la dimension spatiale (hauteur et largeur) pour diminuer le nombre de param√®tres et la complexit√© du calcul, tout en conservant les caract√©ristiques les plus importantes. Elles rendent aussi le mod√®le plus robuste aux petites variations de position. |
| Pourquoi utilise-t-on 'softmax' comme activation de la derni√®re couche ? | Softmax transforme les sorties en une distribution de probabilit√©s (somme = 1) sur les 10 classes (chiffres 0-9), ce qui permet d'interpr√©ter les sorties comme des probabilit√©s d'appartenance √† chaque classe. |

## Partie 4 : Entra√Ænement du mod√®le *(Cellule 5)*

| Question | Observation |
|----------|-------------|
| Combien d'√©poques ont √©t√© effectu√©es ? | 5 √©poques |
| Quelle est la pr√©cision finale sur les donn√©es d'entra√Ænement ? | ~99% (la valeur exacte varie l√©g√®rement √† chaque ex√©cution) |
| Quelle est la pr√©cision finale sur les donn√©es de validation ? | ~98-99% (varie l√©g√®rement) |
| Quelle est la pr√©cision sur l'ensemble de test ? | ~98-99% (varie l√©g√®rement) |
| Y a-t-il un signe de surapprentissage (overfitting) ? Pourquoi ? | Tr√®s l√©ger ou inexistant car l'√©cart entre la pr√©cision d'entra√Ænement et de validation est faible. Le mod√®le g√©n√©ralise bien aux donn√©es non vues pendant l'entra√Ænement. |

## Partie 5 : Visualisation des r√©sultats *(Cellule 6)*

Analysez les graphiques d'apprentissage :

| Question | Observation |
|----------|-------------|
| La courbe de pr√©cision d'entra√Ænement est-elle croissante ? | Oui, elle augmente rapidement au d√©but puis se stabilise. |
| La courbe de perte d'entra√Ænement est-elle d√©croissante ? | Oui, elle diminue rapidement au d√©but puis se stabilise. |
| Y a-t-il un √©cart important entre les courbes d'entra√Ænement et de validation ? | Non, l'√©cart est faible, ce qui indique une bonne g√©n√©ralisation. |
| D'apr√®s vous, l'entra√Ænement a-t-il √©t√© suffisant (nombre d'√©poques) ? | Oui, les courbes semblent se stabiliser apr√®s 3-4 √©poques, donc 5 √©poques semblent suffisantes pour ce mod√®le et ce jeu de donn√©es. Augmenter davantage pourrait amener √† du surapprentissage. |

## Partie 6 : Pr√©dictions sur des exemples de test *(Cellule 7)*

Observez les 10 exemples de pr√©diction :

| Question | Observation |
|----------|-------------|
| Combien de pr√©dictions sont correctes sur les 10 exemples ? | G√©n√©ralement 9-10 sur 10 (peut varier selon l'initialisation al√©atoire) |
| Pour les pr√©dictions incorrectes, quelles pourraient √™tre les raisons d'erreur ? | Similitudes visuelles entre certains chiffres (ex: 4/9, 3/8), √©critures atypiques, bruit dans l'image, ou manque d'exemples similaires dans les donn√©es d'entra√Ænement. |
| Certains chiffres semblent-ils plus difficiles √† reconna√Ætre que d'autres ? | G√©n√©ralement, les chiffres 4, 7, 9 peuvent se confondre, ainsi que 3 et 8. Le mod√®le peut h√©siter davantage sur ces paires de chiffres comme le montrent les barres de probabilit√©. |

## Partie 7 : Test avec votre propre dessin *(Cellule 8)*

| Question | Observation |
|----------|-------------|
| Quels chiffres avez-vous dessin√©s ? | [R√©ponses variables selon les essais des √©tudiants] |
| Combien ont √©t√© correctement pr√©dits ? | [R√©ponses variables] |
| Pour ceux mal pr√©dits, quelle √©tait la pr√©diction et pourquoi selon vous ? | Raisons possibles : style d'√©criture diff√©rent de MNIST, √©paisseur du trait, centrage de l'image, pr√©traitement qui modifie trop l'image, etc. |
| Comment le pr√©traitement de l'image a-t-il transform√© votre dessin ? | L'image est redimensionn√©e √† 28x28 pixels, convertie en niveaux de gris, et les couleurs sont invers√©es si n√©cessaire pour avoir un fond noir et un chiffre blanc comme dans le jeu de donn√©es MNIST original. |

## Partie 8 : Exp√©rimentations *(Cellule 9)*

Documentez vos exp√©rimentations en modifiant le mod√®le ou les param√®tres :

### Exp√©rimentation 1

**Modification effectu√©e :** Augmentation du nombre d'√©poques √† 10

| Param√®tre | Valeur originale | Nouvelle valeur | 
|-----------|------------------|----------------|
| epochs | 5 | 10 |
| | | |

**R√©sultats :**
- Pr√©cision test : ~99%
- Observations : L√©g√®re am√©lioration de la pr√©cision, mais risque d'overfitting accru. Les courbes montrent une stabilisation apr√®s 6-7 √©poques.

### Exp√©rimentation 2

**Modification effectu√©e :** Ajout d'une couche Dropout apr√®s la premi√®re couche Dense

| Param√®tre | Valeur originale | Nouvelle valeur | 
|-----------|------------------|----------------|
| Architecture | Sans Dropout | Avec Dropout (0.2) |
| | | |

**R√©sultats :**
- Pr√©cision test : ~98.8%
- Observations : L√©g√®re diminution de la pr√©cision sur les donn√©es d'entra√Ænement mais meilleure g√©n√©ralisation, √©cart r√©duit entre courbes d'entra√Ænement et validation.

## Conclusion

| Question | R√©ponse |
|----------|---------|
| Quels sont les 3 principaux apprentissages de ce TP ? | 1. Les r√©seaux de neurones convolutifs sont particuli√®rement adapt√©s √† la reconnaissance d'images.<br>2. La pr√©paration des donn√©es (normalisation) est cruciale pour la performance du mod√®le.<br>3. L'architecture du r√©seau (nombre et types de couches) d√©termine sa capacit√© √† extraire des caract√©ristiques pertinentes. |
| Quelles am√©liorations pourriez-vous sugg√©rer pour ce mod√®le ? | Augmentation de donn√©es (rotations, translations), architecture plus profonde, utilisation de techniques de r√©gularisation comme le Dropout, ajout de couches Batch Normalization, ou essayer des architectures plus avanc√©es comme ResNet. |
| Comment ce mod√®le se compare-t-il aux capacit√©s humaines de reconnaissance de chiffres ? | Le mod√®le atteint ~99% de pr√©cision, comparable aux performances humaines sur MNIST (~98%). Cependant, il est moins robuste face aux variations extr√™mes ou aux styles d'√©criture tr√®s diff√©rents de ceux des donn√©es d'entra√Ænement. |
| Quelles autres applications de la vision par ordinateur vous int√©ressent ? | Reconnaissance faciale, d√©tection d'objets, segmentation d'images m√©dicales, syst√®mes de conduite autonome, reconnaissance de gestes, analyse d'images satellites, etc. |

---

## Glossaire des termes cl√©s rencontr√©s

| Terme | Votre d√©finition |
|-------|------------------|
| Convolution | Op√©ration math√©matique qui applique un filtre √† une image pour extraire des caract√©ristiques sp√©cifiques comme les bords, les textures ou les formes. |
| Pooling | Technique de r√©duction de dimension qui conserve les informations importantes tout en diminuant la taille des repr√©sentations. Max pooling prend la valeur maximale dans une r√©gion d√©finie. |
| Epoch (√©poque) | Un passage complet √† travers l'ensemble des donn√©es d'entra√Ænement pendant l'apprentissage d'un mod√®le. |
| Batch | Sous-ensemble des donn√©es d'entra√Ænement trait√© en une seule fois avant la mise √† jour des poids. |
| Dropout | Technique de r√©gularisation qui d√©sactive al√©atoirement certains neurones pendant l'entra√Ænement pour √©viter le surapprentissage. |
| Softmax | Fonction d'activation utilis√©e en sortie qui transforme un vecteur de nombres r√©els en distribution de probabilit√©s. |
| Overfitting (surapprentissage) | Ph√©nom√®ne o√π un mod√®le apprend trop bien les donn√©es d'entra√Ænement, incluant le bruit, ce qui diminue sa capacit√© de g√©n√©ralisation √† de nouvelles donn√©es. |